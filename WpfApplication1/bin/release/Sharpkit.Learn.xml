<?xml version="1.0"?>
<doc>
    <assembly>
        <name>Sharpkit.Learn</name>
    </assembly>
    <members>
        <member name="T:Sharpkit.Learn.ClassWeightEstimator`1">
            <summary>
            Determines the way class weights are calculated.
            </summary>
            <typeparam name="TLabel">Type of class label.</typeparam>
        </member>
        <member name="F:Sharpkit.Learn.ClassWeightEstimator`1.Auto">
            <summary>
            Class weights will be given inverse proportional
            to the frequency of the class in the data.
            </summary>
        </member>
        <member name="F:Sharpkit.Learn.ClassWeightEstimator`1.Uniform">
            <summary>
            All class weights will be 1.0.
            </summary>
        </member>
        <member name="F:Sharpkit.Learn.ClassWeightEstimator`1.func">
            <summary>
            Function which computes class weights.
            </summary>
        </member>
        <member name="M:Sharpkit.Learn.ClassWeightEstimator`1.#ctor(System.Func{`0[],System.Int32[],MathNet.Numerics.LinearAlgebra.Double.Vector})">
            <summary>
            Initializes a new instance of the ClassWeightEstimator class.
            </summary>
            <param name="func">Function which computes class weights.</param>
        </member>
        <member name="M:Sharpkit.Learn.ClassWeightEstimator`1.Explicit(System.Collections.Generic.Dictionary{`0,System.Double})">
            <summary>
            Keys are classes, and values
            are corresponding class weights.
            </summary>
            <param name="classWeights">Dictionary which has class weights
            corresponding to class labels specified.</param>
            <returns>Instance of <see cref="T:Sharpkit.Learn.ClassWeightEstimator`1"/>.</returns>
        </member>
        <member name="M:Sharpkit.Learn.ClassWeightEstimator`1.ComputeWeights(`0[],System.Int32[])">
            <summary>
            Calculates weights for each sample.
            </summary>
            <param name="classes">List of all classes.</param>
            <param name="yInd">
            Target values specified as indixes pointing into <paramref name="classes"/> array.</param>
            <returns>
            Vector with every element containing weight for every item in <paramref name="yInd"/>.
            </returns>
        </member>
        <member name="M:Sharpkit.Learn.ClassWeightEstimator`1.ComputeClassWeightAuto(`0[],System.Int32[])">
            <summary>
            Estimate class weights for unbalanced datasets.
            </summary>
            <param name="classes">Sorted array of the classes occurring in the data.</param>
            <param name="yInd">Array of class indices per sample.</param>
            <returns>Array with ith element - the weight for i-th class (as determined by sorting).</returns>
        </member>
        <member name="M:Sharpkit.Learn.ClassWeightEstimator`1.ComputeClassWeightUniform(`0[])">
            <summary>
            Computes uniform class weights.
            </summary>
            <param name="classes">List of all classes.</param>
            <returns>Class weights for all classes.</returns>
        </member>
        <member name="M:Sharpkit.Learn.ClassWeightEstimator`1.ComputeClassWeightExplicit(System.Collections.Generic.Dictionary{`0,System.Double},`0[])">
            <summary>
            Computes class weights given dictionary which contains weights for certain class labels.
            </summary>
            <param name="classWeight">Dictionary with class weights.</param>
            <param name="classes">List of all classes.</param>
            <returns>Vector with weights corresponding to all items in <paramref name="classes"/>.</returns>
        </member>
        <member name="T:Sharpkit.Learn.Datasets.DiabetesDataset">
            <summary>
            Diabetes sample dataset.
            </summary>
        </member>
        <member name="M:Sharpkit.Learn.Datasets.DiabetesDataset.#ctor">
            <summary>
            Prevents a default instance of the DiabetesDataset class from being created.
            </summary>
        </member>
        <member name="M:Sharpkit.Learn.Datasets.DiabetesDataset.Load">
            <summary>
            Load and return the diabetes dataset (regression).
            ==============      ==================
            Samples total       442
            Dimensionality      10
            Features            real, -.2 &lt; x &lt; .2
            Targets             integer 25 - 346
            ==============      ==================
            </summary>
            <returns>
            Instance of <see cref="T:Sharpkit.Learn.Datasets.DiabetesDataset"/> class.
            </returns>
        </member>
        <member name="P:Sharpkit.Learn.Datasets.DiabetesDataset.Data">
            <summary>
            Gets data to learn;
            </summary>
        </member>
        <member name="P:Sharpkit.Learn.Datasets.DiabetesDataset.Target">
            <summary>
            Gets regression target for each sample.
            </summary>
        </member>
        <member name="T:Sharpkit.Learn.Datasets.DigitsDataset">
            <summary>
            Digits sample dataset.
            </summary>
        </member>
        <member name="M:Sharpkit.Learn.Datasets.DigitsDataset.Load">
            <summary>
            Load and return the digits dataset (classification).
            Each datapoint is a 8x8 image of a digit.
            =================   ==============
            Classes                         10
            Samples per class             ~180
            Samples total                 1797
            Dimensionality                  64
            Features             integers 0-16
            =================   ==============
            </summary>
            <returns>Instance of <see cref="T:Sharpkit.Learn.Datasets.DigitsDataset"/>.</returns>
        </member>
        <member name="P:Sharpkit.Learn.Datasets.DigitsDataset.Data">
            <summary>
            Gets data to learn;
            </summary>
        </member>
        <member name="P:Sharpkit.Learn.Datasets.DigitsDataset.Target">
            <summary>
            Gets regression target for each sample.
            </summary>
        </member>
        <member name="P:Sharpkit.Learn.Datasets.DigitsDataset.Description">
            <summary>
            Gets full description of the dataset.
            </summary>
        </member>
        <member name="T:Sharpkit.Learn.Datasets.IrisDataset">
            <summary>
            The iris dataset is a classic and very easy multi-class classification dataset.
            </summary>
        </member>
        <member name="M:Sharpkit.Learn.Datasets.IrisDataset.Load">
            <summary>
            Load and return the iris dataset (classification).
            =================   ==============
            Classes                          3
            Samples per class               50
            Samples total                  150
            Dimensionality                   4
            Features            real, positive
            =================   ==============
            </summary>
            <returns>Instance of <see cref="T:Sharpkit.Learn.Datasets.IrisDataset"/> class.</returns>
        </member>
        <member name="P:Sharpkit.Learn.Datasets.IrisDataset.Data">
            <summary>
            Gets the data to learn.
            </summary>
        </member>
        <member name="P:Sharpkit.Learn.Datasets.IrisDataset.Target">
            <summary>
            Gets the classification labels.
            </summary>
        </member>
        <member name="P:Sharpkit.Learn.Datasets.IrisDataset.TargetNames">
            <summary>
            Gets the meaning of the labels.
            </summary>
        </member>
        <member name="P:Sharpkit.Learn.Datasets.IrisDataset.FeatureNames">
            <summary>
            Gets the meaning of the features.
            </summary>
        </member>
        <member name="P:Sharpkit.Learn.Datasets.IrisDataset.Descr">
            <summary>
            Gets or sets description of the dataset.
            </summary>
        </member>
        <member name="T:Sharpkit.Learn.Datasets.SampleGenerator">
            <summary>
            Generate samples of synthetic data sets.
            </summary>
        </member>
        <member name="M:Sharpkit.Learn.Datasets.SampleGenerator.MakeRegression(System.Int32,System.Int32,System.Int32,System.Int32,System.Double,System.Nullable{System.Int32},System.Double,System.Double,System.Boolean,System.Boolean,System.Random)">
            <summary>
            Generate a random regression problem.
            <para>
            The input set can either be well conditioned (by default) or have a low
            rank-fat tail singular profile. See <see cref="M:Sharpkit.Learn.Datasets.SampleGenerator.MakeLowRankMatrix(System.Int32,System.Int32,System.Int32,System.Double,System.Random)"/> for
            more details.
            </para>
            <para>
            The output is generated by applying a (potentially biased) random linear
            regression model with `numInformative` nonzero regressors to the previously
            generated input and some gaussian centered noise with some adjustable
            scale.
            </para>
            </summary>
            <param name="numSamples">The number of samples.</param>
            <param name="numFeatures">The number of features.</param>
            <param name="numInformative">The number of informative features, i.e., the number of features used
            to build the linear model used to generate the output.</param>
            <param name="numTargets">The number of regression targets, i.e., the dimension of the y output
            vector associated with a sample. By default, the output is a scalar.</param>
            <param name="bias">The bias term in the underlying linear model.</param>
            <param name="effectiveRank">if not null:
              The approximate number of singular vectors required to explain most
              of the input data by linear combinations. Using this kind of
              singular spectrum in the input allows the generator to reproduce
              the correlations often observed in practice.
              if null:
                  The input set is well conditioned, centered and gaussian with
                  unit variance.</param>
            <param name="tailStrength">Value between 0.0 and 1.0.
            The relative importance of the fat noisy tail of the singular values
            profile if <paramref name="effectiveRank"/> is not None.</param>
            <param name="noise">The standard deviation of the gaussian noise applied to the output.</param>
            <param name="shuffle">Shuffle the samples and the features.</param>
            <param name="coef">If <c>true</c>, the coefficients of the underlying linear model are returned.</param>
            <param name="random">Instance of <see cref="T:System.Random"/>.</param>
            <returns>Instance of <see cref="T:Sharpkit.Learn.Datasets.SampleGenerator.RegressionResult"/>.</returns>
        </member>
        <member name="M:Sharpkit.Learn.Datasets.SampleGenerator.MakeLowRankMatrix(System.Int32,System.Int32,System.Int32,System.Double,System.Random)">
            <summary>
            Generate a mostly low rank matrix with bell-shaped singular values
            <para>
            Most of the variance can be explained by a bell-shaped curve of width
            effective_rank: the low rank part of the singular values profile is:
            </para>
            <para>
            (1 - tailStrength) * exp(-1.0 * (i / effectiveRank) ** 2)
            </para>
            <para>
            The remaining singular values' tail is fat, decreasing as:
            </para>
            <para>
            tailStrength * exp(-0.1 * i / effectiveRank).
            </para>
            <para>
            The low rank part of the profile can be considered the structured
            signal part of the data while the tail can be considered the noisy
            part of the data that cannot be summarized by a low number of linear
            components (singular vectors).
            </para>
            <para>
            This kind of singular profiles is often seen in practice, for instance:
              - gray level pictures of faces
              - TF-IDF vectors of text documents crawled from the web
            </para>
            </summary>
            <param name="numSamples">The number of samples.</param>
            <param name="numFeatures">The number of features.</param>
            <param name="effectiveRank">The approximate number of singular vectors required to explain most of
            the data by linear combinations.</param>
            <param name="tailStrength">The relative importance of the fat noisy tail of the singular values
            profile.</param>
            <param name="randomState">Instance of <see cref="T:System.Random"/>.</param>
            <returns>The matrix.</returns>
        </member>
        <member name="M:Sharpkit.Learn.Datasets.SampleGenerator.MakeSparseUncorrelated(System.Int32,System.Int32,System.Random)">
            <summary>
            Generate a random regression problem with sparse uncorrelated design
            <para>
            This dataset is described in Celeux et al [1]. as::
            </para>
            <para>
            X ~ N(0, 1)
            y(X) = X[:, 0] + 2 * X[:, 1] - 2 * X[:, 2] - 1.5 * X[:, 3]
            </para>
            <para>
            Only the first 4 features are informative. The remaining features are
            useless.
            </para>
            </summary>
            <param name="numSamples">The number of samples.</param>
            <param name="numFeatures">The number of features.</param>
            <param name="random">Instance of <see cref="T:System.Random"/>.</param>
            <returns>Instance of <see cref="T:Sharpkit.Learn.Datasets.SampleGenerator.RegressionResult"/> with <see cref="P:Sharpkit.Learn.Datasets.SampleGenerator.RegressionResult.Coef"/> not populated.</returns>
            <remarks>
                References
                ----------
                 .. [1] G. Celeux, M. El Anbari, J.-M. Marin, C. P. Robert,
                  "Regularization in regression: comparing Bayesian and frequentist
                   methods in a poorly informative situation", 2009.
            </remarks>
        </member>
        <member name="M:Sharpkit.Learn.Datasets.SampleGenerator.MakeClassification(System.Int32,System.Int32,System.Int32,System.Int32,System.Int32,System.Int32,System.Int32,System.Collections.Generic.List{System.Double},System.Double,System.Double,System.Boolean,System.Nullable{System.Double},System.Nullable{System.Double},System.Boolean,System.Random)">
            <summary>
            Generate a random n-class classification problem.
            </summary>
            <param name="nSamples">The number of samples.</param>
            <param name="nFeatures">The total number of features. These comprise <paramref name="nInformative"/>
            informative features, <paramref name="nRedundant"/> redundant features, <paramref name="nRepeated"/>
            dupplicated features and `<paramref name="nFeatures"/>-<paramref name="nInformative"/>-<paramref name="nRedundant"/>-
            <paramref name="nRepeated"/>` useless features drawn at random.</param>
            <param name="nInformative">The number of informative features. Each class is composed of a number
            of gaussian clusters each located around the vertices of a hypercube
            in a subspace of dimension <paramref name="nInformative"/>. For each cluster,
            informative features are drawn independently from  N(0, 1) and then
            randomly linearly combined in order to add covariance. The clusters
            are then placed on the vertices of the hypercube.</param>
            <param name="nRedundant">The number of redundant features. These features are generated as
            random linear combinations of the informative features.</param>
            <param name="nRepeated"> The number of dupplicated features, drawn randomly from the informative
            and the redundant features.
            </param>
            <param name="nClasses">The number of classes (or labels) of the classification problem.</param>
            <param name="nClustersPerClass">The number of clusters per class.</param>
            <param name="weights">The proportions of samples assigned to each class. If None, then
            classes are balanced. Note that if `len(weights) == n_classes - 1`,
            then the last class weight is automatically inferred.
            </param>
            <param name="flipY">The fraction of samples whose class are randomly exchanged.</param>
            <param name="classSep">The factor multiplying the hypercube dimension.</param>
            <param name="hypercube">If True, the clusters are put on the vertices of a hypercube. If
            False, the clusters are put on the vertices of a random polytope.</param>
            <param name="shift">Shift all features by the specified value. If None, then features
            are shifted by a random value drawn in [-class_sep, class_sep].</param>
            <param name="scale">Multiply all features by the specified value. If None, then features
            are scaled by a random value drawn in [1, 100]. Note that scaling
            happens after shifting.
            </param>
            <param name="shuffle">Shuffle the samples and the features.</param>
            <param name="randomState">Random generator.</param>
            <returns>array of shape [n_samples]
            The integer labels for class membership of each sample.</returns>
            <remarks>
            The algorithm is adapted from Guyon [1] and was designed to generate
            the "Madelon" dataset.
            References
            ----------
            .. [1] I. Guyon, "Design of experiments for the NIPS 2003 variable
              selection benchmark", 2003.
            </remarks>
        </member>
        <member name="T:Sharpkit.Learn.Datasets.SampleGenerator.RegressionResult">
            <summary>
            Regression result.
            </summary>
        </member>
        <member name="M:Sharpkit.Learn.Datasets.SampleGenerator.RegressionResult.#ctor">
            <summary>
            Initializes a new instance of the RegressionResult class.
            </summary>
        </member>
        <member name="P:Sharpkit.Learn.Datasets.SampleGenerator.RegressionResult.X">
            <summary>
            Gets the input samples.
            </summary>
        </member>
        <member name="P:Sharpkit.Learn.Datasets.SampleGenerator.RegressionResult.Y">
            <summary>
            Gets the output values.
            </summary>
        </member>
        <member name="P:Sharpkit.Learn.Datasets.SampleGenerator.RegressionResult.Coef">
            <summary>
            Gets the coefficient of the underlying linear model.
            </summary>
        </member>
        <member name="T:Sharpkit.Learn.Datasets.SampleGenerator.Classification">
            <summary>
            Result of <see cref="M:Sharpkit.Learn.Datasets.SampleGenerator.MakeClassification(System.Int32,System.Int32,System.Int32,System.Int32,System.Int32,System.Int32,System.Int32,System.Collections.Generic.List{System.Double},System.Double,System.Double,System.Boolean,System.Nullable{System.Double},System.Nullable{System.Double},System.Boolean,System.Random)"/>.
            </summary>
        </member>
        <member name="P:Sharpkit.Learn.Datasets.SampleGenerator.Classification.X">
            <summary>
            Gets or sets the generated samples.
            Matrix of shape [n_samples, n_features]
            </summary>
        </member>
        <member name="P:Sharpkit.Learn.Datasets.SampleGenerator.Classification.Y">
            <summary>
            Gets or sets target lables.
            </summary>
        </member>
        <member name="T:Sharpkit.Learn.IClassifier`1">
            <summary>
            Interface implemented by all classifiers.
            </summary>
            <typeparam name="TLabel">Type of class label.</typeparam>
        </member>
        <member name="M:Sharpkit.Learn.IClassifier`1.Fit(MathNet.Numerics.LinearAlgebra.Generic.Matrix{System.Double},`0[])">
            <summary>
            Fit the model according to the given training data.
            </summary>
            <param name="x">[nSamples, nFeatures]. Training vectors,
            where nSamples is the number of samples and nFeatures
            is the number of features.</param>
            <param name="y">[nSamples] Target class labels.</param>
            <returns>Reference to itself.</returns>
        </member>
        <member name="M:Sharpkit.Learn.IClassifier`1.Predict(MathNet.Numerics.LinearAlgebra.Generic.Matrix{System.Double})">
            <summary>
            Perform classification on samples in X.
            For an one-class model, +1 or -1 is returned.
            </summary>
            <param name="x">[nSamples, nFeatures]. Samples.</param>
            <returns>[nSamples] Class labels for samples in <paramref name="x"/>.</returns>
        </member>
        <member name="M:Sharpkit.Learn.IClassifier`1.PredictProba(MathNet.Numerics.LinearAlgebra.Generic.Matrix{System.Double})">
            <summary>
            Calculates probability estimates.
            The returned estimates for all classes are ordered by the
            label of classes.
            </summary>
            <param name="x">[nSamples, nFeatures]. Samples.</param>
            <returns>
            [nSamples, nClasses]. The probability of the sample for each class in the model,
            where classes are ordered as they are in <see cref="P:Sharpkit.Learn.IClassifier`1.Classes"/>.
            </returns>
        </member>
        <member name="M:Sharpkit.Learn.IClassifier`1.PredictLogProba(MathNet.Numerics.LinearAlgebra.Generic.Matrix{System.Double})">
            <summary>
            Calculates log of probability estimates.
            The returned estimates for all classes are ordered by the
            label of classes.
            </summary>
            <param name="x">[nSamples, nFeatures]. Samples.</param>
            <returns>[nSamples, nClasses] Log-probability of the sample for each class in the
            model, where classes are ordered as they are in <see cref="P:Sharpkit.Learn.IClassifier`1.Classes"/>.
            </returns>
        </member>
        <member name="M:Sharpkit.Learn.IClassifier`1.DecisionFunction(MathNet.Numerics.LinearAlgebra.Generic.Matrix{System.Double})">
            <summary>
            Predict confidence scores for samples.
            The confidence score for a sample is the signed distance of that
            sample to the hyperplane.
            </summary>
            <param name="x">[nSamples, nFeatures] Samples.</param>
            <returns>[nSamples, nClasses] Confidence scores per (sample, class) combination.
            In the binary case, confidence score for the "positive" class.</returns>
        </member>
        <member name="P:Sharpkit.Learn.IClassifier`1.ClassWeightEstimator">
            <summary>
            Gets or sets class weight estimator.
            </summary>
        </member>
        <member name="P:Sharpkit.Learn.IClassifier`1.Classes">
            <summary>
            Gets ordered list of class labeled discovered int <see cref="M:Sharpkit.Learn.IClassifier`1.Fit(MathNet.Numerics.LinearAlgebra.Generic.Matrix{System.Double},`0[])"/>.
            </summary>
        </member>
        <member name="P:Sharpkit.Learn.LeastSquares.LsqrResult.X">
            <summary>
            The final solution.
            </summary>
        </member>
        <member name="P:Sharpkit.Learn.LeastSquares.LsqrResult.IsStop">
            <summary>
            Gives the reason for termination.
            1 means x is an approximate solution to Ax = b.
            2 means x approximately solves the least-squares problem.
            </summary>
        </member>
        <member name="P:Sharpkit.Learn.LeastSquares.LsqrResult.ItN">
            <summary>
            Iteration number upon termination.
            </summary>
        </member>
        <member name="P:Sharpkit.Learn.LeastSquares.LsqrResult.R1Norm">
            <summary>
            ``norm(r)``, where ``r = b - Ax``.
            </summary>
        </member>
        <member name="P:Sharpkit.Learn.LeastSquares.LsqrResult.R2Norm">
            <summary>
            ``sqrt( norm(r)^2  +  damp^2 * norm(x)^2 )``.  Equal to `r1norm` if
            ``damp == 0``.
            </summary>
        </member>
        <member name="P:Sharpkit.Learn.LeastSquares.LsqrResult.ANorm">
            <summary>
            Estimate of Frobenius norm of ``Abar = [[A]; [damp*I]]``.
            </summary>
        </member>
        <member name="P:Sharpkit.Learn.LeastSquares.LsqrResult.ACond">
            <summary>
            Estimate of ``cond(Abar)``.
            </summary>
        </member>
        <member name="P:Sharpkit.Learn.LeastSquares.LsqrResult.ArNorm">
            <summary>
            Estimate of ``norm(A'*r - damp^2*x)``.
            </summary>
        </member>
        <member name="P:Sharpkit.Learn.LeastSquares.LsqrResult.XNorm">
            <summary>
            ``norm(x)``
            </summary>
        </member>
        <member name="P:Sharpkit.Learn.LeastSquares.LsqrResult.Var">
            <summary>
            If ``calcVar`` is True, estimates all diagonals of
            ``(A'A)^{-1}`` (if ``damp == 0``) or more generally ``(A'A +
            damp^2*I)^{-1}``.  This is well defined if A has full column
            rank or ``damp > 0``.  (Not sure what var means if ``rank(A) &lt; n`` and ``damp = 0.``)
            </summary>
        </member>
        <member name="T:Sharpkit.Learn.LinearModel.LogisticRegression`1">
            <summary>
            <para>
            Logistic Regression (aka logit, MaxEnt) classifier.
            </para>
            <para>
            In the multiclass case, the training algorithm uses a one-vs.-all (OvA)
            scheme, rather than the "true" multinomial LR.
            </para>
            <para>
            This class implements L1 and L2 regularized logistic regression using the
            `liblinear` library. It can handle both dense and sparse input.
            </para>
            </summary>
            <remarks>
            <para>
            The underlying Liblinear implementation uses a random number generator to
            select features when fitting the model. It is thus not uncommon,
            to have slightly different results for the same input data. If
            that happens, try with a smaller tol parameter.
            </para>
            <para>
            References:
            </para>
            <para>
            LIBLINEAR -- A Library for Large Linear Classification
            http://www.csie.ntu.edu.tw/~cjlin/liblinear/
            </para>
            <para>
            Hsiang-Fu Yu, Fang-Lan Huang, Chih-Jen Lin (2011). Dual coordinate descent
               methods for logistic regression and maximum entropy models.
               Machine Learning 85(1-2):41-75.
               http://www.csie.ntu.edu.tw/~cjlin/papers/maxent_dual.pdf
            </para>
            </remarks>
        </member>
        <member name="T:Sharpkit.Learn.LinearModel.LinearClassifier`1">
            <summary>
            Base class for linear classifiers.
            Handles prediction for sparse and dense X.
            </summary>
        </member>
        <member name="T:Sharpkit.Learn.LinearModel.LinearModel">
            <summary>
            Linear model base class.
            </summary>
        </member>
        <member name="M:Sharpkit.Learn.LinearModel.LinearModel.#ctor(System.Boolean)">
            <summary>
            Initializes a new instance of the LinearModel class.
            </summary>
            <param name="fitIntercept">Whether to calculate the intercept for this model. If set
            to false, no intercept will be used in calculations
            (e.g. data is expected to be already centered).</param>
        </member>
        <member name="M:Sharpkit.Learn.LinearModel.LinearModel.CenterData(MathNet.Numerics.LinearAlgebra.Generic.Matrix{System.Double},MathNet.Numerics.LinearAlgebra.Generic.Matrix{System.Double},System.Boolean,System.Boolean,MathNet.Numerics.LinearAlgebra.Generic.Vector{System.Double})">
            <summary>
            Centers data to have mean zero along axis 0. This is here because
            nearly all linear models will want their data to be centered.
            If sample_weight is not None, then the weighted mean of X and y
            is zero, and not the mean itself
            </summary>
            <param name="x"></param>
            <param name="y"></param>
            <param name="fitIntercept"></param>
            <param name="normalize"></param>
            <param name="sampleWeight"></param>
        </member>
        <member name="P:Sharpkit.Learn.LinearModel.LinearModel.FitIntercept">
            <summary>
            Gets or sets a value indicating whether to calculate the intercept for this model. If set
            to false, no intercept will be used in calculations
            (e.g. data is expected to be already centered).
            </summary>
        </member>
        <member name="P:Sharpkit.Learn.LinearModel.LinearModel.Coef">
            <summary>
            shape (n_targets, n_features)
            Estimated coefficients for problem.
            </summary>
        </member>
        <member name="P:Sharpkit.Learn.LinearModel.LinearModel.Intercept">
            <summary>
            Independent term in the linear model.
            </summary>
        </member>
        <member name="M:Sharpkit.Learn.LinearModel.LinearClassifier`1.DecisionFunction(MathNet.Numerics.LinearAlgebra.Generic.Matrix{System.Double})">
             <summary>
             Predict confidence scores for samples.
            
             The confidence score for a sample is the signed distance of that
             sample to the hyperplane.
             </summary>
             <param name="x">shape = [n_samples, n_features] Samples.</param>
             <returns>[n_samples,n_classes]
                Confidence scores per (sample, class) combination. In the binary
                case, confidence score for the "positive" class.</returns>
        </member>
        <member name="M:Sharpkit.Learn.LinearModel.LinearClassifier`1.PredictProbaLr(MathNet.Numerics.LinearAlgebra.Generic.Matrix{System.Double})">
             <summary>
             Probability estimation for OvR logistic regression.
            
             Positive class probabilities are computed as
             1. / (1. + np.exp(-self.decision_function(X)));
             multiclass is handled by normalizing that over all classes.
             </summary>
             <param name="x"></param>
             <returns></returns>
        </member>
        <member name="M:Sharpkit.Learn.LinearModel.LinearClassifier`1.Predict(System.Double[])">
            <summary>
            Predict class labels for samples in X.
            </summary>
            <param name="x">[n_samples, n_features] Samples.</param>
            <returns>[n_samples] Predicted class label per sample.</returns>
        </member>
        <member name="M:Sharpkit.Learn.LinearModel.LinearClassifier`1.Predict(MathNet.Numerics.LinearAlgebra.Generic.Matrix{System.Double})">
            <summary>
            Predict class labels for samples in X.
            </summary>
            <param name="x">[n_samples, n_features] Samples.</param>
            <returns>[n_samples] Predicted class label per sample.</returns>
        </member>
        <member name="M:Sharpkit.Learn.LinearModel.LinearClassifier`1.Fit(MathNet.Numerics.LinearAlgebra.Generic.Matrix{System.Double},`0[])">
            <summary>
            Fit the model according to the given training data.
            </summary>
            <param name="x">[nSamples, nFeatures]. Training vectors,
            where nSamples is the number of samples and nFeatures
            is the number of features.</param>
            <param name="y">[nSamples] Target class labels.</param>
            <returns>Reference to itself.</returns>
        </member>
        <member name="M:Sharpkit.Learn.LinearModel.LinearClassifier`1.PredictProba(MathNet.Numerics.LinearAlgebra.Generic.Matrix{System.Double})">
            <summary>
            Calculates probability estimates.
            The returned estimates for all classes are ordered by the
            label of classes.
            </summary>
            <param name="x">[nSamples, nFeatures]. Samples.</param>
            <returns>
            [nSamples, nClasses]. The probability of the sample for each class in the model,
            where classes are ordered as they are in <see cref="P:Sharpkit.Learn.IClassifier`1.Classes"/>.
            </returns>
        </member>
        <member name="M:Sharpkit.Learn.LinearModel.LinearClassifier`1.PredictLogProba(MathNet.Numerics.LinearAlgebra.Generic.Matrix{System.Double})">
            <summary>
            Calculates log of probability estimates.
            The returned estimates for all classes are ordered by the
            label of classes.
            </summary>
            <param name="x">[nSamples, nFeatures]. Samples.</param>
            <returns>[nSamples, nClasses] Log-probability of the sample for each class in the
            model, where classes are ordered as they are in <see cref="P:Sharpkit.Learn.LinearModel.LinearClassifier`1.Classes"/>.
            </returns>
        </member>
        <member name="P:Sharpkit.Learn.LinearModel.LinearClassifier`1.Classes">
            <summary>
            Gets ordered list of class labeled discovered int <see cref="M:Sharpkit.Learn.LinearModel.LinearClassifier`1.Fit(MathNet.Numerics.LinearAlgebra.Generic.Matrix{System.Double},`0[])"/>.
            </summary>
        </member>
        <member name="P:Sharpkit.Learn.LinearModel.LinearClassifier`1.ClassWeightEstimator">
            <summary>
            Gets or sets class weight estimator.
            </summary>
        </member>
        <!-- Badly formed XML comment ignored for member "M:Sharpkit.Learn.LinearModel.LogisticRegression`1.#ctor(Sharpkit.Learn.LinearModel.Norm,System.Boolean,System.Double,System.Double,System.Boolean,System.Double,Sharpkit.Learn.ClassWeightEstimator{`0},System.Random)" -->
        <member name="M:Sharpkit.Learn.LinearModel.LogisticRegression`1.Fit(MathNet.Numerics.LinearAlgebra.Generic.Matrix{System.Double},`0[])">
            <summary>
            Fit the model according to the given training data.
            </summary>
            <param name="x">[nSamples, nFeatures]. Training vectors,
            where nSamples is the number of samples and nFeatures
            is the number of features.</param>
            <param name="y">[nSamples] Target class labels.</param>
            <returns>Reference to itself.</returns>
        </member>
        <member name="M:Sharpkit.Learn.LinearModel.LogisticRegression`1.PredictProba(MathNet.Numerics.LinearAlgebra.Generic.Matrix{System.Double})">
            <summary>
            Probability estimates.
            The returned estimates for all classes are ordered by the
            label of classes.
            </summary>
            <param name="x">[n_samples, n_features]</param>
            <returns>[n_samples, n_classes]
                  Returns the probability of the sample for each class in the model,
                  where classes are ordered as they are in <see cref="P:Sharpkit.Learn.LinearModel.LogisticRegression`1.Classes"/>.
            </returns>
        </member>
        <member name="M:Sharpkit.Learn.LinearModel.LogisticRegression`1.PredictLogProba(MathNet.Numerics.LinearAlgebra.Generic.Matrix{System.Double})">
             <summary>
             Log of probability estimates.
            
             The returned estimates for all classes are ordered by the
             label of classes.
             </summary>
             <param name="x">[n_samples, n_features].</param>
             <returns>[n_samples, n_classes]
             Returns the log-probability of the sample for each class in the
             model, where classes are ordered as they are in <see cref="P:Sharpkit.Learn.LinearModel.LogisticRegression`1.Classes"/>.
             </returns>
        </member>
        <member name="M:Sharpkit.Learn.LinearModel.LogisticRegression`1.Sparsify">
             <summary>
             Convert coefficient matrix to sparse format.
             Converts the <see cref="P:Sharpkit.Learn.LinearModel.LinearModel.Coef"/> member to a <see cref="T:MathNet.Numerics.LinearAlgebra.Double.SparseMatrix"/>, which for
             L1-regularized models can be much more memory- and storage-efficient
             than the usual numpy.ndarray representation.
            
             The <see cref="P:Sharpkit.Learn.LinearModel.LinearModel.Intercept"/> member is not converted.
             </summary>
             <remarks>
             For non-sparse models, i.e. when there are not many zeros in <see cref="P:Sharpkit.Learn.LinearModel.LinearModel.Coef"/>,
             this may actually *increase* memory usage, so use this method with
             care. A rule of thumb is that the number of zero elements, which can
             be computed with ``(Coef == 0).Sum()``, must be more than 50% for this
             to provide significant benefits.
            
             After calling this method, further fitting with the partial_fit
             method (if any) will not work until you call densify.
             </remarks>
        </member>
        <member name="M:Sharpkit.Learn.LinearModel.LogisticRegression`1.Densify">
             <summary>
             Convert coefficient matrix to dense array format.
            
             Converts the <see cref="P:Sharpkit.Learn.LinearModel.LinearModel.Coef"/> member (back) to a numpy.ndarray. This is the
             default format of <see cref="P:Sharpkit.Learn.LinearModel.LinearModel.Coef"/> and is required for fitting, so calling
             this method is only required on models that have previously been
             sparsified; otherwise, it is a no-op.
             </summary>
             <returns></returns>
        </member>
        <member name="P:Sharpkit.Learn.LinearModel.LogisticRegression`1.Dual">
            <summary>
            Gets or sets a value indicating whether formulation is primal or dual. Dual formulation is only
            implemented for l2 penalty. Prefer dual=false when
            nSamples > nFeatures.
            </summary>
        </member>
        <member name="P:Sharpkit.Learn.LinearModel.LogisticRegression`1.C">
            <summary>
            Gets or sets inverse of regularization strength; must be a positive float.
            Like in support vector machines, smaller values specify stronger
            regularization.
            </summary>
        </member>
        <member name="P:Sharpkit.Learn.LinearModel.LogisticRegression`1.Classes">
            <summary>
            Gets ordered list of class labeled discovered int <see cref="M:Sharpkit.Learn.LinearModel.LogisticRegression`1.Fit(MathNet.Numerics.LinearAlgebra.Generic.Matrix{System.Double},`0[])"/>.
            </summary>
        </member>
        <member name="F:Sharpkit.Learn.LinearModel.Multiclass.Ovr">
            <summary>
            Trains n_classes one-vs-rest classifiers.
            </summary>
        </member>
        <member name="F:Sharpkit.Learn.LinearModel.Multiclass.CrammerSinger">
            <summary>
            Optimizes a joint objective over all classes.
            While `crammer_singer` is interesting from an theoretical perspective
            as it is consistent it is seldom used in practice and rarely leads to
            better accuracy and is more expensive to compute.
            </summary>
        </member>
        <member name="T:Sharpkit.Learn.LinearModel.RidgeSolver">
            <summary>
            Solver to use in the Ridge computational routines.
            </summary>
        </member>
        <member name="F:Sharpkit.Learn.LinearModel.RidgeSolver.Auto">
            <summary>
            Chooses the solver automatically based on the type of data.
            </summary>
        </member>
        <member name="F:Sharpkit.Learn.LinearModel.RidgeSolver.Svd">
            <summary>
            uses a Singular Value Decomposition of X to compute the Ridge
             coefficients. More stable for singular matrices than <see cref="F:Sharpkit.Learn.LinearModel.RidgeSolver.DenseCholesky"/>.
            </summary>
        </member>
        <member name="F:Sharpkit.Learn.LinearModel.RidgeSolver.DenseCholesky">
            <summary>
            Uses the standard Math.Net Cholesky() function to
            obtain a closed-form solution.
            </summary>
        </member>
        <member name="F:Sharpkit.Learn.LinearModel.RidgeSolver.Lsqr">
            <summary>
            Uses the dedicated regularized least-squares routine
            scipy.sparse.linalg.lsqr. It is the fatest but may not be available
            in old scipy versions. It also uses an iterative procedure.
            </summary>
        </member>
        <member name="T:Sharpkit.Learn.Metrics.AverageKind">
            <summary>
            Type of averaging performed on the data.
            </summary>
        </member>
        <member name="F:Sharpkit.Learn.Metrics.AverageKind.Micro">
            <summary>
            Calculate metrics globally by counting the total true positives,
            false negatives and false positives.
            </summary>
        </member>
        <member name="F:Sharpkit.Learn.Metrics.AverageKind.Macro">
            <summary>
            Calculate metrics for each label, and find their unweighted
            mean. This does not take label imbalance into account.
            </summary>
        </member>
        <member name="F:Sharpkit.Learn.Metrics.AverageKind.Weighted">
            <summary>
            Calculate metrics for each label, and find their average, weighted
            by support (the number of true instances for each label). This
            alters 'macro' to account for label imbalance; it can result in an
            F-score that is not between precision and recall.
            </summary>
        </member>
        <member name="T:Sharpkit.Learn.Preprocessing.LabelBinarizer`1">
            <summary>
            Binarize labels in a one-vs-all fashion
            <para>
            Several regression and binary classification algorithms are
            available in the sharpkit. A simple way to extend these algorithms
            to the multi-class classification case is to use the so-called
            one-vs-all scheme.
            </para>
            <para>
            At learning time, this simply consists in learning one regressor
            or binary classifier per class. In doing so, one needs to convert
            multi-class labels to binary labels (belong or does not belong
            to the class). LabelBinarizer makes this process easy with the
            transform method.
            </para>
            <para>
            At prediction time, one assigns the class for which the corresponding
            model gave the greatest confidence. LabelBinarizer makes this easy
            with the InverseTransform method.
            </para>
            </summary>
            <typeparam name="TLabel">Type of class label.</typeparam>
            <example>
            var lb = new LabelBinarizer&lt;int>();
            lb.fit([1, 2, 6, 4, 2]);
                LabelBinarizer(neg_label=0, pos_label=1)
            lb.Classes
                {1, 2, 4, 6}
            lb.Transform(new []{1, 6});
              {{1, 0, 0, 0},
               {0, 0, 0, 1}}
            </example>
        </member>
        <member name="F:Sharpkit.Learn.Preprocessing.LabelBinarizer`1.negLabel">
            <summary>
            Value to use as negative label.
            </summary>
        </member>
        <member name="F:Sharpkit.Learn.Preprocessing.LabelBinarizer`1.posLabel">
            <summary>
            Value to use as positive label.
            </summary>
        </member>
        <member name="M:Sharpkit.Learn.Preprocessing.LabelBinarizer`1.#ctor(System.Int32,System.Int32)">
            <summary>
            Initializes a new instance of the LabelBinarizer class.
            </summary>
            <param name="negLabel">Value with which negative labels must be encoded.</param>
            <param name="posLabel">Value with which positive labels must be encoded.</param>
        </member>
        <member name="M:Sharpkit.Learn.Preprocessing.LabelBinarizer`1.Fit(`0[])">
            <summary>
            Fit label binarizer
            </summary>
            <param name="y">array of shape [n_samples] or sequence of sequences Target values.</param>
            <returns>Instance of self.</returns>
        </member>
        <member name="M:Sharpkit.Learn.Preprocessing.LabelBinarizer`1.Transform(`0[])">
            <summary>
            Transform multi-class labels to binary labels
            The output of transform is sometimes referred to by some authors as the
            1-of-K coding scheme.
            </summary>
            <param name="y">array of shape [n_samples]. Target values.</param>
            <returns>Matrix of shape [n_samples, n_classes]</returns>
        </member>
        <member name="M:Sharpkit.Learn.Preprocessing.LabelBinarizer`1.InverseTransform(MathNet.Numerics.LinearAlgebra.Generic.Matrix{System.Double},System.Nullable{System.Double})">
            <summary>
            Transform binary labels back to multi-class labels.
            </summary>
            <param name="y">Array of shape [nSamples]. Target values.</param>
            <param name="threshold">Threshold used in the binary and multi-label cases.
               Use 0 when:
                   - Y contains the output of decision_function (classifier)
               Use 0.5 when:
                   - Y contains the output of predict_proba
               If None, the threshold is assumed to be half way between
               negLabel and posLabel.
            </param>
            <returns>Array of shape [n_samples]. Target values.</returns>
            <remarks>
            In the case when the binary labels are fractional
            (probabilistic), InverseTransform chooses the class with the
            greatest value. Typically, this allows to use the output of a
            linear model's DecisionFunction method directly as the input
            of InverseTransform.
            </remarks>
        </member>
        <member name="M:Sharpkit.Learn.Preprocessing.LabelBinarizer`1.CheckFitted">
            <summary>
            Ensures that binarizer is fitted.
            </summary>
        </member>
        <member name="P:Sharpkit.Learn.Preprocessing.LabelBinarizer`1.Classes">
            <summary>
            Gets label for each class.
            </summary>
        </member>
        <member name="T:Sharpkit.Learn.LinearModel.LibLinearBase`1">
            <summary>
            Base for classes binding liblinear (dense and sparse versions).
            </summary>
        </member>
        <member name="M:Sharpkit.Learn.LinearModel.LibLinearBase`1.Fit(MathNet.Numerics.LinearAlgebra.Generic.Matrix{System.Double},`0[])">
            <summary>
            Fit the model according to the given training data.
            </summary>
            <param name="x">shape = [n_samples, n_features]
               Training vector, where n_samples in the number of samples and
               n_features is the number of features.</param>
            <param name="y">shape = [n_samples]
               Target vector relative to X</param>
        </member>
        <member name="T:Sharpkit.Learn.LinearModel.LinearRegressor">
            <summary>
            Generalized Linear models.
            </summary>
        </member>
        <member name="M:Sharpkit.Learn.LinearModel.LinearRegressor.#ctor(System.Boolean)">
            <summary>
            Initializes a new instance of the LinearRegressor class.
            </summary>
            <param name="fitIntercept"></param>
        </member>
        <member name="M:Sharpkit.Learn.LinearModel.LinearRegressor.Fit(MathNet.Numerics.LinearAlgebra.Generic.Matrix{System.Double},MathNet.Numerics.LinearAlgebra.Generic.Matrix{System.Double},MathNet.Numerics.LinearAlgebra.Generic.Vector{System.Double})">
            <summary>
            Fit linear model.
            </summary>
            <param name="x">Matrix of shape [n_samples,n_features]. Training data</param>
            <param name="y">Target values.[n_samples, n_targets]</param>
            <param name="sampleWeight">Sample weights.[n_samples]</param>
            <returns>Instance of self.</returns>
        </member>
        <member name="M:Sharpkit.Learn.LinearModel.LinearRegressor.DecisionFunction(MathNet.Numerics.LinearAlgebra.Generic.Matrix{System.Double})">
            <summary>
            Decision function of the linear model.
            </summary>
            <param name="x">Matrix of shape [n_samples, n_features].</param>
            <returns>shape = [n_samples] Returns predicted values.</returns>
        </member>
        <member name="M:Sharpkit.Learn.LinearModel.LinearRegressor.Predict(MathNet.Numerics.LinearAlgebra.Generic.Matrix{System.Double})">
            <summary>
            Predict using the linear model.
            </summary>
            <param name="x">Matrix of shape [n_samples, n_features].</param>
            <returns>shape = [n_samples] Returns predicted values.</returns>
        </member>
        <member name="M:Sharpkit.Learn.LinearModel.LinearRegressor.Predict(System.Double[])">
            <summary>
            Predict using the linear model.
            </summary>
            <param name="x">Array of length [nFeatures].</param>
            <returns>Returns predicted value.</returns>
        </member>
        <member name="T:Sharpkit.Learn.LinearModel.LinearRegression">
            <summary>
            Ordinary least squares Linear Regression.
            </summary>
        </member>
        <member name="M:Sharpkit.Learn.LinearModel.LinearRegression.#ctor(System.Boolean,System.Boolean)">
            <summary>
            Initializes a new instance of the LinearRegression class.
            </summary>
            <param name="fitIntercept">Whether to calculate the intercept for this model. If set
            to false, no intercept will be used in calculations
            (e.g. data is expected to be already centered).</param>
            <param name="normalize">If True, the regressors X will be normalized before regression.</param>
        </member>
        <member name="M:Sharpkit.Learn.LinearModel.LinearRegression.Fit(MathNet.Numerics.LinearAlgebra.Generic.Matrix{System.Double},MathNet.Numerics.LinearAlgebra.Generic.Matrix{System.Double},MathNet.Numerics.LinearAlgebra.Generic.Vector{System.Double})">
            <summary>
            Fit linear model.
            </summary>
            <param name="x">Matrix of shape [n_samples,n_features]. Training data</param>
            <param name="y">Target values.[n_samples, n_targets]</param>
            <param name="sampleWeight">Sample weights.[n_samples]</param>
            <returns>Instance of self.</returns>
        </member>
        <member name="P:Sharpkit.Learn.LinearModel.LinearRegression.Normalize">
            <summary>
            Gets or sets a value indicating whether regressors X shall be normalized
            before regression.
            </summary>
        </member>
        <member name="T:Sharpkit.Learn.LeastSquares.Lsqr">
             <summary>
             Sparse Equations and Least Squares.
             The original Fortran code was written by C. C. Paige and M. A. Saunders as
             described in
             C. C. Paige and M. A. Saunders, LSQR: An algorithm for sparse linear
             equations and sparse least squares, TOMS 8(1), 43--71 (1982).
             C. C. Paige and M. A. Saunders, Algorithm 583; LSQR: Sparse linear
             equations and least-squares problems, TOMS 8(2), 195--209 (1982).
             It is licensed under the following BSD license:
             Copyright (c) 2006, Systems Optimization Laboratory
             All rights reserved.
            
             Redistribution and use in source and binary forms, with or without
             modification, are permitted provided that the following conditions are
                met:
            
                * Redistributions of source code must retain the above copyright
              notice, this list of conditions and the following disclaimer.
            
             * Redistributions in binary form must reproduce the above
               copyright notice, this list of conditions and the following
               disclaimer in the documentation and/or other materials provided
               with the distribution.
             * Neither the name of Stanford University nor the names of its
               contributors may be used to endorse or promote products derived
              from this software without specific prior written permission.
             THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
             "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
             LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
             A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
             OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
             SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
             LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
             DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
             THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
             (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
             OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
             The Fortran code was translated to Python for use in CVXOPT by Jeffery
             Kline with contributions by Mridul Aanjaneya and Bob Myhill.
             Adapted for SciPy by Stefan van der Walt.
             </summary>
        </member>
        <member name="M:Sharpkit.Learn.LeastSquares.Lsqr.SymOrtho(System.Double,System.Double)">
             <summary>
             Stable implementation of Givens rotation.
                 Notes
                -----
                 The routine 'SymOrtho' was added for numerical stability. This is
                 recommended by S.-C. Choi in [1]_.  It removes the unpleasant potential of
                 ``1/eps`` in some important places (see, for example text following
             "Compute the next plane rotation Qk" in minres.py).
            
             References
             ----------
             .. [1] S.-C. Choi, "Iterative Methods for Singular Linear Equations
               and Least-Squares Problems", Dissertation,
               http://www.stanford.edu/group/SOL/dissertations/sou-cheng-choi-thesis.pdf
             </summary>
             <param name="a"></param>
             <param name="b"></param>
        </member>
        <member name="M:Sharpkit.Learn.LeastSquares.Lsqr.lsqr(MathNet.Numerics.LinearAlgebra.Generic.Matrix{System.Double},MathNet.Numerics.LinearAlgebra.Generic.Vector{System.Double},System.Double,System.Double,System.Double,System.Double,System.Nullable{System.Int32},System.Boolean,System.Boolean)">
             <summary>
             Find the least-squares solution to a large, sparse, linear system
             of equations.
            
             The function solves ``Ax = b``  or  ``min ||b - Ax||^2`` or
             ``min ||Ax - b||^2 + d^2 ||x||^2``.
            
             The matrix A may be square or rectangular (over-determined or
             under-determined), and may have any rank.
            
             ::
             1. Unsymmetric equations --    solve  A*x = b
            
             2. Linear least squares  --    solve  A*x = b
                                      in the least-squares sense
            
             3. Damped least squares  --    solve  (   A    )*x = ( b )
                                              ( damp*I )     ( 0 )
                                      in the least-squares sense
             </summary>
             <param name="a">
             Representation of an m-by-n matrix.  It is required that
             the linear operator can produce ``Ax`` and ``A^T x``.
             </param>
             <param name="b">Right-hand side vector ``b``.</param>
             <param name="damp">Damping coefficient.</param>
             <param name="atol">Stopping tolerance.</param>
             <param name="btol">Stopping tolerance. If both atol and btol are 1.0e-9 (say), the final
             residual norm should be accurate to about 9 digits.  (The
             final x will usually have fewer correct digits, depending on
             cond(A) and the size of damp.)
             </param>
             <param name="conlim">
             Another stopping tolerance.  lsqr terminates if an estimate of
             ``cond(A)`` exceeds `conlim`.  For compatible systems ``Ax =
             b``, `conlim` could be as large as 1.0e+12 (say).  For
             least-squares problems, conlim should be less than 1.0e+8.
             Maximum precision can be obtained by setting ``atol = btol =
             conlim = zero``, but the number of iterations may then be
             excessive.
             </param>
             <param name="iterLim">Explicit limitation on number of iterations (for safety).</param>
             <param name="show">Display an iteration log.</param>
             <param name="calcVar">Whether to estimate diagonals of ``(A'A + damp^2*I)^{-1}``.</param>
             <returns>Instance of <see cref="T:Sharpkit.Learn.LeastSquares.LsqrResult"/>.</returns>
             <remarks>
             LSQR uses an iterative method to approximate the solution.  The
             number of iterations required to reach a certain accuracy depends
             strongly on the scaling of the problem.  Poor scaling of the rows
             or columns of A should therefore be avoided where possible.
             
             For example, in problem 1 the solution is unaltered by
             row-scaling.  If a row of A is very small or large compared to
             the other rows of A, the corresponding row of ( A  b ) should be
             scaled up or down.
            
             In problems 1 and 2, the solution x is easily recovered
             following column-scaling.  Unless better information is known,
             the nonzero columns of A should be scaled so that they all have
             the same Euclidean norm (e.g., 1.0).
            
             In problem 3, there is no freedom to re-scale if damp is
             nonzero.  However, the value of damp should be assigned only
             after attention has been paid to the scaling of A.
            
             The parameter damp is intended to help regularize
             ill-conditioned systems, by preventing the true solution from
             being very large.  Another aid to regularization is provided by
             the parameter acond, which may be used to terminate iterations
             before the computed solution becomes very large.
            
             If some initial estimate ``x0`` is known and if ``damp == 0``,
             one could proceed as follows:
            
             1. Compute a residual vector ``r0 = b - A*x0``.
             2. Use LSQR to solve the system  ``A*dx = r0``.
             3. Add the correction dx to obtain a final solution ``x = x0 + dx``.
            
             This requires that ``x0`` be available before and after the call
             to LSQR.  To judge the benefits, suppose LSQR takes k1 iterations
             to solve A*x = b and k2 iterations to solve A*dx = r0.
             If x0 is "good", norm(r0) will be smaller than norm(b).
             If the same stopping tolerances atol and btol are used for each
             system, k1 and k2 will be similar, but the final solution x0 + dx
             should be more accurate.  The only way to reduce the total work
             is to use a larger stopping tolerance for the second system.
             If some value btol is suitable for A*x = b, the larger value
             btol*norm(b)/norm(r0)  should be suitable for A*dx = r0.
            
             Preconditioning is another way to reduce the number of iterations.
             If it is possible to solve a related system ``M*x = b``
             efficiently, where M approximates A in some helpful way (e.g. M -
             A has low rank or its elements are small relative to those of A),
             LSQR may converge more rapidly on the system ``A*M(inverse)*z =
             b``, after which x can be recovered by solving M*x = z.
            
             If A is symmetric, LSQR should not be used!
            
             Alternatives are the symmetric conjugate-gradient method (cg)
             and/or SYMMLQ.  SYMMLQ is an implementation of symmetric cg that
             applies to any symmetric A and will converge more rapidly than
             LSQR.  If A is positive definite, there are other implementations
             of symmetric cg that require slightly less work per iteration than
             SYMMLQ (but will take the same number of iterations).
            
             References
             ----------
             .. [1] C. C. Paige and M. A. Saunders (1982a).
               "LSQR: An algorithm for sparse linear equations and
               sparse least squares", ACM TOMS 8(1), 43-71.
             .. [2] C. C. Paige and M. A. Saunders (1982b).
               "Algorithm 583.  LSQR: Sparse linear equations and least
               squares problems", ACM TOMS 8(2), 195-209.
            .. [3] M. A. Saunders (1995).  "Solution of sparse rectangular
               systems using LSQR and CRAIG", BIT 35, 588-604.
             </remarks>
        </member>
        <member name="M:Sharpkit.Learn.LinearModel.RidgeBase.RidgeRegression(MathNet.Numerics.LinearAlgebra.Generic.Matrix{System.Double},MathNet.Numerics.LinearAlgebra.Generic.Matrix{System.Double},System.Double,MathNet.Numerics.LinearAlgebra.Generic.Vector{System.Double},Sharpkit.Learn.LinearModel.RidgeSolver,System.Nullable{System.Int32},System.Double)">
            <summary>
            Solve the ridge equation by the method of normal equations.
            </summary>
            <param name="x">[n_samples, n_features]
            Training data</param>
            <param name="y">[n_samples, n_targets]
            Target values</param>
            <param name="alpha"></param>
            <param name="sampleWeight">Individual weights for each sample.</param>
            <param name="solver">Solver to use in the computational routines.</param>
            <param name="maxIter">Maximum number of iterations for least squares solver. </param>
            <param name="tol">Precision of the solution.</param>
            <returns>[n_targets, n_features]
            Weight vector(s)</returns>
            <remarks>
            This function won't compute the intercept;
            </remarks>
        </member>
        <member name="T:Sharpkit.Learn.LinearModel.RidgeClassifier`1">
            <summary>
            Classifier using Ridge regression.
            </summary>
            <remarks>
            For multi-class classification, n_class classifiers are trained in
            a one-versus-all approach. Concretely, this is implemented by taking
            advantage of the multi-variate response support in Ridge.
            </remarks>
            <typeparam name="TLabel"></typeparam>
        </member>
        <member name="M:Sharpkit.Learn.LinearModel.RidgeClassifier`1.#ctor(System.Double,System.Boolean,System.Boolean,System.Nullable{System.Int32},System.Double,Sharpkit.Learn.ClassWeightEstimator{`0},Sharpkit.Learn.LinearModel.RidgeSolver)">
            <summary>
            Initializes a new instance of the RidgeClassifier class.
            </summary>
            <param name="alpha"> Small positive values of alpha improve the conditioning of the problem
            and reduce the variance of the estimates.  Alpha corresponds to
            ``(2*C)^-1`` in other linear models such as LogisticRegression or
            LinearSVC.</param>
            <param name="fitIntercept">Whether to calculate the intercept for this model. If set to false, no
            intercept will be used in calculations (e.g. data is expected to be
            already centered).</param>
            <param name="normalize">If True, the regressors X will be normalized before regression.</param>
            <param name="maxIter">Maximum number of iterations for conjugate gradient solver.
            The default value is determined by Math.Net.</param>
            <param name="tol">Precision of the solution.</param>
            <param name="classWeightEstimator">Weights associated with classes in the form
            {class_label : weight}. If not given, all classes are
            supposed to have weight one.</param>
            <param name="solver">Solver to use in the computational.</param>
        </member>
        <member name="M:Sharpkit.Learn.LinearModel.RidgeClassifier`1.Fit(System.Double[0:,0:],`0[])">
            <summary>
            Fit Ridge regression model.
            </summary>
            <param name="x">[n_samples,n_features]. Training data</param>
            <param name="y">Target values.</param>
            <returns>Instance of self.</returns>
        </member>
        <member name="M:Sharpkit.Learn.LinearModel.RidgeClassifier`1.Fit(MathNet.Numerics.LinearAlgebra.Generic.Matrix{System.Double},`0[])">
            <summary>
            Fit Ridge regression model.
            </summary>
            <param name="x">[n_samples,n_features]. Training data</param>
            <param name="y">Target values.</param>
            <returns>Instance of self.</returns>
        </member>
        <member name="T:Sharpkit.Learn.LinearModel.RidgeRegression">
             <summary>
             Linear least squares with l2 regularization.
            
             This model solves a regression model where the loss function is
             the linear least squares function and regularization is given by
             the l2-norm. Also known as Ridge Regression or Tikhonov regularization.
             This estimator has built-in support for multi-variate regression
             (i.e., when y is a 2d-array of shape [n_samples, n_targets]).
             </summary>
        </member>
        <member name="M:Sharpkit.Learn.LinearModel.RidgeRegression.#ctor(System.Double,System.Boolean,System.Boolean,System.Nullable{System.Int32},System.Double,Sharpkit.Learn.LinearModel.RidgeSolver)">
            <summary>
            Initializes a new instance of the RidgeRegression class.
            </summary>
            <param name="alpha">
            Small positive values of alpha improve the conditioning of the problem
            and reduce the variance of the estimates.  Alpha corresponds to
            ``(2*C)^-1`` in other linear models such as LogisticRegression or
            LinearSVC.
            </param>
            <param name="fitIntercept">
            Whether to calculate the intercept for this model. If set
            to false, no intercept will be used in calculations
            (e.g. data is expected to be already centered).
            </param>
            <param name="normalize">
            If True, the regressors X will be normalized before regression.
            </param>
            <param name="maxIter">
            Maximum number of iterations for conjugate gradient solver.
            The default value is determined by Math.Net.
            </param>
            <param name="tol">Precision of the solution.</param>
            <param name="solver">Solver to use in the computational routines.</param>
        </member>
        <member name="T:Sharpkit.Learn.MatrixExtensions">
            <summary>
            <see cref="T:MathNet.Numerics.LinearAlgebra.Double.Matrix"/> and <see cref="T:MathNet.Numerics.LinearAlgebra.Double.Vector"/> extention methods.
            </summary>
        </member>
        <member name="M:Sharpkit.Learn.MatrixExtensions.Shape(MathNet.Numerics.LinearAlgebra.Generic.Matrix{System.Double})">
            <summary>
            Returns dimensions of a matrix as Tuple.
            </summary>
            <param name="matrix">Matrix to get dimensions.</param>
            <returns>Tuple (rows, columns).</returns>
        </member>
        <member name="M:Sharpkit.Learn.MatrixExtensions.Sqr(MathNet.Numerics.LinearAlgebra.Generic.Matrix{System.Double})">
            <summary>
            Returns matrix where every element is square of corresponding
            element in the original matrix.
            </summary>
            <param name="matrix">Matrix which elements will be squared.</param>
            <returns>Matrix with squared elements.</returns>
        </member>
        <member name="M:Sharpkit.Learn.MatrixExtensions.Sqrt(MathNet.Numerics.LinearAlgebra.Generic.Vector{System.Double})">
            <summary>
            Returns <see cref="T:MathNet.Numerics.LinearAlgebra.Double.Vector"/> where every element is square root of corresponding
            element in the original vector.
            </summary>
            <param name="vector">Original vector.</param>
            <returns>Vector with square root elements.</returns>
        </member>
        <member name="M:Sharpkit.Learn.MatrixExtensions.Log(MathNet.Numerics.LinearAlgebra.Generic.Matrix{System.Double})">
            <summary>
            Returns matrix where every element is Log of corresponding
            element in the original matrix.
            </summary>
            <param name="matrix">Matrix which elements will be transformed.</param>
            <returns>Resulting matrix.</returns>
        </member>
        <member name="M:Sharpkit.Learn.MatrixExtensions.Exp(MathNet.Numerics.LinearAlgebra.Generic.Matrix{System.Double})">
            <summary>
            Returns matrix where every element is exponent of corresponding
            element in the original matrix.
            </summary>
            <param name="matrix">Matrix which elements will be transformed.</param>
            <returns>Resulting matrix.</returns>
        </member>
        <member name="M:Sharpkit.Learn.MatrixExtensions.Sqr(MathNet.Numerics.LinearAlgebra.Generic.Vector{System.Double})">
            <summary>
            Returns vector where every element is square of corresponding
            element in the original vector.
            </summary>
            <param name="vector">Vector which elements will be squared.</param>
            <returns>Vector with squared elements.</returns>
        </member>
        <member name="M:Sharpkit.Learn.MatrixExtensions.Sum(MathNet.Numerics.LinearAlgebra.Generic.Matrix{System.Double})">
            <summary>
            Returns sum of all values of a matrix.
            </summary>
            <param name="matrix">Matrix to calculate sum for.</param>
            <returns>Sum of all elements of the matrix.</returns>
        </member>
        <member name="M:Sharpkit.Learn.MatrixExtensions.SubtractRowVector(MathNet.Numerics.LinearAlgebra.Generic.Matrix{System.Double},MathNet.Numerics.LinearAlgebra.Generic.Vector{System.Double},MathNet.Numerics.LinearAlgebra.Generic.Matrix{System.Double})">
            <summary>
            Subtracts <paramref name="vector"/> from all elements of <paramref name="matrix"/>
            and places result into <paramref name="destMatrix"/>.
            </summary>
            <param name="matrix">Source matrix.</param>
            <param name="vector">Vector to subtract.</param>
            <param name="destMatrix">Resulting matrix.</param>
        </member>
        <member name="M:Sharpkit.Learn.MatrixExtensions.SubtractRowVector(MathNet.Numerics.LinearAlgebra.Generic.Matrix{System.Double},MathNet.Numerics.LinearAlgebra.Generic.Vector{System.Double})">
            <summary>
            Subtracts <paramref name="vector"/> from all rows elements of <paramref name="matrix"/>.
            </summary>
            <param name="matrix">Source matrix.</param>
            <param name="vector">Vector to subtract.</param>
            <returns>
            Resulting matrix.
            </returns>
        </member>
        <member name="M:Sharpkit.Learn.MatrixExtensions.DivRowVector(MathNet.Numerics.LinearAlgebra.Generic.Matrix{System.Double},MathNet.Numerics.LinearAlgebra.Generic.Vector{System.Double},MathNet.Numerics.LinearAlgebra.Generic.Matrix{System.Double})">
            <summary>
            Divides rows of matrix <paramref name="matrix"/> by <paramref name="vector"/> pointwise
            and places result into <paramref name="destMatrix"/>.
            </summary>
            <param name="matrix">Source matrix.</param>
            <param name="vector">Divider.</param>
            <param name="destMatrix">Resulting matrix.</param>
        </member>
        <member name="M:Sharpkit.Learn.MatrixExtensions.DivColumnVector(MathNet.Numerics.LinearAlgebra.Generic.Matrix{System.Double},MathNet.Numerics.LinearAlgebra.Generic.Vector{System.Double},MathNet.Numerics.LinearAlgebra.Generic.Matrix{System.Double})">
            <summary>
            Divides all columns of matrix <paramref name="matrix"/> by <paramref name="vector"/> pointwise
            and places result into <paramref name="destMatrix"/>.
            </summary>
            <param name="matrix">Source matrix.</param>
            <param name="vector">Divider.</param>
            <param name="destMatrix">Resulting matrix.</param>
        </member>
        <member name="M:Sharpkit.Learn.MatrixExtensions.AddRowVector(MathNet.Numerics.LinearAlgebra.Generic.Matrix{System.Double},MathNet.Numerics.LinearAlgebra.Generic.Vector{System.Double},MathNet.Numerics.LinearAlgebra.Generic.Matrix{System.Double})">
            <summary>
            Adds <paramref name="vector"/> to all rows of <paramref name="matrix"/>.
            </summary>
            <param name="matrix">Source matrix.</param>
            <param name="vector">Vector to add.</param>
            <param name="destMatrix">Resulting matrix.</param>
        </member>
        <member name="M:Sharpkit.Learn.MatrixExtensions.AddRowVector(MathNet.Numerics.LinearAlgebra.Generic.Matrix{System.Double},MathNet.Numerics.LinearAlgebra.Generic.Vector{System.Double})">
            <summary>
            Adds <paramref name="vector"/> to all rows of <paramref name="matrix"/>.
            </summary>
            <param name="matrix">Source matrix.</param>
            <param name="vector">Vector to add.</param>
            <returns>Resulting matrix.</returns>
        </member>
        <member name="M:Sharpkit.Learn.MatrixExtensions.AddColumnVector(MathNet.Numerics.LinearAlgebra.Generic.Matrix{System.Double},MathNet.Numerics.LinearAlgebra.Generic.Vector{System.Double},MathNet.Numerics.LinearAlgebra.Generic.Matrix{System.Double})">
            <summary>
            Adds <paramref name="vector"/> to all columns of <paramref name="matrix"/>.
            </summary>
            <param name="matrix">Source matrix.</param>
            <param name="vector">Vector to add.</param>
            <param name="destMatrix">Resulting matrix.</param>
        </member>
        <member name="M:Sharpkit.Learn.MatrixExtensions.AddColumnVector(MathNet.Numerics.LinearAlgebra.Generic.Matrix{System.Double},MathNet.Numerics.LinearAlgebra.Generic.Vector{System.Double})">
            <summary>
            Adds <paramref name="vector"/> to all columns of <paramref name="matrix"/>.
            </summary>
            <param name="matrix">Source matrix.</param>
            <param name="vector">Vector to add.</param>
            <returns>Resulting matrix.</returns>
        </member>
        <member name="M:Sharpkit.Learn.MatrixExtensions.MulRowVector(MathNet.Numerics.LinearAlgebra.Generic.Matrix{System.Double},MathNet.Numerics.LinearAlgebra.Generic.Vector{System.Double})">
            <summary>
            Multiplies rows of matrix <paramref name="matrix"/> by <paramref name="vector"/> pointwise.
            </summary>
            <param name="matrix">Source matrix.</param>
            <param name="vector">Multiplier.</param>
            <returns>Resulting matrix.</returns>
        </member>
        <member name="M:Sharpkit.Learn.MatrixExtensions.MulColumnVector(MathNet.Numerics.LinearAlgebra.Generic.Matrix{System.Double},MathNet.Numerics.LinearAlgebra.Generic.Vector{System.Double})">
            <summary>
            Multiplies columns of matrix <paramref name="matrix"/> by <paramref name="vector"/> pointwise.
            </summary>
            <param name="matrix">Source matrix.</param>
            <param name="vector">Multiplier.</param>
            <returns>Resulting matrix.</returns>
        </member>
        <member name="M:Sharpkit.Learn.MatrixExtensions.MeanOfEveryRow(MathNet.Numerics.LinearAlgebra.Generic.Matrix{System.Double})">
            <summary>
            Computes mean of every row.
            </summary>
            <param name="matrix">Matrix to compute means of rows.</param>
            <returns>Vector where every element is a mean of corresponding row in the source matrix.</returns>
        </member>
        <member name="M:Sharpkit.Learn.MatrixExtensions.MeanOfEveryColumn(MathNet.Numerics.LinearAlgebra.Generic.Matrix{System.Double})">
            <summary>
            Computes mean of every column.
            </summary>
            <param name="matrix">Matrix to compute means of columns.</param>
            <returns>Vector where every element is a mean of corresponding column in the source matrix.</returns>
        </member>
        <member name="M:Sharpkit.Learn.MatrixExtensions.ToDenseMatrix(System.Double[0:,0:])">
            <summary>
            Converts array of double to <see cref="T:MathNet.Numerics.LinearAlgebra.Double.DenseMatrix"/>.
            </summary>
            <param name="data">Double array.</param>
            <returns>Instance of <see cref="T:MathNet.Numerics.LinearAlgebra.Double.DenseMatrix"/>.</returns>
        </member>
        <member name="M:Sharpkit.Learn.MatrixExtensions.ToDenseVector(System.Double[])">
            <summary>
            Converts array of double to <see cref="T:MathNet.Numerics.LinearAlgebra.Double.DenseVector"/>.
            </summary>
            <param name="data">Double array.</param>
            <returns>Instance of <see cref="T:MathNet.Numerics.LinearAlgebra.Double.DenseVector"/>.</returns>
        </member>
        <member name="M:Sharpkit.Learn.MatrixExtensions.ToColumnMatrix(MathNet.Numerics.LinearAlgebra.Generic.Vector{System.Double})">
            <summary>
            Converts <see cref="T:MathNet.Numerics.LinearAlgebra.Double.Vector"/> to column matrix.
            </summary>
            <param name="vector">Vector to convert.</param>
            <returns>Column matrix.</returns>
        </member>
        <member name="M:Sharpkit.Learn.MatrixExtensions.FrobeniusNorm(MathNet.Numerics.LinearAlgebra.Generic.Vector{System.Double})">
            <summary>
            Computes frobenious norm for the vector.
            </summary>
            <param name="vector">Vector to compute norm for.</param>
            <returns>Norm for the vector.</returns>
        </member>
        <member name="T:Sharpkit.Learn.Metrics.Metrics">
            <summary>
            Score functions, performance metrics
            and pairwise metrics and distance computations.
            </summary>
        </member>
        <member name="M:Sharpkit.Learn.Metrics.Metrics.R2Score(MathNet.Numerics.LinearAlgebra.Generic.Matrix{System.Double},MathNet.Numerics.LinearAlgebra.Generic.Matrix{System.Double})">
            <summary>
            R (coefficient of determination) regression score function.
            Best possible score is 1.0, lower values are worse.
            </summary>
            <param name="yTrue">[n_samples, n_outputs]
            Ground truth (correct) target values.</param>
            <param name="yPred">[n_samples, n_outputs]
            Estimated target values.</param>
            <returns>The R score.</returns>
            <remarks>
            <para>
            This is not a symmetric function.
            Unlike most other scores, R score may be negative (it need not actually
            be the square of a quantity R).
            </para>
            <para>
            References
            ----------
            [1] `Wikipedia entry on the Coefficient of determination
               http://en.wikipedia.org/wiki/Coefficient_of_determination
            </para>
            </remarks>
            <example>
             var yTrue = new[]{ 3, -0.5, 2, 7 };
             var yPred = new[]{ 2.5, 0.0, 2, 8 };
             Metrics.R2Score(yTrue, yPred)
                0.948...
            </example>
        </member>
        <member name="M:Sharpkit.Learn.Metrics.Metrics.F1ScoreAvg(System.Int32[],System.Int32[],System.Int32[],System.Int32,Sharpkit.Learn.Metrics.AverageKind)">
             <summary>
             <para>
             Compute the F1 score, also known as balanced F-score or F-measure
             </para>
             <para>
             The F1 score can be interpreted as a weighted average of the precision and
             recall, where an F1 score reaches its best value at 1 and worst score at 0.
             The relative contribution of precision and recall to the F1 score are
             equal. The formula for the F1 score is::
             </para>
             <para>
             F1 = 2 * (precision * recall) / (precision + recall)
             </para>
             <para>
             In the multi-class and multi-label case, this is the weighted average of
             the F1 score of each class.
             </para>
             <para>
                References
                ----------
                Wikipedia entry for the F1-score
                http://en.wikipedia.org/wiki/F1_score
             </para>
             </summary>
             <param name="yTrue">List of labels. Ground truth (correct) target values.</param>
             <param name="yPred">Estimated targets as returned by a classifier.</param>
             <param name="labels">Integer array of labels.</param>
             <param name="posLabel">If classification target is binary,
             only this class's scores will be returned.</param>
             <param name="average">Unless ``posLabel`` is given in binary classification, this
             determines the type of averaging performed on the data.</param>
             <returns>Weighted average of the F1 scores of each class for the multiclass task.
             </returns>
             <example>
              In the binary case:
            
               var yPred = new[] { 0, 1, 0, 0 };
               var yTrue = new[] { 0, 1, 0, 1 };
               Metrics.F1ScoreAvg(yTrue, yPred);
                   0.666...
              
              In multiclass case:
             
              var yTrue = new[] { 0, 1, 2, 0, 1, 2 };
              var yPred = new[] { , 2, 1, 0, 0, 1 };
              Metrics.F1ScoreAvg(yTrue, yPred, average : AverageKind.Micro)
              0.26...
              Metrics.F1ScoreAvg(yTrue, yPred, average : AverageKind.Macro)
              0.33...
              Metrics.F1ScoreAvg(yTrue, yPred, average : AverageKind.Weighted)
              0.26...
             </example>
        </member>
        <member name="M:Sharpkit.Learn.Metrics.Metrics.F1Score(System.Int32[],System.Int32[],System.Int32[])">
             <summary>
             <para>
             Compute the F1 score, also known as balanced F-score or F-measure for each class.
             </para>
             <para>
             The F1 score can be interpreted as a weighted average of the precision and
             recall, where an F1 score reaches its best value at 1 and worst score at 0.
             The relative contribution of precision and recall to the F1 score are
             equal. The formula for the F1 score is::
             </para>
             <para>
             F1 = 2 * (precision * recall) / (precision + recall)
             </para>
             <para>
                 References
                ----------
             .. [1] `Wikipedia entry for the F1-score
                http://en.wikipedia.org/wiki/F1_score
             </para>
             </summary>
             <param name="yTrue">List of labels. Ground truth (correct) target values.</param>
             <param name="yPred">Estimated targets as returned by a classifier.</param>
             <param name="labels">Integer array of labels.</param>
             <returns>[nUniqueLabels] F1 score for each class.
             </returns>
             <example>
            
              In the multiclass case:
            
              var yTrue = new[] { 0, 1, 2, 0, 1, 2 };
              var yPred = new[] { 0, 2, 1, 0, 0, 1 };
              F1Score(yTrue, yPred)
             array([ 0.8,  0. ,  0. ])
             </example>
        </member>
        <member name="M:Sharpkit.Learn.Metrics.Metrics.FBetaScoreAvg(System.Int32[],System.Int32[],System.Double,System.Int32[],System.Int32,Sharpkit.Learn.Metrics.AverageKind)">
             <summary>
             Compute the F-beta score
            
             The F-beta score is the weighted harmonic mean of precision and recall,
             reaching its optimal value at 1 and its worst value at 0.
            
             The `beta` parameter determines the weight of precision in the combined
             score. ``beta &lt; 1`` lends more weight to precision, while ``beta > 1``
             favors precision (``beta == 0`` considers only precision, ``beta == inf``
             only recall).
             </summary>
             <param name="yTrue">List of labels. Ground truth (correct) target values.</param>
             <param name="yPred">Estimated targets as returned by a classifier.</param>
             <param name="beta">Weight of precision in harmonic mean.</param>
             <param name="labels">Integer array of labels.</param>
             <param name="posLabel">If the classification target is binary,
             only this class's scores will be returned.</param>
             <param name="average">Unless ``posLabel`` is given in binary classification, this
             determines the type of averaging performed on the data.</param>
             <returns> F-beta score of the positive class in binary classification or weighted
             average of the F-beta score of each class for the multiclass task.</returns>
             <remarks>
                 References
              ----------
              .. [1] R. Baeza-Yates and B. Ribeiro-Neto (2011).
               Modern Information Retrieval. Addison Wesley, pp. 327-328.
            
               .. [2] `Wikipedia entry for the F1-score
               http://en.wikipedia.org/wiki/F1_score
             </remarks>
             <example>
             In the binary case:
             var yPred = new[] { 0, 1, 0, 0 };
             var yTrue = new[] { 0, 1, 0, 1 };
             Metrics.FBetaScoreAvg(yTrue, yPred, beta: 0.5)
                 0.83...
             
             Metrics.FBetaScoreAvg(yTrue, yPred, beta: 1)
                 0.66...
             Metrics.FBetaScoreAvg(yTrue, yPred, beta: 2)
                 0.55...
            
             In the multiclass case:
              
             yTrue = new[] { 0, 1, 2, 0, 1, 2 };
             yPred = new[] { 0, 2, 1, 0, 0, 1 };
             Metrics.FBetaScoreAvg(yTrue, yPred, average: AverageKind.Macro, beta: 0.5);
                0.23...
             Metrics.FBetaScoreAvg(yTrue, yPred, average: AverageKind.Micro, beta: 0.5);
                0.33...
             Metrics.FBetaScoreAvg(yTrue, y_pred, average: AverageKind.Weighted, beta: 0.5);
                0.23...
             </example>
        </member>
        <member name="M:Sharpkit.Learn.Metrics.Metrics.FBetaScore(System.Int32[],System.Int32[],System.Double,System.Int32[])">
             <summary>
             Compute the F-beta score for each class.
            
             The F-beta score is the weighted harmonic mean of precision and recall,
             reaching its optimal value at 1 and its worst value at 0.
            
             The `beta` parameter determines the weight of precision in the combined
             score. ``beta &lt; 1`` lends more weight to precision, while ``beta > 1``
             favors precision (``beta == 0`` considers only precision, ``beta == inf``
             only recall).
             </summary>
             <param name="yTrue">List of labels. Ground truth (correct) target values.</param>
             <param name="yPred">Estimated targets as returned by a classifier.</param>
             <param name="beta">Weight of precision in harmonic mean.</param>
             <param name="labels">Integer array of labels.</param>
             <returns>F-beta score for each class.</returns>
             <remarks>
                 References
              ----------
              .. [1] R. Baeza-Yates and B. Ribeiro-Neto (2011).
               Modern Information Retrieval. Addison Wesley, pp. 327-328.
            
               .. [2] `Wikipedia entry for the F1-score
               http://en.wikipedia.org/wiki/F1_score
             </remarks>
             <example>
             In the multiclass case:
              
             yTrue = new[]{ 0, 1, 2, 0, 1, 2 };
             yPred = new[] { 0, 2, 1, 0, 0, 1 };
             Metrics.FBetaScore(yTrue, yPred, beta=0.5)
                    {0.71...,  0.        ,  0.        }
             </example>
        </member>
        <member name="M:Sharpkit.Learn.Metrics.Metrics.PrecisionScoreAvg(System.Int32[],System.Int32[],System.Int32[],System.Int32,Sharpkit.Learn.Metrics.AverageKind)">
             <summary>
             Compute the precision
            
             The precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of
             true positives and ``fp`` the number of false positives. The precision is
             intuitively the ability of the classifier not to label as positive a sample
             that is negative.
            
             The best value is 1 and the worst value is 0.
             </summary>
             <param name="yTrue">List of labels. Ground truth (correct) target values.</param>
             <param name="yPred">Estimated targets as returned by a classifier.</param>
             <param name="labels">Integer array of labels.</param>
             <param name="posLabel">classification target is binary,
             only this class's scores will be returned.</param>
             <param name="average">Unless ``posLabel`` is given in binary classification, this
             determines the type of averaging performed on the data.</param>
             <returns>
             Precision of the positive class in binary classification or weighted
             average of the precision of each class for the multiclass task.
             </returns>
             <example>
             In the binary case:
             var yPred = new[] { 0, 1, 0, 0 };
             var yTrue = new[] { 0, 1, 0, 1 };
             Metrics.PrecisionScore(yTrue, yPred)
               1.0
            
              In the multiclass case:
               var yTrue = new[] { 0, 1, 2, 0, 1, 2 };
               var yPred = new[] { 0, 2, 1, 0, 0, 1 };
               Metrics.PrecisionScoreAvg(yTrue, yPred, average: AverageKind.Macro);
                 0.22...
               Metrics.PrecisionScoreAvg(yTrue, yPred, average: AverageKind.Micro);
                 0.33...
               Metrics.PrecisionScoreAvg(yTrue, yPred, average: AverageKind.Weighted);
                 0.22...
               Metrics.PrecisionScore(yTrue, yPred)
                { 0.66...,  0.        ,  0.        }
             </example>
        </member>
        <member name="M:Sharpkit.Learn.Metrics.Metrics.PrecisionScore(System.Int32[],System.Int32[],System.Int32[])">
             <summary>
             Compute the precision
            
             The precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of
             true positives and ``fp`` the number of false positives. The precision is
             intuitively the ability of the classifier not to label as positive a sample
             that is negative.
            
             The best value is 1 and the worst value is 0.
             </summary>
             <param name="yTrue">List of labels. Ground truth (correct) target values.</param>
             <param name="yPred">Estimated targets as returned by a classifier.</param>
             <param name="labels">Integer array of labels.</param>
             <returns>
             Precision of the positive class in binary classification or weighted
             average of the precision of each class for the multiclass task.
             </returns>
             <example>
              In the multiclass case:
               var yTrue = new[] { 0, 1, 2, 0, 1, 2 };
               var yPred = new[] { 0, 2, 1, 0, 0, 1 };
               Metrics.PrecisionScore(yTrue, yPred)
                { 0.66...,  0.        ,  0.        }
             </example>
        </member>
        <member name="M:Sharpkit.Learn.Metrics.Metrics.RecallScoreAvg(System.Int32[],System.Int32[],System.Int32[],System.Int32,Sharpkit.Learn.Metrics.AverageKind)">
             <summary>
             Compute the recall
            
             The recall is the ratio ``tp / (tp + fn)`` where ``tp`` is the number of
             true positives and ``fn`` the number of false negatives. The recall is
             intuitively the ability of the classifier to find all the positive samples.
             
             The best value is 1 and the worst value is 0.
             </summary>
             <param name="yTrue">List of labels. Ground truth (correct) target values.</param>
             <param name="yPred">Estimated targets as returned by a classifier.</param>
             <param name="labels">Integer array of labels.</param>
             <param name="posLabel">classification target is binary,
             only this class's scores will be returned.</param>
             <param name="average">Unless ``posLabel`` is given in binary classification, this
             determines the type of averaging performed on the data.</param>
             <returns>Recall of the positive class in binary classification or weighted
             average of the recall of each class for the multiclass task.</returns>
             <examples>
                In the binary case:
            
                var yPred = new[] { 0, 1, 0, 0 };
                var yTrue = new[] { 0, 1, 0, 1 };
                Metrics.RecallScoreAvg(yTrue, yPred);
                   0.5
            
             In the multiclass case:
            
                var yTrue = new[] { 0, 1, 2, 0, 1, 2 };
                var yPred = new[] { 0, 2, 1, 0, 0, 1 };
                Metrics.RecallScoreAvg(yTrue, yPred, average: AverageKind.Macro)
                   0.33...
                Metrics.RecallScoreAvg(yTrue, yPred, average: AverageKind.Micro)
                   0.33...
                Metrics.RecallScoreAvg(yTrue, yPred, average: AverageKind.Weighted)
                   0.33...
                Metrics.RecallScore(yTrue, yPred)
                  { 1.,  0.,  0. }
             </examples>
        </member>
        <member name="M:Sharpkit.Learn.Metrics.Metrics.RecallScore(System.Int32[],System.Int32[],System.Int32[])">
             <summary>
             Compute the recall
            
             The recall is the ratio ``tp / (tp + fn)`` where ``tp`` is the number of
             true positives and ``fn`` the number of false negatives. The recall is
             intuitively the ability of the classifier to find all the positive samples.
             
             The best value is 1 and the worst value is 0.
             </summary>
             <param name="yTrue">List of labels. Ground truth (correct) target values.</param>
             <param name="yPred">Estimated targets as returned by a classifier.</param>
             <param name="labels">Integer array of labels.</param>
             <returns>Recall for each class.</returns>
             <examples>
             In the multiclass case:
            
                var yTrue = new[] { 0, 1, 2, 0, 1, 2 };
                var yPred = new[] { 0, 2, 1, 0, 0, 1 };
                Metrics.RecallScore(yTrue, yPred)
                  { 1.,  0.,  0. }
             </examples>
        </member>
        <member name="M:Sharpkit.Learn.Metrics.Metrics.PrecisionRecallFScoreSupportAvg(System.Int32[],System.Int32[],System.Double,System.Int32[],System.Nullable{System.Int32},Sharpkit.Learn.Metrics.AverageKind)">
             <summary>
             Compute average precision, recall, F-measure and support.
            
             The precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of
             true positives and ``fp`` the number of false positives. The precision is
             intuitively the ability of the classifier not to label as positive a sample
             that is negative.
             
             The recall is the ratio ``tp / (tp + fn)`` where ``tp`` is the number of
             true positives and ``fn`` the number of false negatives. The recall is
             intuitively the ability of the classifier to find all the positive samples.
            
             The F-beta score can be interpreted as a weighted harmonic mean of
             the precision and recall, where an F-beta score reaches its best
             value at 1 and worst score at 0.
            
             The F-beta score weights recall more than precision by a factor of
             ``beta``. ``beta == 1.0`` means recall and precsion are equally important.
            
             The support is the number of occurrences of each class in ``y_true``.
             </summary>
             <param name="yTrue">List of labels. Ground truth (correct) target values.</param>
             <param name="yPred">Estimated targets as returned by a classifier.</param>
             <param name="beta">Weight of precision in harmonic mean.</param>
             <param name="labels">Integer array of labels.</param>
             <param name="posLabel">If the classification target is binary,
             only this class's scores will be returned.</param>
             <param name="average">Unless ``posLabel`` is given in binary classification, this
             determines the type of averaging performed on the data.</param>
             <returns>Instance of <see cref="T:Sharpkit.Learn.Metrics.PrecisionRecallResultAvg"/>.</returns>
             <remarks>
             .. [1] `Wikipedia entry for the Precision and recall
                http://en.wikipedia.org/wiki/Precision_and_recall_
            
             .. [2] `Wikipedia entry for the F1-score
                http://en.wikipedia.org/wiki/F1_score
            
             .. [3] `Discriminative Methods for Multi-labeled Classification Advances
               in Knowledge Discovery and Data Mining (2004), pp. 22-30 by Shantanu
               Godbole, Sunita Sarawagi
               http://www.godbole.net/shantanu/pubs/multilabelsvm-pakdd04.pdf
             </remarks>
             <example>
            
             In the multiclass case:
            
             yTrue = new[] { 0, 1, 2, 0, 1, 2 };
             yPred = new[] { 0, 2, 1, 0, 0, 1 };
             var r = Metrics.PrecisionRecallFscoreSupportAvg(yTrue, yPred, average: AverageKind.Macro);
                 (Precision = 0.22, Recall = 0.33, FScore = 0.26)
             r = Metrics.PrecisionRecallFscoreSupportAvg(yTrue, yPred, average: AverageKind.Micro);
                 (Precision = 0.33, Recall = 0.33, FScore = 0.33)
             r = Metrics.PrecisionRecallFscoreSupport(yTrue, yPred, average: AverageKind.Weighted);
                 (Precision = 0.22, Recall = 0.33, FScore = 0.26)
             </example>
        </member>
        <member name="M:Sharpkit.Learn.Metrics.Metrics.PrecisionRecallFScoreSupport(System.Int32[],System.Int32[],System.Double,System.Int32[])">
             <summary>
             Compute precision, recall, F-measure and support for each class.
            
             The precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of
             true positives and ``fp`` the number of false positives. The precision is
             intuitively the ability of the classifier not to label as positive a sample
             that is negative.
             
             The recall is the ratio ``tp / (tp + fn)`` where ``tp`` is the number of
             true positives and ``fn`` the number of false negatives. The recall is
             intuitively the ability of the classifier to find all the positive samples.
            
             The F-beta score can be interpreted as a weighted harmonic mean of
             the precision and recall, where an F-beta score reaches its best
             value at 1 and worst score at 0.
            
             The F-beta score weights recall more than precision by a factor of
             ``beta``. ``beta == 1.0`` means recall and precsion are equally important.
            
             The support is the number of occurrences of each class in ``y_true``.
             </summary>
             <param name="yTrue">List of labels. Ground truth (correct) target values.</param>
             <param name="yPred">Estimated targets as returned by a classifier.</param>
             <param name="beta">Weight of precision in harmonic mean.</param>
             <param name="labels">Integer array of labels.</param>
             <returns>Instance of <see cref="T:Sharpkit.Learn.Metrics.PrecisionRecallResult"/>.</returns>
             <remarks>
             .. [1] `Wikipedia entry for the Precision and recall
                http://en.wikipedia.org/wiki/Precision_and_recall_
            
             .. [2] `Wikipedia entry for the F1-score
                http://en.wikipedia.org/wiki/F1_score
            
             .. [3] `Discriminative Methods for Multi-labeled Classification Advances
               in Knowledge Discovery and Data Mining (2004), pp. 22-30 by Shantanu
               Godbole, Sunita Sarawagi
               http://www.godbole.net/shantanu/pubs/multilabelsvm-pakdd04.pdf
             </remarks>
             <example>
                 In the binary case:
                 yPred = new[]{ 0, 1, 0, 0 };
                 yTrue = new[]{ 0, 1, 0, 1 };
                 r = Metrics.PrecisionRecallFscoreSupport(yTrue, yPred, beta: 0.5);
                 r.Precision
                    { 0.66...,  1.        }
                 r.Recall 
                    { 1. ,  0.5 }
                 r.FScore
                    { 0.71...,  0.83...}
                 r.Support s
                    {[2, 2]...}
             </example>
        </member>
        <member name="T:Sharpkit.Learn.Preprocessing.LabelEncoder`1">
            <summary>
            Encode labels with value between 0 and n_classes-1.
            </summary>
            <typeparam name="TLabel">Type of class label.</typeparam>
            <example>
            `LabelEncoder` can be used to normalize labels.
            <para>
            var le = new.LabelEncoder();
            le.Fit(new [] {1, 2, 2, 6});
            le.Classes
                {1, 2, 6}
            le.Transform(new[] {1, 1, 2, 6})
                {0, 0, 1, 2}
            le.InverseTransform(new[] {0, 0, 1, 2})
                {1, 1, 2, 6}
            </para>
            <para>
            It can also be used to transform non-numerical labels to numerical labels.
            </para>
            <para>
            var le = new LabelEncoder();
            le.Fit(new[] {"paris", "paris", "tokyo", "amsterdam"});
            le.Classes
                {"amsterdam', "paris", "tokyo"}
            le.Transform(new[] {"tokyo", "tokyo", "paris"})
                {2, 2, 1}
            le.InverseTransform(new[] {2, 2, 1});
                {"tokyo", "tokyo", "paris"}
            </para>
            </example>
        </member>
        <member name="M:Sharpkit.Learn.Preprocessing.LabelEncoder`1.Fit(`0[])">
            <summary>
            Fit label encoder.
            </summary>
            <param name="y">Target values. [n_samples]</param>
            <returns>Returns an instance of self.</returns>
        </member>
        <member name="M:Sharpkit.Learn.Preprocessing.LabelEncoder`1.FitTransform(`0[])">
            <summary>
            Fit label encoder and return encoded labels.
            </summary>
            <param name="y">Target values. [n_samples]</param>
            <returns>Array [n_samples].</returns>
        </member>
        <member name="M:Sharpkit.Learn.Preprocessing.LabelEncoder`1.Transform(`0[])">
            <summary>
            Transform labels to normalized encoding.
            </summary>
            <param name="y">Target values. [n_samples]</param>
            <returns>Array [n_samples].</returns>
        </member>
        <member name="M:Sharpkit.Learn.Preprocessing.LabelEncoder`1.InverseTransform(System.Int32[])">
            <summary>
            Transform labels back to original encoding.
            </summary>
            <param name="y">Target values. [n_samples]</param>
            <returns>Array. [n_samples]</returns>
        </member>
        <member name="M:Sharpkit.Learn.Preprocessing.LabelEncoder`1.CheckFitted">
            <summary>
            Ensures that <see cref="M:Sharpkit.Learn.Preprocessing.LabelEncoder`1.Fit(`0[])"/> was called.
            </summary>
        </member>
        <member name="P:Sharpkit.Learn.Preprocessing.LabelEncoder`1.Classes">
            <summary>
            Gets or sets an array with labels for each class.
            </summary>
        </member>
        <member name="T:Sharpkit.Learn.Svm.Kernel">
            <summary>
            Kernel to be used with Support Vector Machines classifiers.
            </summary>
        </member>
        <member name="M:Sharpkit.Learn.Svm.Kernel.FromFunction(System.Func{MathNet.Numerics.LinearAlgebra.Generic.Matrix{System.Double},MathNet.Numerics.LinearAlgebra.Generic.Matrix{System.Double},MathNet.Numerics.LinearAlgebra.Generic.Matrix{System.Double}})">
            <summary>
            Creates precomputed kernel.
            </summary>
            <param name="f">Function used to precompute the kernel matrix.</param>
            <returns>Instance of <see cref="T:Sharpkit.Learn.Svm.Kernel"/>.</returns>
        </member>
        <member name="M:Sharpkit.Learn.Svm.Kernel.Equals(Sharpkit.Learn.Svm.Kernel)">
            <summary>
            Indicates whether the current object is equal to another object of the same type.
            </summary>
            <returns>
            true if the current object is equal to the <paramref name="other"/> parameter; otherwise, false.
            </returns>
            <param name="other">An object to compare with this object.</param>
        </member>
        <member name="M:Sharpkit.Learn.Svm.Kernel.FromLibSvmKernel(Sharpkit.Learn.Svm.LibSvmKernel)">
            <summary>
            Creates <see cref="T:Sharpkit.Learn.Svm.Kernel"/> from <see cref="T:Sharpkit.Learn.Svm.LibSvmKernel"/>.
            </summary>
            <param name="kernel">LibSvm kernel.</param>
            <returns>Instance of <see cref="T:Sharpkit.Learn.Svm.Kernel"/>.</returns>
        </member>
        <member name="P:Sharpkit.Learn.Svm.Kernel.Linear">
            <summary>
            Gets linear kernel.
            </summary>
        </member>
        <member name="P:Sharpkit.Learn.Svm.Kernel.Poly">
            <summary>
            Gets polynomial kernel.
            </summary>
        </member>
        <member name="P:Sharpkit.Learn.Svm.Kernel.Rbf">
            <summary>
            Gets rbf kernel.
            </summary>
        </member>
        <member name="P:Sharpkit.Learn.Svm.Kernel.Sigmoid">
            <summary>
            Gets sigmoid kernel.
            </summary>
        </member>
        <member name="P:Sharpkit.Learn.Svm.Kernel.Precomputed">
            <summary>
            Gets precomputed kernel.
            </summary>
        </member>
        <member name="P:Sharpkit.Learn.Svm.Kernel.LibSvmKernel">
            <summary>
            Gets or sets LibSvm kernel type.
            </summary>
        </member>
        <member name="P:Sharpkit.Learn.Svm.Kernel.KernelFunction">
            <summary>
            Gets or sets function which calculates kernel matrix.
            </summary>
        </member>
        <member name="T:Sharpkit.Learn.Svm.LibSvmBase">
            <summary>
            <para>
            Base class for estimators that use libsvm as backing library
            </para>
            <para>
            This implements support vector machine classification and regression.
            </para>
            </summary>
        </member>
        <member name="F:Sharpkit.Learn.Svm.LibSvmBase.fitShape">
            <summary>
            Shape of matrix which was passed to <see cref="M:Sharpkit.Learn.Svm.LibSvmBase.Fit(MathNet.Numerics.LinearAlgebra.Generic.Matrix{System.Double},MathNet.Numerics.LinearAlgebra.Generic.Vector{System.Double})"/>.
            </summary>
        </member>
        <member name="F:Sharpkit.Learn.Svm.LibSvmBase.xFit">
            <summary>
            Matrix which was passed to <see cref="M:Sharpkit.Learn.Svm.LibSvmBase.Fit(MathNet.Numerics.LinearAlgebra.Generic.Matrix{System.Double},MathNet.Numerics.LinearAlgebra.Generic.Vector{System.Double})"/>.
            </summary>
        </member>
        <member name="M:Sharpkit.Learn.Svm.LibSvmBase.Fit(MathNet.Numerics.LinearAlgebra.Generic.Matrix{System.Double},MathNet.Numerics.LinearAlgebra.Generic.Vector{System.Double})">
            <summary>
            Fit the Svm model according to the given training data.
            </summary>
            <param name="x">
            [nSamples, nFeatures]
            Training vectors, where nSamples is the number of samples
            and nFeatures is the number of features.</param>
            <param name="y">[nSamples]
            Target values (class labels in classification, real numbers in
            regression)
            </param>
        </member>
        <member name="M:Sharpkit.Learn.Svm.LibSvmBase.predict(MathNet.Numerics.LinearAlgebra.Generic.Matrix{System.Double})">
            <summary>
            <para>
            Perform regression on samples in X.
            </para>
            <para>
            For an one-class model, +1 or -1 is returned.
            </para>
            </summary>
            <param name="x">[nSamples, nFeatures]</param>
            <returns>[nSamples]</returns>
        </member>
        <member name="M:Sharpkit.Learn.Svm.LibSvmBase.ValidateTargets(MathNet.Numerics.LinearAlgebra.Generic.Vector{System.Double})">
            <summary>
            Validation of y and class_weight.
            </summary>
            <param name="y">Target values.</param>
        </member>
        <member name="M:Sharpkit.Learn.Svm.LibSvmBase.ComputeKernel(MathNet.Numerics.LinearAlgebra.Generic.Matrix{System.Double})">
            <summary>
            Return the data transformed by a callable kernel.
            </summary>
            <param name="x"></param>
        </member>
        <member name="M:Sharpkit.Learn.Svm.LibSvmBase.OneVsOneCoef(MathNet.Numerics.LinearAlgebra.Generic.Matrix{System.Double},System.Int32[],MathNet.Numerics.LinearAlgebra.Generic.Matrix{System.Double})">
            <summary>
            Generate primal coefficients from dual coefficients
            for the one-vs-one multi class LibSVM in the case
            of a linear kernel.
            </summary>
        </member>
        <member name="P:Sharpkit.Learn.Svm.LibSvmBase.Kernel">
            <summary>
            Gets or sets kernel.
            </summary>
        </member>
        <member name="P:Sharpkit.Learn.Svm.LibSvmBase.Degree">
            <summary>
            Gets or sets the degree of kernel function.
            It is significant only in <see cref="P:Sharpkit.Learn.Svm.LibSvmBase.Kernel"/> == <see cref="P:Sharpkit.Learn.Svm.Kernel.Poly"/>.
            </summary>
        </member>
        <member name="P:Sharpkit.Learn.Svm.LibSvmBase.Gamma">
            <summary>
            Gets or sets kernel coefficient for <see cref="P:Sharpkit.Learn.Svm.Kernel.Rbf"/>,
            <see cref="P:Sharpkit.Learn.Svm.Kernel.Poly"/> and <see cref="P:Sharpkit.Learn.Svm.Kernel.Sigmoid"/>.
            If gamma is 0.0 then 1/nFeatures will be used instead.
            </summary>
        </member>
        <member name="P:Sharpkit.Learn.Svm.LibSvmBase.Coef0">
            <summary>
            Gets or sets independent term in kernel function.
            It is only significant in <see cref="P:Sharpkit.Learn.Svm.Kernel.Poly"/> and <see cref="P:Sharpkit.Learn.Svm.Kernel.Sigmoid"/>.
            </summary>
        </member>
        <member name="P:Sharpkit.Learn.Svm.LibSvmBase.Tol">
            <summary>
            Gets or sets tolerance for stopping criterion.
            </summary>
        </member>
        <member name="P:Sharpkit.Learn.Svm.LibSvmBase.C">
            <summary>
            Gets or sets penalty parameter C of the error term.
            </summary>
        </member>
        <member name="P:Sharpkit.Learn.Svm.LibSvmBase.Shrinking">
            <summary>
            Gets or sets a value indicating whether to use the shrinking heuristic.
            </summary>
        </member>
        <member name="P:Sharpkit.Learn.Svm.LibSvmBase.Probability">
            <summary>
            Gets or sets a value indicating whether to enable probability estimates.
            </summary>
        </member>
        <member name="P:Sharpkit.Learn.Svm.LibSvmBase.CacheSize">
            <summary>
            Gets or sets size of the kernel cache (in MB).
            </summary>
        </member>
        <member name="P:Sharpkit.Learn.Svm.LibSvmBase.Verbose">
            <summary>
            Gets or sets a value indicating whether to enable verbose output.
            Note that this setting takes advantage of a
            per-process runtime setting in libsvm that, if enabled, may not work
            properly in a multithreaded context.
            </summary>
        </member>
        <member name="P:Sharpkit.Learn.Svm.LibSvmBase.NSupport">
            <summary>
            Gets or sets number of support vectors in each class.
            </summary>
        </member>
        <member name="P:Sharpkit.Learn.Svm.LibSvmBase.ProbA">
            <summary>
            Gets or sets probability estimates, empty array if <see cref="P:Sharpkit.Learn.Svm.LibSvmBase.Probability"/> == false.
            </summary>
        </member>
        <member name="P:Sharpkit.Learn.Svm.LibSvmBase.ProbB">
            <summary>
            Gets or sets probability estimates, empty array if <see cref="P:Sharpkit.Learn.Svm.LibSvmBase.Probability"/> == false.
            </summary>
        </member>
        <member name="P:Sharpkit.Learn.Svm.LibSvmBase.Label">
            <summary>
            Gets or sets labels for different classes (only relevant in classification).
            </summary>
        </member>
        <member name="P:Sharpkit.Learn.Svm.LibSvmBase.DualCoef">
            <summary>
            Gets or sets coefficients of support vectors in decision function.
            </summary>
        </member>
        <member name="P:Sharpkit.Learn.Svm.LibSvmBase.Support">
            <summary>
            Gets [nSupport] index of support vectors.
            </summary>
        </member>
        <member name="P:Sharpkit.Learn.Svm.LibSvmBase.SupportVectors">
            <summary>
            Gets support vectors ([nSupport, nFeatures]) (equivalent to X.RowsAt(Support)). Will return an
            empty array in the case of precomputed kernel.
            </summary>
        </member>
        <member name="P:Sharpkit.Learn.Svm.LibSvmBase.Intercept">
            <summary>
            Gets intercept in decision function
            </summary>
        </member>
        <member name="P:Sharpkit.Learn.Svm.LibSvmBase.Coef">
            <summary>
            <para>
            Gets weights asigned to the features (coefficients in the primal
            problem). This is only available in the case of linear kernel.
            [nClass-1, nFeatures]
            </para>
            <para>
            This is readonly property derived from <see cref="P:Sharpkit.Learn.Svm.LibSvmBase.DualCoef"/> and <see cref="P:Sharpkit.Learn.Svm.LibSvmBase.SupportVectors"/>.
            </para>
             </summary>
        </member>
        <member name="P:Sharpkit.Learn.Svm.LibSvmBase.Nu">
            <summary>
            Gets or sets nu.
            </summary>
        </member>
        <member name="P:Sharpkit.Learn.Svm.LibSvmBase.Epsilon">
            <summary>
            Gets or sets epsilon.
            </summary>
        </member>
        <member name="P:Sharpkit.Learn.Svm.LibSvmBase.IsSparse">
            <summary>
            Gets a value indicating whether this class was trained using sparse matrix.
            </summary>
        </member>
        <member name="P:Sharpkit.Learn.Svm.LibSvmBase.Model">
            <summary>
            Gets LibSvm model.
            </summary>
        </member>
        <member name="P:Sharpkit.Learn.Svm.LibSvmBase.Param">
            <summary>
            Gets or sets LibSvm parameter class.
            </summary>
        </member>
        <member name="T:Sharpkit.Learn.Svm.LibSvmImpl">
            <summary>
            TODO: Update summary.
            </summary>
        </member>
        <member name="T:Sharpkit.Learn.Svm.LibSvmKernel">
            <summary>
            Libsvm kernel types.
            </summary>
        </member>
        <member name="T:Sharpkit.Learn.Svm.Svc`1">
             <summary>
             <para>
             C-Support Vector Classification.
             </para>
             <para>
             The implementations is a based on libsvm. The fit time complexity
             is more than quadratic with the number of samples which makes it hard
             to scale to dataset with more than a couple of 10000 samples.
             </para>
             <para>
             The multiclass support is handled according to a one-vs-one scheme.
             </para>
             <para>
             For details on the precise mathematical formulation of the provided
             kernel functions and how `gamma`, `coef0` and `degree` affect each,
             see the corresponding section in the narrative documentation:
             :ref:`svm_kernels`.
            </para>
             <para>
             .. The narrative documentation is available at http://scikit-learn.org/
             </para>
             </summary>
             <example>
                 var x = DenseMatrix.OfArray(new[,] { {-1, -1}, {-2, -1}, {1, 1}, {2, 1}});
                 var y = new[] { 1, 1, 2, 2 };
                 var clf = new Svc&lt;int>();
                 clf.Fit(X, y)
                 Console.WriteLine(clf.Predict(DenseMatrix.OfArray(new[,] {{-0.8, -1}}));
                      { 1 }
            
             </example>
        </member>
        <member name="T:Sharpkit.Learn.Svm.SvcBase`1">
            <summary>
            ABC for LibSVM-based classifiers.
            </summary>
            <typeparam name="TLabel">Type of class label.</typeparam>
        </member>
        <member name="F:Sharpkit.Learn.Svm.SvcBase`1.enc">
            <summary>
            Label encoder.
            </summary>
        </member>
        <member name="M:Sharpkit.Learn.Svm.SvcBase`1.Fit(MathNet.Numerics.LinearAlgebra.Generic.Matrix{System.Double},`0[])">
            <summary>
            Fit the model according to the given training data.
            </summary>
            <param name="x">[nSamples, nFeatures]. Training vectors,
            where nSamples is the number of samples and nFeatures
            is the number of features.</param>
            <param name="y">[nSamples] Target class labels.</param>
            <returns>Reference to itself.</returns>
        </member>
        <member name="M:Sharpkit.Learn.Svm.SvcBase`1.Predict(MathNet.Numerics.LinearAlgebra.Generic.Matrix{System.Double})">
            <summary>
            Perform classification on samples in X.
            For an one-class model, +1 or -1 is returned.
            </summary>
            <param name="x">[n_samples, n_features]</param>
            <returns>Class labels for samples in X.</returns>
        </member>
        <member name="M:Sharpkit.Learn.Svm.SvcBase`1.PredictProba(MathNet.Numerics.LinearAlgebra.Generic.Matrix{System.Double})">
            <summary>
            <para>
            Compute probabilities of possible outcomes for samples in X.
            </para>
            <para>
            The model need to have probability information computed at training
            time: fit with attribute <see cref="P:Sharpkit.Learn.Svm.LibSvmBase.Probability"/> set to True.
            </para>
            </summary>
            <param name="x">[nSamples, nFeatures]</param>
            <returns>[nSamples, nClasses]
               Returns the probability of the sample for each class in
              the model. The columns correspond to the classes in sorted
               order, as they appear in the attribute `classes_`.</returns>
            <remarks>
                    The probability model is created using cross validation, so
            the results can be slightly different than those obtained by
            predict. Also, it will produce meaningless results on very small
            datasets.
            </remarks>
        </member>
        <member name="M:Sharpkit.Learn.Svm.SvcBase`1.DecisionFunction(MathNet.Numerics.LinearAlgebra.Generic.Matrix{System.Double})">
            <summary>
            Distance of the samples X to the separating hyperplane.
            </summary>
            <param name="x">[nSamples, nFeatures]</param>
            <returns>
            [nSamples, nClass * (nClass-1) / 2]
                   Returns the decision function of the sample for each class
                   in the model.
            </returns>
        </member>
        <member name="M:Sharpkit.Learn.Svm.SvcBase`1.PredictLogProba(MathNet.Numerics.LinearAlgebra.Generic.Matrix{System.Double})">
            <summary>
            Compute log probabilities of possible outcomes for samples in X.
            The model need to have probability information computed at training
            time: fit with attribute `probability` set to True.
            </summary>
            <param name="x">[nSamples, nFeatures]</param>
            <returns>[nSamples, nClasses]
               Returns the log-probabilities of the sample for each class in
               the model. The columns correspond to the classes in sorted
               order, as they appear in the attribute `classes_`.</returns>
            <remarks>
                    The probability model is created using cross validation, so
            the results can be slightly different than those obtained by
            predict. Also, it will produce meaningless results on very small
            datasets.
            </remarks>
        </member>
        <member name="P:Sharpkit.Learn.Svm.SvcBase`1.ClassWeightEstimator">
            <summary>
            Gets or sets class weight estimator.
            </summary>
        </member>
        <member name="P:Sharpkit.Learn.Svm.SvcBase`1.Classes">
            <summary>
            Gets array with labels for each class.
            </summary>
        </member>
        <member name="P:Sharpkit.Learn.Svm.SvcBase`1.Intercept">
            <summary>
            Gets intercept in decision function
            </summary>
        </member>
        <member name="M:Sharpkit.Learn.Svm.Svc`1.#ctor(System.Double,Sharpkit.Learn.Svm.Kernel,System.Int32,System.Double,System.Double,System.Boolean,System.Boolean,System.Double,System.Int32,Sharpkit.Learn.ClassWeightEstimator{`0},System.Boolean)">
            <summary>
            Initializes a new instance of the Svc class.
            </summary>
            <param name="c">Penalty parameter C of the error term.</param>
            <param name="kernel">Specifies the kernel type to be used in the algorithm.</param>
            <param name="degree"> Degree of kernel function.
            It is significant only in <see cref="P:Sharpkit.Learn.Svm.Kernel.Poly"/>.</param>
            <param name="gamma">
            Kernel coefficient for <see cref="P:Sharpkit.Learn.Svm.Kernel.Rbf"/>, <see cref="P:Sharpkit.Learn.Svm.Kernel.Poly"/> and <see cref="P:Sharpkit.Learn.Svm.Kernel.Sigmoid"/>.
            If gamma is 0.0 then 1/nFeatures will be used instead.
            </param>
            <param name="coef0">
            Independent term in kernel function.
            It is only significant in <see cref="P:Sharpkit.Learn.Svm.Kernel.Poly"/> and <see cref="P:Sharpkit.Learn.Svm.Kernel.Sigmoid"/>.
            </param>
            <param name="shrinking"> Whether to use the shrinking heuristic.</param>
            <param name="probability">
            Whether to enable probability estimates. This must be enabled prior
            to calling <see cref="M:Sharpkit.Learn.Svm.SvcBase`1.PredictProba(MathNet.Numerics.LinearAlgebra.Generic.Matrix{System.Double})"/>.
            </param>
            <param name="tol">Tolerance for stopping criterion.</param>
            <param name="cacheSize">Size of the kernel cache (in MB).</param>
            <param name="classWeightEstimator"> Set the parameter C of class i to class_weight[i]*C for
            Svc. If not given, all classes are supposed to have
            weight one. The 'auto' mode uses the values of y to
            automatically adjust weights inversely proportional to
            class frequencies.</param>
            <param name="verbose">Enable verbose output. Note that this setting takes advantage of a
            per-process runtime setting in libsvm that, if enabled, may not work
            properly in a multithreaded context.</param>
        </member>
        <member name="T:Sharpkit.Learn.Utils.Fixes">
            <summary>
            Auxiliary utility methods.
            </summary>
        </member>
        <member name="M:Sharpkit.Learn.Utils.Fixes.Unique``1(``0[],System.Int32[]@)">
            <summary>
            Returns sorted array of unique items in <paramref name="vals"/>,
            populates <paramref name="indices"/> with indices of items in the
            returned array which correspond to every item in <paramref name="vals"/>.
            </summary>
            <typeparam name="T">Type of elements in the array.</typeparam>
            <param name="vals">Array of items.</param>
            <param name="indices">'Out' array which contains index into returned array for every item in <paramref name="vals"/>.
            </param>
            <returns>sorted array of unique items in <paramref name="vals"/>.</returns>
        </member>
        <member name="T:Sharpkit.Learn.Utils.Multiclass">
            <summary>
            TODO: Update summary.
            </summary>
        </member>
    </members>
</doc>
