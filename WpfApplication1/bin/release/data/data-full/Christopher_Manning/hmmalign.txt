Extensions to HMM-based Statistical Word Alignment Models
Kristina Toutanova, H. Tolga Ilhan and Christopher D. Manning
Department of Computer Science
Stanford University
Stanford, CA 94305-9040 USA
kristina@cs.stanford.edu
ilhan@stanford.edu
manning@cs.stanford.edu

§ 

¥
§£

§ 

This paper was supported in part by the National Science
Foundation under Grants IIS-0085896 and IIS-9982226. The
authors would also like to thank the various reviewers for their
helpful comments on earlier versions.

"
#

¥
§£
 § ¨ ¥ ¡
 ©§¦¤£¢ 

1

$"
§ ##

The main task in statistical machine translation is
to model the string translation probability
where the string
in one language is translated
into another language as string . We refer to
as the source language string and
as the target
language string in accordance with the noisy channel terminology used in the IBM models of (Brown
et al., 1993). Word-level translation models assume
a pairwise mapping between the words of the source
and target strings. This mapping is generated by
alignment models. In this paper we present extensions to the HMM alignment model of (Vogel et al.,
1996; Och and Ney, 2000b). Some of our extensions
are applicable to other alignment models as well and
are of general utility.1
For most language pairs huge amounts of parallel
corpora are not readily available whereas monolingual resources such as taggers are more often available. Little research has gone into exploring the po-

£

1 Introduction


! ¨£ ¡ 

This paper describes improved HMM-based word
level alignment models for statistical machine
translation. We present a method for using part of
speech tag information to improve alignment accuracy, and an approach to modeling fertility and correspondence to the empty word in an HMM alignment model. We present accuracy results from evaluating Viterbi alignments against human-judged
alignments on the Canadian Hansards corpus, as
compared to a bigram HMM, and IBM model 4.
The results show up to 16% alignment error reduction.

tential of part of speech information to better model
translation probabilities and permutation probabilities. Melamed (2000) uses a very broad classiﬁcation of words (content, function and several punctuation classes) to estimate class-speciﬁc parameters
for translation models. Fung and Wu (1995) adapt
English tags for Chinese language modeling using
Coerced Markov Models. They use English POS
classes as states of the Markov Model to generate
Chinese language words. In this paper we use POS
tag information to incorporate prior knowledge of
word translation and to model local word order variation. We show that using this information can help
in the translation modeling task.
Many alignment models assume a one to many
mapping from source language words to target language words, such as the IBM models 1-5 of Brown
et al. (1993) and the HMM alignment model of (Vogel et al., 1996). In addition, the IBM Models 3,
where is
4 and 5 include a fertility model
the number of words aligned to a source word . In
HMM-based alignment word fertilities are not modeled. The alignment positions of target words are the
states in an HMM. The alignment probabilities for
word
depend only on the alignment of the previous word
if using a ﬁrst order HMM. Therefore, source words are not awarded/penalized for being aligned to more than one target word. We present
an extension to HMM alignment that approximately
models word fertility.
Another assumption of existing alignment models is that there is a special Null word in the source
sentence from which all target words that do not
have other correspondences in the source language
are generated. Use of such a Null word has proven
problematic in many models. We also assume the



Abstract

 ¨ ¡
& §  §¥ ¤£% 

with part of
Augmenting the model
speech tag information leads to the following equaor vector notation e, f to detions. We use ,
note English and French strings. ( and represent
the lengths of the French and English strings respectively.) Let us deﬁne eT and fT as possible POS tag
sequences of the sentences e and f. We can rewrite
as folthe string translation probability
lows (using Bayes rule to give the last line):

One possible way to rewrite
without loss of generality is:

"
#
g
Df£
"
©
e &tB@sB@ ( @ § A@ $ § @ §  P¤¢)
 C0 0
  ¨" ¡
 &PBAB@ ( @ " § A@ § #" § @ " §  A¤¢)   §  "
C 0@ 0
$   ¨" ¡
 0 @ 0 ( $( #"  $ $#"  " ¨ "
d  CPBAB@ @ § § " § A@ § § #"§ @ § $§ "  ©¡¢)  
 C 0@
&tBs ¨0 ¡%3RP0BA ¨0 §B@ A@ ¤%)
)  C @   § § ¡
 @ §#£
¥§  £ ¥

2 Part of Speech Tags in a Translation
Model

¨ @  @ ¡
§ BE § AS § ¤%)

existence of a special Null word in the source language that generates words in the target language.
However, we deﬁne a different model that better
constrains and conditions generation from Null. We
assume that the generation probability of words by
Null depends on other words in the target sentence.
Next we present the general equations for decomposition of the translation probability using part of
speech tags and later we will go into more detail of
our extensions.

(1)

Here each
gives the index of the word
to
is aligned. The models we present in
which
this paper will differ in the decompositions of alignment probabilities, tag translation and word translation probabilities in Eqn. 1. Section 3 describes
the baseline model in more detail. Section 4 illustrates examples where the baseline model performs
poorly. Section 5 presents our extensions and Section 6 presents experimental results.

3 Baseline Model
Translation of French and English sentences shows a
strong localization effect. Words close to each other
in the source language remain close in the translation. Furthermore, most of the time the alignment
shows monotonicity. This means that pairwise alignments stay close to the diagonal line of the
plane. It has been shown (Vogel et al., 1996; Och
et al., 1999; Och and Ney, 2000a) that HMM based
alignment models are effective at capturing such localization.
We use as a baseline the model presented by
(Och and Ney, 2000a). A basic bigram HMM-based
model gives us

(

 §  §¥ £

'

 ¨ ¡
 §  §¥ ¤£¢ 

j@
lk¦ih¡

(2)

In this HMM model,2 alignment probabilities are
independent of word position and depend only on
jump width (
).3 The Och and Ney (2000a)
model includes reﬁnements including special treatment of a jump to Null and smoothing with a uniform prior which we also included in our initial
model. As in their model we set the probability for
jump from any state to Null to a ﬁxed value (
)
which we estimated from held-out data.

$" r "
§ #©ts¦

g



~}{y
lk|zg

x

2

Twv3 
u

 C2@ ¨ ¡ ) ¨ C ¡ 4
FBG2 10¢F2 EF2¢) 876 53
Y
a `X
ca Y X  ¨ C ¡ )  ¡ ) 7  ¨ C ¡ ) 4
d876 6 a `V0 WP0¢FV0¢UIH T4¦2 SF2¢R876 53
c7
`IH H b 876 6
 C2@ ¨ C 0@ ¡ 7  ¨ C ¡ )
DBQ2 EPBA0¢) IH 4 2 EF2¢987 46 3
 C 0 @ ¨ C 2 @ ¡ )  ¡ q eH h g f 3  ¨ ¡ q eH h g f
PBA0 WDBQ2¢FV0¢) ¢rB©e A©2 U0¢) piB©e
This is the decomposition of the string translation
probability into a language model and translation
model. In this paper we only address the translation model and assume that there exists a one-to-one
alignment from target to source words. Therefore,

 m
o § " nf
p gf ¨ " ¡ ) $" ¨ " ¡
q©D£ P¤¢F ' @ § # P¢)   4 0 P2%)
3  ¨ ¡

 ¨ C2@ ¡ ) 4 3  ¨ ¡
©2 EDBA0¢9876 52 10¢)
If we also assume that the taggers in both languages
generate a single tag sequence for each sentence then
the equation for machine translation by the noisy
channel model simpliﬁes to

Each HMM state is [
] emitting as output.
In order for the model not to be deﬁcient, we normalize the
jump probabilities at each EM step so that jumping outside of
the borders of the sentence is not possible.
3

 C 0@ ¨ x@ C2@ ¡ ) v
PBA0 yB%DBQ2¢wI4

3  C 0@ ¨ C2@ ¡
utBs0 EDBQ2¢)

j−2

PRE

ADV

PON

PRP

en

outre

,

VB

elle

VB

pourrait

DT

JJ

VB

constituer une serieuse

menace

PRE

NN

DT

pour la

CC

DT

et

le

confederation

j−1

j

NN

JJ

PON

unite nationale

.

Aln1
Aln2

VB

 ~}
|FB{

 ~}
¤lk{

national
unity

JJ

DT

P(

unity
unity

x x
y   
|I¦Bg Bn kg

MD

threat

to

NN

TO

)

0.0292
0.0862

P(



~}
lk{
.
.

PRP

a serious

confederation
NN

 ~}  
¤lk{ Bn kg

Aln1
Aln2

,

could become

0.5741
0.2789

and
CC

)

P(

national
JJ

x x
 
|y n kg  g

NN

it

0.05357
0.31

unity
NN

)



IN

,

.
.

~}
lk{  g

addition

P(
)
0.9766
0.9766


l
 
© k

¦

NULL in

total
8.77x
7.2x

Figure 1: The baseline model makes a simple alignment error.
sequences (
), (
) for Aln1
and Aln2 are
, 2 and 0, 1 respectively. The
table shows that the gain from use of monotonic
alignment probabilities dominates over the lowered
word translation probability. Although national and
nationale are strongly correlated according to the
and 2
translation probabilities, jump widths of
are less probable than jump widths of 0 and 1.


r

5 Extensions
In this section we describe our improvements on the
HMM model. We present evaluation results in Section 6 after describing the technical details of our
models here.
5.1

Our model with part of speech tags for translation
probabilities uses the following simpliﬁcation of the
translation probability shown in Eqn. 1. 4

  § I " nm f 4 &PBA0 SFBG2¢)
3  C 0@ ¨ C2@ ¡
 

(3)

4



Since we are only concerned with alignment here and not
generation of candidate translations the factor P( e,eT) can
be ignored and we omit it from the equations for the rest of the
paper.





"

In this model we introduce tag translation probabilities as an extra factor to Eqn. 2. Intuitively the role
of this factor is to boost the translation probabilities
for words of parts of speech that can often be translations of each other. Thus this probability distribution provides prior knowledge of the possible translations of a word based only on its part of speech.
However, P(
) should not be too sharp or

g  
Df z ¨£ " A



Figure 1 illustrates how our baseline HMM model
makes an alignment mistake of this sort. The table in the ﬁgure displays alignment and translation
probabilities of two competing alignments (namely
Aln1 and Aln2) for the last three words. In both
alignments, the shown
and
are periods at the
end of the French and English sentences. The ﬁrst
alignment maps nationale to national and unit´ to
e
unity. (i.e.
national and
=unity). The
second alignment maps both nationale and unit´ to
e
unity and
unity). Startunity (i.e.
ing from the unity-unit´ alignment, the jump width
e

POS Tags for Translation Probabilities

e gf ¨ " ¡
 g F#£ P ¤¡¢)
' Ff $ "¨£ " ¨ A¤¡¢)
"
d   @ § # t©¢)

Although the baseline Hidden Markov alignment
model successfully generates smooth alignments,
there are a fair number of alignment examples where
pairwise match shows local irregularities. One instance of this is the transition of the NP JJ NN
rule to NP NN JJ from English to French. We can
list two main reasons why word translation probabilities may not catch such irregularities to monotonicity. First, it may be the case that both the
English adjective and noun are words that are unknown. In this case the translation probabilities will
be close to each other after smoothing. Second, the
adjective-noun pair may consist of words that are
frequently seen together in English. National reserve and Canadian parliament, are examples of
such pairs. As a result there will be an indirect association between the English noun and the translation
of the English adjective. In both cases, word translation probabilities will not be differentiating enough
and alignment probabilities become the dominating
factor to determine where aligns.

 
  
 § #©w© s#wr #r$ © 
$" r " $" § "

4 Alignment Irregularities

g
Df £

3  gf
GkF£
 gf
¦BD!£

"

3 n kF£
gf
3 n kF£
gf

$"
§ #©
"
©
&CPB@AB@ ( @ § " § A@ § #"§ @ § § "  ©¢)
0 0
$  $  $ ¨" ¡
 C 0@ ¨ x@ C2@ ¡
&tBs0 `BpFBG2¢)
$"
§ #©

 § Fn BD1z@ n kFU@ § $ n BD1z@ § # t©¢)
 gf £ gf £ gf £ $" ¨ " ¡
3  C 0@ 0
s&PBAB@ ( @ § " § A@ § #"§ @ § #§ " ¨ " ¢)
$  $  $ ¡
we could model probabilities of transpositions and
insertion of function words in the target language
that have no corresponding words in the source lanis Null) similarly to the channel operguage (
ations of the (Yamada and Knight, 2001) syntax-


o § " nm f
p gf ¨ " ¡ ) ¡  @ "    ¨ " ¡
qF#£ t¤¢F ' @ n Fg A©A@ n kg  ©¢)   V F2¢)
4 C ¡
3  ¨ C2@ ¡
sV0 DBQ2¢)

This section describes an extension to the bigram
HMM model that uses source and target language
tag sequences as conditioning information when
predicting the alignment of target language words.
In the decomposition of the joint probashown in Eqn. 1
bility
the factor for alignment probabilities is
.
A bigram HMM model assumes independence of
from anything but the previous alignment position
and the length of the English sentence.
Brown et al. (1993) and Och et al. (1999) variably
condition this probability on the English word in
and/or the French word in position
position
. As conditioning directly on words would yield
a large number of parameters and would be impractical, they cluster the words automatically into bilingual word classes.
The question arises then whether we would have
larger gains by conditioning on the part of speech
tags of those words or even more words around the
alignment position. For example, if we use the following conditioning information:

" ¡
!AA¤¢)

Tag Sequences for Jump Probabilities

"
@" 
§  "AA
"  @ "   $"    gf £ gf £ gf 
§  VA©A@ § A@ § Fn kFU@ n BD1z@ § $ n BD1z£

5.2

3 § Fn BD1z£
 gf 


   r  ¡  g   ¡ )  3 g   ¡
§ l1 Df z ¨£ " A¤¢wR Df z ¨£ " A¤¢)

T is the size of the French tag set and is set to be
0.1 in our experiments. The tag translation model is
so heavily smoothed with a uniform distribution because in EM the tag translation probabilities quickly
become very sharp and can easily overrule the alignment and word translation probabilities. The Results
section shows that the addition of this factor reduces
the alignment error rate, with the improvement being
especially large when the training data size is small.

3 n BDz£
gf 
3 "
A #
3 "

 ¨ ¡
!£ R¤%)

(4)

based statistical translation model. Since the syntactic knowledge provided by POS tags is quite limited,
this is a crude model of transpositions and Null insertions at the preterminal level. However we could
still expect that it would help in modeling local
word order variations. For example, in the sentence
J’aime la chute ‘I love the fall’ the probability of
aligning
la (
DT) to the will be boosted
by knowing
VBP and
DT.
Similarly, in the sentence J’aime des chiens ‘I love
dogs’ the probability of aligning
la to Null
will be increased by knowing
VBP and
NNS. VBP followed by NNS crudely
conducts the information that the verb is followed by
a noun phrase which does not include a determiner.
We conducted a series of experiments where
the alignment probabilities are conditioned on
different subsets of the part of speech tags
.
In order to be able to condition on
when generating an alignment position for
,
we have to change the generative model for the
sentence f and its tag sequence fT to generate the
part of speech tags for the French words before
choosing alignment positions for them. The French
POS tags could be generated for example from
or from the previous
a prior distribution
French tags as in an HMM for part-of-speech tagging. The generative model becomes:

3 n kFf z£
g 
3 "
!
3 § Dn kF1z£
 gf 

it will dominate the alignment probabilities and the
probabilities
. We use the following linear
interpolation to smooth tag translation probabilities:

This model makes the assumption that target words
are independent of their tags given the corresponding source word and models only the dependence of
alignment positions on part of speech tags.
5.3

Modeling Fertility

A major advantage of the IBM models 3–5 over the
HMM alignment model is the presence of a model
of source word fertility. Thus knowledge that some
words translate as phrases in the target language is
incorporated in the model.
The HMM model has no memory, apart from the
previous alignment, about how many words it has
aligned to a source word. Yet even this memory is
not used to decide whether to generate more words
from a given English word. The decision to gener-

gf
F£

h

¡ ) r
¢¦

c
F§ b §

Y
`X $
§

gf
n kF ¨£

(6)


 n Bg } ¨£



¯« ® ­« ¯« ® ­«
¦tq° G1t|¬ « ª
n
n
¡ )  r¡  © ¨ )  3 
¢F`llEV¦st n Bg } ¨£

¡
¢)

5
+
+ ...
E[X] = = +
where X is the number of Bernoulli trials until the ﬁrst success.

3 gf£ " ¨ " ¡
F#@ §   P¤¢)


 ¨£

stay

Null (7)

re-estimate
the
probabilities
Null from the training corpus using EM. The dependence of a French
word on the next French word requires a change
in the generative model to ﬁrst propose alignments for all words in the French sentence and
to then generate the French words given their
alignments, starting from the end of the sentence
and going towards the beginning. For the new
model there is an efﬁcient dynamic programming
algorithm for computations in EM similar to the
forward-backward algorithm.
The probability



 ' @ § © ©¢)
$" ¨" ¡
 § # tP£ ¤
$" ¨" ¡
e
 ' @ § © ¨©¢)
$" " ¡

 g
k n BDf ¨£ ¢)5l¡  
¡ r 
"@ " ¡ ¤ 
d  k § $©BQP¥rl¡ 
d n BgD ¨£ ¡¢F § ©BG©P¤53A ' @ n BD#@ § #" ¨P"¡%)
 f
) $"@" ¡
gf£ $
£
stay

We

Null



¢  m
Fo § " nf
p gf ¨ " ¡ ) gf£ $" ¨ " ¡
q©D#£ t¤¢F ' @ n kF#@ § © ©¢)   4 s0 t2¢)
3 ¨ ¡
in Eqn. 5 is the Kronecker delta funcBasically, the new alignment probabilities
state that a jump width of zero depends on the English word. If we deﬁne the fertility
of a word as the number of consecutive words from
the target language it generates, then the probability distribution for the fertility of an English word e
according to this model is geometric with a probability of success
stay . The expectation is
.5 Even though the ﬁt of this distribution
stay
to the real fertility distribution may not be very good,
this approximation improves alignment accuracy in
practice.
Sparsity is a problem in estimating stay probabil). We use the probability of a jump
ities P(stay
of size zero from the baseline model as our prior to
do smoothing as follows:
tion.

As originally proposed by Brown et al. (1993),
words in the target sentence for which there are no
corresponding English words are assumed to be generated by the special English word Null. Null appears in every English sentence and often serves to
generate syntactic elements in the target language
that are missing in the source. A probability distribution
Null for generation probabilities of the
Null is re-estimated from a training corpus.
Modeling a Null word has proven problematic. It
has required many special ﬁxes to keep models from
aligning everything to Null or to keep them from
aligning nothing to Null (Och and Ney, 2000b). This
might stem from the problem that the Null is responsible for generating syntactic elements of the target
language as well as generating words that make the
target language sentence more idiomatic and stylistic. The intuition for our model of translation probabilities for target words that do not have corresponding source words is that these words are generated
from the special English Null and also from the next
word in the target language by a mixture model. The
´ ´
pair la confederation in Figure 1 is an example of
´ ´
such case where confederation contributes extra information in generation of la. The formula for the
probability of a target word given that it does not
have a corresponding aligning word in the source is:

3gF#£ ¤%FAµlp
f ¨ " ¡ )  r ¡
3 Ff @ §  " ¨ " ¤¢s´Q3 " ¨ " ¤¢)
¡ ) 3 ³
¡
g £

¡
¢)

$"
§ ##

(5)

Translation Model for Null



 n BDf ¨£
g

stay

5.4

¨ ¡
R¤¢)

gf
n BD£

stay

in this equation is the alignment probability
from the baseline model with zero jump distance.
.



gf
n BD¦£

where

 ' Bj²3 § #" zj²3"©¡¢)3  )
@
$ ¨
±
 )
±

ate again (to make a jump of size 0) is independent
of the word and is estimated over all words in the
corpus.
We extended the HMM model to decide whether
to generate more words from the previous English
word
or to move on to a different word depending on the identity of the English word
.
We introduced a factor
stay
where the
boolean random variable stay depends on the English word
aligned to. Since in most cases
words with fertility greater than one generate words
that are consecutive in the target language, this
extension approximates fertility modeling. More
speciﬁcally, the baseline model (i.e., Eqn. 2) is
changed as follows:

¿ Ï
¦Î @ Î
Ñ
#V³B@ Ð
³³
¦Gw

³³
¦Gw
³³
¦
¨ ¨ ¨
ÆTÅw#& ¨Ã
¨ Å Ä ¨ ¨   Ä 3 Ì Ë
ÆW¦Ã w#%¦ ¨Ã ÍÃ
''  Â Á
¨
& ¨Ã
¨
Á  ÆTÅ¨
¨   Ä 3 É Èj ÇjÂ
%9 ¨Ã ÊqD#£ E¦ ¨Ã 3 z#£
¨ Å Ä
Ò
¦Ð w

Ó
Ó

Tags is an HMM model that includes tags for
translation probabilities (section 5.1)
6

We want to thank Franz Och for sharing the annotated data
with us.

Table 2 shows AER results for our improved
models on training corpora of increasing size. The
model Null outperforms the baseline at every data
set size,with the error reduction being larger for bigger training sets (up to 9.2% error reduction). The
SG model reduces the baseline error rate by up to
10%. The model Tags reduces the error rate for the
smallest dataset by 7.6%. The combination of Tags
and the SG or Null models outperforms the individual models in the combination since they address
different problems and make orthogonal mistakes.
The combination of SG and Tags reduces the baseline error rate by up to 16% and the combination of
Null and Tags reduces the error rate by up to 12.3%.
All of these error reductions are statistically signiﬁconﬁdence level according to the
cant at the
paired t-test. The combination Tags+Null+SG further reduces the error rate. For small datasets, there
seems to be a stronger overlap between the strengths
of the Null and SG models because some fertility
related phenomena can be accounted for by both
models. When an English word is wrongly aligning to several consecutive French words because of
indirect association, while the correct alignment of
some of them is to the empty word, both the Null and
SG models can combat the problem— one by better
modeling correspondence to Null, and the other by
discouraging large fertilities.
Figure 2 displays learning curves for three models: Och, Tags, and Tags+Null. Och is the HMM
alignment model of (Och and Ney, 2000b). To obtain results from the Och model we ran GIZA++. 7
Both the Tags and Och models use word classes.
However the word classes used in the latter are
learned automatically from parallel bilingual corpora while the classes used in the former are human deﬁned part of speech tags. Figure 2 shows
that the Tags model outperforms the Och model
when the training data size is small. As the train7

GIZA++ can be downloaded from http://www-i6.
informatik.rwth-aachen.de/ och/software/GIZA++.html

Õ

À

Baseline is the baseline HMM model described
in section 2

Tags+Null, Tags+SG, and Tags+Null+SG are
combinations of the above models

¿ Ô³vt
u 3

³
¦³ ¿

We divided this annotated data into a validation
sentences and a ﬁnal test set of
senset of
tences. The validation set was used to select tuning
parameters such as in Eqn. 4, 6 and 7. We report
AER results on the ﬁnal test set of
sentences
which contain a total of
English and
French words. We experimented with training corpora of different sizes ranging from 5K to 50K sentences. We concentrated on small to medium data
sets to assess the ability of our models to deal with
sparse data.
Table 1 shows the percentage of words in the corpus that were seen less than the speciﬁed number of
times. For example, in our 10K training corpus
of all word types were seen only once. As seen from
the table the sparsity is great even for large corpora.
The models we implemented and compare in this
section are the following:

Null is an HMM model that includes the new generation model for words by Null (section 5.4)

Ó

We present results on word level alignment accuracy using the Hansards corpus. Our test data conmanually aligned sentences which are
sists of
the same data set used by (Och and Ney, 2000b). 6
In the annotated sentences every alignment between
two words is labeled as either a sure (S) or possible
(P) alignment. (S P). We used the following quantity (called alignment error rate or AER) to evaluate
the alignment quality of our models, which is also
the evaluation metric used by (Och and Ney, 2000b):

SG is an HMM model that includes stay probabilities (section 5.3)

Ó

tBj¾3 " ¨  u §  " ¤%½tFk¦ih¡ ¼
0@
uu
¡ ) 3 j@
 0 "@
tB@ §  Bj 3  Iu § ¤¢)
" ¨"uu ¡
 j
»lº3t"G¡¢)¹%Fk¦ih¡ ¸
3 j@
 uuu "@j 3 "@"uu ¡
t ¨0  @ §  ·¶B¦Iu § ¤¢)

6 Results

Ó

again decomposes into forward and backward probabilities.
The forward probability is
and the backward
probability is
.
These can be computed recursively and used for
efﬁcient computation of posteriors in EM.

Corpus
10K
25K
50K

Table 1: Percentage of words in the corpus by frequency
=1
3
5
10
English French English French English French English French
47%
50%
61%
66%
74%
77%
84%
87%
43%
44%
57%
59%
69%
72%
80%
83%
42%
44%
55%
57%
67%
69%
78%
81%

Corpus
5K
15K
25K
35K
50K

Table 2: Alignment Error Rate by Model and Corpus Size
Baseline Null
SG
Tags Tags+SG Tags+Null Tags+Null+SG
17.53
16.86 16.72 16.20
15.31
15.36
15.14
15.03
14.29 13.52 13.90
12.63
13.22
12.52
13.85
13.05 12.79 13.10
11.91
12.30
11.79
13.19
11.98 12.03 12.60
11.45
11.56
11.07
12.63
11.76 11.78 12.10
11.19
11.11
10.69

Ö

25K

35K

45K

Training Set Size

Figure 2: Och vs. Tags and Tags+Null.

AER

gf
n BD£

15K

We also assessed the gains from using part of
speech tags in the alignment probabilities according
to the model described in section 5.2. Table 3 shows
the error rate of the basic HMM alignment model
as compared to an HMM model that conditions on
tag sequences of source and target word tags in the
neighborhood of the French word and the English
word
for a training set size of 10K. The results
we achieved showed an improvement of our model
over a model that does not include conditioning on
tags. The improvement in accuracy is best when
using the current and previous French word parts
of speech and does not increase when adding more
conditioning information. The improvement from
part of speech tag sequences for alignment probabilities was not as good as we had expected, however, which leads us to believe that more sophisti-

"
!

10.5
5K

¿ Ô³tt
u³ 3

12.5

positive signs. Ties are assigned the average rank of
the tied group. Since there are 400 test sentences, we
have 400 paired samples where the elements of each
pair are the AERs of the models being compared.
The difference between Och and Tags at 5K, 10K,
and 15K is signiﬁcant at the
level according to both tests. The difference between Och and
Tags+Null is signiﬁcant for all training set sizes at
the
level.

¿ Ô³tÍÝ
u³ 3

14.5

×Y
 Ü c ¡ ¡ × Y BsfØÛ $ Ú ¡ Ù ×

16.5

both paired t-test and Wilcoxon signed rank tests to
show the improvements are statistically signiﬁcant.
The signed rank test uses the normalized test statistic
.
is the sum of the ranks that have

c

Och
Tags
Null+Tags

Ö

18.5

Ö

ing size increases the Och model catches up with
the Tags model and even surpasses it slightly. This
suggests that when large amounts of parallel text are
not available monolingual part of speech classes can
improve alignment quality more than automatically
induced classes. When more data is available automatically induced bilingual word classes seem to
provide more improvement but it still remains to be
explored whether the combination of part-of-speech
knowledge with induction of bilingual classes will
perform even better. The third curve in the ﬁgure for
Tags+Null illustrates the relative improvement of
the Null model over the Tags model as the training
set size increases. We see that the performance gap
between the two models becomes wider for larger
training data sizes. This reﬂects the improved estimation of the generation probabilities for Null which
require target word speciﬁc parameters. We used

Table 3: POS Conditioning of Jump Probabilities
Model
AER
Baseline
16.37
15.97
15.74
15.86
15.88
15.94

 f
$"
n kgFU£¦ § #A¦
g £ g £
§ Fn kFf ¦ n kFf ¦
gf £
n kFU¦
$"
§ #A¦

"A


" A
"A

"A

"
A

cated syntax is needed to model local word order
variation.

7 Conclusions
In this paper we presented three extensions to
HMM-based alignment models. We showed that
incorporating part of speech tag information of the
source and target languages in the translation model
improves word alignment accuracy. We also presented a method for approximately modeling fertility in an HMM-based model and a new generative
model for target language words that do not have
correspondences in the source language. The proposed models do not increase signiﬁcantly the complexity of the learning algorithms while providing a
better account for some phenomena in natural language translation.

20

References

AER

18

IBM4
SG+Tags

16
14

P. Brown, S. Della Pietra, V. Della Pietra, and R. Mercer. 1993. The mathematics of statistical machine
translation: Parameter estimation. In Computational
Linguistics, volume 19(2), pages 263–311.
Pascale Fung and Dekai Wu. 1995. Coerced markov
models for cross-lingual tag relations. In Sixth International Conference on Theoretical and Methodological Issues in Machine Translation, volume 1, pages
240–255.

12
10
5K

15K

25K

35K

45K

Training Set Size

w

Figure 3: IBM- vs SG+Tags

w

In Figure 3 we compare the IBM- model to our
SG+Tags model. Such a comparison makes sense
because IBM- uses a fertility model for English
words and SG approximates fertility modeling and
because IBM- uses word classes as does our Tags
model. For smaller training set sizes our model performs much better than IBM- but when more data
is available IBM- becomes slightly better. This
conﬁrms the observation from Figure 2 that automatically induced bilingual classes perform better
when trained on large amounts of data. Also as our
fertility model estimates one parameter for each English word and IBM- estimates as many parameters as the maximum fertility allowed, at small training set sizes our model parameters can be estimated
more reliably.

Dan I. Melamed. 2000. Models of translational equivalence among words. In Computational Linguistics,
volume 26(2), pages 221–249.
F. Och and H. Ney. 2000a. A comparison of alignment
models for statistical machine translation. In Proc.
COLING ’00: The 18th Int. Conf. on Computational
Linguistics, pages 1086–1090.
F. Josef Och and H. Ney. 2000b. Improved statistical
alignment models. In Proc. of the 39th Annual Meeting of the ACL.
F. Och, C. Tillmann, and H. Ney. 1999. Improved alignment models for statistical machine translation. In
Proc. of the Joint Conf. of Empirical Methods in Natural Language Processing and Very Large Corpora,
pages 20–28.
S. Vogel, H. Ney, and C. Tillmann. 1996. Hmm-based
word alignment in statistical translation. In Proc.
COLING ’96: The 16th Int. Conf. on Computational
Linguistics, pages 836– 841.
K. Yamada and K. Knight. 2001. A syntax-based statistical translation model. In Proc. of the 39th Annual
Meeting of the ACL, pages 523–530.

w

w

w

w

w

