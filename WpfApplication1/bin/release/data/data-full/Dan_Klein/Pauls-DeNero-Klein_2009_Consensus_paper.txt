Consensus Training for Consensus Decoding in Machine Translation
Adam Pauls, John DeNero and Dan Klein
Computer Science Division
University of California at Berkeley
{adpauls,denero,klein}@cs.berkeley.edu

Abstract
We propose a novel objective function for discriminatively tuning log-linear machine translation models. Our objective explicitly optimizes the BLEU score of expected n-gram
counts, the same quantities that arise in forestbased consensus and minimum Bayes risk decoding methods. Our continuous objective
can be optimized using simple gradient ascent. However, computing critical quantities
in the gradient necessitates a novel dynamic
program, which we also present here. Assuming BLEU as an evaluation measure, our
objective function has two principle advantages over standard max BLEU tuning. First,
it speciﬁcally optimizes model weights for
downstream consensus decoding procedures.
An unexpected second beneﬁt is that it reduces
overﬁtting, which can improve test set BLEU
scores when using standard Viterbi decoding.

1

Introduction

Increasing evidence suggests that machine translation decoders should not search for a single
top scoring Viterbi derivation, but should instead
choose a translation that is sensitive to the model’s
entire predictive distribution. Several recent consensus decoding methods leverage compact representations of this distribution by choosing translations according to n-gram posteriors and expected
counts (Tromble et al., 2008; DeNero et al., 2009;
Li et al., 2009; Kumar et al., 2009). This change
in decoding objective suggests a complementary
change in tuning objective, to one that optimizes
expected n-gram counts directly. The ubiquitous
minimum error rate training (MERT) approach optimizes Viterbi predictions, but does not explicitly
boost the aggregated posterior probability of desirable n-grams (Och, 2003).
We therefore propose an alternative objective

function for parameter tuning, which we call consensus BLEU or CoBLEU, that is designed to
maximize the expected counts of the n-grams that
appear in reference translations. To maintain consistency across the translation pipeline, we formulate CoBLEU to share the functional form of
BLEU used for evaluation. As a result, CoBLEU
optimizes exactly the quantities that drive efﬁcient
consensus decoding techniques and precisely mirrors the objective used for fast consensus decoding
in DeNero et al. (2009).
CoBLEU is a continuous and (mostly) differentiable function that we optimize using gradient
ascent. We show that this function and its gradient
are efﬁciently computable over packed forests of
translations generated by machine translation systems. The gradient includes expectations of products of features and n-gram counts, a quantity that
has not appeared in previous work. We present a
new dynamic program which allows the efﬁcient
computation of these quantities over translation
forests. The resulting gradient ascent procedure
does not require any k-best approximations. Optimizing over translation forests gives similar stability beneﬁts to recent work on lattice-based minimum error rate training (Macherey et al., 2008)
and large-margin training (Chiang et al., 2008).
We developed CoBLEU primarily to complement consensus decoding, which it does; it produces higher BLEU scores than coupling MERT
with consensus decoding. However, we found
an additional empirical beneﬁt: CoBLEU is less
prone to overﬁtting than MERT, even when using
Viterbi decoding. In experiments, models trained
to maximize tuning set BLEU using MERT consistently degraded in performance from tuning to
test set, while CoBLEU-trained models generalized more robustly. As a result, we found that optimizing CoBLEU improved test set performance
reliably using consensus decoding and occasionally using Viterbi decoding.

(a) Tuning set sentence and translation
(a) Hypotheses ranked by !TM = !LM = 1
Sentence f: Il était une rime
Reference r: Once upon a rhyme
TM

LM

Pr

H1) Once on a rhyme

-3

-7

0.67

H2) Once upon a rhyme

-5

-6

0.24

H3) Once upon a time

-9

-3

Viterbi derivations1 under a log-linear model with
parameters θ. Let d∗ (fi ) = arg maxd Pθ (d|fi ) be
θ
the highest scoring derivation d of fi . For a system
employing Viterbi decoding and evaluated by bigram precision, we would want to select θ to maximize MaxBP(R, F, θ):

0.09

m
i=1

g2

min{c(g2 , d∗ (fi )), c(g2 , ri )}
θ
m
i=1

(b) Computing Consensus Bigram Precision

0.24 + 0.09 = 0.33

Eθ [c(“upon a”, d)|f ]

=

0.24 + 0.09 = 0.33

Eθ [c(“a rhyme”, d)|f ]

=

0.67 + 0.24 = 0.91

Eθ [c(g, d)|f ]

=

3[0.67 + 0.24 + 0.09]

=

0.33 + 0.33 + 0.91
3

g
g

min{Eθ [c(g, d)|f ], c(g, r)}
g Eθ [c(g, d)|f ]

Figure 1: (a) A simple hypothesis space of translations
for a single sentence containing three alternatives, each
with two features. The hypotheses are scored under a
log-linear model with parameters θ equal to the identity
vector. (b) The expected counts of all bigrams that appear in the computation of consensus bigram precision.

2

Consensus Objective Functions

Our proposed objective function maximizes ngram precision by adapting the BLEU evaluation
metric as a tuning objective (Papineni et al., 2002).
To simplify exposition, we begin by adapting a
simpler metric: bigram precision.
2.1

Bigram Precision Tuning

Let the tuning corpus consist of source sentences
F = f1 . . . fm and human-generated references
R = r1 . . . rm , one reference for each source
sentence. Let ei be a translation of fi , and let
E = e1 . . . em be a corpus of translations, one for
each source sentence. A simple evaluation score
for E is its bigram precision BP(R, E):
BP(R, E) =

m
i=1

g2

min{c(g2 , ei ), c(g2 , ri )}
m
i=1

g2

c(g2 , ei )

where g2 iterates over the set of bigrams in the target language, and c(g2 , e) is the count of bigram
g2 in translation e. As in BLEU, we “clip” the bigram counts of e in the numerator using counts of
bigrams in the reference sentence.
Modern machine translation systems are typically tuned to maximize the evaluation score of

c(g2 , d∗ (fi ))
θ

On the other hand, for a system that uses expected bigram counts for a function of we would pre(a) Model score as decoding, !LM
fer to choose θ such that expected bigram counts
match bigrams0 in the reference sentence. To this
H1
end, we can evaluate an entire posterior distri-6
bution over derivations by computing the same
H2
clipped precision H3 expected bigram counts usfor
-12 θ):
ing CoBP(R, F,
m
i=1

where

Model: TM + !LM • LM

=

-18

min{Eθ [c(g2 , d)|fi ], LM 2 , ri )}
0
2
Parameter: !c(g
m
Eθ [c(g2 , d)|fi ]
i=1
g2
(b) Objectives as functions of !LM
g2

Viterbi & Consensus Objectives

Eθ [c(“Once upon”, d)|f ]

g2

Eθ [c(g2 , d)|fi ] =

(1)

Pθ (d|fi )c(g2 , d)
d

is the expected count of bigram g2 in all derivations d of fi . We deﬁne the precise parametric
form of Pθ (d|fi ) in Section 3. Figure 1 shows proParameter: !LM
posed translations for a single sentence along with
the bigram expectations needed to compute CoBP.
Equation 1 constitutes an objective function for
tuning the parameters of a machine translation
model. Figure 2 contrasts the properties of CoBP
and MaxBP as tuning objectives, using the simple
example from Figure 1.
Consensus bigram precision is an instance of a
general recipe for converting n-gram based evaluation metrics into consensus objective functions
for model tuning. For the remainder of this paper, we focus on consensus BLEU. However, the
techniques herein, including the optimization approach of Section 3, are applicable to many differentiable functions of expected n-gram counts.
1
By derivation, we mean a translation of a foreign sentence along with any latent structure assumed by the model.
Each derivation corresponds to a particular English translation, but many derivations may yield the same translation.

1.0

-14

0.8
0.6

CoBP
MaxBP
H3

H1

0.0

-16

H3

H2

0.4

H3

H1

0.2

Value of Objective

H2

-12

Log Model Score

-10

H1

1.0

1.5

2.0

2.5

0

3.0

2

4

6

8

10

θLM

θLM

(a)

(b)

Figure 2: These plots illustrate two properties of the objectives max bigram precision (MaxBP) and consensus
bigram precision (CoBP) on the simple example from Figure 1. (a) MaxBP is only sensitive to the convex hull (the
solid line) of model scores. When varying the single parameter θLM , it entirely disregards the correct translation
H2 because H2 never attains a maximal model score. (b) A plot of both objectives shows their differing characteristics. The horizontal segmented line at the top of the plot indicates the range over which consensus decoding would
select each hypothesis, while the segmented line at the bottom indicates the same for Viterbi decoding. MaxBP
is only sensitive to the single point of discontinuity between H1 and H3 , and disregards H2 entirely. CoBP peaks
when the distribution most heavily favors H2 while suppressing H1 . Though H2 never has a maximal model score,
if θLM is in the indicated range, consensus decoding would select H2 , the desired translation.

2.2 CoBLEU
The logarithm of the single-reference2 BLEU metric (Papineni et al., 2002) has the following form:
ln BLEU(R, E) =
4

+

1
ln
4 n=1

m
i=1

1−

m
i=1

|R|
g1 c(g1 , ei )

3
−

gn min{c(gn , ei ), c(gn , ri )}
m
i=1
gn c(gn , ei )

Above, |R| denotes the number of words in the
reference corpus. The notation (·)− is shorthand
for min(·, 0). In the inner sums, gn iterates over
all n-grams of order n. In order to adapt BLEU
to be a consensus tuning objective, we follow the
recipe of Section 2.1: we replace n-gram counts
from a candidate translation with expected n-gram
counts under the model.
CoBLEU(R, F, θ) = 1−
4

+

1
ln
4 n=1

m
i=1

m
i=1

equals the sum of all expected unigram counts.
We call this objective function consensus BLEU,
or CoBLEU for short.

|R|
Eθ [c(g1 , d)|fi ]
g1

−

gn min{Eθ [c(gn , d)|fi ], c(gn , ri )}
m
i=1
gn Eθ [c(gn , d)|fi ]

The brevity penalty term in BLEU is calculated
using the expected length of the corpus, which
2
Throughout this paper, we use only a single reference,
but our objective readily extends to multiple references.

Optimizing CoBLEU

Unlike the more common MaxBLEU tuning objective optimized by MERT, CoBLEU is continuous. For distributions Pθ (d|fi ) that factor
over synchronous grammar rules and n-grams, we
show below that it is also analytically differentiable, permitting a straightforward gradient ascent
optimization procedure.3 In order to perform gradient ascent, we require methods for efﬁciently
computing the gradient of the objective function
for a given parameter setting θ. Once we have the
gradient, we can perform an update at iteration t
of the form
θ(t+1) ← θ(t) + ηt θ CoBLEU(R, F, θ(t) )
where ηt is an adaptive step size.4

3
Technically, CoBLEU is non-differentiable at some
points because of clipping. At these points, we must compute a sub-gradient, and so our optimization is formally subgradient ascent. See the Appendix for details.
4
After each successful step, we grow the step size by a
constant factor. Whenever the objective does not decrease
after a step, we shrink the step size by a constant factor and
try again until a decrease is attained.

u=OnceSrhyme
2 (h)

=2

head(h)
c(“Once upon”, h) = 1
c(“upon a”, h)
=1

lowing expectations:
Eθ [c(φk , d)|fi ]

(2)

Eθ [ n (d)|fi ]

(3)

Eθ [c(φk , d) ·

v1=OnceRBOnce v2=uponINupon

v3=aNPrhyme

tail(h)
Figure 3: A hyperedge h represents a “rule” used in
syntactic machine translation. tail(h) refers to the “children” of the rule, while head(h) refers to the “head” or
“parent”. A forest of translations is built by combining
the nodes vi using h to form a new node u = head(h).
Each forest node consists of a grammar symbol and target language boundary words used to track n-grams. In
the above, we keep one boundary word for each node,
which allows us to track bigrams.

n (d)|fi ]

(4)

where n (d) = gn c(gn , d) is the sum of all ngrams on derivation d (its “length”). The ﬁrst expectation is an expected count of the kth feature
φk over all derivations of fi . The second is an expected length, the total expected count of all ngrams in derivations of fi . We call the ﬁnal expectation an expected product of counts. We now
present the computation of each of these expectations in turn.
3.2

Computing Feature Expectations

The expected feature counts Eθ [c(φk , d)|fi ] can be
written as

In this section, we develop an analytical expression for the gradient of CoBLEU, then discuss
how to efﬁciently compute the value of the objective function and gradient.

Pθ (d|fi )c(φk , d)

Eθ [c(φk , d)|fi ] =
d

=

Pθ (h|fi )c(φk , h)
h

3.1

Translation Model Form

We ﬁrst assume the general hypergraph setting of
Huang and Chiang (2007), namely, that derivations under our translation model form a hypergraph. This framework allows us to speak about
both phrase-based and syntax-based translation in
a uniﬁed framework.
We deﬁne a probability distribution over derivations d via θ as:
w(d)
Pθ (d|fi ) =
Z(fi )
with
Z(fi ) =

w(d )
d

where w(d) = exp(θ Φ(d, fi )) is the weight of a
derivation and Φ(d, fi ) is a featurized representation of the derivation d of fi . We further assume
that these features decompose over hyperedges in
the hypergraph, like the one in Figure 3. That is,
Φ(d, fi ) = h∈d Φ(h, fi ).
In this setting, we can analytically compute the
gradient of CoBLEU. We provide a sketch of the
derivation of this gradient in the Appendix. In
computing this gradient, we must calculate the fol-

We can justify the second step since feature counts are local to hyperedges, i.e.
The posterior
c(φk , d) =
h∈d c(φk , h).
probability Pθ (h|fi ) can be efﬁciently computed
with inside-outside scores. Let I(u) and O(u) be
the standard inside and outside scores for a node
u in the forest.5
Pθ (h|fi ) =

1
w(h) O(head(h))
Z(f )

I(v)

v∈tail(h)

where w(h) is the weight of hyperedge h, given
by exp(θ Φ(h)), and Z(f ) = I(root) is the inside score of the root of the forest. Computing
these inside-outside quantities takes time linear in
the number of hyperedges in the forest.
3.3

Computing n-gram Expectations

We can compute the expectations of any speciﬁc
n-grams, or of total n-gram counts , in the same
way as feature expectations, provided that targetside n-grams are also localized to hyperedges (e.g.
consider to be a feature of a hyperedge whose
value is the number of n-grams on h). If the
nodes in our forests are annotated with target-side
5

Appendix Figure 7 gives recursions for I(u) and O(u).

boundary words as in Figure 3, then this will be the
case. Note that this is the same approach used by
decoders which integrate a target language model
(e.g. Chiang (2007)). Other work has computed
n-gram expectations in the same way (DeNero et
al., 2009; Li et al., 2009).
3.4

Computing Expectations of Products of
Counts

While the previous two expectations can be computed using techniques known in the literature, the
expected product of counts Eθ [c(φk , d) · n (d)|fi ]
is a novel quantity. Fortunately, an efﬁcient dynamic program exists for computing this expectation as well. We present this dynamic program
here as one of the contributions of this paper,
though we omit a full derivation due to space restrictions.
To see why this expectation cannot be computed
in the same way as the expected feature or n-gram
counts, we expand the deﬁnition of the expectation
above to get
Pθ (d|fi ) [c(φk , d) n (d)]
d

Unlike feature and n-gram counts, the product of
counts in brackets above does not decompose over
hyperedges, at least not in an obvious way. We
can, however, still decompose the feature counts
c(φk , d) over hyperedges. After this decomposition and a little re-arranging, we get
=
=
=

Pθ (d|fi ) n (d)

c(φk , h)
h

1
Z(fi )
1
Z(fi )

d:h∈d

w(d) n (d)

c(φk , h)
h

d:h∈d

ˆn
c(φk , h)Dθ (h|fi )
h

ˆn
The quantity Dθ (h|fi ) = d:h∈d w(d) n (d) is the
sum of the weight-length products of all derivations d containing hyperedge h. In the same
way that Pθ (h|fi ) can be efﬁciently computed
from inside and outside probabilities, this quanˆn
tity Dθ (h|fi ) can be efﬁciently computed with two
new inside and outside quantities, which we call
ˆ
ˆn (u) and On (u). We provide recursions for these
I
quantities in Figure 4. Like the standard inside and
outside computations, these recursions run in time
linear in the number of hyperedges in the forest.
While a full exposition of the algorithm is not
possible in the available space, we give some brief

intuition behind this dynamic program. We ﬁrst
deﬁne ˆn (u):
I
ˆn (u) =
I

w(du ) n (d)
du

where du is a derivation rooted at node u. This is
ˆ
a sum of weight-length products similar to D. To
give a recurrence for ˆ we rewrite it:
I,
ˆn (u) =
I

[w(du ) n (h)]
du h∈du

Here, we have broken up the total value of n (d)
across hyperedges in d. The bracketed quantity
is a score of a marked derivation pair (d, h) where
the edge h is some speciﬁc element of d. The score
of a marked derivation includes the weight of the
derivation and the factor n (h) for the marked hyperedge.
This sum over marked derivations gives the inside recurrence in Figure 4 by the following decomposition. For ˆn (u) to sum over all marked
I
derivation pairs rooted at u, we must consider two
cases. First, the marked hyperedge could be at the
root, in which case we must choose child derivations from regular inside scores and multiply in the
local n , giving the ﬁrst summand of ˆn (u). AlterI
natively, the marked hyperedge is in exactly one
of the children; for each possibility we recursively
choose a marked derivation for one child, while
the other children choose regular derivations. The
second summand of ˆn (u) compactly expresses
I
ˆ
a sum over instances of this case. On (u) decomposes similarly: the marked hyperedge could
be local (ﬁrst summand), under a sibling (second
summand), or higher in the tree (third summand).
Once we have these new inside-outside quantiˆ
ties, we can compute D as in Figure 5. This combination states that marked derivations containing
h are either marked at h, below h, or above h.
As a ﬁnal detail, computing the gradient
clip
Cn (θ) (see the Appendix) involves a clipped
version of the expected product of counts, for
ˆ
which a clipped D is required. This quantity can
be computed with the same dynamic program with
a slight modiﬁcation. In Figure 4, we show the difference as a choice point when computing n (h).
3.5

Implementation Details

As stated, the runtime of computing the required
expectations for the objective and gradient is linear in the number of hyperedges in the forest. The

ˆn (u) =
I
h∈IN(u)





w(h) n (h)

ˆn (v)
I

I(v) +
v∈tail(h)

w=v

v∈tail(h)



I(w)







ˆ n (head(h))
 n (h) O(head(h))
ˆn (v)
I
I(w) + O
I(w)
I(v) + O(head(h))
w(h)


w∈tail(h) 
v∈tail(h)
v∈tail(h) w∈tail(h)
h∈OUT(u)

ˆ
On (u) =

v=u

n (h)

v=u

w=v
w=u

w=u

c(gn , h)
computing unclipped counts
c(gn , h)1 [Eθ [c(gn , d)] ≤ c(gn , ri )] computing clipped counts
gn

=

gn

ˆ
Figure 4: Inside and Outside recursions for ˆn (u) and On (u). IN(u) and OUT(u) refer to the incoming and
I
outgoing hyperedges of u, respectively. I(·) and O(·) refer to standard inside and outside quantities, deﬁned in
ˆ
Appendix Figure 7. We initialize with ˆn (u) = 0 for all terminal forest nodes u and On (root) = 0 for the root
I
node. n (h) computes the sum of all n-grams of order n on a hyperedge h.

ˆn
Dθ (h|fi ) =




w(h)  n (h)O(head(h))


ˆn (v)
I

I(v) + O(head(h))

v∈tail(h)

v∈tail(h)

ˆ
I(w) + On (head(h))

v∈tail(h)
w=v


I(w)


w∈tail(h)

ˆ
ˆn
I
Figure 5: Calculation of Dθ (h|fi ) after ˆn (u) and On (u) have been computed.

number of hyperedges is very large, however, because we must track n-gram contexts in the nodes,
just as we would in an integrated language model
decoder. These contexts are required both to correctly compute the model score of derivations and
to compute clipped n-gram counts. To speed our
computations, we use the cube pruning method of
Huang and Chiang (2007) with a ﬁxed beam size.
For regularization, we added an L2 penalty on
the size of θ to the CoBLEU objective, a simple
addition for gradient ascent. We did not ﬁnd that
our performance varied very much for moderate
levels of regularization.
3.6

Related Work

The calculation of expected counts can be formulated using the expectation semiring framework of Eisner (2002), though that work does
not show how to compute expected products of
counts which are needed for our gradient calculations. Concurrently with this work, Li and Eisner (2009) have generalized Eisner (2002) to compute expected products of counts on translation
forests. The training algorithm of Kakade et al.
(2002) makes use of a dynamic program similar to

ours, though specialized to the case of sequence
models.

4



Consensus Decoding

Once model parameters θ are learned, we must
select an appropriate decoding objective. Several new decoding approaches have been proposed
recently that leverage some notion of consensus
over the many weighted derivations in a translation forest. In this paper, we adopt the fast consensus decoding procedure of DeNero et al. (2009),
which directly complements CoBLEU tuning. For
a source sentence f , we ﬁrst build a translation
forest, then compute the expected count of each
n-gram in the translation of f under the model.
We extract a k-best list from the forest, then select
the translation that yields the highest BLEU score
relative to the forest’s expected n-gram counts.
Speciﬁcally, let BLEU(e; r) compute the similarity of a sentence e to a reference r based on
the n-gram counts of each. When training with
CoBLEU, we replace e with expected counts and
maximize θ. In consensus decoding, we replace r
with expected counts and maximize e.
Several other efﬁcient consensus decoding pro-

1.0

Fraction of Value at Convergence

5

Figure 6: Trajectories of MERT and CoBLEU during optimization show that MERT is initially unstable,
while CoBLEU training follows a smooth path to convergence. Because these two training procedures optimize different functions, we have normalized each
trajectory by the ﬁnal objective value at convergence.
Therefore, the absolute values of this plot do not reﬂect the performance of either objective, but rather
the smoothness with which the ﬁnal objective is approached. The rates of convergence shown in this plot
are not directly comparable. Each iteration for MERT
above includes 10 iterations of coordinate ascent, followed by a decoding pass through the training set. Each
iteration of CoBLEU training involves only one gradient step.

Experiments

We compared CoBLEU training with an implementation of minimum error rate training on two
language pairs.
5.1

Model

Our optimization procedure is in principle
tractable for any syntactic translation system. For
simplicity, we evaluate the objective using an Inversion Transduction Grammar (ITG) (Wu, 1997)
that emits phrases as terminal productions, as in
(Cherry and Lin, 2007). Phrasal ITG models have
been shown to perform comparably to the state-ofthe art phrase-based system Moses (Koehn et al.,
2007) when using the same phrase table (Petrov et
al., 2008).
We extract a phrase table using the Moses
pipeline, based on Model 4 word alignments generated from GIZA++ (Och and Ney, 2003). Our ﬁnal ITG grammar includes the ﬁve standard Moses
features, an n-gram language model, a length feature that counts the number of target words, a feature that counts the number of monotonic ITG
rewrites, and a feature that counts the number of
inverted ITG rewrites.
5.2

Data

We extracted phrase tables from the SpanishEnglish and French-English sections of the Europarl corpus, which include approximately 8.5
million words of bitext for each of the language
pairs (Koehn, 2002). We used a trigram language model trained on the entire corpus of English parliamentary proceedings provided with the
Europarl distribution and generated according to
the ACL 2008 SMT shared task speciﬁcations.6
For tuning, we used all sentences from the 2007
SMT shared task up to length 25 (880 sentences
6

See http://www.statmt.org/wmt08 for details.

0.2

0.4

0.6

0.8

cedures would similarly beneﬁt from a tuning procedure that aggregates over derivations. For instance, Blunsom and Osborne (2008) select the
translation sentence with highest posterior probability under the model, summing over derivations.
Li et al. (2009) propose a variational approximation maximizing sentence probability that decomposes over n-grams. Tromble et al. (2008) minimize risk under a loss function based on the linear Taylor approximation to BLEU, which decomposes over n-gram posterior probabilities.

0.0

CoBLEU
MERT

2

4

6

8

10

Iterations

for Spanish and 923 for French), and we tested on
the subset of the ﬁrst 1000 development set sentences which had length at most 25 words (447
sentences for Spanish and 512 for French).
5.3

Tuning Optimization

We compared two techniques for tuning the nine
log-linear model parameters of our ITG grammar.
We maximized CoBLEU using gradient ascent, as
described above. As a baseline, we maximized
BLEU of the Viterbi translation derivations using
minimum error rate training. To improve optimization stability, MERT used a cumulative k-best
list that included all translations generated during
the tuning process.
One of the beneﬁts of CoBLEU training is that
we compute expectations efﬁciently over an entire
forest of translations. This has substantial stability beneﬁts over methods based on k-best lists. In
Figure 6, we show the progress of CoBLEU as
compared to MERT. Both models are initialized
from 0 and use the same features. This plot exhibits a known issue with MERT training: because
new k-best lists are generated at each iteration,
the objective function can change drastically between iterations. In contrast, CoBLEU converges

Consensus Decoding

MERT
CoBLEU
MERT→CoBLEU

Tune
32.5
31.4
31.7

MERT
CoBLEU
MERT→CoBLEU

Tune
32.5
31.9
32.4

Spanish
Test
∆
30.2
-2.3
30.4
-1.0
30.8
-0.9
French
Test
∆
31.1* -1.4
30.9
-1.0
31.2* -0.8

Viterbi Decoding

Br.
0.992
0.992
0.992
Br.
0.972
0.954
0.953

Table 1: Performance measured by BLEU using a consensus decoding method over translation forests shows
an improvement over MERT when using CoBLEU
training. The ﬁrst two conditions were initialized by
0 vectors. The third condition was initialized by the
ﬁnal parameters of MERT training. Br. indicates the
brevity penalty on the test set. The * indicates differences which are not statistically signiﬁcant.

smoothly to its ﬁnal objective because the forests
do not change substantially between iterations, despite the pruning needed to track n-grams. Similar
stability beneﬁts have been observed for latticebased MERT (Macherey et al., 2008).
5.4

Results

We performed experiments from both French and
Spanish into English under three conditions. In the
ﬁrst two, we initialized both MERT and CoBLEU
training uniformly with zero weights and trained
until convergence. In the third condition, we initialized CoBLEU with the ﬁnal parameters from
MERT training, denoted MERT→CoBLEU in the
results tables. We evaluated each of these conditions on both the tuning and test sets using the consensus decoding method of DeNero et al. (2009).
The results appear in Table 1.
In Spanish-English, CoBLEU slightly outperformed MERT under the same initialization, while
the opposite pattern appears for French-English.
The best test set performance in both language
pairs was the third condition, in which CoBLEU
training was initialized with MERT. This condition also gave the highest CoBLEU objective
value. This pattern indicates that CoBLEU is a
useful objective for translation with consensus decoding, but that the gradient ascent optimization is
getting stuck in local maxima during tuning. This
issue can likely be addressed with annealing, as
described in (Smith and Eisner, 2006).
Interestingly, the brevity penatly results in
French indicate that, even though CoBLEU did

MERT
MERT→CoBLEU

MERT
MERT→CoBLEU

Spanish
Test
30.2
30.9
French
Tune Test
32.0 31.0
31.7 30.9
Tune
32.5
30.5

∆
-2.3
+0.4
∆
-1.0
-0.8

Table 2: Performance measured by BLEU using Viterbi
decoding indicates that CoBLEU is less prone to overﬁtting than MERT.

not outperform MERT in a statistically signiﬁcant
way, CoBLEU tends to ﬁnd shorter sentences with
higher n-gram precision than MERT.
Table 1 displays a second beneﬁt of CoBLEU
training: compared to MERT training, CoBLEU
performance degrades less from tuning to test
set. In Spanish, initializing with MERT-trained
weights and then training with CoBLEU actually
decreases BLEU on the tuning set by 0.8 points.
However, this drop in tuning performance comes
with a corresponding increase of 0.6 on the test
set, relative to MERT training. We see the same
pattern in French, albeit to a smaller degree.
While CoBLEU ought to outperform MERT using consensus decoding, we expected that MERT
would give better performance under Viterbi decoding. Surprisingly, we found that CoBLEU
training actually outperformed MERT in SpanishEnglish and performed equally well in FrenchEnglish. Table 2 shows the results. In these experiments, we again see that CoBLEU overﬁt the
training set to a lesser degree than MERT, as evidenced by a smaller drop in performance from tuning to test set. In fact, test set performance actually
improved for Spanish-English CoBLEU training
while dropping by 2.3 BLEU for MERT.

6

Conclusion

CoBLEU takes a fundamental quantity used in
consensus decoding, expected n-grams, and trains
to optimize a function of those expectations.
While CoBLEU can therefore be expected to increase test set BLEU under consensus decoding, it
is more surprising that it seems to better regularize
learning even for the Viterbi decoding condition.
It is also worth emphasizing that the CoBLEU approach is applicable to functions of expected ngram counts other than BLEU.

Appendix: The Gradient of CoBLEU
We would like to compute the gradient of
1−

m
i=1

1
ln
4 n=1

−

gn min{Eθ [c(gn , d)|fi ], c(gn , ri )}
m
i=1
gn Eθ [c(gn , d)|fi ]

To simplify notation, we use Ei [·] to mean
θ
Eθ [·|fi ]. We also introduce the functions
m

m

Cn (θ) =

Ei
θ

[ n (d)] =

Ei [c(gn , e)]
θ
i=1 gn

i=1

=
h∈IN(u)

O(u)

m
i=1

4

+

|R|
g1 Eθ [c(g1 , d)|fi ]

I(u)

w(h) 

min{Ei [c(gn , d)], c(r, gn )}
θ
i=1 gn

h∈OU T (u)

1−
1
4

|R|
C1 (θ)

4
n=1

clip
ln Cn (θ) −

1
4

−
4

ln Cn (θ)
n=1

Ei [c(φk , d) ·
θ

∂ i
E [c(gn , d)] =
∂θk θ

n (d)]

− Ei [ n (d)] · Ei [c(φk , d)]
θ
θ

which can be derived by expanding the expectation on
the left-hand side

gn

d

∂
Pθ (d|fi )c(gn , d)
∂θk

and substituting
∂
Pθ (d|fi ) =
∂θk
Pθ (d|fi )c(φk , d) − Pθ (d|fi )

d

Pθ (d |fi )c(φk , d )

Using this identity and some basic calculus, the
gradient Cn (θ) is
m
i=1

Ei [c(φk , d) ·
θ

I(v)


w(h) O(head(h))


v∈tail(h)
v=u




I(v)


Figure 7: Standard Inside-Outside recursions which
compute I(u) and O(u). IN(u) and OUT(u) refer to the
incoming and outgoing hyperedges of u, respectively.
We initialize with I(u) = 1 for all terminal forest nodes
u and O(root) = 1 for the root node. These quantities
are referenced in Figure 4.
clip

Cn (θ) is given by

and the gradient

n (d)]

i=1 gn

Ei [c(gn , d) · c(φk , d)]
θ

·1 Ei [c(gn , d)] ≤ c(gn , ri )
θ
− min{Ei [c(gn , d)], c(r, gn )}Ei [c(φk , d)]
θ
θ

where 1 denotes an indicator function. At the top
level, the gradient of the ﬁrst term (the brevity
penalty) is
|R| C1 (θ)
1 C1 (θ) ≤ |R|
C1 (θ)2
The gradient of the second term is

We ﬁrst state an identity:

gn



m

Cn (θ) represents the sum of the expected counts
of all n-grams or order n in all translations of
clip
the source corpus F , while Cn (θ) represents the
sum of the same expected counts, but clipped with
reference counts c(gn , ri ).
With this notation, we can write our objective
function CoBLEU(R, F, θ) in three terms:

+

v∈tail(h)



=

m
clip
Cn (θ) =



− Ei [ n (d)] Ei [c(φk , d)]
θ
θ

1
4

4

clip

Cn (θ)
clip

n=1

Cn (θ)

and the gradient of the third term is
1
−
4

4
n=1

Cn (θ)
Cn (θ)

Note that, because of the indicator functions, CoBLEU is non-differentiable when
Eθ [c(gn , d)|fi ] = c(gn , ri ) or Cn (θ) = |R|.
Formally, we must compute a sub-gradient at
these points. In practice, we can choose between
the gradients calculated assuming the indicator
function is 0 or 1; we always choose the latter.

References
Phil Blunsom and Miles Osborne. 2008. Probabilistic
inference for machine translation. In Proceedings
of the Conference on Emprical Methods for Natural
Language Processing.
Colin Cherry and Dekang Lin. 2007. Inversion transduction grammar for joint phrasal translation modeling. In The Annual Conference of the North American Chapter of the Association for Computational
Linguistics Workshop on Syntax and Structure in
Statistical Translation.
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and structural translation features. In The Conference on Empirical Methods in Natural Language Processing.
David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics.
John DeNero, David Chiang, and Kevin Knight. 2009.
Fast consensus decoding over translation forests. In
The Annual Conference of the Association for Computational Linguistics.
Jason Eisner. 2002. Parameter estimation for probabilistic ﬁnite-state transducers. In Proceedings of
the 40th Annual Meeting on Association for Computational Linguistics.
Liang Huang and David Chiang. 2007. Forest rescoring: Faster decoding with integrated language models. In The Annual Conference of the Association for
Computational Linguistics.
Sham Kakade, Yee Whye Teh, and Sam T. Roweis.
2002. An alternate objective function for markovian
ﬁelds. In Proceedings of ICML.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
The Annual Conference of the Association for Computational Linguistics.
Philipp Koehn. 2002. Europarl: A multilingual corpus
for evaluation of machine translation.
Shankar Kumar, Wolfgang Macherey, Chris Dyer,
and Franz Och. 2009. Efﬁcient minimum error
rate training and minimum Bayes-risk decoding for
translation hypergraphs and lattices. In The Annual
Conference of the Association for Computational
Linguistics.
Zhifei Li and Jason Eisner. 2009. First- and secondorder expectation semirings with applications to
minimum-risk training on translation forests. In
Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing.
Zhifei Li, Jason Eisner, and Sanjeev Khudanpur. 2009.
Variational decoding for statistical machine translation. In The Annual Conference of the Association
for Computational Linguistics.

W. Macherey, F. Och, I. Thayer, and J. Uszkoreit.
2008. Lattice-based minimum error rate training for
statistical machine translation. In In Proceedings of
Empirical Methods in Natural Language Processing.
Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment
models. Computational Linguistics, 29:19–51.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st Annual Meeting on Association for Computational Linguistics (ACL), pages 160–167, Morristown, NJ, USA. Association for Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: A method for automatic
evaluation of machine translation. In The Annual
Conference of the Association for Computational
Linguistics.
Slav Petrov, Aria Haghighi, and Dan Klein. 2008.
Coarse-to-ﬁne syntactic machine translation using language projections. In Proceedings of the
2008 Conference on Empirical Methods in Natural Language Processing, pages 108–116, Honolulu, Hawaii, October. Association for Computational Linguistics.
David Smith and Jason Eisner. 2006. Minimum risk
annealing for training log-linear models. In In Proceedings of the Association for Computational Linguistics.
Roy Tromble, Shankar Kumar, Franz Och, and Wolfgang Macherey. 2008. Lattice minimum Bayes-risk
decoding for statistical machine translation. In The
Conference on Empirical Methods in Natural Language Processing.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23:377–404.

