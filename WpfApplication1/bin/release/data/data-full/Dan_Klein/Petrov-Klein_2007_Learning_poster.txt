Learning and Inference for Hierarchically Split Probabilistic Context-Free Grammars
Slav Petrov and Dan Klein
University of California, Berkeley
Inference

Grammar Extraction:

Parsing:
Grammar
S → NP VP .
1.0
NP → PRP
0.5
NP → DT NN 0.5
...

PRP VBD
She

.

VP
NP

heard DT

.

All NPs

NPs under S

NPs under VP
23%

21%
11%

9%

9%

9%

7%

6%

NN

Lexicon
PRP → She
DT → the
...

the noise

S

Observed categories are too coarse:

1.0
1.0

NP PP

DT NN

The Golden Gate
bridge is red.

DT

NP PP

PRP

DT NN

PRP

NP PP

DT NN

NP-x

VP-x

heard

NP-x

G0
this (0.14)

the (0.50)

this (0.39)
that (0.28)
That (0.11)

DT-x NN-x
noise

the (0.54)

some (0.20)
all (0.19)
those (0.12)

a (0.25)

The (0.09)

the (0.80)
The (0.15)
a (0.01)

a (0.61)
the (0.19)
an (0.10)

The (0.93)
A(0.02)
No(0.01)

X-Bar=G0

Evolution of the DT tag during hierarchical splitting and merging.
Shown are the top three words for each subcategory and their respective probability.

Merging:
Roll back the least useful
splits in order to allocate
complexity only where
needed.

π1(G)

G2

π2(G)

G3

π3(G)
π4(G)
π5(G)

G= G6
VP

L

2

1

NP
1

DT

2

NN
1

2

1

2

L

With split at NP node

2

NP
1

DT

NN
1

G

2

1

2

With split at NP node reversed

He

VP-11

was

NP

88

89.6
90.1
80.1

86
84
82
50% Merging and Smoothing

80

50% Merging

78

Hierarchical Training

76

Flat Training

74
100

76.6
83.4

300

500
700
900
Total number of grammar symbols

1100

DT

gives
comes
includes
puts
says
believes
expects
plans
is
’s
has
does

sells
goes
owns
provides
adds
means
makes
expects
’s
is
’s
Is
NNP
Jr.
Goldman
Bush
Noriega
J.
E.
York
Francisco
Inc
Exchange
Inc.
Corp.
Stock
Exchange
Corp.
Inc.
Congress
Japan
Friday
September
Shearson
D.
U.S.
Treasury
John
Robert
Mr.
Ms.
Oct.
Nov.
New
San
JJS
largest
latest
least
best
most
Most

takes
works
is
takes
Says
thinks
calls
wants
gets
remains
is
Does

DT-0
DT-1
DT-2
DT-3
DT-4
DT-5
DT-6
DT-7
DT-8
DT-9
DT-10
DT-11

INC.
Peters
L.
Street
Co
Co.
York
Group
IBM
August
Ford
Senate
James
President
Sept.
Wall

CD-0
CD-1
CD-2
CD-3
CD-4
CD-5
CD-6
CD-7
CD-8
CD-9
CD-10
CD-11

biggest
worst
least

RBR-0
RBR-1
RBR-2

PRP-0
PRP-1
PRP-2

the
A
The
The
all
some
That
this
the
no
an
a

The
An
No
Some
those
these
This
that
The
any
a
this
CD
1
50
8.50
15
8
10
1
30
1989
1990
1988
1987
two
three
one
One
12
34
78
58
one
two
million billion
PRP
It
He
it
he
it
them
RBR
further lower
more
less
earlier Earlier

IN
a
Another
This
These
some
both
each
each
a
some
the
the
100
1.2
20
31
1988
1990
ﬁve
Three
14
34
three
trillion
I
they
him
higher
More
later

IN-0
IN-1
IN-2
IN-3
IN-4
IN-5
IN-6
IN-7
IN-8
IN-9
IN-10
IN-11
IN-12
IN-13
IN-14
IN-15

In
In
in
of
from
at
by
for
If
because
whether
that
about
as
than
out

RB-0
RB-1
RB-2
RB-3
RB-4
RB-5
RB-6
RB-7
RB-8
RB-9
RB-10
RB-11
RB-12
RB-13
RB-14
RB-15

recently
here
very
so
also
however
much
even
as
only
ago
rather
back
up
not
n’t

With
For
for
for
on
for
in
with
While
if
if
like
over
de
ago
up
RB
previously
back
highly
too
now
Now
far
well
about
just
earlier
instead
close
down
Not
not

After
At
on
on
with
by
with
on
As
while
That
whether
between
Up
until
down
still
now
relatively
as
still
However
enough
then
nearly
almost
later
because
ahead
off
maybe
also

Extensions

NP

right

Hierarchical Dirichlet Processes as a
nonparametric Bayesian alternative
to split and merge:

S
VP

.

PRP VBD ADJP .

VP-23

.

He

was

right

He

was

right

Parse Tree

Parse Derivations

d
b

ae
c

b

d
c

b

c

Start

φT
z

β

z2

φE
z
z

Parameters

Reference:
Slav Petrov and Dan Klein,
"Improved Inference for Unlexicalized Parsing", in NAACL-HLT '07

Automatic reﬁnement of acoustic models
learns phone-internal structure as well as
phone-external context:

z1

φB
z

PRP-2 VBD-12 ADJP-11 .

Reference:
Slav Petrov, Leon Barrett, Romain Thibaux and Dan Klein,
"Learning accurate, compact and interpretable tree annotation", in ACL-COLING '06

90

The most frequent three words in the subcategories of several part-of-speech tags.

.

S-2
NP-17

NP1 NP2 NP3 NP4 NP5 NP6 NP7 NP8

Parsing accuracy for different models

S-1

PRP-2 VBD-16 ADJP-14 .

Smoothing:

JJS-0
JJS-1
JJS-2

Use a variational approximation to select the tree with
the maximum number of expected correct rules (since
computing the best parse tree is intractable and
selecting the best derivation is a poor approximation).

NP-10

Reduce overﬁtting by shrinking the
productions of each subcategory
towards their common base
category.

NNP-0
NNP-1
NNP-2
NNP-3
NNP-4
NNP-5
NNP-6
NNP-7
NNP-8
NNP-9
NNP-10
NNP-11
NNP-12
NNP-13
NNP-14
NNP-15

Parse Selection:

VP
1

VBZ-0
VBZ-1
VBZ-2
VBZ-3
VBZ-4
VBZ-5
VBZ-6
VBZ-7
VBZ-8
VBZ-9
VBZ-10
VBZ-11

π0(G)

G1

a (0.75)
an (0.12)
the (0.03)

all
F1

VBZ

Projection πi

the (0.96)
a (0.01)
The (0.01)

≤ 40 words
Parser
F1
ENGLISH
Charniak & Johnson ’05
90.1
This Work
90.6
GERMAN
Dubey ’05
76.3
This Work
80.8
CHINESE
Chiang & Bikel ’02
80.0
This Work
86.3

Learned grammars are compact and interpretable:

Compute grammars of intermediate
complexity by projecting the most
reﬁned grammar.

Learning

these (0.27)
both (0.21)
Some (0.15)

Interactive demo and download at http://nlp.cs.berkeley.edu

NP3 NP4 ... G3

G5

some (0.37)
all (0.29)
those (0.14)

G1

... VP3 VP4 ... NP3 NP4 G2

G4

3

That (0.38)
This (0.34)
each (0.07)

G0

... VP1 VP2 NP1 NP2 ...

The (0.08)

some (0.11)

JJ

.

... VP6 VP7 ...
DT
a (0.24)

is

Bracket posterior
probabilities during
coarse-to-ﬁne decoding

... VP NP QP ...

Hierarchical Splitting:

this (0.52)
that (0.36)
another (0.04)

bridge

VBZ ADJP .

red

.-x

the

G2

Gate

Rapidly pre-parse the sentence in a
hierarchical coarse-to-ﬁne fashion
pruning away unlikely chart items.

PRP-x VBD-x
She

that (0.15)

NN

PRP

S-x

.

VP

NNP

Parse Efﬁciency:

Split each category in k subcategories
and ﬁt grammar with the EM algorithm.

G1

NNP

The Golden

4%

Adaptive Grammar Reﬁnement:

Repeatedly split each
annotation symbol in
two and retrain the
grammar, initializing
with the previous
grammar.
G

NP

General technique for learning reﬁned, structured models when only
the trace of a complex underlying process is observed.
Learns compact and accurate grammars from a treebank without
additional human input.
Gives best known parsing accuracy on a variety of languages, while
being extremely efﬁcient.

Influential
members
of
the
House
Ways
and
Means
Committee
introduced
legislation
that
would
restrict
how
the
new
s&l
bailout
agency
can
raise
capital
;
creating
another
potential
obstacle
to
the
government
‘s
sale
of
sick
thrifts
.

S
NP

Results

Parsing accuracy (F1)

Learning

∞

a

z3

x2

End
d

a

d

a

d

x3
T

Trees

Reference:
Percy Liang, Slav Petrov, Michael Jordan and Dan Klein,
"The Infinite PCFG using Hierarchical Dirichlet Processes",
in EMNLP-CoNLL '07

Reference:
Slav Petrov, Adam Pauls and Dan Klein,
"Learning Structured Models for Phone Recognition",
in EMNLP-CoNLL '07

