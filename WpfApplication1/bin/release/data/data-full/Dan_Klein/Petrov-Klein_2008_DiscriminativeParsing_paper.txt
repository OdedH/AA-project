Sparse Multi-Scale Grammars
for Discriminative Latent Variable Parsing
Slav Petrov and Dan Klein
Computer Science Division, EECS Department
University of California at Berkeley
Berkeley, CA 94720
{petrov, klein}@eecs.berkeley.edu

Abstract

category-splitting approach, in which a coarse initial grammar is reﬁned by iteratively splitting each
grammar category into two subcategories using the
EM algorithm. Of course, each time the number of
grammar categories is doubled, the number of binary productions is increased by a factor of eight.
As a result, while our ﬁnal grammars used few categories, the number of total active (non-zero) productions was still substantial (see Section 7). In addition, it is reasonable to assume that some generatively learned splits have little discriminative utility.
In this paper, we present a discriminative approach
which addresses both of these limitations.

We present a discriminative, latent variable
approach to syntactic parsing in which rules
exist at multiple scales of reﬁnement. The
model is formally a latent variable CRF grammar over trees, learned by iteratively splitting
grammar productions (not categories). Different regions of the grammar are reﬁned to
different degrees, yielding grammars which
are three orders of magnitude smaller than
the single-scale baseline and 20 times smaller
than the split-and-merge grammars of Petrov
et al. (2006). In addition, our discriminative
approach integrally admits features beyond local tree conﬁgurations. We present a multiscale training method along with an efﬁcient
CKY-style dynamic program. On a variety of
domains and languages, this method produces
the best published parsing accuracies with the
smallest reported grammars.

1 Introduction
In latent variable approaches to parsing (Matsuzaki
et al., 2005; Petrov et al., 2006), one models an observed treebank of coarse parse trees using a grammar over more reﬁned, but unobserved, derivation
trees. The parse trees represent the desired output
of the system, while the derivation trees represent
the typically much more complex underlying syntactic processes. In recent years, latent variable methods have been shown to produce grammars which
are as good as, or even better than, earlier parsing
work (Collins, 1999; Charniak, 2000). In particular,
in Petrov et al. (2006) we exhibited a very accurate

We introduce multi-scale grammars, in which
some productions reference ﬁne categories, while
others reference coarse categories (see Figure 2).
We use the general framework of hidden variable
CRFs (Lafferty et al., 2001; Koo and Collins, 2005),
where gradient-based optimization maximizes the
likelihood of the observed variables, here parse
trees, summing over log-linearly scored derivations.
With multi-scale grammars, it is natural to reﬁne
productions rather than categories. As a result, a
category such as NP can be complex in some regions of the grammar while remaining simpler in
other regions. Additionally, we exploit the ﬂexibility
of the discriminative framework both to improve the
treatment of unknown words as well as to include
span features (Taskar et al., 2004), giving the beneﬁt of some input features integrally in our dynamic
program. Our multi-scale grammars are 3 orders
of magnitude smaller than the fully-split baseline
grammar and 20 times smaller than the generative
split-and-merge grammars of Petrov et al. (2006).

867
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 867–876,
Honolulu, October 2008. c 2008 Association for Computational Linguistics

In addition, we exhibit the best parsing numbers on
several metrics, for several domains and languages.
Discriminative parsing has been investigated before, such as in Johnson (2001), Clark and Curran
(2004), Henderson (2004), Koo and Collins (2005),
Turian et al. (2007), Finkel et al. (2008), and, most
similarly, in Petrov and Klein (2008). However, in
all of these cases, the ﬁnal parsing performance fell
short of the best generative models by several percentage points or only short sentences were used.
Only in combination with a generative model was
a discriminative component able to produce high
parsing accuracies (Charniak and Johnson, 2005;
Huang, 2008). Multi-scale grammars, in contrast,
give higher accuracies using smaller grammars than
previous work in this direction, outperforming top
generative models in grammar size and in parsing
accuracy.

2.2

Log-Linear Latent Variable Grammars

In a log-linear latent variable grammar, each production r = Ax → By Cz is associated with a
multiplicative weight φr (Johnson, 2001; Petrov and
Klein, 2008) (sometimes we will use the log-weight
θr when convenient). The probability of a derivation
t of a sentence w is proportional to the product of the
weights of its productions r:
P (t|w) ∝

φr
r∈t

The score of a parse T is then the sum of the scores
of its derivations:

2 Latent Variable Parsing
Treebanks are typically not annotated with fully detailed syntactic structure. Rather, they present only
a coarse trace of the true underlying processes. As
a result, learning a grammar for parsing requires
the estimation of a more highly articulated model
than the naive CFG embodied by such treebanks.
A manual approach might take the category NP and
subdivide it into one subcategory NPˆS for subjects
and another subcategory NPˆVP for objects (Johnson, 1998; Klein and Manning, 2003). However,
rather than devising linguistically motivated features
or splits, latent variable parsing takes a fully automated approach, in which each symbol is split into
unconstrained subcategories.
2.1

(Matsuzaki et al., 2005; Liang et al., 2007) and discriminative approaches (Petrov and Klein, 2008).
We take the discriminative log-linear approach here.
Note that the comparison is only between estimation
methods, as Smith and Johnson (2007) show that the
model classes are the same.

Latent Variable Grammars

Latent variable grammars augment the treebank
trees with latent variables at each node. This creates a set of (exponentially many) derivations over
split categories for each of the original parse trees
over unsplit categories. For each observed category
A we now have a set of latent subcategories Ax . For
example, NP might be split into NP1 through NP8 .
The parameters of the reﬁned productions
Ax → By Cz , where Ax is a subcategory of A, By
of B, and Cz of C, can then be estimated in various ways; past work has included both generative
868

P (t|w)

P (T |w) =
t∈T

3 Hierarchical Reﬁnement
Grammar reﬁnement becomes challenging when the
number of subcategories is large. If each category
is split into k subcategories, each (binary) production will be split into k3 . The resulting memory limitations alone can prevent the practical learning of
highly split grammars (Matsuzaki et al., 2005). This
issue was partially addressed in Petrov et al. (2006),
where categories were repeatedly split and some
splits were re-merged if the gains were too small.
However, while the grammars are indeed compact
at the (sub-)category level, they are still dense at the
production level, which we address here.
As in Petrov et al. (2006), we arrange our subcategories into a hierarchy, as shown in Figure 1. In
practice, the construction of the hierarchy is tightly
coupled to a split-based learning process (see Section 5). We use the naming convention that an original category A becomes A0 and A1 in the ﬁrst round;
A0 then becoming A00 and A01 in the second round,
and so on. We will use x ≻ x to indicate that the
ˆ
subscript or subcategory x is a reﬁnement of x.1 We
ˆ
1

Conversely, x is a coarser version of x, or, in the language
ˆ
of Petrov and Klein (2007), x is a projection of x.
ˆ

θr
ˆ

s

n

o

i

t

c

u

d

o

r

.
.

3

5

a

c

s



i

t

l

u

s

M

1

t

→

0

0

T

D

n

o

i

t

c

u

d

o

r

+

7

e

h

t

→

0

1

0

T

.

0

.

0

.

3

h

e
2

l

p

e

l

a

c

s



e

l

g

n

i

S
→

e
0

.

e

*

5

h

t

→

0

0

0

T

D

+
e

5

h

t

→

1

0

0

T

*

D

+

D

e

+

7

h

t

→

0

1

0

T

D

+

0

1

0

+
e
2
1

0

0
0

.

5

1

1
1

0

0
+

3

.

1
7

h

t

→

1

1

0

T

D

.

e

+

2

+
1

2

1

0
2

+

e

h

t

→

1

T

0
+

D

}

1

p

}

→

θr
¯

1

h

.

1

.

1

.

1

t

→

1

1

0

T

D

+
e

.

1

2

h

t

→

0

0

1

T

D

+
1
e

2

h

→

1

0

1

T

h

1

1

0

0

t

→

0

1

1

T

0

D
D

+
1
1
e

2

0

+
e

2

t

1

h

t

→

1

1

1

T

.

1
2

1

0
+

1

.

1
2

1

1
+

1

.

0
2

1

0
+

1

.

0
2

1

1
+

2

1

1

0

0
+

3

.

1
7

0

1
+

0

.

0
5

0

0
+

0

.

0
5

0
+

D

+

Figure 1: Multi-scale reﬁnement of the DT → the production. The multi-scale grammar can be encoded much more
compactly than the equally expressive single scale grammar by using only the shaded features along the fringe.

will also say that x dominates x, and x will refer to
ˆ
fully reﬁned subcategories. The same terminology
can be applied to (binary) productions, which split
into eight reﬁnements each time the subcategories
are split in two.
The core observation leading to multi-scale grammars is that when we look at the reﬁnements of a
production, many are very similar in weight. It is
therefore advantageous to record productions only at
the level where they are distinct from their children
in the hierarchy.

4 Multi-Scale Grammars
A multi-scale grammar is a grammar in which some
productions reference ﬁne categories, while others
reference coarse categories. As an example, consider the multi-scale grammar in Figure 2, where the
NP category has been split into two subcategories
(NP0 , NP1 ) to capture subject and object distinctions. Since it can occur in subject and object position, the production NP → it has remained unsplit.
In contrast, in a single-scale grammar, two productions NP0 → it and NP1 → it would have been necessary. We use * as a wildcard, indicating that NP∗
can combine with any other NP, while NP1 can only
combine with other NP1 . Whenever subcategories
of different granularity are combined, the resulting
constituent takes the more speciﬁc label.
In terms of its structure, a multi-scale grammar is
a set of productions over varyingly reﬁned symbols,
where each production is associated with a weight.
Consider the reﬁnement of the production shown in
Figure 1. The original unsplit production (at top)
would naively be split into a tree of many subproductions (downward in the diagram) as the grammar
categories are incrementally split. However, it may
be that many of the fully reﬁned productions share
869

the same weights. This will be especially common
in the present work, where we go out of our way to
achieve it (see Section 5). For example, in Figure 1,
the productions DTx → the have the same weight
for all categories DTx which reﬁne DT1 .2 A multiscale grammar can capture this behavior with just 4
productions, while the single-scale grammar has 8
productions. For binary productions the savings will
of course be much higher.
In terms of its semantics, a multi-scale grammar is
simply a compact encoding of a fully reﬁned latent
variable grammar, in which identically weighted reﬁnements of productions have been collapsed to the
coarsest possible scale. Therefore, rather than attempting to control the degree to which categories
are split, multi-scale grammars simply encode productions at varying scales. It is hence natural to
speak of reﬁning productions, while considering
the categories to exist at all degrees of reﬁnement.
Multi-scale grammars enable the use of coarse (even
unsplit) categories in some regions of the grammar,
while requiring very speciﬁc subcategories in others,
as needed. As we will see in the following, this ﬂexibility results in a tremendous reduction of grammar
parameters, as well as improved parsing time, because the vast majority of productions end up only
partially split.
Since a multi-scale grammar has productions
which can refer to different levels of the category
hierarchy, there must be constraints on their coherence. Speciﬁcally, for each fully reﬁned production, exactly one of its dominating coarse productions must be in the grammar. More formally, the
multi-scale grammar partitions the space of fully reﬁned base rules such that each r maps to a unique
2

We deﬁne dominating productions and reﬁning productions
analogously as for subcategories.

:

5.1

Hierarchical Training

*

S

*

P

V

0

P

N

*

P

V

0

P

P

*

V

P

N

0

P

V

0

P

m
*

V

m

G

a

S

N
1

0

r
a

0
*

r

S
P

P

*

N

*

V

P

V

0

P

N

N
:

n

o

c

i

x

e
L

t
P

1

P

1

N

*

V

i

e
1

P

h

N
*

V

s
0

r

N
e

0

*

V

h

P
t

w

a

s

N

0

*

V

P

N

1

P

N

0

P

N

V

i

t
w

a

s

w

a

s

i

r

e

h

e

h
s

Figure 2: In multi-scale grammars, the categories exist
at varying degrees of reﬁnement. The grammar in this
example enforces the correct usage of she and her, while
allowing the use of it in both subject and object position.

We learn discriminative multi-scale grammars in an
iterative fashion (see Figure 1). As in Petrov et al.
(2006), we start with a simple X-bar grammar from
an input treebank. The parameters θ of the grammar
(production log-weights for now) are estimated in a
log-linear framework by maximizing the penalized
log conditional likelihood Lcond − R(θ), where:
Lcond (θ) = log

P(Ti |wi )
i

R(θ) =

|θr |
r

dominating rule r, and for all base rules r′ such that
ˆ
r ≻ r′ , r′ maps to r as well. This constraint is alˆ
ˆ
ways satisﬁed if the multi-scale grammar consists of
fringes of the production reﬁnement hierarchies, indicated by the shading in Figure 1.
A multi-scale grammar straightforwardly assigns
scores to derivations in the corresponding fully reﬁned single scale grammar: simply map each reﬁned
derivation rule to its dominating abstraction in the
multi-scale grammar and give it the corresponding
weight. The fully reﬁned grammar is therefore trivially (though not compactly) reconstructable from
its multi-scale encoding.
It is possible to directly deﬁne a derivational semantics for multi-scale grammars which does not
appeal to the underlying single scale grammar.
However, in the present work, we use our multiscale grammars only to compute expectations of the
underlying grammars in an efﬁcient, implicit way.

5 Learning Sparse Multi-Scale Grammars
We now consider how to discriminatively learn
multi-scale grammars by iterative splitting productions. There are two main concerns. First, because multi-scale grammars are most effective when
many productions share the same weight, sparsity
is very desirable. In the present work, we exploit
L1 -regularization, though other techniques such as
structural zeros (Mohri and Roark, 2006) could
also potentially be used. Second, training requires
repeated parsing, so we use coarse-to-ﬁne chart
caching to greatly accelerate each iteration.
870

We directly optimize this non-convex objective
function using a numerical gradient based method
(LBFGS (Nocedal and Wright, 1999) in our implementation). To handle the non-diferentiability of the
L1 -regularization term R(θ) we use the orthant-wise
method of Andrew and Gao (2007). Fitting the loglinear model involves the following derivatives:
∂Lcond (θ)
=
∂θr

Eθ [fr (t)|Ti ] − Eθ [fr (t)|wi ]
i

where the ﬁrst term is the expected count fr of a production r in derivations corresponding to the correct
parse tree Ti and the second term is the expected
count of the production in all derivations of the sentence wi . Note that r may be of any scale. As we
will show below, these expectations can be computed exactly using marginals from the chart of the
inside/outside algorithm (Lari and Young, 1990).
Once the base grammar has been estimated, all
categories are split in two, meaning that all binary
productions are split in eight. When splitting an already reﬁned grammar, we only split productions
whose log-weight in the previous grammar deviates
from zero.3 This creates a reﬁnement hierarchy over
productions. Each newly split production r is given
a unique feature, as well as inheriting the features of
its parent productions r ≻ r:
ˆ
φr = exp

θr
ˆ
r ≻r
ˆ

The parent productions r are then removed from the
ˆ
grammar and the new features are ﬁt as described
3

L1 -regularization drives more than 95% of the feature
weights to zero in each round.

I(S0 , i, j)

I(S11 , i, j)

in terms of the standard inside scores of the most
reﬁned subcategories Ax :

S
P

V
P

I(Ax , i, j) =

I(Ax , i, j)

P

N

V
1

0

P

V

1

P

N

→

0

S
N
P

N

T

D

D

B

V

N

N

T

D

N
j

k

i

x≺x

Figure 3: A multi-scale chart can be used to efﬁciently
compute inside/outside scores using productions of varying speciﬁcity.

above. We detect that we have split a production too
far when all child production features are driven to
zero under L1 regularization. In such cases, the children are collapsed to their parent production, which
forms an entry in the multi-scale grammar.
5.2

When working with multi-scale grammars, we
expand the standard three-dimensional chart over
spans and grammar categories to store the scores of
all subcategories of the reﬁnement hierarchy, as illustrated in Figure 3. This allows us to compute the
scores more efﬁciently by summing only over rules
r = Ax → By Cz ≻ r:
ˆ
ˆ
ˆ ˆ
I(Ax , i, j) =
=

Efﬁcient Multi-Scale Inference

I(Ax , i, j) =

I(By , i, k)I(Cz , k, j)

φr
r

k

Note that this involves summing over all relevant
fully reﬁned grammar productions.
The key quantities we will need are marginals of
the form I(Ax , i, j), the sum of the scores of all fully
reﬁned derivations rooted at any Ax dominated by
Ax and spanning i, j . We deﬁne these marginals
4
These scores lack any probabilistic interpretation, but can
be normalized to compute the necessary expectations for training (Petrov and Klein, 2008).

871

=

k

I(By , i, k)I(Cz , k, j)

φr
ˆ
r≺ˆ k
r

r
ˆ

In order to compute the expected counts needed for
training, we need to parse the training set, score
all derivations and compute posteriors for all subcategories in the reﬁnement hierarchy. The inside/outside algorithm (Lari and Young, 1990) is an
efﬁcient dynamic program for summing over derivations under a context-free grammar. It is fairly
straightforward to adapt this algorithm to multiscale grammars, allowing us to sum over an exponential number of derivations without explicitly reconstructing the underlying fully split grammar.
For single-scale latent variable grammars, the inside score I(Ax , i, j) of a fully reﬁned category Ax
spanning i, j is computed by summing over all
possible productions r = Ax → By Cz with weight
φr , spanning i, k and k, j respectively:4

I(By , i, k)I(Cz , k, j)

φr
r r≺ˆ
ˆ
r

I(By , i, k)I(Cz , k, j)

φr
ˆ
y≺ˆ z≺ˆ k
y
z

r
ˆ

=

I(By , i, k)

φr
ˆ
y
k y≺ˆ

r
ˆ

=

φr
ˆ
r
ˆ

I(Cz , k, j)
z≺ˆ
z

I(By , i, k)I(Cz , k, j)
ˆ
ˆ
k

Of course, some of the same quantities are computed
repeatedly in the above equation and can be cached
in order to obtain further efﬁciency gains. Due to
space constraints we omit these details, and also the
computation of the outside score, as well as the handling of unary productions.
5.3

Feature Count Approximations

Estimating discriminative grammars is challenging,
as it requires repeatedly taking expectations over all
parses of all sentences in the training set. To make
this computation practical on large data sets, we
use the same approach as Petrov and Klein (2008).
Therein, the idea of coarse-to-ﬁne parsing (Charniak
et al., 1998) is extended to handle the repeated parsing of the same sentences. Rather than computing
the entire coarse-to-ﬁne history in every round of
training, the pruning history is cached between training iterations, effectively avoiding the repeated calculation of similar quantities and allowing the efﬁcient approximation of feature count expectations.

with the following example and the span 1, 4 :

6 Additional Features
The discriminative framework gives us a convenient
way of incorporating additional, overlapping features. We investigate two types of features: unknown word features (for predicting the part-ofspeech tags of unknown or rare words) and span features (for determining constituent boundaries based
on individual words and the overall sentence shape).
6.1

Unknown Word Features

Building a parser that can process arbitrary sentences requires the handling of previously unseen
words. Typically, a classiﬁcation of rare words into
word classes is used (Collins, 1999). In such an approach, the word classes need to be manually deﬁned a priori, for example based on discriminating
word shape features (sufﬁxes, preﬁxes, digits, etc.).
While this component of the parsing system is
rarely talked about, its importance should not be underestimated: when using only one unknown word
class, ﬁnal parsing performance drops several percentage points. Some unknown word features are
universal (e.g. digits, dashes), but most of them
will be highly language dependent (preﬁxes, sufﬁxes), making additional human expertise necessary
for training a parser on a new language. It is therefore beneﬁcial to automatically learn what the discriminating word shape features for a language are.
The discriminative framework allows us to do that
with ease. In our experiments we extract preﬁxes
and sufﬁxes of length ≤ 3 and add those features to
words that occur 25 times or less in the training set.
These unknown word features make the latent variable grammar learning process more language independent than in previous work.
6.2

Span Features

There are many features beyond local tree conﬁgurations which can enhance parsing discrimination;
Charniak and Johnson (2005) presents a varied list.
In reranking, one can incorporate any such features,
of course, but even in our dynamic programming approach it is possible to include features that decompose along the dynamic program structure, as shown
by Taskar et al. (2004). We use non-local span features, which condition on properties of input spans
(Taskar et al., 2004). We illustrate our span features
872

0

“

1[

Yes

2

”

3

, ]

4

he

5

said

6

.

7

We ﬁrst added the following lexical features:
• the ﬁrst (Yes), last (comma), preceding (“) and
following (he) words,
• the word pairs at the left edge “,Yes , right
edge comma,he , inside border Yes,comma
and outside border “,he .
Lexical features were added for each span of length
three or more. We used two groups of span features,
one for natural constituents and one for synthetic
ones.5 We found this approach to work slightly
better than anchoring the span features to particular
constituent labels or having only one group.
We also added shape features, projecting the
sentence to abstract shapes to capture global sentence structures. Punctuation shape replaces every non-punctuation word with x and then further
collapses strings of x to x+. Our example becomes #‘‘x’’,x+.#, and the punctuation feature
for our span is ‘‘[x’’,]x. Capitalization shape
projects the example sentence to #.X..xx.#, and
.[X..]x for our span. Span features are a rich
source of information and our experiments should
be seen merely as an initial investigation of their effect in our system.

7 Experiments
We ran experiments on a variety of languages and
corpora using the standard training and test splits,
as described in Table 1. In each case, we start
with a completely unannotated X-bar grammar, obtained from the raw treebank by a simple rightbranching binarization scheme. We then train multiscale grammars of increasing latent complexity as
described in Section 5, directly incorporating the
additional features from Section 6 into the training
procedure. Hierarchical training starting from a raw
treebank grammar and proceeding to our most reﬁned grammars took three days in a parallel implementation using 8 CPUs. At testing time we
marginalize out the hidden structure and extract the
tree with the highest number of expected correct productions, as in Petrov and Klein (2007).
5

Synthetic constituents are nodes that are introduced during
binarization.

Training Set
Dev. Set
Test Set
Sections
Section 22
Section 23
(Marcus et al., 1993)
2-21
ENGLISH-BROWN
see
10% of
10% of the
(Francis et al. 2002) ENGLISH-WSJ
the data6
the data6
7
FRENCH
Sentences
Sentences
Sentences
(Abeille et al., 2000)
1-18,609
18,610-19,609 19,609-20,610
GERMAN
Sentences
Sentences
Sentences
(Skut et al., 1997)
1-18,602
18,603-19,602 19,603-20,602
ENGLISH-WSJ

Parsing accuracy (F1)

90

85

80

75

Table 1: Corpora and standard experimental setups.

Discriminative Multi-Scale Grammars
+ Lexical Features
+ Span Features
Generative Split-Merge Grammars
Flat Discriminative Grammars
10000

We compare to a baseline of discriminatively
trained latent variable grammars (Petrov and Klein,
2008). We also compare our discriminative multiscale grammars to their generative split-and-merge
cousins, which have been shown to produce the
state-of-the-art ﬁgures in terms of accuracy and efﬁciency on many corpora. For those comparisons we
use the grammars from Petrov and Klein (2007).
7.1

Sparsity

One of the main motivations behind multi-scale
grammars was to create compact grammars. Figure 4 shows parsing accuracies vs. grammar sizes.
Focusing on the grammar size for now, we see that
multi-scale grammars are extremely compact - even
our most reﬁned grammars have less than 50,000 active productions. This is 20 times smaller than the
generative split-and-merge grammars, which use explicit category merging. The graph also shows that
this compactness is due to controlling production
sparsity, as the single-scale discriminative grammars
are two orders of magnitude larger.
7.2

Accuracy

Figure 4 shows development set results for English. In terms of parsing accuracy, multi-scale
grammars signiﬁcantly outperform discriminatively
trained single-scale latent variable grammars and
perform on par with the generative split-and-merge
grammars. The graph also shows that the unknown
word and span features each add about 0.5% in ﬁnal
parsing accuracy. Note that the span features improve the performance of the unsplit baseline grammar by 8%, but not surprisingly their contribution
6

See Gildea (2001) for the exact setup.
This setup contains only sentences without annotation errors, as in (Arun and Keller, 2005).
7

873

100000

1000000

Number of grammar productions

Figure 4: Discriminative multi-scale grammars give similar parsing accuracies as generative split-merge grammars, while using an order of magnitude fewer rules.

gets smaller when the grammars get more reﬁned.
Section 8 contains an analysis of some of the learned
features, as well as a comparison between discriminatively and generatively trained grammars.

7.3

Efﬁciency

Petrov and Klein (2007) demonstrates how the idea
of coarse-to-ﬁne parsing (Charniak et al., 1998;
Charniak et al., 2006) can be used in the context of
latent variable models. In coarse-to-ﬁne parsing the
sentence is rapidly pre-parsed with increasingly reﬁned grammars, pruning away unlikely chart items
in each pass. In their work the grammar is projected onto coarser versions, which are then used
for pruning. Multi-scale grammars, in contrast, do
not require projections. The reﬁnement hierarchy is
built in and can be used directly for coarse-to-ﬁne
pruning. Each production in the grammar is associated with a set of hierarchical features. To obtain a
coarser version of a multi-scale grammar, one therefore simply limits which features in the reﬁnement
hierarchy can be accessed. In our experiments, we
start by parsing with our coarsest grammar and allow an additional level of reﬁnement at each stage of
the pre-parsing. Compared to the generative parser
of Petrov and Klein (2007), parsing with multi-scale
grammars requires the evaluation of 29% fewer productions, decreasing the average parsing time per
sentence by 36% to 0.36 sec/sentence.

≤ 40 words
all
F1
EX
F1 EX
ENGLISH-WSJ
Petrov and Klein (2008)
88.8 35.7 88.3 33.1
Charniak et al. (2005)
90.3 39.6 89.7 37.2
Petrov and Klein (2007)
90.6 39.1 90.1 37.1
This work w/o span features 89.7 39.6 89.2 37.2
This work w/ span features 90.0 40.1 89.4 37.7
ENGLISH-WSJ (reranked)
Huang (2008)
92.3 46.2 91.7 43.5
ENGLISH-BROWN
Charniak et al. (2005)
84.5 34.8 82.9 31.7
Petrov and Klein (2007)
84.9 34.5 83.7 31.2
This work w/o span features 85.3 35.6 84.3 32.1
This work w/ span features 85.6 35.8 84.5 32.3
ENGLISH-BROWN (reranked)
Charniak et al. (2005)
86.8 39.9 85.2 37.8
FRENCH
Arun and Keller (2005)
79.2 21.2 75.6 16.4
This Paper
80.1 24.2 77.2 19.2
GERMAN
Petrov and Klein (2007)
80.8 40.8 80.1 39.1
This Paper
81.5 45.2 80.7 43.9
Parser

Table 2: Our ﬁnal test set parsing accuracies compared to
the best previous work on English, French and German.

7.4

Final Results

For each corpus we selected the grammar that gave
the best performance on the development set to parse
the ﬁnal test set. Table 2 summarizes our ﬁnal test
set performance, showing that multi-scale grammars
achieve state-of-the-art performance on most tasks.
On WSJ-English, the discriminative grammars perform on par with the generative grammars of Petrov
et al. (2006), falling slightly short in terms of F1, but
having a higher exact match score. When trained
on WSJ-English but tested on the Brown corpus,
the discriminative grammars clearly outperform the
generative grammars, suggesting that the highly regularized and extremely compact multi-scale grammars are less prone to overﬁtting. All those methods fall short of reranking parsers like Charniak and
Johnson (2005) and Huang (2008), which, however,
have access to many additional features, that cannot
be used in our dynamic program.
When trained on the French and German treebanks, our multi-scale grammars achieve the best
ﬁgures we are aware of, without any language speciﬁc modiﬁcations. This conﬁrms that latent vari874

able models are well suited for capturing the syntactic properties of a range of languages, and also
shows that discriminative grammars are still effective when trained on smaller corpora.

8 Analysis
It can be illuminating to see the subcategories that
are being learned by our discriminative multi-scale
grammars and to compare them to generatively estimated latent variable grammars. Compared to the
generative case, the lexical categories in the discriminative grammars are substantially less reﬁned. For
example, in the generative case, the nominal categories were fully reﬁned, while in the discriminative case, fewer nominal clusters were heavily used.
One reason for this can be seen by inspecting the
ﬁrst two-way split in the NNP tag. The generative model split into initial NNPs (San, Wall) and
ﬁnal NNPs (Francisco, Street). In contrast, the discriminative split was between organizational entities
(Stock, Exchange) and other entity types (September,
New, York). This constrast is unsurprising. Generative likelihood is advantaged by explaining lexical
choice – New and York occur in very different slots.
However, they convey the same information about
the syntactic context above their base NP and are
therefore treated the same, discriminatively, while
the systematic attachment distinctions between temporals and named entities are more predictive.
Analyzing the syntactic and semantic patterns
learned by the grammars shows similar trends. In
Table 3 we compare the number of subcategories
in the generative split-and-merge grammars to the
average number of features per unsplit production
with that phrasal category as head in our multi-scale
grammars after 5 split (and merge) rounds. These
quantities are inherently different: the number of
features should be roughly cubic in the number of
subcategories. However, we observe that the numbers are very close, indicating that, due to the sparsity of our productions, and the efﬁcient multi-scale
encoding, the number of grammar parameters grows
linearly in the number of subcategories. Furthermore, while most categories have similar complexity in those two cases, the complexity of the two
most reﬁned phrasal categories are ﬂipped. Generative grammars split NPs most highly, discrimina-

QP

PRN

ADVP

ADJP

SBAR

S

PP

VP

NP

Generative
32 24 20 12 12 12 8
subcategories
Discriminative
19 32 20 14 14 8 7
production parameters

7

5

9

6

Table 3: Complexity of highly split phrasal categories in
generative and discriminative grammars. Note that subcategories are compared to production parameters, indicating that the number of parameters grows cubicly in the
number of subcategories for generative grammars, while
growing linearly for multi-scale grammars.

tive grammars split the VP. This distinction seems
to be because the complexity of VPs is more syntactic (e.g. complex subcategorization), while that of
NPs is more lexical (noun choice is generally higher
entropy than verb choice).
It is also interesting to examine the automatically
learned word class features. Table 4 shows the sufﬁxes with the highest weight for a few different categories across the three languages that we experimented with. The learning algorithm has selected
discriminative sufﬁxes that are typical derviational
or inﬂectional morphemes in their respective languages. Note that the highest weighted sufﬁxes will
typically not correspond to the most common sufﬁx
in the word class, but to the most discriminative.
Finally, the span features also exhibit clear patterns. The highest scoring span features encourage
the words between the last two punctuation marks
to form a constituent (excluding the punctuation
marks), for example ,[x+]. and :[x+]. Words
between quotation marks are also encouraged to
form constituents: ‘‘[x+]’’ and x[‘‘x+’’]x.
Span features can also discourage grouping words
into constituents. The features with the highest negative weight involve single commas: x[x,x+],
and x[x+,x+]x and so on (indeed, such spans
were structurally disallowed by the Collins (1999)
parser).

9 Conclusions
Discriminatively trained multi-scale grammars give
state-of-the-art parsing performance on a variety of
languages and corpora. Grammar size is dramatically reduced compared to the baseline, as well as to
875

Adjectives

Nouns
Verbs
Adverbs
Numbers

ENGLISH
-ous
-ble
-nth
-ion
-en
-cle
-ed
-s
-ly
-ty

GERMAN
-los
-bar
-ig
-t¨ t
a
-ung
-rei
-st
-eht
-mal
-zig

FRENCH
-ien
-ble
-ive
-t´
e
-eur
-ges
-´ es
e
-´
e
-ent
—

Table 4: Automatically learned sufﬁxes with the highest
weights for different languages and part-of-speech tags.

methods like split-and-merge (Petrov et al., 2006).
Because fewer parameters are estimated, multi-scale
grammars may also be less prone to overﬁtting, as
suggested by a cross-corpus evaluation experiment.
Furthermore, the discriminative framework enables
the seamless integration of additional, overlapping
features, such as span features and unknown word
features. Such features further improve parsing performance and make the latent variable grammars
very language independent.
Our parser, along with trained grammars
for a variety of languages, is available at
http://nlp.cs.berkeley.edu.

References
A. Abeille, L. Clement, and A. Kinyon. 2000. Building a
treebank for French. In 2nd International Conference
on Language Resources and Evaluation.
G. Andrew and J. Gao. 2007. Scalable training of L1regularized log-linear models. In ICML ’07.
A. Arun and F. Keller. 2005. Lexicalization in crosslinguistic probabilistic parsing: the case of french. In
ACL ’05.
E. Charniak and M. Johnson. 2005. Coarse-to-Fine NBest Parsing and MaxEnt Discriminative Reranking.
In ACL’05.
E. Charniak, S. Goldwater, and M. Johnson. 1998. Edgebased best-ﬁrst chart parsing. 6th Workshop on Very
Large Corpora.
E. Charniak, M. Johnson, D. McClosky, et al. 2006.
Multi-level coarse-to-ﬁne PCFG Parsing. In HLTNAACL ’06.
E. Charniak. 2000. A maximum–entropy–inspired
parser. In NAACL ’00.
S. Clark and J. R. Curran. 2004. Parsing the WSJ using
CCG and log-linear models. In ACL ’04.
M. Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, UPenn.

J. Finkel, A. Kleeman, and C. Manning. 2008. Efﬁcient, feature-based, conditional random ﬁeld parsing.
In ACL ’08.
W. N. Francis and H. Kucera. 2002. Manual of information to accompany a standard corpus of present-day
edited american english. In TR, Brown University.
D. Gildea. 2001. Corpus variation and parser performance. EMNLP ’01.
J. Henderson. 2004. Discriminative training of a neural
network statistical parser. In ACL ’04.
L. Huang. 2008. Forest reranking: Discriminative parsing with non-local features. In ACL ’08.
M. Johnson. 1998. PCFG models of linguistic tree representations. Computational Linguistics, 24:613–632.
M. Johnson. 2001. Joint and conditional estimation of
tagging and parsing models. In ACL ’01.
D. Klein and C. Manning. 2003. Accurate unlexicalized
parsing. In ACL ’03, pages 423–430.
T. Koo and M. Collins. 2005. Hidden-variable models
for discriminative reranking. In EMNLP ’05.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional Random Fields: Probabilistic models for segmenting and labeling sequence data. In ICML ’01.
K. Lari and S. Young. 1990. The estimation of stochastic context-free grammars using the inside-outside algorithm. Computer Speech and Language.
P. Liang, S. Petrov, M. I. Jordan, and D. Klein. 2007. The
inﬁnite PCFG using hierarchical Dirichlet processes.
In EMNLP ’07.

876

M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993.
Building a large annotated corpus of English: The
Penn Treebank. In Computational Linguistics.
T. Matsuzaki, Y. Miyao, and J. Tsujii. 2005. Probabilistic CFG with latent annotations. In ACL ’05.
M. Mohri and B. Roark. 2006. Probabilistic context-free
grammar induction based on structural zeros. In HLTNAACL ’06.
J. Nocedal and S. J. Wright. 1999. Numerical Optimization. Springer.
S. Petrov and D. Klein. 2007. Improved inference for
unlexicalized parsing. In HLT-NAACL ’07.
S. Petrov and D. Klein. 2008. Discriminative log-linear
grammars with latent variables. In NIPS ’08.
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning accurate, compact, and interpretable tree annotation. In ACL ’06.
W. Skut, B. Krenn, T. Brants, and H. Uszkoreit. 1997.
An annotation scheme for free word order languages.
In Conf. on Applied Natural Language Processing.
N. A. Smith and M. Johnson. 2007. Weighted and probabilistic context-free grammars are equally expressive.
Computational Lingusitics.
B. Taskar, D. Klein, M. Collins, D. Koller, and C. Manning. 2004. Max-margin parsing. In EMNLP ’04.
J. Turian, B. Wellington, and I. D. Melamed. 2007. Scalable discriminative learning for natural language parsing and translation. In NIPS ’07.

