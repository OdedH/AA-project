Type-Based MCMC

Percy Liang
UC Berkeley
pliang@cs.berkeley.edu

Michael I. Jordan
UC Berkeley
jordan@cs.berkeley.edu

Dan Klein
UC Berkeley
klein@cs.berkeley.edu

Most existing algorithms for learning latentvariable models—such as EM and existing
Gibbs samplers—are token-based, meaning
that they update the variables associated with
one sentence at a time. The incremental nature of these methods makes them susceptible to local optima/slow mixing. In this paper,
we introduce a type-based sampler, which updates a block of variables, identiﬁed by a type,
which spans multiple sentences. We show improvements on part-of-speech induction, word
segmentation, and learning tree-substitution
grammars.

1

2

1 2 2 1

2 1 2 2 1

2

1 2 2 1

2

Abstract

2

2

1

2

2

2

2

1

2

2

2

1

2

1

2

2

1

2

1

2

2

1 2 1 2 2

(a) token-based

(b) sentence-based

2

1 2

(c) type-based

Figure 1: Consider a dataset of 3 sentences, each of
length 5. Each variable is labeled with a type (1 or 2). The
unshaded variables are the ones that are updated jointly
by a sampler. The token-based sampler updates the variable for one token at a time (a). The sentence-based sampler updates all variables in a sentence, thus dealing with
intra-sentential dependencies (b). The type-based sampler updates all variables of a particular type (1 in this example), thus dealing with dependencies due to common
parameters (c).

Introduction

A long-standing challenge in NLP is the unsupervised induction of linguistic structures, for example, grammars from raw sentences or lexicons
from phoneme sequences. A fundamental property
of these unsupervised learning problems is multimodality. In grammar induction, for example, we
could analyze subject-verb-object sequences as either ((subject verb) object) (mode 1) or (subject
(verb object)) (mode 2).
Multimodality causes problems for token-based
procedures that update variables for one example at
a time. In EM, for example, if the parameters already assign high probability to the ((subject verb)
object) analysis, re-analyzing the sentences in E-step
only reinforces the analysis, resulting in EM getting
stuck in a local optimum. In (collapsed) Gibbs sampling, if all sentences are already analyzed as ((subject verb) object), sampling a sentence conditioned

on all others will most likely not change its analysis,
resulting in slow mixing.
To combat the problems associated with tokenbased algorithms, we propose a new sampling algorithm that operates on types. Our sampler would, for
example, be able to change all occurrences of ((subject verb) object) to (subject (verb object)) in one
step. These type-based operations are reminiscent of
the type-based grammar operations of early chunkmerge systems (Wolff, 1988; Stolcke and Omohundro, 1994), but we work within a sampling framework for increased robustness.
In NLP, perhaps the the most simple and popular sampler is the token-based Gibbs sampler,1 used
in Goldwater et al. (2006), Goldwater and Grifﬁths
(2007), and many others. By sampling only one
1

In NLP, this is sometimes referred to as simply the collapsed Gibbs sampler.

2

Basic Idea via a Motivating Example

The key technical problem we solve in this paper is
ﬁnding a block of variables which are both highly
coupled and yet tractable to sample jointly. This
section illustrates the main idea behind type-based
sampling on a small word segmentation example.
Suppose our dataset x consists of n occurrences
of the sequence a b. Our goal is infer z =
(z1 , . . . , zn ), where zi = 0 if the sequence is one
word ab, and zi = 1 if the sequence is two, a
and b. We can model this situation with a simple
generative model: for each i = 1, . . . , n, generate one or two words with equal probability. Each
word is drawn independently based on probabilities
θ = (θa , θb , θab ) which we endow with a uniform
prior θ ∼ Dirichlet(1, 1, 1).
We marginalize out θ to get the following standard
expression (Goldwater et al., 2009):
p(z | x) ∝

1(m) 1(m) 1(n−m) def
= g(m),
3(n+m)

(1)

800

-709.1

600

-1060.3

Token
Type

400

-1411.4

200
200 400 600 8001000

2

m

4

6

8 10

iteration

(a) bimodal posterior

(b) sampling run

Figure 2: (a) The posterior (1) is sharply bimodal (note
the log-scale). (b) A run of the token-based and typebased samplers. We initialize both samplers with m = n
(n = 1000). The type-based sampler mixes instantly
(in fact, it makes independent draws from the posterior)
whereas the token-based sampler requires ﬁve passes
through the data before ﬁnding the high probability region m 0.

ascending factorial.2 Figure 2(a) depicts the resulting bimodal posterior.
A token-based sampler chooses one zi to update
according to the posterior p(zi | z−i , x). To illustrate the mixing problem, consider the case where
m = n, i.e., all sequences are analyzed as two
words. From (1), we can verify that p(zi = 0 |
1
z−i , x) = O( n ). When n = 1000, this means that
there is only a 0.002 probability of setting zi = 0,
a very unlikely but necessary ﬁrst step to take to escape this local optimum. Indeed, Figure 2(b) shows
how the token-based sampler requires ﬁve passes
over the data to ﬁnally escape.
Type-based sampling completely eradicates the
local optimum problem in this example. Let us take
a closer look at (1). Note that p(z | x) only depends
on a single integer m, which only takes one of n + 1
values, not on the particular z. This shows that the
n
zi s are exchangeable. There are m possible values of z satisfying m = i zi , each with the same
probability g(m). Summing, we get:
p(m | x) ∝

p(x, z) =
z:m=

P

i zi

n
g(m). (2)
m

A sampling strategy falls out naturally: First, sample
the number m via (2). Conditioned on m, choose
2

where m = n zi is the number of two-word sei=1
quences and a(k) = a(a + 1) · · · (a + k − 1) is the

1000

m

-6.8
-358.0

log g(m)

variable at a time, this sampler is prone to slow mixing due to the strong coupling between variables.
A general remedy is to sample blocks of coupled
variables. For example, the sentence-based sampler
samples all the variables associated with a sentence
at once (e.g., the entire tag sequence). However, this
blocking does not deal with the strong type-based
coupling (e.g., all instances of a word should be
tagged similarly). The type-based sampler we will
present is designed exactly to tackle this coupling,
which we argue is stronger and more important to
deal with in unsupervised learning. Figure 1 depicts
the updates made by each of the three samplers.
We tested our sampler on three models: a
Bayesian HMM for part-of-speech induction (Goldwater and Grifﬁths, 2007), a nonparametric
Bayesian model for word segmentation (Goldwater
et al., 2006), and a nonparametric Bayesian model of
tree substitution grammars (Cohn et al., 2009; Post
and Gildea, 2009). Empirically, we ﬁnd that typebased sampling improves performance and is less
sensitive to initialization (Section 5).

The ascending factorial function arises from marginalizing Dirichlet distributions and is responsible the rich-gets-richer
phenomenon: the larger n is, more we gain by increasing it.

n
the particular z uniformly out of the m possibilities. Figure 2(b) shows the effectiveness of this typebased sampler.
This simple example exposes the fundamental
challenge of multimodality in unsupervised learning. Both m = 0 and m = n are modes due to the
rich-gets-richer property which arises by virtue of
all n examples sharing the same parameters θ. This
sharing is a double-edged sword: It provides us with
clustering structure but also makes inference hard.
Even though m = n is much worse (by a factor exponential in n) than m = 0, a na¨ve algorithm can
ı
easily have trouble escaping m = n.

3

Setup

We will now present the type-based sampler in full
generality. Our sampler is applicable to any model
which is built out of local multinomial choices,
where each multinomial has a Dirichlet process prior
(a Dirichlet prior if the number of choices is ﬁnite).
This includes most probabilistic models in NLP (excluding ones built from log-linear features).
As we develop the sampler, we will provide concrete examples for the Bayesian hidden
Markov model (HMM), the Dirichlet process unigram segmentation model (USM) (Goldwater et al.,
2006), and the probabilistic tree-substitution grammar (PTSG) (Cohn et al., 2009; Post and Gildea,
2009).
3.1

Model parameters

A model is speciﬁed by a collection of multinomial parameters θ = {θr }r∈R , where R is an index set. Each vector θr speciﬁes a distribution over
outcomes: outcome o has probability θro .
• HMM: Let K is the number of states. The set
R = {(q, k) : q ∈ {T, E}, k = 1, . . . , K}
indexes the K transition distributions {θ(T,k) }
(each over outcomes {1, . . . , K}) and K emission distributions {θ(E,k) } (each over the set of
words).
• USM: R = {0}, and θ0 is a distribution over (an
inﬁnite number of) words.
• PTSG: R is the set of grammar symbols, and
each θr is a distribution over labeled tree fragments with root label r.

R
θ = {θr }r∈R
µ = {µr }r∈R
S
b = {bs }s∈S
z
z−s
zs:b
∆zs:b
S⊂S
m
n = {nro }

index set for parameters
multinomial parameters
base distributions (ﬁxed)
set of sites
binary variables (to be sampled)
latent structure (set of choices)
choices not depending on site s
choices after setting bs = b
zs:b \z−s : new choices from bs = b
sites selected for sampling
# sites in S assigned bs = 1
counts (sufﬁcient statistics of z)

Table 1: Notation used in this paper. Note that there is a
one-to-one mapping between z and (b, x). The information relevant for evaluating the likelihood is n. We use
the following parallel notation: n−s = n(z−s ), ns:b =
n(zs:b ), ∆ns = n(∆zs ).

3.2

Choice representation of latent structure z

We represent the latent structure z as a set of local
choices:3
• HMM: z contains elements of the form
(T, i, a, b), denoting a transition from state
a at position i to state b at position i + 1; and
(E, i, a, w), denoting an emission of word w
from state a at position i.
• USM: z contains elements of the form (i, w), denoting the generation of word w at character position i extending to position i + |w| − 1.
• PTSG: z contains elements of the form (x, t), denoting the generation of tree fragment t rooted at
node x.
The choices z are connected to the parameters θ
as follows: p(z | θ) = z∈z θz.r,z.o . Each choice
z ∈ z is identiﬁed with some z.r ∈ R and outcome z.o. Intuitively, choice z was made by drawing
drawing z.o from the multinomial distribution θz.r .
3.3

Prior

We place a Dirichlet process prior on θr (Dirichlet
prior for ﬁnite outcome spaces): θr ∼ DP(αr , µr ),
where αr is a concentration parameter and µr is a
ﬁxed base distribution.
3

We assume that z contains both a latent part and the observed input x, i.e., x is a deterministic function of z.

Let nro (z) = |{z ∈ z : z.r = r, z.o = o}| be the
number of draws from θr resulting in outcome o, and
nr· = o nro be the number of times θr was drawn
from. Let n(z) = {nro (z)} denote the vector of
sufﬁcient statistics associated with choices z. When
it is clear from context, we simply write n for n(z).
Using these sufﬁcient statistics, we can write p(z |
nro (z)
θ) = r,o θro
.
We now marginalize out θ using Dirichletmultinomial conjugacy, producing the following expression for the likelihood:
p(z) =
r∈R

(nro (z))
o (αro µro )
,
αr (nr· (z))

(3)

where a(k)

= a(a+1) · · · (a+k−1) is the ascending
factorial. (3) is the distribution that we will use for
sampling.

4

Type-Based Sampling

Having described the setup of the model, we now
turn to posterior inference of p(z | x).
4.1

Binary Representation

We ﬁrst deﬁne a new representation of the latent
structure based on binary variables b so that there is
a bijection between z and (b, x); z was used to deﬁne the model, b will be used for inference. We will
use b to exploit the ideas from Section 2. Speciﬁcally, let b = {bs }s∈S be a collection of binary variables indexed by a set of sites S.
• HMM: If the HMM has K = 2 states, S is the set
of positions in the sequence. For each s ∈ S, bs
is the hidden state at s. The extension to general
K is considered at the end of Section 4.4.
• USM: S is the set of non-ﬁnal positions in the
sequence. For each s ∈ S, bs denotes whether
a word boundary exists between positions s and
s + 1.

of choices that do not depend on the value of bs , and
def
n−s = n(z−s ) be the corresponding counts.
• HMM: z−s includes all but the transitions into
and out of the state at s plus the emission at s.
• USM: z−s includes all except the word ending at
s and the one starting at s + 1 if there is a boundary (bs = 1); except the word covering s if no
boundary exists (bs = 0).
• PTSG: z−s includes all except the tree fragment
rooted at node s and the one with leaf s if bs = 1;
except the single fragment containing s if bs = 0.
4.2

Sampling One Site

A token-based sampler considers one site s at a time.
Speciﬁcally, we evaluate the likelihoods of zs:0 and
zs:1 according to (3) and sample bs with probability
proportional to the likelihoods. Intuitively, this can
be accomplished by removing choices that depend
on bs (resulting in z−s ), evaluating the likelihood resulting from setting bs to 0 or 1, and then adding the
appropriate choices back in.
def
More formally, let ∆zs:b = zs:b \z−s be the new
choices that would be added if we set bs = b ∈
def
{0, 1}, and let ∆ns:b = n(∆zs:b ) be the corresponding counts. With this notation, we can write
the posterior as follows:
p(bs = b | b\bs ) ∝

r∈R

(4)

s:b
−s (∆nro )
o (αro µro + nro )
.
(∆ns:b )
(αr + n−s ) r·
r·

The form of the conditional (4) follows from the
joint (3) via two properties: additivity of counts
(ns:b = n−s + ∆ns:b ) and a simple property of ascending factorials (a(k+δ) = a(k) (a + k)(δ) ).
In practice, most of the entries of ∆ns:b are zero.
For the HMM, ns:b would be nonzero only for
ro
the transitions into the new state (b) at position s
(zs−1 → b), transitions out of that state (b → zs+1 ),
and emissions from that state (b → xs ).

• PTSG: S is the set of internal nodes in the parse
tree. For s ∈ S, bs denotes whether a tree fragment is rooted at node s.

4.3

For each site s ∈ S, let zs:0 and zs:1 denote the
choices associated with the structures obtained by
setting the binary variable bs = 0 and bs = 1, redef
spectively. Deﬁne z−s = zs:0 ∩ zs:1 to be the set

We would like to sample multiple sites jointly as in
Section 2, but we cannot choose any arbitrary subset
S ⊂ S, as the likelihood will in general depend on
def
the exact assignment of bS = {bs }s∈S , of which

Sampling Multiple Sites

a

b

c

a

a

b

c

a

b

c

b

(a) USM
a
b
1 1 2 2 1 1 2 2
a b a b c b b e

a

c
a

b

(b) HMM

d
c

d

b

e
c

a

b

e

shows that types depend on z. For example, s, s ∈
S conﬂict when s = s + 1 in the HMM or when
s and s are boundaries of one segment (USM) or
one tree fragment (PTSG). Therefore, one additional
concept is necessary: We say two sites s and s conﬂict if there is some choice that depends on both bs
and bs ; formally, (z\z−s ) ∩ (z\z−s ) = ∅.
Our key mathematical result is as follows:
Proposition 1 For any set S ⊂ S of non-conﬂicting
sites with the same type,

(c) PTSG
Figure 3: The type-based sampler jointly samples all variables at a set of sites S (in green boxes). Sites in S are
chosen based on types (denoted in red). (a) HMM: two
sites have the same type if they have the same previous
and next states and emit the same word; they conﬂict unless separated by at least one position. (b) USM: two sites
have the same type if they are both of the form ab|c or
abc; note that occurrences of the same letters with other
segmentations do not match the type. (c) PTSG: analogous to the USM, only for tree rather than sequences.

there are an exponential number. To exploit the exchangeability property in Section 2, we need to ﬁnd
sites which look “the same” from the model’s point
of view, that is, the likelihood only depends on bS
def
via m = s∈S bs .
To do this, we need to deﬁne two notions, type and
conﬂict. We say sites s and s have the same type if
the counts added by setting either bs or bs are the
same, that is, ∆ns:b = ∆ns :b for b ∈ {0, 1}. This
motivates the following deﬁnition of the type of site
s with respect to z:
def

t(z, s) = (∆ns:0 , ∆ns:1 ),

p(bS | b\bS ) ∝ g(m)
|S|
p(m | b\bS ) ∝
g(m),
m

(7)

for some easily computable g(m), where m =
s∈S bs .
We will derive g(m) shortly, but ﬁrst note from
(6) that the likelihood for a particular setting of bS
depends on bS only via m as desired. (7) sums
over all |S| settings of bS with m =
s∈S bs .
m
The algorithmic consequences of this result is that
to sample bS , we can ﬁrst compute (7) for each
m ∈ {0, . . . , |S|}, sample m according to the normalized distribution, and then choose the actual bS
uniformly subject to m.
Let us now derive g(m) by generalizing (4).
Imagine removing all sites S and their dependent
choices and adding in choices corresponding to
some assignment bS . Since all sites in S are nonconﬂicting and of the same type, the count contribution ∆ns:b is the same for every s ∈ S (i.e., sites
in S are exchangeable). Therefore, the likelihood
of the new assignment bS depends only on the new
counts:

(5)

We say that s and s have the same type if t(z, s) =
t(z, s ). Note that the actual choices added (∆zs:b
and ∆zs :b ) are in general different as s and s correspond to different parts of the latent structure, but
the model only depends on counts and is indifferent
to this. Figure 3 shows examples of same-type sites
for our three models.
However, even if all sites in S have the same
type, we still cannot sample bS jointly, since changing one bs might change the type of another site s ;
indeed, this dependence is reﬂected in (5), which

(6)

def

∆nS:m = m∆ns:1 + (|S| − m)∆ns:0 .

(8)

Using these new counts in place of the ones in (4),
we get the following expression:
o (αro µro

g(m) =

S:m )

r∈R

4.4

(∆nS:m )
ro

+ nro (z−S ))

αr + nr· (z−S )(∆nr·

. (9)

Full Algorithm

Thus far, we have shown how to sample bS given
a set S ⊂ S of non-conﬂicting sites with the same
type. To complete the description of the type-based

Type-Based Sampler
for each iteration t = 1, . . . , T :
−for each pivot site s0 ∈ S:
−−S ← TB(z, s0 ) (S is the type block centered at s0 )
−−decrement n and remove from z based on bS
−−sample m according to (7)
−−sample M ⊂ S with |M | = m uniformly at random
−−set bs = I[s ∈ M ] for each s ∈ S
−−increment n and add to z accordingly

Figure 4: Pseudocode for the general type-based sampler.
We operate in the binary variable representation b of z.
Each step, we jointly sample |S| variables (of the same
type).

sampler, we need to specify how to choose S. Our
general strategy is to ﬁrst choose a pivot site s0 ∈ S
uniformly at random and then set S = TB(z, s0 ) for
some function TB. Call S the type block centered at
s0 . The following two criteria on TB are sufﬁcient
for a valid sampler: (A) s0 ∈ S, and (B) the type
blocks are stable, which means that if we change bS
to any bS (resulting in a new z ), the type block centered at s0 with respect to z does not change (that
is, TB(z , s0 ) = S). (A) ensures ergodicity; (B),
reversibility.
Now we deﬁne TB as follows: First set S = {s0 }.
Next, loop through all sites s ∈ S with the same type
as s0 in some ﬁxed order, adding s to S if it does
not conﬂict with any sites already in S. Figure 4
provides the pseudocode for the full algorithm.
Formally, this sampler cycles over |S| transition
kernels, one for each pivot site. Each kernel (indexed by s0 ∈ S) deﬁnes a blocked Gibbs move,
i.e. sampling from p(bTB(z,s0 ) | · · · ).
Efﬁcient Implementation There are two operations we must perform efﬁciently: (A) looping
through sites with the same type as the pivot site s0 ,
and (B) checking whether such a site s conﬂicts with
any site in S. We can perform (B) in O(1) time by
checking if any element of ∆zs:bs has already been
removed; if so, there is a conﬂict and we skip s. To
do (A) efﬁciently, we maintain a hash table mapping
type t to a doubly-linked list of sites with type t.
There is an O(1) cost for maintaining this data structure: When we add or remove a site s, we just need
to add or remove neighboring sites s from their respective linked lists, since their types depend on bs .

For example, in the HMM, when we remove site s,
we also remove sites s−1 and s+1.
For the USM, we use a simpler solution: maintain a hash table mapping each word w to a list of
positions where w occurs. Suppose site (position) s
straddles words a and b. Then, to perform (A), we
retrieve the list of positions where a, b, and ab occur,
intersecting the a and b lists to obtain a list of positions where a b occurs. While this intersection is
often much smaller than the pre-intersected lists, we
found in practice that the smaller amount of bookkeeping balanced out the extra time spent intersecting. We used a similar strategy for the PTSG, which
signiﬁcantly reduces the amount of bookkeeping.
Skip Approximation Large type blocks mean
larger moves. However, such a block S is also sampled more frequently—once for every choice of a
pivot site s0 ∈ S. However, we found that empirically, bS changes very infrequently. To eliminate
this apparent waste, we use the following approximation of our sampler: do not consider s0 ∈ S as
a pivot site if s0 belongs to some block which was
already sampled in the current iteration. This way,
each site is considered roughly once per iteration.4
Sampling Non-Binary Representations We can
sample in models without a natural binary representation (e.g., HMMs with with more than two states)
by considering random binary slices. Speciﬁcally,
suppose bs ∈ {1, . . . , K} for each site s ∈ S.
We modify Figure 4 as follows: After choosing a
pivot site s0 ∈ S, let k = bs0 and choose k uniformly from {1, . . . , K}. Only include sites in one
of these two states by re-deﬁning the type block to
be S = {s ∈ TB(z, s0 ) : bs ∈ {k, k }}, and sample bS restricted to these two states by drawing from
p(bS | bS ∈ {k, k }|S| , · · · ). By choosing a random
k each time, we allow b to reach any point in the
space, thus achieving ergodicity just by using these
binary restrictions.

5

Experiments

We now compare our proposed type-based sampler
to various alternatives, evaluating on marginal like4

A site could be sampled more than once if it belonged to
more than one type block during the iteration (recall that types
depend on z and thus could change during sampling).

lihood (3) and accuracy for our three models:
• HMM: We learned a K = 45 state HMM on
the Wall Street Journal (WSJ) portion of the Penn
Treebank (49208 sentences, 45 tags) for part-ofspeech induction. We ﬁxed αr to 0.1 and µr to
uniform for all r.
For accuracy, we used the standard metric based
on greedy mapping, where each state is mapped
to the POS tag that maximizes the number of correct matches (Haghighi and Klein, 2006). We did
not use a tagging dictionary.
• USM: We learned a USM model on the
Bernstein-Ratner corpus from the CHILDES
database used in Goldwater et al. (2006) (9790
sentences) for word segmentation. We ﬁxed α0 to
0.1. The base distribution µ0 penalizes the length
of words (see Goldwater et al. (2009) for details).
For accuracy, we used word token F1 .
• PTSG: We learned a PTSG model on sections 2–
21 of the WSJ treebank.5 For accuracy, we used
EVALB parsing F1 on section 22.6 Note this is a
supervised task with latent-variables, whereas the
other two are purely unsupervised.
5.1

Basic Comparison

Figure 5(a)–(c) compares the likelihood and accuracy (we use the term accuracy loosely to also include F1 ). The initial observation is that the typebased sampler (T YPE) outperforms the token-based
sampler (T OKEN) across all three models on both
metrics.
We further evaluated the PTSG on parsing. Our
standard treebank PCFG estimated using maximum
likelihood obtained 79% F1 . T OKEN obtained an F1
of 82.2%, and T YPE obtained a comparable F1 of
83.2%. Running the PTSG for longer continued to
5

Following Petrov et al. (2006), we performed an initial preprocessing step on the trees involving Markovization, binarization, and collapsing of unary chains; words occurring once are
replaced with one of 50 “unknown word” tokens, using base
distributions {µr } that penalize the size of trees, and sampling
the hyperparameters (see Cohn et al. (2009) for details).
6
To evaluate, we created a grammar where the rule probabilities are the mean values under the PTSG distribution: this
involves taking a weighted combination (based on the concentration parameters) of the rule counts from the PTSG samples
and the PCFG-derived base distribution. We used the decoder
of DeNero et al. (2009) to parse.

improve the likelihood but actually hurt parsing accuracy, suggesting that the PTSG model is overﬁtting.
To better understand the gains from T YPE
over T OKEN, we consider three other alternative samplers. First, annealing (T OKENanneal ) is
a commonly-used technique to improve mixing,
where (3) is raised to some inverse temperature.7
In Figure 5(a)–(c), we see that unlike T YPE,
T OKENanneal does not improve over T OKEN uniformly: it hurts for the HMM, improves slightly for
the USM, and makes no difference for the PTSG. Although annealing does increase mobility of the sampler, this mobility is undirected, whereas type-based
sampling increases mobility in purely model-driven
directions.
Unlike past work that operated on types (Wolff,
1988; Brown et al., 1992; Stolcke and Omohundro, 1994), type-based sampling makes stochastic
choices, and moreover, these choices are reversible.
Is this stochasticity important? To answer this, we
consider a variant of T YPE, T YPEgreedy : instead
of sampling from (7), T YPEgreedy considers a type
block S and sets bs to 0 for all s ∈ S if p(bS =
(0, . . . , 0) | · · · ) > p(bS = (1, . . . , 1) | · · · ); else
it sets bs to 1 for all s ∈ S. From Figure 5(a)–(c),
we see that greediness is disastrous for the HMM,
hurts a little for USM, and makes no difference on
the PTSG. These results show that stochasticity can
indeed be important.
We consider another block sampler, S ENTENCE,
which uses dynamic programming to sample all
variables in a sentence (using Metropolis-Hastings
to correct for intra-sentential type-level coupling).
For USM, we see that S ENTENCE performs worse
than T YPE and is comparable to T OKEN, suggesting
that type-based dependencies are stronger and more
important to deal with than intra-sentential dependencies.
5.2

Initialization

We initialized all samplers as follows: For the USM
and PTSG, for each site s, we place a boundary (set
bs = 1) with probability η. For the HMM, we set bs
to state 1 with probability η and a random state with
7

We started with a temperature of 10 and gradually decreased it to 1 during the ﬁrst half of the run, and kept it at 1
thereafter.

-1.1e7

0.2
0.1

3

6

9 12

3

time (hr.)

6

-2.4e5

0.5
Token
Tokenanneal
Typegreedy

-2.8e5
-3.2e5

Type
Sentence

-3.7e5

9 12

2

time (hr.)

4

6

8

2

4

6

-5.8e6
-6.0e6
-6.2e6

8

3

time (min.)

0.3
0.2

0.2 0.4 0.6 0.8 1.0

η

0.6

-2.3e5

0.5

-2.7e5

0.4

F1

0.4

-3.1e5
-3.5e5

0.2 0.4 0.6 0.8 1.0

0.3

0.2 0.4 0.6 0.8 1.0

η

(d) HMM

9 12

-5.5e6

0.2

η

6

time (hr.)

(c) PTSG

-1.9e5

log-likelihood

0.5

accuracy

log-likelihood

0.6

-6.8e6

-7.1e6

0.1

-5.7e6

(b) USM

-6.7e6

-7.0e6

0.2

time (min.)

(a) HMM

-6.9e6

0.4

log-likelihood

-0.9e7

0.4

-5.5e6

log-likelihood

0.5

-9.1e6

0.6

F1

-7.9e6

-1.9e5

log-likelihood

0.6

accuracy

log-likelihood

-6.7e6

0.2 0.4 0.6 0.8 1.0

η

(e) USM

-5.5e6
-5.6e6
-5.6e6
-5.7e6
0.2 0.4 0.6 0.8 1.0

η

(f) PTSG

Figure 5: (a)–(c): Log-likelihood and accuracy over time. T YPE performs the best. Relative to T YPE, T YPEgreedy
tends to hurt performance. T OKEN generally works worse. Relative to T OKEN, T OKENanneal produces mixed results.
S ENTENCE behaves like T OKEN. (d)–(f): Effect of initialization. The metrics were applied to the current sample after
15 hours for the HMM and PTSG and 10 minutes for the USM. T YPE generally prefers larger η and outperform the
other samplers.

probability 1 − η. Results in Figure 5(a)–(c) were
obtained by setting η to maximize likelihood.
Since samplers tend to be sensitive to initialization, it is important to explore the effect of initialization (parametrized by η ∈ [0, 1]). Figure 5(d)–(f)
shows that T YPE is consistently the best, whereas
other samplers can underperform T YPE by a large
margin. Note that T YPE favors η = 1 in general.
This setting maximizes the number of initial types,
and thus creates larger type blocks and thus enables
larger moves. Larger type blocks also mean more
dependencies that T OKEN is unable to deal with.

6

Related Work and Discussion

Block sampling, on which our work is built, is a classical idea, but is used restrictively since sampling
large blocks is computationally expensive. Past
work for clustering models maintained tractability by using Metropolis-Hastings proposals (Dahl,
2003) or introducing auxiliary variables (Swendsen
and Wang, 1987; Liang et al., 2007). In contrast,
our type-based sampler simply identiﬁes tractable

blocks based on exchangeability.
Other methods for learning latent-variable models
include EM, variational approximations, and uncollapsed samplers. All of these methods maintain distributions over (or settings of) the latent variables of
the model and update the representation iteratively
(see Gao and Johnson (2008) for an overview in the
context of POS induction). However, these methods
are at the core all token-based, since they only update variables in a single example at a time.8
Blocking variables by type—the key idea of
this paper—is a fundamental departure from tokenbased methods. Though type-based changes have
also been proposed (Brown et al., 1992; Stolcke and
Omohundro, 1994), these methods operated greedily, and in Section 5.1, we saw that being greedy led
to more brittle results. By working in a sampling
framework, we were able bring type-based changes
to fruition.
8

While EM technically updates all distributions over latent
variables in the E-step, this update is performed conditioned on
model parameters; it is this coupling (made more explicit in
collapsed samplers) that makes EM susceptible to local optima.

References
P. F. Brown, V. J. D. Pietra, P. V. deSouza, J. C. Lai, and
R. L. Mercer. 1992. Class-based n-gram models of
natural language. Computational Linguistics, 18:467–
479.
T. Cohn, S. Goldwater, and P. Blunsom. 2009. Inducing
compact but accurate tree-substitution grammars. In
North American Association for Computational Linguistics (NAACL), pages 548–556.
D. B. Dahl. 2003. An improved merge-split sampler for
conjugate Dirichlet process mixture models. Technical report, Department of Statistics, University of Wisconsin.
J. DeNero, M. Bansal, A. Pauls, and D. Klein. 2009.
Efﬁcient parsing for transducer grammars. In North
American Association for Computational Linguistics
(NAACL), pages 227–235.
J. Gao and M. Johnson. 2008. A comparison of
Bayesian estimators for unsupervised hidden Markov
model POS taggers. In Empirical Methods in Natural
Language Processing (EMNLP), pages 344–352.
S. Goldwater and T. Grifﬁths. 2007. A fully Bayesian
approach to unsupervised part-of-speech tagging. In
Association for Computational Linguistics (ACL).
S. Goldwater, T. Grifﬁths, and M. Johnson. 2006. Contextual dependencies in unsupervised word segmentation. In International Conference on Computational
Linguistics and Association for Computational Linguistics (COLING/ACL).
S. Goldwater, T. Grifﬁths, and M. Johnson. 2009. A
Bayesian framework for word segmentation: Exploring the effects of context. Cognition, 112:21–54.
A. Haghighi and D. Klein. 2006. Prototype-driven learning for sequence models. In North American Association for Computational Linguistics (NAACL), pages
320–327.
P. Liang, M. I. Jordan, and B. Taskar. 2007. A
permutation-augmented sampler for Dirichlet process
mixture models. In International Conference on Machine Learning (ICML).
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning accurate, compact, and interpretable tree annotation. In International Conference on Computational Linguistics and Association for Computational
Linguistics (COLING/ACL), pages 433–440.
M. Post and D. Gildea. 2009. Bayesian learning of a
tree substitution grammar. In Association for Computational Linguistics and International Joint Conference on Natural Language Processing (ACL-IJCNLP).
A. Stolcke and S. Omohundro. 1994. Inducing probabilistic grammars by Bayesian model merging. In
International Colloquium on Grammatical Inference
and Applications, pages 106–118.

R. H. Swendsen and J. S. Wang. 1987. Nonuniversal
critical dynamics in MC simulations. Physics Review
Letters, 58:86–88.
J. G. Wolff. 1988. Learning syntax and meanings
through optimization and distributional analysis. In
Categories and processes in language acquisition,
pages 179–215.

