Joint Parsing and Alignment with Weakly Synchronized Grammars
David Burkett

John Blitzer
Dan Klein
Computer Science Division
University of California, Berkeley
{dburkett,blitzer,klein}@cs.berkeley.edu

Abstract
Syntactic machine translation systems extract
rules from bilingual, word-aligned, syntactically parsed text, but current systems for parsing and word alignment are at best cascaded
and at worst totally independent of one another. This work presents a uniﬁed joint model
for simultaneous parsing and word alignment.
To ﬂexibly model syntactic divergence, we develop a discriminative log-linear model over
two parse trees and an ITG derivation which
is encouraged but not forced to synchronize
with the parses. Our model gives absolute
improvements of 3.3 F1 for English parsing, 2.1 F1 for Chinese parsing, and 5.5 F1
for word alignment over each task’s independent baseline, giving the best reported results
for both Chinese-English word alignment and
joint parsing on the parallel portion of the Chinese treebank. We also show an improvement
of 1.2 BLEU in downstream MT evaluation
over basic HMM alignments.

1

Introduction

Current syntactic machine translation (MT) systems build synchronous context free grammars from
aligned syntactic fragments (Galley et al., 2004;
Zollmann et al., 2006). Extracting such grammars
requires that bilingual word alignments and monolingual syntactic parses be compatible. Because of
this, much recent work in both word alignment and
parsing has focused on changing aligners to make
use of syntactic information (DeNero and Klein,
2007; May and Knight, 2007; Fossum et al., 2008)
or changing parsers to make use of word alignments (Smith and Smith, 2004; Burkett and Klein,

2008; Snyder et al., 2009). In the ﬁrst case, however, parsers do not exploit bilingual information.
In the second, word alignment is performed with a
model that does not exploit syntactic information.
This work presents a single, joint model for parsing
and word alignment that allows both pieces to inﬂuence one another simultaneously.
While building a joint model seems intuitive,
there is no easy way to characterize how word alignments and syntactic parses should relate to each
other in general. In the ideal situation, each pair
of sentences in a bilingual corpus could be syntactically parsed using a synchronous context-free grammar. Of course, real translations are almost always
at least partially syntactically divergent. Therefore,
it is unreasonable to expect perfect matches of any
kind between the two sides’ syntactic trees, much
less expect that those matches be well explained at
a word level. Indeed, it is sometimes the case that
large pieces of a sentence pair are completely asynchronous and can only be explained monolingually.
Our model exploits synchronization where possible to perform more accurately on both word
alignment and parsing, but also allows independent models to dictate pieces of parse trees and
word alignments when synchronization is impossible. This notion of “weak synchronization” is parameterized and estimated from data to maximize
the likelihood of the correct parses and word alignments. Weak synchronization is closely related to
the quasi-synchronous models of Smith and Eisner (2006; 2009) and the bilingual parse reranking
model of Burkett and Klein (2008), but those models
assume that the word alignment of a sentence pair is
known and ﬁxed.
To simultaneously model both parses and align-

ments, our model loosely couples three separate
combinatorial structures: monolingual trees in the
source and target languages, and a synchronous ITG
alignment that links the two languages (but is not
constrained to match linguistic syntax). The model
has no hard constraints on how these three structures must align, but instead contains a set of “synchronization” features that are used to propagate
inﬂuence between the three component grammars.
The presence of synchronization features couples
the parses and alignments, but makes exact inference
in the model intractable; we show how to use a variational mean ﬁeld approximation, both for computing approximate feature expectations during training, and for performing approximate joint inference
at test time.
We train our joint model on the parallel, gold
word-aligned portion of the Chinese treebank.
When evaluated on parsing and word alignment, this
model signiﬁcantly improves over independentlytrained baselines: the monolingual parser of Petrov
and Klein (2007) and the discriminative word
aligner of Haghighi et al. (2009). It also improves
over the discriminative, bilingual parsing model
of Burkett and Klein (2008), yielding the highest
joint parsing F1 numbers on this data set. Finally,
our model improves word alignment in the context
of translation, leading to a 1.2 BLEU increase over
using HMM word alignments.

grammar (SCFG) (Shieber and Schabes, 1990). Figure 1(a) gives a simple example of generation from
a log-linearly parameterized synchronous grammar,
together with its features. With the SCFG restriction, we can sum over the necessary structures using
the O(n6 ) bitext inside-outside algorithm, making
P(t, a, t |s, s ) relatively efﬁcient to compute expectations under.
Unfortunately, an SCFG requires that all the constituents of each tree, from the root down to the
words, are generated perfectly in tandem. The resulting inability to model any level of syntactic divergence prevents accurate modeling of the individual monolingual trees. We will consider the running example from Figure 2 throughout the paper.
Here, for instance, the verb phrase established in
such places as Quanzhou, Zhangzhou, etc. in English does not correspond to any single node in the
Chinese tree. A synchronous grammar has no choice
but to analyze this sentence incorrectly, either by ignoring this verb phrase in English or postulating an
incorrect Chinese constituent that corresponds to it.
Therefore, instead of requiring strict synchronization, our model treats the two monolingual trees and
the alignment as separate objects that can vary arbitrarily. However, the model rewards synchronization
appropriately when the alignment brings the trees
into correspondence.

3
2

Weakly Synchronized Grammars

Joint Parsing and Alignment

Given a source-language sentence, s, and a targetlanguage sentence, s , we wish to predict a source
tree t, a target tree t , and some kind of alignment
a between them. These structures are illustrated in
Figure 1.
To facilitate these predictions, we deﬁne a conditional distribution P(t, a, t |s, s ). We begin with a
generic conditional exponential form:
P(t, a, t |s, s ) ∝ exp θ φ(t, a, t , s, s )

(1)

Unfortunately, a generic model of this form is intractable, because we cannot efﬁciently sum over
all triples (t, a, t ) without some assumptions about
how the features φ(t, a, t , s, s ) decompose.
One natural solution is to restrict our candidate
triples to those given by a synchronous context free

We propose a joint model which still gives probabilities on triples (t, a, t ). However, instead of using
SCFG rules to synchronously enforce the tree constraints on t and t , we only require that each of t
and t be well-formed under separate monolingual
CFGs.
In order to permit efﬁcient enumeration of all possible alignments a, we also restrict a to the set of
unlabeled ITG bitrees (Wu, 1997), though again we
do not require that a relate to t or t in any particular
way. Although this assumption does limit the space
of possible word-level alignments, for the domain
we consider (Chinese-English word alignment), the
reduced space still contains almost all empirically
observed alignments (Haghighi et al., 2009).1 For
1

See Section 8.1 for some new terminal productions required to make this true for the parallel Chinese treebank.

S

S

Features
NP

!( (IP, b0, S), s, s’ )
!( (NP, b1, NP), s, s’ )
!( (VP, b2, VP), s, s’ )

VP

AP

NP
b1

b0

VP

NP

IP

b0

Parsing

IP

b1

VP

b2

Features
φF (IP, s)
φF (NP, s)

b2

VP

φA (b0, s, s’)
φA (b1, s, s’)

φF (VP, s)

NP

φA (b2, s, s’)

φ (IP, b0)
φE (NP, s’) φ (b0, S)
φE (AP, s’) φ (b1, NP)

Alignment

φE (S, s’)

Synchronization

φE (VP, s’) φ (IP, b0, S)

(a) Synchronous Rule

(b) Asynchronous Rule

Figure 1: Source trees, t (right), alignments, a (grid), and target trees, t (top), and feature decompositions for synchronous (a) and weakly synchronous (b) grammars. Features always condition on bispans and/or anchored syntactic
productions, but weakly synchronous grammars permit more general decompositions.

example, in Figure 2, the word alignment is ITGderivable, and each of the colored rectangles is a bispan in that derivation.
There are no additional constraints beyond the
independent, internal structural constraints on t, a,
and t . This decoupling permits derivations like that
in Figure 1(b), where the top-level syntactic nodes
align, but their children are allowed to diverge. With
the three structures separated, our ﬁrst model is a
completely factored decomposition of (1).
Formally, we represent a source tree t as a set of
nodes {n}, each node representing a labeled span.
Likewise, a target tree t is a set of nodes {n }.2 We
represent alignments a as sets of bispans {b}, indicated by rectangles in Figure 1.3 Using this notation,
the initial model has the following form:

P(t, a, t |s, s ) ∝ exp 

θ φF (n, s)+
n∈t


θ φA (b, s, s )+
b∈a

(2)

θ φE (n , s )
n ∈t

Here φF (n, s) indicates a vector of source node features, φE (n , s ) is a vector of target node features,
and φA (b, s, s ) is a vector of alignment bispan features. Of course, this model is completely asyn2

For expositional clarity, we describe n and n as labeled
spans only. However, in general, features that depend on n or
n are permitted to depend on the entire rule, and do in our ﬁnal
system.
3
Alignments a link arbitrary spans of s and s (including
non-constituents and individual words). We discuss the relation
to word-level alignments in Section 4.

chronous so far, and fails to couple the trees and
alignments at all. To permit soft constraints between
the three structures we are modeling, we add a set of
synchronization features.
For n ∈ t and b ∈ a, we say that n £ b if n and b
both map onto the same span of s. We deﬁne b ¡ n
analogously for n ∈ t . We now consider three
different types of synchronization features. Sourcealignment synchronization features φ£ (n, b) are extracted whenever n £ b. Similarly, target-alignment
features φ¡ (b, n ) are extracted if b ¡ n . These
features capture phenomena like that of bispan b7
in Figure 2. Here the Chinese noun 地 synchronizes
with the ITG derivation, but the English projection
of b7 is a distituent. Finally, we extract source-target
features φ (n, b, n ) whenever n£b¡n . These features capture complete bispan synchrony (as in bispan b8 ) and can be expressed over triples (n, b, n )
which happen to align, allowing us to reward synchrony, but not requiring it. All of these licensing
conditions are illustrated in Figure 1(b).
With these features added, the ﬁnal form of the
model is:

P(t, a, t |s, s ) ∝ exp 

θ φF (n, s)+
n∈t

θ φA (b, s, s )+

θ φE (n , s )+
n ∈t

b∈a

θ φ£ (n, b)+
n£b

θ φ¡ (b, n )+
b¡n


θ φ (n, b, n )
n£b¡n

(3)

We emphasize that because of the synchronization
features, this ﬁnal form does not admit any known
efﬁcient dynamic programming for the exact computation of expectations. We will therefore turn to a
variational inference method in Section 6.

4

Features

With the model’s locality structure deﬁned, we
just need to specify the actual feature function,
φ. We divide the features into three types: parsing features (φF (n, s) and φE (n , s )), alignment
features (φA (b, s, s )) and synchronization features
(φ£ (n, b), φ¡ (b, n ), and φ (n, b, n )). We detail
each of these in turn here.
4.1

Parsing

The monolingual parsing features we use are simply parsing model scores under the parser of Petrov
and Klein (2007). While that parser uses heavily reﬁned PCFGs with rule probabilities deﬁned at the
reﬁned symbol level, we interact with its posterior
distribution via posterior marginal probabilities over
unreﬁned symbols. In particular, to each unreﬁned
anchored production i Aj → i Bk Cj , we associate a
single feature whose value is the marginal quantity
log P(i Bk Cj |i Aj , s) under the monolingual parser.
These scores are the same as the variational rule
scores of Matsuzaki et al. (2005).4
4.2

Alignment

We begin with the same set of alignment features
as Haghighi et al. (2009), which are deﬁned only for
terminal bispans. In addition, we include features on
nonterminal bispans, including a bias feature, features that measure the difference in size between
the source and target spans, features that measure
the difference in relative sentence position between
the source and target spans, and features that measure the density of word-to-word alignment posteriors under a separate unsupervised word alignment
model.

4.3

Our synchronization features are indicators for the
syntactic types of the participating nodes. We determine types at both a coarse (more collapsed
than Treebank symbols) and ﬁne (Treebank symbol) level. At the coarse level, we distinguish between phrasal nodes (e.g. S, NP), synthetic nodes
introduced in the process of binarizing the grammar
(e.g. S , NP ), and part-of-speech nodes (e.g. NN,
VBZ). At the ﬁne level, we distinguish all nodes
by their exact label. We use coarse and ﬁne types
for both partially synchronized (source-alignment or
target-alignment) features and completely synchronized (source-alignment-target) features. The inset
of Figure 2 shows some sample features. Of course,
we could devise even more sophisticated features by
using the input text itself. As we shall see, however,
our model gives signiﬁcant improvements with these
simple features alone.

5

Learning

We learn the parameters of our model on the parallel portion of the Chinese treebank. Although our
model assigns probabilities to entire synchronous
derivations of sentences, the parallel Chinese treebank gives alignments only at the word level (1 by
1 bispans in Figure 2). This means that our alignment variable a is not fully observed. Because of
this, given a particular word alignment w, we maximize the marginal probability of the set of derivations A(w) that are consistent with w (Haghighi et
al., 2009).5
L(θ) = log

P(ti , a, ti |si , si )
a∈A(wi )

We maximize this objective using standard gradient
methods (Nocedal and Wright, 1999). As with fully
visible log-linear models, the gradient for the ith sentence pair with respect to θ is a difference of feature
expectations:
L(θ) = EP(a|ti ,wi ,ti ,si ,si ) φ(ti , a, ti , si , si )
− EP(t,a,t |si ,si ) φ(t, a, t , si , si )

4

Of course the structure of our model permits any of the
additional rule-factored monolingual parsing features that have
been described in the parsing literature, but in the present work
we focus on the contributions of joint modeling.

Synchronization

5

(4)

We also learn from non-ITG alignments by maximizing the
marginal probability of the set of minimum-recall error alignments in the same way as Haghighi et al. (2009)

S

Sample Synchronization Features

NP

VP
VBD

φ (NP, b8 , NP )

VP
PP
IN

φ (NN, b7 )

NP
NP

established

in

NNS

IN

such

places

as

=

CoarseSourceAlign pos : 1
FineSourceAlign NN : 1

PP

JJ
were

CoarseSourceTarget phrasal, phrasal : 1
FineSourceTarget NP, NP : 1

VBN

...

=

NP
Quanzhou Zhangzhou

etc.

在

P

泉州
漳州
等

b8

地

b7

!立

PP

NP
NP

VP

NN
VV

了

AS

...

b4

NP

VP

Figure 2: An example of a Chinese-English sentence pair with parses, word alignments, and a subset of the full optimal
ITG derivation, including one totally unsynchronized bispan (b4 ), one partially synchronized bispan (b7 ), and and fully
synchronized bispan (b8 ). The inset provides some examples of active synchronization features (see Section 4.3) on
these bispans. On this example, the monolingual English parser erroneously attached the lower PP to the VP headed by
established, and the non-syntactic ITG word aligner misaligned 等 to such instead of to etc. Our joint model corrected
both of these mistakes because it was rewarded for the synchronization of the two NPs joined by b8 .

We cannot efﬁciently compute the model expectations in this equation exactly. Therefore we turn next
to an approximate inference method.

6

Instead of computing the model expectations from
(4), we compute the expectations for each sentence
pair with respect to a simpler, fully factored distribution Q(t, a, t ) = q(t)q(a)q(t ). Rewriting Q in
log-linear form, we have:



ψn +
n∈t

EQ(a|wi ) φ(ti , a, ti , si , si )
− EQ(t,a,t |si ,si ) φ(t, a, t , si , si )

Mean Field Inference

Q(t, a, t ) ∝ exp 

expectations under Q:

ψb +
b∈a

ψn 
n ∈t

Here, the ψn , ψb and ψn are variational parameters
which we set to best approximate our weakly synchronized model from (3):
ψ ∗ = argmin KL Qψ ||Pθ (t, a, t |s, s )
ψ

Once we have found Q, we compute an approximate
gradient by replacing the model expectations with

Now, we will brieﬂy describe how we compute Q.
First, note that the parameters ψ of Q factor along
individual source nodes, target nodes, and bispans.
The combination of the KL objective and our particular factored form of Q make our inference procedure a structured mean ﬁeld algorithm (Saul and
Jordan, 1996). Structured mean ﬁeld techniques are
well-studied in graphical models, and our adaptation
in this section to multiple grammars follows standard techniques (see e.g. Wainwright and Jordan,
2008).
Rather than derive the mean ﬁeld updates for ψ,
we describe the algorithm (shown in Figure 3) procedurally. Similar to block Gibbs sampling, we iteratively optimize each component (source parse,
target parse, and alignment) of the model in turn,
conditioned on the others. Where block Gibbs sampling conditions on ﬁxed trees or ITG derivations,
our mean ﬁeld algorithm maintains uncertainty in

Input:

sentence pair (s, s )
parameter vector θ

Output:

variational parameters ψ

1.

2.

Initialize
0
ψn ← θ φF (n, s)
0
ψb ← θ φA (b, s, s )
0
ψn ← θ φE (n , s )
P
µ0 ← t qψ0 (t)I(n ∈ t), etc for µ0 , µ0
n
b
n
While not converged, for each n, n , b in
the monolingual and ITG charts
“
P
i
ψn ← θ
φF (n, s) + b,n£b µi−1 φ£ (n, b)+
b
”
P
P
µi−1 µi−1 φ (n, b, n )
b
b,n£b
n ,b¡n
n
P
µi ← t qψ (t)I(n ∈ t) (inside-outside)
n
i
ψb ← θ
P

“

φA (b, s, s ) +

P

n,n£b

µi−1 φ£ (n, b)+
n

µi−1 φ¡ (b, n )+
n
”
P
i−1 i−1
n,n£b
n ,b¡n µn µn φ (n, b, n )
P
µb ← a qψ (a)I(b ∈ a) (bitext inside-outside)
n ,b¡n

P

i
i
updates for ψn , µi analogous to ψn , µi
n
n

3.

Return variational parameters ψ

Figure 3: Structured mean ﬁeld inference for the weakly
synchronized model. I(n ∈ t) is an indicator value for
the presence of node n in source tree t.

the form of monolingual parse forests or ITG forests.
The key components to this uncertainty are the
expected counts of particular source nodes, target
nodes, and bispans under the mean ﬁeld distribution:
µn =

qψ (t)I(n ∈ t)
t

µn =

qψ (t )I(n ∈ t )
t

µb =

qψ (a)I(b ∈ a)
a

Since dynamic programs exist for summing over
each of the individual factors, these expectations can
be computed in polynomial time.
6.1

Pruning

Although we can approximate the expectations from
(4) in polynomial time using our mean ﬁeld distribution, in practice we must still prune the ITG forests
and monolingual parse forests to allow tractable inference. We prune our ITG forests using the same

basic idea as Haghighi et al. (2009), but we employ a technique that allows us to be more aggressive. Where Haghighi et al. (2009) pruned bispans
based on how many unsupervised HMM alignments
were violated, we ﬁrst train a maximum-matching
word aligner (Taskar et al., 2005) using our supervised data set, which has only half the precision errors of the unsupervised HMM. We then prune every bispan which violates at least three alignments
from the maximum-matching aligner. When compared to pruning the bitext forest of our model with
Haghighi et al. (2009)’s HMM technique, this new
technique allows us to maintain the same level of accuracy while cutting the number of bispans in half.
In addition to pruning the bitext forests, we also
prune the syntactic parse forests using the monolingual parsing model scores. For each unreﬁned
anchored production i Aj → i Bk Cj , we compute the marginal probability P(i Aj ,i Bk ,k Cj |s) under the monolingual parser (these are equivalent to
the maxrule scores from Petrov and Klein 2007). We
only include productions where this probability is
greater than 10−20 . Note that at training time, we are
not guaranteed that the gold trees will be included
in the pruned forest. Because of this, we replace the
gold trees ti , ti with oracle trees from the pruned forest, which can be found efﬁciently using a variant of
the inside algorithm (Huang, 2008).

7

Testing

Once the model has been trained, we still need to
determine how to use it to predict parses and word
alignments for our test sentence pairs. Ideally, given
the sentence pair (s, s ), we would ﬁnd:
(t∗ , w∗ , t ∗ ) = argmax P(t, w, t |s, s )
t,w,t

= argmax
t,w,t

P(t, a, t |s, s )
a∈A(w)

Of course, this is also intractable, so we once again
resort to our mean ﬁeld approximation. This yields
the approximate solution:
(t∗ , w∗ , t ∗ ) = argmax
t,w,t

Q(t, a, t )
a∈A(w)

However, recall that Q incorporates the model’s mutual constraint into the variational parameters, which

factor into q(t), q(a), and q(t ). This allows us to
simplify further, and ﬁnd the maximum a posteriori
assignments under the variational distribution. The
trees can be found quickly using the Viterbi inside
algorithm on their respective qs. However, the sum
for computing w∗ under q is still intractable.
As we cannot ﬁnd the maximum probability word
alignment, we provide two alternative approaches
for ﬁnding w∗ . The ﬁrst is to just ﬁnd the Viterbi
ITG derivation a∗ = argmaxa q(a) and then set w∗
to contain exactly the 1x1 bispans in a∗ . The second
method, posterior thresholding, is to compute posterior marginal probabilities under q for each 1x1 cell
beginning at position i, j in the word alignment grid:
m(i, j) =

q(a)I((i, i + 1, j, j + 1) ∈ a)
a

We then include w(i, j) in w∗ if m(w(i, j)) > τ ,
where τ is a threshold chosen to trade off precision
and recall. For our experiments, we found that the
Viterbi alignment was uniformly worse than posterior thresholding. All the results from the next section use the threshold τ = 0.25.

8

Experiments

We trained and tested our model on the translated
portion of the Chinese treebank (Bies et al., 2007),
which includes hand annotated Chinese and English
parses and word alignments. We separated the data
into three sets: train, dev, and test, according to the
standard Chinese treebank split. To speed up training, we only used training sentences of length ≤ 50
words, which left us with 1974 of 2261 sentences.
We measured the results in two ways. First, we
directly measured F1 for English parsing, Chinese
parsing, and word alignment on a held out section of
the hand annotated corpus used to train the model.
Next, we further evaluated the quality of the word
alignments produced by our model by using them as
input for a machine translation system.
8.1

Dataset-speciﬁc ITG Terminals

The Chinese treebank gold word alignments include
signiﬁcantly more many-to-many word alignments
than those used by Haghighi et al. (2009). We are
able to produce some of these many-to-many alignments by including new many-to-many terminals in

h
th dtesth es es
bo siobosidsid
b

岸岸
岸
(a) 2x2

t nt t
en arsen rsars
e
n ec a
in riecinryerecye ye

近年近年
近年
来来
来

(b) 2x3

e i etry trytry
e
tir c urn ir unun
the ehethentent o co
tn e o c

全全
全
国国
国

(c) Gapped 2x3

Figure 4: Examples of phrasal alignments that can be represented by our new ITG terminal bispans.

our ITG word aligner, as shown in Figure 4. Our
terminal productions sometimes capture non-literal
translation like both sides or in recent years. They
also can allow us to capture particular, systematic
changes in the annotation standard. For example,
the gapped pattern from Figure 4 captures the standard that English word the is always aligned to the
Chinese head noun in a noun phrase. We featurize
these non-terminals with features similar to those
of Haghighi et al. (2009), and all of the alignment
results we report in Section 8.2 (both joint and ITG)
employ these features.
8.2

Parsing and Word Alignment

To compute features that depend on external models,
we needed to train an unsupervised word aligner and
monolingual English and Chinese parsers. The unsupervised word aligner was a pair of jointly trained
HMMs (Liang et al., 2006), trained on the FBIS corpus. We used the Berkeley Parser (Petrov and Klein,
2007) for both monolingual parsers, with the Chinese parser trained on the full Chinese treebank, and
the English parser trained on a concatenation of the
Penn WSJ corpus (Marcus et al., 1993) and the English side of train.6
We compare our parsing results to the monolingual parsing models and to the English-Chinese
bilingual reranker of Burkett and Klein (2008),
trained on the same dataset. The results are in
Table 1. For word alignment, we compare to
6

To avoid overlap in the data used to train the monolingual
parsers and the joint model, at training time, we used a separate
version of the Chinese parser, trained only on articles 400-1151
(omitting articles in train). For English parsing, we deemed it
insufﬁcient to entirely omit the Chinese treebank data from the
monolingual parser’s training set, as otherwise the monolingual
parser would be trained entirely on out-of-domain data. Therefore, at training time we used two separate English parsers: to
compute model scores for the ﬁrst half of train, we used a parser
trained on a concatenation of the WSJ corpus and the second
half of train, and vice versa for the remaining sentences.

Monolingual
Reranker
Joint

Ch F1
83.6
86.0
85.7

Test Results
Eng F1 Tot F1
81.2
82.5
83.8
84.9
84.5
85.1

Table 1: Parsing results. Our joint model has the highest
reported F1 for English-Chinese bilingual parsing.

HMM
ITG
Joint

Precision
86.0
86.8
85.5

Test Results
Recall AER
58.4
30.0
73.4
20.2
84.6
14.9

F1
69.5
79.5
85.0

Table 2: Word alignment results. Our joint model has the
highest reported F1 for English-Chinese word alignment.

the baseline unsupervised HMM word aligner and
to the English-Chinese ITG-based word aligner
of Haghighi et al. (2009). The results are in Table 2.
As can be seen, our model makes substantial improvements over the independent models. For parsing, we improve absolute F1 over the monolingual
parsers by 2.1 in Chinese, and by 3.3 in English.
For word alignment, we improve absolute F1 by 5.5
over the non-syntactic ITG word aligner. In addition, our English parsing results are better than those
of the Burkett and Klein (2008) bilingual reranker,
the current top-performing English-Chinese bilingual parser, despite ours using a much simpler set
of synchronization features.
8.3

Machine Translation

We further tested our alignments by using them to
train the Joshua machine translation system (Li and
Khudanpur, 2008). Table 3 describes the results of
our experiments. For all of the systems, we tuned

HMM
ITG
Joint

Rules
1.1M
1.5M
1.5M

Tune
29.0
29.9
29.6

Test
29.4
30.4†
30.6

Table 3: Tune and test BLEU results for machine translation systems built with different alignment tools. † indicates a statistically signiﬁcant difference between a system’s test performance and the one above it.

on 1000 sentences of the NIST 2004 and 2005 machine translation evaluations, and tested on 400 sentences of the NIST 2006 MT evaluation. Our training set consisted of 250k sentences of newswire distributed with the GALE project, all of which were
sub-sampled to have high Ngram overlap with the
tune and test sets. All of our sentences were of
length at most 40 words. When building the translation grammars, we used Joshua’s default “tight”
phrase extraction option. We ran MERT for 4 iterations, optimizing 20 weight vectors per iteration on
a 200-best list.
Table 3 gives the results. On the test set, we also
ran the approximate randomization test suggested by
Riezler and Maxwell (2005). We found that our joint
parsing and alignment system signiﬁcantly outperformed the HMM aligner, but the improvement over
the ITG aligner was not statistically signiﬁcant.

9

Conclusion

The quality of statistical machine translation models depends crucially on the quality of word alignments and syntactic parses for the bilingual training
corpus. Our work presented the ﬁrst joint model
for parsing and alignment, demonstrating that we
can improve results on both of these tasks, as well
as on downstream machine translation, by allowing
parsers and word aligners to simultaneously inform
one another. Crucial to this improved performance
is a notion of weak synchronization, which allows
our model to learn when pieces of a grammar are
synchronized and when they are not. Although exact inference in the weakly synchronized model is
intractable, we developed a mean ﬁeld approximate
inference scheme based on monolingual and bitext
parsing, allowing for efﬁcient inference.

Acknowledgements
We thank Adam Pauls and John DeNero for their
help in running machine translation experiments.
We also thank the three anonymous reviewers for
their helpful comments on an earlier draft of this
paper. This project is funded in part by NSF
grants 0915265 and 0643742, an NSF graduate research fellowship, the CIA under grant HM1582-091-0021, and BBN under DARPA contract HR001106-C-0022.

References
Ann Bies, Martha Palmer, Justin Mott, and Colin Warner.
2007. English Chinese translation treebank v 1.0.
Web download. LDC2007T02.
David Burkett and Dan Klein. 2008. Two languages are
better than one (for syntactic parsing). In EMNLP.
John DeNero and Dan Klein. 2007. Tailoring word
alignments to syntactic machine translation. In ACL.
Victoria Fossum, Kevin Knight, and Steven Abney. 2008.
Using syntax to improve word alignment for syntaxbased statistical machine translation. In ACL MT
Workshop.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What’s in a translation rule? In HLTNAACL.
Aria Haghighi, John Blitzer, John DeNero, and Dan
Klein. 2009. Better word alignments with supervised
ITG models. In ACL.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In ACL.
Zhifei Li and Sanjeev Khudanpur. 2008. A scalable
decoder for parsing-based machine translation with
equivalent language model state maintenance. In ACL
SSST.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Alignment by agreement. In HLT-NAACL.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational
Linguistics, 19(2):313–330.
Takuya Matsuzaki, Yusuki Miyao, and Jun’ichi Tsujii.
2005. Probabilistic CFG with latent annotations. In
ACL.
Jon May and Kevin Knight. 2007. Syntactic realignment models for machine translation. In EMNLP.
Jorge Nocedal and Stephen J. Wright. 1999. Numerical
Optimization. Springer.

Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In HLT-NAACL.
Stefan Riezler and John Maxwell. 2005. On some pitfalls in automatic evaluation and signiﬁcance testing
for MT. In Workshop on Intrinsic and Extrinsic Evaluation Methods for MT and Summarization, ACL.
Lawrence Saul and Michael Jordan. 1996. Exploiting tractable substructures in intractable networks. In
NIPS.
Stuart M. Shieber and Yves Schabes. 1990. Synchronous
tree-adjoining grammars. In ACL.
David A. Smith and Jason Eisner. 2006. Quasisynchronous grammars: Alignment by soft projection
of syntactic dependencies. In HLT-NAACL.
David A. Smith and Jason Eisner. 2009. Parser adaptation and projection with quasi-synchronous grammar
features. In EMNLP.
David A. Smith and Noah A. Smith. 2004. Bilingual parsing with factored estimation: using English
to parse Korean. In EMNLP.
Benjamin Snyder, Tahira Naseem, and Regina Barzilay.
2009. Unsupervised multilingual grammar induction.
In ACL.
Ben Taskar, Simon Lacoste-Julien, and Dan Klein. 2005.
A discriminative matching approach to word alignment. In EMNLP.
Martin J Wainwright and Michael I Jordan. 2008.
Graphical Models, Exponential Families, and Variational Inference. Now Publishers Inc., Hanover, MA,
USA.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377–404.
Andreas Zollmann, Ashish Venugopal, Stephan Vogel,
and Alex Waibel. 2006. The CMU-AKA syntax augmented machine translation system for IWSLT-06. In
IWSLT.

