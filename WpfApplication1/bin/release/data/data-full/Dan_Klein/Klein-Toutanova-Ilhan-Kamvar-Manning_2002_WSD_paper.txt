Combining Heterogeneous Classiﬁers for Word-Sense Disambiguation

Dan Klein, Kristina Toutanova, H. Tolga Ilhan,
Sepandar D. Kamvar and Christopher D. Manning
Computer Science Department
Stanford University
Stanford, CA 94305-9040, USA

Abstract
This paper discusses ensembles of simple but heterogeneous classiﬁers for word-sense disambiguation, examining the Stanford-CS 224 N system entered in the S ENSEVAL -2 English lexical sample
task. First-order classiﬁers are combined by a
second-order classiﬁer, which variously uses majority voting, weighted voting, or a maximum entropy model. While individual ﬁrst-order classiﬁers
perform comparably to middle-scoring teams’ systems, the combination achieves high performance.
We discuss trade-offs and empirical performance.
Finally, we present an analysis of the combination,
examining how ensemble performance depends on
error independence and task difﬁculty.

1 Introduction
The problem of supervised word sense disambiguation (WSD) has been approached using many different classiﬁcation algorithms, including naive-Bayes,
decision trees, decision lists, and memory-based
learners. While it is unquestionable that certain algorithms are better suited to the WSD problem than
others (for a comparison, see Mooney (1996)), it
seems that, given similar input features, various algorithms exhibit roughly similar accuracies. 1 This
was supported by the S ENSEVAL -2 results, where a
This paper is based on work supported in part by the National Science Foundation under Grants IIS-0085896 and IIS9982226, by an NSF Graduate Fellowship, and by the Research
Collaboration between NTT Communication Science Laboratories, Nippon Telegraph and Telephone Corporation and CSLI,
Stanford University.
1 In fact, we have observed that differences between implementations of a single classiﬁer type, such as smoothing or window size, impacted accuracy far more than the choice of classiﬁcation algorithm.

large fraction of systems had scores clustered in a
fairly narrow region (Senseval-2, 2001).
We began building our system with 23 supervised
WSD systems, each submitted by a student taking
the natural language processing course (CS 224 N) at
Stanford University in Spring 2000. Students were
free to implement whatever WSD method they chose.
While most implemented variants of naive-Bayes,
others implemented a range of other methods, including n-gram models, vector space models, and
memory-based learners. Taken individually, the best
of these systems would have turned in an accuracy
of 61.2% in the S ENSEVAL -2 English lexical sample task (which would have given it 6th place), while
others would have produced middling to low performance. In this paper, we investigate how these classiﬁers behave in combination.
In section 2, we discuss the ﬁrst-order classiﬁers
and describe our methods of combination. In section 3, we discuss performance, analyzing what beneﬁt was found from combination, and when. We also
discuss aspects of the component systems which
substantially inﬂuenced overall performance.

2 The System
2.1

Training Procedure

Figure 1 shows the high-level organization of our
system. Individual ﬁrst-order classiﬁers each map
lists of context word tokens to word-sense predictions, and are self-contained WSD systems. The ﬁrstorder classiﬁers are combined in a variety of ways
with second-order classiﬁers. Second-order classiﬁers are selectors, taking a list of ﬁrst-order out-

Final classifier

Chosen
Classifier

2nd. order ranking

Cross
Validation

Majority
Voting

2nd. order classifiers

Weighted
Voting

Maximum
Entropy

1st. order ranking
1st. order classifiers

1

2

3

4

5

6

7

8

Figure 1: High-level system organization.
1
2
3
4
5
6
7
8
9
10
11
12
13

Split data into multiple training and held-out parts.
Rank ﬁrst-order classiﬁers globally (across all words).
Rank ﬁrst-order classiﬁers locally (per word),
breaking ties with global ranks.
For each word w
For each size k
Choose the ensemble E w,k to be the top k classiﬁers
For each voting method m
Train the (k, m) second-order classiﬁer with E w,k
Rank the second-order classiﬁer types (k, m) globally.
Rank the second-order classiﬁer instances locally.
Choose the top-ranked second-order classiﬁer for each word.
Retrain chosen per-word classiﬁers on entire training data.
Run these classiﬁers on test data, and evaluate results.

Table 1: The classiﬁer construction process.

puts and choosing from among them. An outline
of the classiﬁer construction process is given in table 1. First, the training data was split into training
and held-out sets for each word. This was done using 5 random bootstrap splits. Each split allocated
75% of the examples to training and 25% to heldout testing.2 Held-out data was used both to select
the subsets of ﬁrst-order classiﬁers to be combined,
and to select the combination methods.
For each word and each training split, the 23 ﬁrstorder classiﬁers were (independently) trained and
tested on held-out data. For each word, the ﬁrstorder classiﬁers were ranked by their average performance on the held-out data, with the most accurate classiﬁers at the top of the rankings. Ties were
broken by the classiﬁers’ (weighted) average perfomance across all words.
For each word, we then constructed a set of can2 Bootstrap splits were used rather than standard n-fold
cross-validation for two reasons. First, it allowed us to generate an arbitrary number of training/held-out pairs while still
leaving substantial held-out data set sizes. Second, this approach is commonly used in the literature on ensembles. Its
well-foundedness and theoretical properties are discussed in
Breiman (1996). In retrospect, since we did not take proper advantage of the ability to generate numerous splits, it might have
been just as well to use cross-validation.

didate second-order classiﬁers. Second-order classiﬁer types were identiﬁed by an ensemble size k
and a combination method m. One instance of each
second-order type was constructed for each word.
We originally considered ensemble sizes k in the
range {1, 3, 5, 7, 9, 11, 13, 15}. For a second-order
classiﬁer with ensemble size k, the ensemble members were the top k ﬁrst-order classiﬁers according
to the local rank described above.
We combined ﬁrst-order ensembles using one of
three methods m:
• Majority voting: The sense output by the most
ﬁrst-order classiﬁers in the ensemble was chosen.
Ties were broken by sense frequency, in favor of
more frequent senses.
• Weighted voting: Each ﬁrst-order classiﬁer was
assigned a voting weight (see below). The sense
receiving the greatest total weighted vote was
chosen.
• Maximum entropy: A maximum entropy classiﬁer
was trained (see below) and run on the outputs of
the ﬁrst-order classiﬁers.
We considered all pairs of k and m, and so for
each word there were 24 possible second-order classiﬁers, though for k = 1 all three values of m are
equivalent and were merged. The k = 1 ensemble,
as well as the larger ensembles (k ∈ {9, 11, 13, 15}),
did not help performance once we had good ﬁrstorder classiﬁer rankings (see section 3.4).
For m = Majority, there are no parameters to set.
For the other two methods, we set the parameters of
the (k, m) second-order classiﬁer for a word w using
the bootstrap splits of the training data for w.
In the same manner as for the ﬁrst-order classiﬁers, we then ranked the second-order classiﬁers.
For each word, there was the local ranking of the
second-order classiﬁers, given by their (average) accuracy on held-out data. Ties in these rankings were
broken by the average performance of the classiﬁer
type across all words. The top second-order classiﬁer for each word was selected from these tie-broken
rankings.
At this point, all ﬁrst-order ensemble members
and chosen second-order combination methods were
retrained on the unsplit training data and run on the
ﬁnal test data.

It is important to stress that each target word was
considered an entirely separate task, and different
ﬁrst- and second-order choices could be, and were,
made for each word (see the discussion of table 2
below). Aggregate performance across words was
only used for tie-breaking.

ticed that, for certain words, simple majority voting
performed better than the maximum entropy model.
It also turned out that the most complex features we
could get value from were features of the form:

2.2

That is, for each ﬁrst-order classiﬁer, there is a single feature which is true exactly when that classiﬁer is correct. With only these features, the maximum entropy approach also reduces to a weighted
vote; the s which maximizes the posterior probability P(s|s1 , . . . , sk ) also maximizes the vote:

Combination Methods

Our second-order classiﬁers take training instances
of the form s = (s, s1 , . . . , sk ) where s is the correct
¯
sense and each si is the sense chosen by classiﬁer i.
All three of the combination schemes which we used
can be seen as weighted voting, with different ways
of estimating the voting weights λi of the ﬁrst-order
voters. In the simplest case, majority voting, we skip
any attempt at statistical estimation and simply set
each λi to be 1/k.
For the method we actually call “weighted voting,” we view the combination output as a mixture
model in which each ﬁrst-order system is a mixture
component:
P(s|s1 , . . . , sk ) =

λi P(s|si )
i

The conditional probabilties P(s|s i ) assign mass
one to the sense si chosen by classiﬁer i. The mixture weights λi were estimated using EM to maximize the likelihood of the second-order training
instances. In testing, the sense with the highest
weighted vote, and hence highest posterior likelihood, is the selected sense.
For the maximum entropy classiﬁer, we have a
different model for the chosen sense s. In this case,
it is an exponential model of the form:
P(s|s1 , . . . , sk ) =

exp
t exp

x

λx f x (s, s1 , . . . , sk )
x λ x f x (t, s1 , . . . , sk )

The features f x are functions which are true over
some subset of vectors s . The original intent was to
¯
design features to recognize and exploit “sense expertise” in the individual classiﬁers. For example,
one classiﬁer might be trustworthy when reporting
a certain sense but less so for other senses. However, there was not enough data to accurately estimate parameters for such models.3 In fact, we no3 The number of features was not large: only one for each

(classiﬁer, chosen sense, correct sense) triple. However, most
senses are rarely chosen and rarely correct, so most features
had zero or singleton support.

f i (s, s1 , . . . , sk ) = 1 ⇐⇒ s = si

v(s) =

i

λi δ(si = s)

The indicators δ are true for exactly one sense, and
correspond to the simple f i deﬁned above.4 The
sense with the largest vote v(s) will be the sense
with the highest posterior probability P(s|s 1 , . . . sk )
and will be chosen.
For the maximum entropy classiﬁer, we estimate
the weights by maximizing the likelihood of a heldout set, using the standard IIS algorithm (Berger et
al., 1996). For both weighted schemes, we found
that stopping the iterative procedures before convergence gave better results. IIS was halted after 50
rounds, while EM was halted after a single round.
Both methods were initialized to uniform starting
weights.
More importantly than changing the exact weight
estimates, moving from method to method triggers
broad qualitative changes in what kind of weights
are allowed. With majority voting, classiﬁers all
have equal, positive weights. With weighted voting, the weights are no longer required to be equal,
but are still non-negative. With maximum entropy
weighting, this non-negativity constraint is also relaxed, allowing classiﬁers’ votes to actually reduce
the score for the sense that classiﬁer has chosen.
Negative weights are in fact assigned quite frequently, and often seem to have the effect of using
poor classiﬁers as “error masks” to cancel out common errors.
As we move from majority voting to weighted
voting to maximum entropy, the estimation becomes
4 If the ith classiﬁer returns the correct sense s, then

δ(si = s) is 1, otherwise it is zero.

more sophisticated, but also more prone to overﬁtting. Since solving the overﬁtting problem is hard,
while choosing between classiﬁers based on heldout data is relatively easy, this spectrum gives us a
way to gracefully handle the range of sparsities in
the training corpora for different words.
2.3

Individual Classiﬁers

While our ﬁrst-order classiﬁers implemented a variety of classiﬁcation algorithms, the differences in
their individual accuracies did not primarily stem
from the algorithm chosen. Rather, implementation
details led to the largest differences. For example,
naive-Bayes classiﬁers which chose sensible window sizes, or dynamically chose between window
sizes, tended to outperform those which chose poor
sizes. Generally, the optimal windows were either
of size one (for words with strong local syntactic or
collocational cues) or of very large size (which detected more topical cues). Programs with hard-wired
window sizes of, say, 5, performed poorly. Ironically, such middle-size windows were commonly
chosen by students, but rarely useful; either extreme
was a better design.5
Another implementation choice dramatically affecting performance of naive-Bayes systems was the
amount and type of smoothing. Heavy smoothing
and smoothing which backed off conditional distributions P(w j |si ) to the relevant marginal P(w j )
gave good results, while insufﬁcient smoothing or
backing off to uniform marginals gave substantially
degraded results.6
There is one signiﬁcant way in which our ﬁrstorder classiﬁers were likely different from other
teams’ systems. In the original class project, students were guaranteed that the ambiguous word
would appear only in a single orthographic form,
and many of the systems depended on the input satisfying this guarantee. Since this was not true of
the S ENSEVAL -2 data, we mapped the ambiguous
5 Such window sizes were also apparently chosen by other
S ENSEVAL -2 systems, which commonly used “long distance”
and “local” features, but deﬁned local as a window size of 3–5
words on each side of the ambiguous word.
6 In particular, there is a defective behavior with naive-Bayes
where, when one smooths far too little, the chosen sense is the
one which has occurred with the most words in the context
window. For small training sets of skewed-prior data like the
S ENSEVAL -2 sets, this is invariably the common sense, regardless of the context words.

words (but not context words) to a citation form.
We suspect that this lost quite a bit of information
and negatively affected the system’s overall performance, since there is considerable correlation between form and sense, especially for verbs. Nevertheless, we have made no attempt to re-engineer
the student systems, and have not thoroughly investigated how big a difference this stemming made.

3 Results and Discussion
3.1

Results

Table 2 shows the results per word, and table 3
shows results by part-of-speech and overall, for the
S ENSEVAL -2 English lexical sample task. It also
shows what second-order classiﬁers were selected
for each word. 54.2% of the time, we made an optimal second-order classiﬁer choice. When we chose
wrong, we usually made a mistake in either ensemble size or method, rarely both. A wide range of
second-order classiﬁer types were chosen. As an
overview of the beneﬁt of combination, the globally
best single classiﬁer scored 61.2%, the locally best
single classiﬁer (best on test data) scored 62.2%, the
globally best second order classiﬁer (ME-7, best on
test data) scored 63.2%, and our dynamic selection
method scored 63.9%. Section 3.3 examines combination effectiveness more closely.
3.2

Changes from S ENSEVAL -2

The system we originally submitted to the
S ENSEVAL -2 competition had an overall accuracy of 61.7%, putting it in 4th place in the revised
rankings (among 21 supervised and 28 total systems). Assuming that our ﬁrst-order classiﬁers
were ﬁxed black-boxes, we wanted an idea of how
good our combination and selection methods were.
To isolate the effectiveness of our second-order
classiﬁer choices, we compared our system to an
oracle method (OR-B EST) which chose a word’s
second-order classiﬁer based on test data (rather
than held-out data). The overall accuracy of this
oracle method was 65.4% at the time, a jump of
3.7%.7 This gap was larger than the gap between
the various top-scoring teams’ systems. Therefore,
while the test-set performance of the second-order
classiﬁers is obviously not available, it was clear
7 With other changes, OR-B EST rose to 66.1%.

Combination
M J -7 W T-7 ME-7
66.4
67.9
67.8
69.0
69.4
69.9
53.4
54.7
55.8
61.5
62.7
63.2

OR
B EST
71.9
71.6
58.2
68.9

UB
S OME
81.2
81.0
71.2
72.0

System
ACC
69.7
69.9
55.7
63.9

65
64



Chosen
Combination
Maximum
Entropy

63
62
61

Vote
Vote

60

Single

59
58
1

3

5

7

9

11

13

15

 © §  © ¢¤¢©¤¦¤¢ 
¨      ¨§ ¥ £ ¡

that a more sophisticated or better-tuned method
of selecting combination models could lead to
signiﬁcant improvement. In fact, changing only
ranking methods, which are discussed further in the
next section, resulted in an increase in ﬁnal accuracy for our system to the current score of 63.9%,
which would have placed it 1st in the S ENSEVAL -2
preliminary results or 2nd in the revised results. Our

Baselines
MFS S NG
50.5 63.8
57.8 66.7
40.2 48.7
47.5 62.2

Table 3: Results by part-of-speech, and overall.

!"

Table 2: Results by word for the S ENSEVAL -2 English lexical sample task. Lower bound (LB): A LL is how often all of
the ﬁrst-orders chose correctly. Baselines (BL): MFS is the
most-frequent-sense baseline, S NG is the best single ﬁrst-order
classiﬁer as chosen on held-out data for that word. Fixed combinations: majority vote (M J), weighted vote (W T), maximum
entropy (ME). Oracle bound (OR): B EST is the best secondorder classiﬁer as measured on the test data. Upper bound (UB):
S OME is how often at least one ﬁrst-order classiﬁer produced
the correct answer. Methods which are ensemble-size dependent are shown for k = 7. System choices: ACC is the accuracy
of the selection the system makes based on held-out data. C L is
the 2nd-order classiﬁer selected.

noun
adj.
verb
avg.

LB
A LL
42.5
45.1
28.8
46.5

9 T ©S 5R8E PH
2 IBQ I

System
ACC
CL
58.2
W T-5
66.3
W T-3
72.2
ME-7
83.6
M J -7
83.6
W T-7
77.8
ME-7
30.3
W T-7
33.3
M J -5
81.2
ME-3
67.1
ME-3
71.9
W T-5
73.4
W T-7
78.8
ME-5
90.0
W T-5
68.6
ME-5
59.6
ME-5
63.4
W T-3
42.0
M J -3
26.8
W T-5
55.9
ME-7
37.5
W T-5
42.9
M J -3
92.9
W T-3
83.9
W T-5
70.7
W T-7
78.3
M J -3
90.7
M J -5
72.5
W T-3
32.4
W T-3
52.9
M J -3
93.1
M J -5
64.6
ME-3
79.3
W T-5
83.0
M J -3
60.8
M J -7
62.5
W T-3
83.9
W T-5
52.2
W T-5
75.5
W T-3
54.5
W T-5
71.6
M J -3
71.1
M J -7
47.6
W T-3
66.7
W T-3
53.3
M J -5
70.3
W T-5
55.3
W T-3
60.9
M J -5
79.3
W T-5
51.5
W T-5
67.1
ME-3
23.3
W T-3
57.8
M J -7
66.7
ME-5
42.0
M J -5
64.2
M J -7
62.7
W T-5
51.5
ME-3
96.0
W T-3
78.8
W T-3
48.7
W T-5
31.5
M J -5
57.1
W T-7
52.3
W T-3
31.3
M J -5
72.4
ME-3
92.1
W T-5
80.0
ME-3
58.3
M J -7
45.0
W T-3
78.6
W T-5

#

UB
S OME
69.4
78.3
81.5
94.6
90.9
82.2
62.1
62.1
84.1
78.1
90.6
85.9
80.0
90.0
82.9
80.8
82.8
68.1
41.5
72.9
65.6
61.9
96.4
84.9
86.2
100.0
93.0
86.3
48.5
67.1
96.6
74.4
82.8
88.3
84.3
84.4
87.1
65.7
81.1
68.2
77.6
92.1
78.6
73.9
78.3
70.3
76.7
82.6
89.7
62.1
73.4
46.7
77.8
82.2
55.1
79.2
74.5
54.5
96.0
81.8
82.1
55.6
76.2
70.5
52.2
75.0
92.1
84.0
83.3
63.3
82.1

$ %&

OR
B EST
58.2
69.6
72.2
88.2
85.5
77.8
34.8
37.9
82.6
67.1
75.0
75.0
78.8
90.0
68.6
59.6
69.0
43.5
29.3
59.3
37.5
45.2
92.9
83.9
74.1
82.6
90.7
74.5
32.4
57.1
93.1
65.9
79.3
85.1
74.5
75.0
83.9
52.2
77.4
54.5
71.6
71.1
57.1
66.7
56.7
70.3
58.3
67.4
86.2
51.5
67.1
28.3
60.0
71.1
42.0
64.2
66.7
51.5
96.0
78.8
51.3
40.7
57.1
54.5
35.8
72.4
92.1
82.0
58.3
45.0
78.6

G 9 4 8 DE CA
F B

Combination
W T-7
ME-7
54.1
52.0
69.6
65.2
69.5
72.2
84.3
88.2
83.6
85.5
75.6
77.8
30.3
27.3
33.3
33.3
82.6
82.6
60.3
65.8
70.3
70.3
73.4
75.0
65.9
78.8
90.0
90.0
68.6
68.6
55.8
59.6
68.3
66.2
43.5
42.0
26.8
24.4
52.5
55.9
37.5
34.4
45.2
40.5
89.3
89.3
83.9
82.8
70.7
65.5
78.3
78.3
90.7
90.7
70.6
72.5
27.9
30.9
57.1
54.3
89.7
86.2
65.9
61.0
79.3
79.3
83.0
83.0
60.8
58.8
71.9
65.6
83.9
80.6
49.3
52.2
75.5
77.4
53.0
50.0
59.7
65.7
68.4
68.4
52.4
57.1
55.1
50.7
53.3
45.0
70.3
70.3
50.5
58.3
67.4
65.2
82.8
82.8
51.5
50.0
57.0
65.8
21.7
28.3
53.3
60.0
68.9
71.1
42.0
42.0
60.4
50.9
64.7
66.7
51.5
51.5
96.0
96.0
75.8
78.8
43.6
35.9
29.6
29.6
57.1
54.0
54.5
52.3
29.9
32.8
65.8
72.4
92.1
92.1
82.0
82.0
58.3
25.0
43.3
41.7
78.6
78.6

@ 92 86 531
7 42

M J -7
52.0
69.6
61.6
83.6
83.6
75.6
25.8
34.8
82.6
60.3
67.2
73.4
65.9
90.0
68.6
57.7
69.0
42.0
29.3
52.5
37.5
45.2
89.3
83.9
67.2
78.3
88.4
62.7
30.9
51.4
89.7
65.9
79.3
83.0
60.8
75.0
83.9
38.8
75.5
43.9
53.7
71.1
52.4
55.1
53.3
70.3
49.5
63.0
82.8
40.9
49.4
21.7
57.8
71.1
42.0
64.2
60.8
51.5
96.0
75.8
43.6
31.5
57.1
54.5
32.8
65.8
92.1
80.0
58.3
43.3
78.6

0

Baselines
MFS
S NG
41.8
50.6
33.7
61.3
39.7
63.7
58.6
70.0
83.6
77.8
75.6
71.3
25.8
33.3
22.7
27.8
79.7
84.2
27.4
61.1
54.7
57.9
53.1
63.1
27.1
70.9
90.0
92.9
65.7
80.0
46.2
65.0
59.3
58.4
29.0
35.2
9.8
23.4
42.4
49.9
25.0
31.7
28.6
40.0
89.3
86.5
83.9
80.9
48.3
70.5
78.3
65.0
76.7
83.9
56.9
76.7
14.7
37.6
38.6
46.9
51.7
87.7
39.0
58.2
75.9
81.4
78.7
80.0
54.9
49.2
75.0
56.3
83.9
89.7
37.3
36.1
69.8
67.7
31.8
29.1
50.7
54.6
57.9
76.8
35.7
30.4
42.0
56.0
45.0
40.5
70.3
71.1
27.2
50.4
45.7
51.3
69.0
73.7
19.7
35.6
31.6
66.5
21.7
27.7
53.3
49.0
31.1
53.9
31.9
40.0
22.6
46.3
29.4
54.4
51.5
43.0
96.0
89.2
63.6
81.8
46.2
47.0
16.7
32.3
30.2
48.3
38.6
51.8
14.9
38.8
65.8
69.6
92.1
91.5
80.0
83.2
25.0
40.0
26.7
28.1
78.6
81.4

'"

LB
A LL
28.6
45.7
31.1
50.0
65.5
71.1
1.5
9.1
76.8
46.6
34.4
56.2
52.9
90.0
48.6
15.4
36.6
11.6
4.9
25.4
3.1
16.7
85.7
82.8
36.2
56.5
67.4
29.4
7.4
32.9
51.7
26.8
62.1
69.1
25.5
46.9
77.4
19.4
60.4
21.2
20.9
15.8
11.9
39.1
15.0
70.3
18.4
23.9
51.7
12.1
26.6
1.7
28.9
35.6
29.0
18.9
35.3
51.5
96.0
66.7
7.7
5.6
22.2
36.4
1.5
61.8
84.2
70.0
16.7
10.0
75.0

'()

Word
art-n
authority-n
bar-n
begin-v
blind-a
bum-n
call-v
carry-v
chair-n
channel-n
child-n
church-n
circuit-n
collaborate-v
colorless-a
cool-a
day-n
develop-v
draw-v
dress-v
drift-v
drive-v
dyke-n
face-v
facility-n
faithful-a
fatigue-n
feeling-n
ﬁnd-v
ﬁne-a
ﬁt-a
free-a
graceful-a
green-a
grip-n
hearth-n
holiday-n
keep-v
lady-n
leave-v
live-v
local-a
match-v
material-n
mouth-n
nation-n
natural-a
nature-n
oblique-a
play-v
post-n
pull-v
replace-v
restraint-n
see-v
sense-n
serve-v
simple-a
solemn-a
spade-n
stress-n
strike-v
train-v
treat-v
turn-v
use-v
vital-a
wander-v
wash-v
work-v
yew-n

Figure 2: Accuracy of the various combination methods as
the ensemble size varies. The three combination methods are
shown. In addition, the globally best single classiﬁer is the single ﬁrst-order classiﬁer with the highest overall accuracy on the
test data. Chosen combination is our ﬁnal system’s score. These
two are both independent of k in this graph.

ﬁnal accuracy is thus higher than the ﬁrst draft of
the system, and, in particular, the classiﬁer selection
gap between actual performance and the OR-B EST
oracle has been substantially decreased.
In addition, since the top ﬁrst-order classiﬁers
were more reliably identiﬁed, larger ensembles were
no longer beneﬁcial in the revised system, for an interesting reason. When the ﬁrst-order rankings were
poorly estimated, large ensembles and weighted
methods were important for achieving good accuracy, because the weighting scheme could “rescue”
good classiﬁers which had been incorrectly ranked
low. In our current system, however, ﬁrst-order classiﬁers were ranked reliably enough that we could restrict our ensemble sizes to k ∈ {1, 3, 5, 7}. Furthermore, since k = 1 was only chosen a few times,
usually among ties, we removed that option as well.
3.3

Combination Methods and Ensemble Size

Our system differs from the typical ensemble of
classiﬁers in that the ﬁrst-order classiﬁers are not
merely perturbations of each other, but are highly
varied in both quality and character. This scenario
has been investigated before, e.g. (Zhang et al.,
1992), but is not the common case. With such heterogeneity, having more classiﬁers is not always better. Figure 2 shows how the three combination methods’ average scores varied with the number of com-

ponent classiﬁers used. Initially, accuracy increases
as added classiﬁers bring value to the ensemble.
However, as lower-quality classiﬁers are added in,
the better classiﬁers are steadily drowned out. The
weighted vote and maximum entropy combinations
are much less affected by low-quality classiﬁers than
the majority vote, being able to suppress them with
low weights. Still, majority vote over small ensembles was effective for some words where weights
could not be usefully set by the other methods.
3.4

Ranking Methods

Because of the effects described above, it was necessary to identify which classiﬁers were worth including for a given word. A global ranking of ﬁrstorder classiﬁers, averaged over all words, was not
effective because the strengths of the classiﬁers were
so different. In fact, every single ﬁrst-order classiﬁer was a top-5 performer on at least one word.
On the other hand, S ENSEVAL -2 training sets were
often very small, and very skewed towards a frequent most-frequent-sense. As a result, accuracy estimates based on single words’ held-out data produced frequent ties. The average size of the perword largest set of tied ﬁrst-order classiﬁers was
3.6 (with a maximum of 23 on the word collaborate where all tied). The second-order local rankings also produced many ties. For the top position
(the most important for second-order ranks) 43.1%
of the words had local ties.
In our submitted entry, all ties were broken unintelligently (in an arbitrary manner based on the order
in which systems were listed in a ﬁle). The approach
of local ranking with global tie-breaking presented
in this paper was much more successful according to two distinct measures. First, it predicted the
true ranks more accurately, (measured by the Spearman rank correlation: 0.08 for global ranks, 0.63
for globally-broken local ranks) and gave better ﬁnal accuracy scores (63.5% with global, 63.9% with
globally-broken local – signiﬁcant only at p=0.1 by
a sign test at the word type level).
The other ranking that our system attempts to estimate is the per-word ranking of the second-order
classiﬁers. In this case, however, we are only
ever concerned with which classiﬁer ends up being ranked ﬁrst, as only that classiﬁer is chosen.
Again, globally-broken local ranks were the most

effective, choosing a second-order classiﬁer which
was actually top-performing on test data for 54% of
the words, as opposed to 50% for global selection
(and increasing the overall accuracy from 62.8% to
63.9% – signiﬁcant at p=0.01, sign test).
These results stress that ranking, and effective tiebreaking, are important for a system such as ours
where the classiﬁers are so divergent in behavior.
3.5

Combination

When combining classiﬁers, one would like to know
when and how the combination will outperform the
individuals. One factor (Tumer and Ghosh, 1996)
is how complementary the mistakes of the individual classiﬁers are. If all make the same mistakes,
combination can do no good. We can measure this
complementarity by averaging, over all pairs of ﬁrstorder classiﬁers, the fraction of errors that pair has in
common. This gives average pairwise error independence. Another factor is the difﬁculty of the word
being disambiguated. A high most-frequent-sense
baseline (BL - MFS) means that there is little room
for improvement by combining classiﬁers. Figure 3
shows, for the global top 7 ﬁrst-order classiﬁers, the
absolute gain between their average accuracy (BL AVG -7) and the accuracy of their majority combination (M J -7). The quantity on the x-axis is the difference between the pairwise independence and the
baseline accuracy. The pattern is loose, but clear.
When either independence increases or the word’s
difﬁculty (as indicated by the BL - MFS baseline) increases, the combination tends to win by a greater
amount.
Figure 4 shows how the average pairwise independent error fraction (api) varies as we add classiﬁers. Here classiﬁers are added in an order based on
their accuracy on the entire test set. For each k, the
average is over all pairs of classiﬁers in the top k and
all samples of all words. This graph should be compared to ﬁgure 2. After the third classiﬁer, adding
classiﬁers reduces the api, and the performance of
the majority vote begins to drop at exactly this point.
However, the weighted methods continue to gain in
accuracy since they have the capacity to downweight
classiﬁers which hurt held-out accuracy.
The drop in api reﬂects that the newly added systems are no longer bringing many new correct answers to the collection. However, they can still add

25

of the available training data. Careful but greedy determination of rankings proved to be effective, capturing the highly word-dependent strengths of our
classiﬁers. The resulting system’s overall accuracy
is very high, despite the medium level of accuracy
of the component systems.

5E
3
97D

20

4 A23

15

47@9

10

D BC
78 67

5

3) 5 2
4

3 12

0

()

-5

0

-100

-80

-60

-40

-20

0

20

40

' ¢ ¢§ %§¥  ¢" §©¥ §¢©§¢§¢©§¥ ¢¢£ ¡ ¢ 
& ¥ ¨ ¡ ¨ $ ¨ # ¨  !     ¨  ¥ ¨ ¦ ¥ ¨  ¨ ¦ ¤ ¡ ¡

Figure 3: Gain in accuracy of majority vote over the average
component performance as (pairwise independence − baseline
accuracy) grows.
0.45

st
dpi

gqi

0.43

qi

0.41

gpi

0.39

gf

0.37

r
gi h
ih
ed

dd c
0.35
2

4

6

8

10

12

14

16

18

20

22

5 Acknowledgments
We would like to thank the following people for
contributing their classiﬁers to the Stanford CS 224 N
system: Zoe Abrams, Jenny Berglund, Dmitri Bobrovnikoff, Chris Callison-Burch, Marcos Chavira,
Shipra Dingare, Elizabeth Douglas, Sarah Harris, Ido Milstein, Jyotirmoy Paul, Soumya Raychaudhuri, Paul Ruhlen, Magnus Sandberg, Adil
Sherwani, Philip Shilane, Joshua Solomin, Patrick
Sutphin, Yuliya Tarnikova, Ben Taskar, Kristina
Toutanova, Christopher Unkel, and Vincent Vanhoucke.

24

a "R b V ¢""` Y XIU"IQI%F
S b a a W V T S R P H G

Figure 4: The average pairwise error independence of classiﬁers
as their number is increased.

deciding votes in areas where the ensemble had the
right answer, but did not choose it. The ﬁnal gradual
rise in api reﬂects the somewhat patternless new errors that substantially lower-performing systems unfortunately bring to the ensemble.

4 Conclusions
In this paper, we have explored ensemble sizes, combination methods, bounds for what can be expected
from combinations, factors in the performance of individual classiﬁers, and methods of improving performance by effective tie-breaking. In accord with
much recent work on classiﬁer combination, e.g.
(Breiman, 1996; Bauer and Kohavi, 1999), we have
demonstrated that the combination of classiﬁers can
lead to a substantial performance increase over the
individual classiﬁers within the domain of WSD. In
addition, we have shown that highly varying component systems augment each other well and that
adding lower-scoring systems can still improve ensemble performance, at least to a certain point. A
particular emphasis of our research has been how to
make the combination robust to both the wide range
of ﬁrst-order classiﬁer accuracies and to the sparsity

References
Alan Agresti. 1990. Categorical Data Analysis. John Wiley &
Sons.
Eric Bauer and Ron Kohavi. 1999. An empirical comparison of
voting classication algorithms: Bagging, boosting and variants. Machine Learning, 36:105–139.
Adam L. Berger, Stephen A. Della Pietra, and Vincent J. Della
Pietra. 1996. A maximum entropy approach to natural language processing. Computational Linguistics, 22(1):39–71.
Leo Breiman. 1996. Bagging predictors. Machine Learning,
24:123–140.
Raymond J. Mooney. 1996. Comparative experiments on disambiguating word senses: An illustration of the role of bias
in machine learning. In EMNLP 1.
Senseval-2. 2001. Senseval-2 proceedings, in publication.
Kagan Tumer and Joydeep Ghosh. 1996. Error correlation and
error reduction in ensemble classiﬁers. Connection Science,
8:385–404.
Xiru Zhang, Jill Mesirov, and David L. Waltz. 1992. Hybrid
system for protein structure prediction. Journal of Molecular
Biology, 225:1049–1063.

