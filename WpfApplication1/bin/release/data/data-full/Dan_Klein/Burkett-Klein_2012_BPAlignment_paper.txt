Fast Inference in Phrase Extraction Models with Belief Propagation
David Burkett and Dan Klein
Computer Science Division
University of California, Berkeley
{dburkett,klein}@cs.berkeley.edu

Abstract
Modeling overlapping phrases in an alignment model can improve alignment quality
but comes with a high inference cost. For
example, the model of DeNero and Klein
(2010) uses an ITG constraint and beam-based
Viterbi decoding for tractability, but is still
slow. We ﬁrst show that their model can be
approximated using structured belief propagation, with a gain in alignment quality stemming from the use of marginals in decoding.
We then consider a more ﬂexible, non-ITG
matching constraint which is less efﬁcient for
exact inference but more efﬁcient for BP. With
this new constraint, we achieve a relative error
reduction of 40% in F5 and a 5.5x speed-up.

1

Introduction

Modern statistical machine translation (MT) systems most commonly infer their transfer rules from
word-level alignments (Koehn et al., 2007; Li and
Khudanpur, 2008; Galley et al., 2004), typically
using a deterministic heuristic to convert these to
phrase alignments (Koehn et al., 2003). There have
been many attempts over the last decade to develop
model-based approaches to the phrase alignment
problem (Marcu and Wong, 2002; Birch et al., 2006;
DeNero et al., 2008; Blunsom et al., 2009). However, most of these have met with limited success
compared to the simpler heuristic method. One key
problem with typical models of phrase alignment
is that they choose a single (latent) segmentation,
giving rise to undesirable modeling biases (DeNero
et al., 2006) and reducing coverage, which in turn
reduces translation quality (DeNeefe et al., 2007;
DeNero et al., 2008). On the other hand, the extraction heuristic identiﬁes many overlapping options,
and achieves high coverage.

In response to these effects, the recent phrase
alignment work of DeNero and Klein (2010) models extraction sets: collections of overlapping phrase
pairs that are consistent with an underlying word
alignment. Their extraction set model is empirically
very accurate. However, the ability to model overlapping – and therefore non-local – features comes
at a high computational cost. DeNero and Klein
(2010) handle this in part by imposing a structural
ITG constraint (Wu, 1997) on the underlying word
alignments. This permits a polynomial-time algorithm, but it is still O(n6 ), with a large constant
factor once the state space is appropriately enriched
to capture overlap. Therefore, they use a heavily
beamed Viterbi search procedure to ﬁnd a reasonable alignment within an acceptable time frame. In
this paper, we show how to use belief propagation
(BP) to improve on the model’s ITG-based structural formulation, resulting in a new model that is
simultaneously faster and more accurate.
First, given the model of DeNero and Klein
(2010), we decompose it into factors that admit
an efﬁcient BP approximation. BP is an inference
technique that can be used to efﬁciently approximate posterior marginals on variables in a graphical
model; here the marginals of interest are the phrase
pair posteriors. BP has only recently come into use
in the NLP community, but it has been shown to be
effective in other complex structured classiﬁcation
tasks, such as dependency parsing (Smith and Eisner, 2008). There has also been some prior success
in using BP for both discriminative (Niehues and
Vogel, 2008) and generative (Cromi` res and Kuroe
hashi, 2009) word alignment models.
By aligning all phrase pairs whose posterior under
BP exceeds some ﬁxed threshold, our BP approximation of the model of DeNero and Klein (2010) can

achieve a comparable phrase pair F1 . Furthermore,
because we have posterior marginals rather than a
single Viterbi derivation, we can explicitly force the
aligner to choose denser extraction sets simply by
lowering the marginal threshold. Therefore, we also
show substantial improvements over DeNero and
Klein (2010) in recall-heavy objectives, such as F5 .
More importantly, we also show how the BP factorization allows us to relax the ITG constraint, replacing it with a new set of constraints that permit a wider family of alignments. Compared to
ITG, the resulting model is less efﬁcient for exact
inference (where it is exponential), but more efﬁcient for our BP approximation (where it is only
quadratic). Our new model performs even better
than the ITG-constrained model on phrase alignment metrics while being faster by a factor of 5.5x.

2

Extraction Set Models

Figure 1 shows part of an aligned sentence pair, including the word-to-word alignments, and the extracted phrase pairs licensed by those alignments.
Formally, given a sentence pair (e, f), a word-level
alignment a is a collection of links between target
words ei and source words fj . Following past work,
we further divide word links into two categories:
sure and possible, shown in Figure 1 as solid and
hatched grey squares, respectively. We represent a
as a grid of ternary word link variables aij , each of
which can take the value sure to represent a sure link
between ei and fj , poss to represent a possible link,
or off to represent no link.
An extraction set π is a set of aligned phrase pairs
to be extracted from (e, f), shown in Figure 1 as
green rounded rectangles. We represent π as a set of
boolean variables πghk , which each have the value
true when the target span [g, h] is phrase-aligned to
the source span [k, ]. Following previous work on
phrase extraction, we limit the size of π by imposing
a phrase length limit d: π only contains a variable
πghk if h − g < d and − k < d.

There is a deterministic mapping π(a) from a
word alignment to the extraction set licensed by that
word alignment. We will brieﬂy describe it here, and
then present our factorized model.

f
σ5 = [7, 7]

f5

f
σ6 = [5, 6]

f6

f
σ7 = [5, 6]

f7

f
σ8 = [4, 4]

f8

f
σ9 = [−1, ∞]

f9
e3

e4

e5

e6

e7

Figure 1: A schematic representation of part of a sentence pair. Solid grey squares indicate sure links (e.g.
a48 = sure), and hatched squares possible links (e.g.
a67 = poss). Rounded green rectangles are extracted
phrase pairs (e.g. π5667 = true). Target spans are shown
as blue vertical lines and source spans as red horizontal
f
lines. Because there is a sure link at a48 , σ8 = [4, 4] does
not include the possible link at a38 . However, f7 only
f
has possible links, so σ7 = [5, 6] is the span containing
f
those. f9 is null-aligned, so σ9 = [−1, ∞], which blocks
all phrase pairs containing f9 from being extracted.

2.1

Extraction Sets from Word Alignments

The mapping from a word alignment to the set of
licensed phrase pairs π(a) is based on the standard
rule extraction procedures used in most modern statistical systems (Koehn et al., 2003; Galley et al.,
2006; Chiang, 2007), but extended to handle possible links (DeNero and Klein, 2010). We start by
using a to ﬁnd a projection from each target word ei
onto a source span, represented as blue vertical lines
in Figure 1. Similarly, source words project onto
target spans (red horizontal lines in Figure 1). π(a)
contains a phrase pair iff every word in the target
span projects within the source span and vice versa.
Figure 1 contains an example for d = 2.
Formally, the mapping introduces a set of spans
σ. We represent the spans as variables whose values
e
are intervals, where σi = [k, ] means that the target word ei projects to the source span [k, ]. The
e
set of legal values for σi includes any interval with
0 ≤ k ≤ < |f| and − k < d, plus the special interval [−1, ∞] that indicates ei is null-aligned. The
f
span variables for source words σj have target spans
[g, h] as values and are deﬁned analogously.
For a set I of positions, we deﬁne the range func-

tion:
[−1, ∞]
I=∅
[mini∈I i, maxi∈I i] else

range(I) =

(1)

For a ﬁxed word alignment a we set the target
e
span variable σi :
e
σi,s = range({j : aij = sure})

(2)

e
σi,p
e
σi

(3)

= range({j : aij = off})
=

e
σi,s

∩

e
σi,p

(4)

e
As illustrated in Figure 1, this sets σi to the minimal span containing all the source words with a
sure link to ei if there are any. Otherwise, because
of the special case for range(I) when I is empty,
e
e
σi,s = [−1, ∞], so σi is the minimal span containing
all poss-aligned words. If all word links to ei are off,
e
indicating that ei is null-aligned, then σi is [−1, ∞],
preventing the alignment of any phrase pairs containing ei .
Finally, we specify which phrase pairs should be
included in the extraction set π. Given the spans σ
based on a, π(a) sets πghk = true iff every word in
each phrasal span projects within the other:
e
σi ⊆ [k, ]

∀i ∈ [g, h]

⊆ [g, h]

∀j ∈ [k, ]

f
σj

2.2

(5)

Formulation as a Graphical Model

We score triples (a, π, σ) as the dot product of a
weight vector w that parameterizes our model and a
feature vector φ(a, π, σ). The feature vector decomposes into word alignment features φa , phrase pair
features φπ and target and source null word features
φe and φf :1
∅
∅
φ(a, π, σ) =

φa (aij ) +
i,j

g,h,k,
e
φe (σi )
∅

i

φπ (πghk )+
f
φf (σj )
∅

+

(6)

j

This feature function is exactly the same as that
used by DeNero and Klein (2010).2 However, while
1
In addition to the arguments we write out explicitly, all feature functions have access to the observed sentence pair (e, f).
2
Although the null word features are not described in DeNero and Klein (2010), all of their reported results include these
features (DeNero, 2010).

they formulated their inference problem as a search
for the highest scoring triple (a, π, σ) for an observed sentence pair (e, f), we wish to derive a conditional probability distribution p(a, π, σ|e, f). We
do this with the standard transformation for linear
models: p(a, π, σ|e, f) ∝ exp(w·φ(a, π, σ)). Due to
the factorization in Eq. (6), this exponentiated form
becomes a product of local multiplicative factors,
and hence our model forms an undirected graphical
model, or Markov random ﬁeld.
In addition to the scoring function, our model
also includes constraints on which triples (a, π, σ)
have nonzero probability. DeNero and Klein (2010)
implicitly included these constraints in their representation: instead of sets of variables, they used a
structured representation that only encodes triples
(a, π, σ) satisfying both the mapping π = π(a) and
the structural constraint that a can be generated by
a block ITG grammar. However, our inference procedure, BP, requires that we represent (a, π, σ) as an
assignment of values to a set of variables. Therefore,
we must explicitly encode all constraints into the
multiplicative factors that deﬁne the model. To accomplish this, in addition to the soft scoring factors
we have already mentioned, our model also includes
a set of hard constraint factors. Hard constraint factors enforce the relationships between the variables
of the model by taking a value of 0 when the constraints they encode are violated and a value of 1
when they are satisﬁed. The full factor graph representation of our model, including both soft scoring factors and hard constraint factors, is drawn
schematically in Figure 2.
2.2.1

Soft Scoring Factors

The scoring factors all take the form exp(w · φ),
and so can be described in terms of their respective
local feature vectors, φ. Depending on the values of
the variables each factor depends on, the factor can
be active or inactive. Features are only extracted for
active factors; otherwise φ is empty and the factor
produces a value of 1.
S URE L INK . Each word alignment variable aij
has a corresponding S URE L INK factor Lij to incorporate scores from the features φa (aij ). Lij is active whenever aij = sure. φa (aij ) includes posteriors from unsupervised jointly trained HMM word
alignment models (Liang et al., 2006), dictionary

A

agk

a11

a21
L11

a12

a|e|k

ahk

f
Sk

f
σk
f
Nk

ai1
L21

Li1

ah￿

ag￿

a|e|￿

f
S￿

f
σ￿
f
N￿

a22
L12

L22

a1j

ag|f|
e
Sg

aij
L1j

ah|f|
e
Sh

e
σg

e
σh

Pghk￿

πghk￿

Lij

e
Ng

(a) ITG factor

e
Nh

Rghk￿

(b) S PAN and E XTRACT factors

Figure 2: A factor graph representation of the ITG-based extraction set model. For visual clarity, we draw the graph
separated into two components: one containing the factors that only neighbor word link variables, and one containing
the remaining factors.

and identical word features, a position distortion feature, and features for numbers and punctuation.
P HRASE PAIR . For each phrase pair variable
πghk , scores from φπ (πghk ) come from the factor
Rghk , which is active if πghk = true. Most of the
model’s features are on these factors, and include
relative frequency statistics, lexical template indicator features, and indicators for numbers of words and
Chinese characters. See DeNero and Klein (2010)
for a more comprehensive list.
N ULLW ORD . We can determine if a word is
null-aligned by looking at its corresponding span
e
variable. Thus, we include features from φe (σi ) in
∅
e that is active if σ e = [−1, ∞]. The
a factor Ni
i
features are mostly indicators for common words.
f
There are also factors Nj for source words, which
are deﬁned analogously.

e
e
a factor Si to ensure that the span variable σi has
a value that agrees with the projection of the word
e
alignment a. As shown in Figure 2b, Si depends
e and all the word alignment variables a in
on σi
ij
e
column i of the word alignment grid. Si has value
1 iff the equality in Eq. (4) holds. Our model also
f
includes a factor Sj to enforce the analogous rela-

2.2.2

ITG. Finally, to enforce the structural constraint
on a, we include a single global factor A that depends on all the word link variables in a (see Figure 2a). A is satisﬁed iff a is in the family of
block inverse transduction grammar (ITG) alignments. The block ITG family permits multiple links
to be on (aij = off) for a particular word ei via terminal block productions, but ensures that every word is

Hard Constraint Factors

We encode the hard constraints on relationships
between variables in our model using three families of factors, shown graphically in Figure 2. The
S PAN and E XTRACT factors together ensure that
π = π(a). The ITG factor encodes the structural
constraint on a.
S PAN . First, for each target word ei we include

f
tionship between each σj and corresponding row j
of a.

E XTRACT. For each phrase pair variable πghk
we have a factor Pghk to ensure that πghk = true
iff it is licensed by the span projections σ. As shown
in Figure 2b, in addition to πghk , Pghk depends on
f
e
the range of span variables σi for i ∈ [g, h] and σj
for j ∈ [k, ]. Pghk is satisﬁed when πghk = true
and the relations in Eq. (5) all hold, or when πghk =
false and at least one of those relations does not hold.

in at most one such terminal production, and that the
full set of terminal block productions is consistent
with ITG reordering patterns (Zhang et al., 2008).

e
U1

f
U1

3

e
U2

Uie

a11

a21

ai1

Relaxing the ITG Constraint

L11
f
U2

The ITG factor can be viewed as imposing two different types of constraints on allowable word alignments a. First, it requires that each word is aligned
to at most one relatively short subspan of the other
sentence. This is a linguistically plausible constraint, as it is rarely the case that a single word will
translate to an extremely long phrase, or to multiple
widely separated phrases.3
The other constraint imposed by the ITG factor
is the ITG reordering constraint. This constraint
is imposed primarily for reasons of computational
tractability: the standard dynamic program for bitext parsing depends on ITG reordering (Wu, 1997).
While this constraint is not dramatically restrictive (Haghighi et al., 2009), it is plausible that removing it would permit the model to produce better
alignments. We tested this hypothesis by developing a new model that enforces only the constraint
that each word align to one limited-length subspan,
which can be viewed as a generalization of the atmost-one-to-one constraint frequently considered in
the word-alignment literature (Taskar et al., 2005;
Cromi` res and Kurohashi, 2009).
e
Our new model has almost exactly the same form
as the previous one. The only difference is that A is
replaced with a new family of simpler factors:
O NE S PAN . For each target word ei (and each
source word fj ) we include a hard constraint factor
f
e
Uie (respectively Uj ). Uie is satisﬁed iff |σi,p | < d
e
(length limit) and either σi,p = [−1, ∞] or ∀j ∈
e , a = off (no gaps), with σ e as in Eq. (3). Figσi,p ij
i,p
ure 3 shows the portion of the factor graph from Figure 2a redrawn with the O NE S PAN factors replacing
the ITG factor. As Figure 3 shows, there is no longer
a global factor; each Uie depends only on the word
link variables from column i.
3

Short gaps can be accomodated within block ITG (and in
our model are represented as possible links) as long as the total
aligned span does not exceed the block size.

a12

L21
a22

L12

f
Uj

Li1

L22

a1j

aij
L1j

Lij

Figure 3: O NE S PAN factors

4

Belief Propagation

Belief propagation is a generalization of the well
known sum-product algorithm for undirected graphical models. We will provide only a procedural
sketch here, but a good introduction to BP for inference in structured NLP models can be found
in Smith and Eisner (2008), and Chapters 16 and 23
of MacKay (2003) contain a general introduction to
BP in the more general context of message-passing
algorithms.
At a high level, each variable maintains a local
distribution over its possible values. These local distribution are updated via messages passed between
variables and factors. For a variable V , N (V ) denotes the set of factors neighboring V in the factor graph. Similarly, N (F ) is the set of variables
neighboring the factor F . During each round of BP,
messages are sent from each variable to each of its
neighboring factors:
(k+1)

(k)

qV →F (v) ∝

rG→V (v)

(7)

G∈N (V ),G=F

and from each factor to each of its neighboring variables:
(k+1)

rF →V (v) ∝

F (XF )

XF ,XF [V ]=v

(k)

qU →F (v) (8)

U ∈N (F ),U =V

where XF is a partial assignment of values to just
the variables in N (F ).

Marginal beliefs at time k can be computed by
simply multiplying together all received messages
and normalizing:
(k)

(k)

bV (v) ∝

rG→V (v)

(9)

G∈N (V )

Although messages can be updated according to
any schedule, generally one iteration of BP updates
each message once. The process iterates until some
stopping criterion has been met: either a ﬁxed number of iterations or some convergence metric.
For our models, we say that BP has converged
whenever

(k)

V,v

(k−1)

bV (v) − bV

(v)

2

< δ for

0.4

some small δ >
While we have no theoretical
convergence guarantees, it usually converges within
10 iterations in practice.

5

Efﬁcient BP for Extraction Set Models

In general, the efﬁciency of BP depends directly on
the arity of the factors in the model. Performed
na¨vely, the sum in Eq. (8) will take time that grows
ı
exponentially with the size of N (F ). For the softscoring factors, which each depend only on a single
variable, this isn’t a problem. However, our model
also includes factors whose arity grows with the input size: for example, explicitly enumerating all assignments to the word link variables that the ITG
2
factor depends on would take O(3n ) time.5
To run BP in a reasonable time frame, we need
efﬁcient factor-speciﬁc propagators that can exploit
the structure of the factor functions to compute outgoing messages in polynomial time (Duchi et al.,
2007; Smith and Eisner, 2008). Fortunately, all of
our hard constraints permit dynamic programs that
accomplish this propagation. Space does not permit
a full description of these dynamic programs, but we
will brieﬂy sketch the intuitions behind them.
e
S PAN and O NE S PAN. Marginal beliefs for Si or
Uie can be computed in O(nd2 ) time. The key obsere
e
vation is that for any legal value σi = [k, ], Si and
e require that a = off for all j ∈ [k, ].6 Thus, we
Ui
/
ij
start by computing the product of all the off beliefs:
4

We set δ = 0.001.
For all asymptotic analysis, we deﬁne n = max(|e|, |f|).
6
For ease of exposition, we assume that all alignments are
either sure or off ; the modiﬁcations to account for the general
case are straightforward.
5

Factor
S URE L INK
P HRASE PAIR
N ULLW ORD
S PAN
E XTRACT
ITG
O NE S PAN

Runtime
O(1)
O(1)
O(nd)
O(nd2 )
O(d3 )
O(n6 )
O(nd2 )

Count
O(n2 )
O(n2 d2 )
O(n)
O(n)
O(n2 d2 )
1
O(n)

Total
O(n2 )
O(n2 d2 )
O(n2 d)
O(n2 d2 )
O(n2 d5 )
O(n6 )
O(n2 d2 )

Table 1: Asymptotic complexity for all factors.

¯=
b
j qaij (off). Then, for each of the O(nd) legal
source spans [k, ] we can efﬁciently ﬁnd a joint belief by summing over consistent assignments to the
O(d) link variables in that span.
E XTRACT. Marginal beliefs for Pghk can be
computed in O(d3 ) time. For each of the O(d) target
e
words, we can ﬁnd the total incoming belief that σi
2 ) values
is within [k, ] by summing over the O(d
[k , ] where [k , ] ⊆ [k, ]. Likewise for source
words. Multiplying together these per-word beliefs
and the belief that πghk = true yields the joint belief of a consistent assignment with πghk = true,
which can be used to efﬁciently compute outgoing
messages.
ITG. To build outgoing messages, the ITG factor A needs to compute marginal beliefs for all of the
word link variables aij . These can all be computed
in O(n6 ) time by using a standard bitext parser to
run the inside-outside algorithm. By using a normal
form grammar for block ITG with nulls (Haghighi
et al., 2009), we ensure that there is a 1-1 correspondence between the ITG derivations the parser sums
over and word alignments a that satisfy A.
The asymptotic complexity for all the factors is
shown in Table 1. The total complexity for inference
in each model is simply the sum of the complexities
of its factors, so the complexity of the ITG model is
O(n2 d5 + n6 ), while the complexity of the relaxed
model is just O(n2 d5 ). The complexity of exact inference, on the other hand, is exponential in d for the
ITG model and exponential in both d and n for the
relaxed model.

6

Training and Decoding

We use BP to compute marginal posteriors, which
we use at training time to get expected feature counts
and at test time for posterior decoding. For each sentence pair, we continue to pass messages until either
the posteriors converge, or some maximum number
of iterations has been reached.7 After running BP,
the marginals we are interested in can all be computed with Eq. (9).
6.1

Training

We train the model to maximize the log likelihood of
manually word-aligned gold training sentence pairs
(with L2 regularization). Because π and σ are determined when a is observed, the model has no latent
variables. Therefore, the gradient takes the standard
form for loglinear models:
LL = φ(a, π, σ) −
a ,π ,σ

(10)

p(a , π , σ |e, f)φ(a , π , σ ) − λw

The feature vector φ contains features on sure
word links, extracted phrase pairs, and null-aligned
words. Approximate expectations of these features
can be efﬁciently computed using the marginal bee
liefs baij (sure), bπghk (true), and bσi ([−1, ∞]) and
bσf ([−1, ∞]), respectively. We learned our ﬁnal
j

weight vector w using AdaGrad (Duchi et al., 2010),
an adaptive subgradient version of standard stochastic gradient ascent.
6.2

Testing

We evaluate our model by measuring precision and
recall on extracted phrase pairs. Thus, the decoding problem takes a sentence pair (e, f) as input, and
must produce an extraction set π as output. Our approach, posterior thresholding, is extremely simple:
we set πghk = true iff bπghk (true) ≥ τ for some
ﬁxed threshold τ . Note that this decoding method
does not require that there be any underlying word
alignment a licensing the resulting extraction set π,8
7
See Section 7.2 for an empirical investigation of this maximum.
8
This would be true even if we computed posteriors exactly, but is especially true with approximate marginals from
BP, which are not necessarily consistent.

but the structure of the model is such that two conﬂicting phrase pairs are unlikely to simultaneously
have high posterior probability.
Most publicly available translation systems expect word-level alignments as input. These can
also be generated by applying posterior thresholding, aligning target word i to source word j whenever baij (sure) ≥ t.9

7

Experiments

Our experiments are performed on Chinese-toEnglish alignment. We trained and evaluated all
models on the NIST MT02 test set, which consists
of 150 training and 191 test sentences and has been
used previously in alignment experiments (Ayan and
Dorr, 2006; Haghighi et al., 2009; DeNero and
Klein, 2010). The unsupervised HMM word aligner
used to generate features for the model was trained
on 11.3 million words of FBIS newswire data. We
test three models: the Viterbi ITG model of DeNero
and Klein (2010), our BP ITG model that uses the
ITG factor, and our BP Relaxed model that replaces
the ITG factor with the O NE S PAN factors. In all of
our experiments, the phrase length d was set to 3.10
7.1

Phrase Alignment

We tested the models by computing precision and
recall on extracted phrase pairs, relative to the gold
phrase pairs of up to length 3 induced by the gold
word alignments. For the BP models, we trade
off precision and recall by adjusting the decoding
threshold τ . The Viterbi ITG model was trained to
optimize F5 , a recall-biased measure, so in addition
to F1 , we also report the recall-biased F2 and F5
measures. The maximum number of BP iterations
was set to 5 for the BP ITG model and to 10 for the
BP Relaxed model.
The phrase alignment results are shown in Figure 4. The BP ITG model performs comparably to
the Viterbi ITG model. However, because posterior
decoding permits explicit tradeoffs between precision and recall, it can do much better in the recallbiased measures, even though the Viterbi ITG model
was explicitly trained to maximize F5 (DeNero and
9

For our experiments, we set t = 0.2.
Because the runtime of the Viterbi ITG model grows exponentially with d, it was not feasible to perform comparisons for
higher phrase lengths.
10

0.5

1

2

4

8

16

Time (seconds per sentence)
Viterbi ITG

75

Best F1

Recall

80

70
65
60
60

65

70

75

80

85

73
72
71
70
69
68
67
0.0625 0.125

Model
Viterbi ITG
BP ITG
BP Relaxed

BP ITG

Sentences
per Second
0.21
0.11
1.15

Figure 4: Phrase alignment results. A portion of the Precision/Recall curve is plotted for the BP models, with the
result from the Viterbi ITG model provided for reference.

Klein, 2010). The BP Relaxed model performs the
best of all, consistently achieving higher recall for
ﬁxed precision than either of the other models. Because of its lower asymptotic runtime, it is also much
faster: over 5 times as fast as the Viterbi ITG model
and over 10 times as fast as the BP ITG model.11
7.2

Timing

BP approximates marginal posteriors by iteratively
updating beliefs for each variable based on current beliefs about other variables. The iterative nature of the algorithm permits us to make an explicit
speed/accuracy tradeoff by limiting the number of
iterations. We tested this tradeoff by limiting both
of the BP models to run for 2, 3, 5, 10, and 20 iterations. The results are shown in Figure 5. Neither
model beneﬁts from running more iterations than
used to obtain the results in Figure 4, but each can
be sped up by a factor of almost 1.5x in exchange
for a modest (< 1 F1 ) drop in accuracy.
11

Viterbi ITG

BP Relaxed

Best Scores
F1
F2
F5
71.6 73.1 74.0
71.8 74.8 83.5
72.6 75.2 84.5

The speed advantage of Viterbi ITG over BP ITG comes
from Viterbi ITG’s aggressive beaming.

0.25

0.5

BP Relaxed

1

2

Speed (sentences per second)

Precision
Viterbi ITG

BP ITG

BP ITG

BP Relaxed

Figure 5: Speed/accuracy tradeoff. The speed axis is on
a logarithmic scale. From fastest to slowest, data points
correspond to maximums of 2, 5, 10, and 20 BP iterations. F1 for the BP Relaxed model was very low when
limited to 2 iterations, so that data point is outside the
visible area of the graph.

Model

BLEU

Baseline
Viterbi ITG
BP Relaxed

32.8
33.5
33.6

Relative
Improve.
+0.0
+0.7
+0.8

Hours to
Train/Align
5
831
39

Table 2: Machine translation results.

7.3

Translation

We ran translation experiments using Moses (Koehn
et al., 2007), which we trained on a 22.1 million word parallel corpus from the GALE program.
We compared alignments generated by the baseline
HMM model, the Viterbi ITG model and the Relaxed BP model.12 The systems were tuned and
evaluated on sentences up to length 40 from the
NIST MT04 and MT05 test sets. The results, shown
in Table 2, show that the BP Relaxed model achives
a 0.8 BLEU improvement over the HMM baseline,
comparable to that of the Viterbi ITG model, but taking a fraction of the time,13 making the BP Relaxed
model a practical alternative for real translation applications.
12
Following a simpliﬁed version of the procedure described
by DeNero and Klein (2010), we added rule counts from the
HMM alignments to the extraction set aligners’ counts.
13
Some of the speed difference between the BP Relaxed and
Viterbi ITG models comes from better parallelizability due to
drastically reduced memory overhead of the BP Relaxed model.

8

Conclusion

For performing inference in a state-of-the-art, but inefﬁcient, alignment model, belief propagation is a
viable alternative to greedy search methods, such as
beaming. BP also results in models that are much
more scalable, by reducing the asymptotic complexity of inference. Perhaps most importantly, BP permits the relaxation of artiﬁcial constraints that are
generally taken for granted as being necessary for
efﬁcient inference. In particular, a relatively modest relaxation of the ITG constraint can directly be
applied to any model that uses ITG-based inference
(e.g. Zhang and Gildea, 2005; Cherry and Lin, 2007;
Haghighi et al., 2009).

Acknowledgements
This project is funded by an NSF graduate research
fellowship to the ﬁrst author and by BBN under
DARPA contract HR0011-06-C-0022.

References
Necip Fazil Ayan and Bonnie J. Dorr. 2006. Going beyond AER: An extensive analysis of word alignments
and their impact on MT. In ACL.
Alexandra Birch, Chris Callison-Burch, and Miles Osborne. 2006. Constraining the phrase-based, joint
probability statistical translation model. In AMTA.
Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles Osborne. 2009. A gibbs sampler for phrasal synchronous
grammar induction. In ACL-IJCNLP.
Colin Cherry and Dekang Lin. 2007. Inversion transduction grammar for joint phrasal translation modeling. In
NAACL Workshop on Syntax and Structure in Statistical Translation.
David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2):201–228.
Fabien Cromi` res and Sadao Kurohashi. 2009. An
e
alignment algorithm using belief propagation and a
structure-based distortion model. In EACL.
Steve DeNeefe, Kevin Knight, Wei Wang, and Daniel
Marcu. 2007. What can syntax-based MT learn from
phrase-based MT? In EMNLP-CoNLL.
John DeNero and Dan Klein. 2010. Discriminative modeling of extraction sets for machine translation. In
ACL.
John DeNero, Dan Gillick, James Zhang, and Dan Klein.
2006. Why generative phrase models underperform
surface heuristics. In NAACL Workshop on Statistical
Machine Translation.

John DeNero, Alexandre Bouchard-Cˆ t´ , and Dan Klein.
oe
2008. Sampling alignment structure under a Bayesian
translation model. In EMNLP.
John DeNero. 2010. Personal Communication.
John Duchi, Danny Tarlow, Gal Elidan, and Daphne
Koller. 2007. Using combinatorial optimization
within max-product belief propagation. In NIPS 2006.
John Duchi, Elad Hazan, and Yoram Singer. 2010.
Adaptive subgradient methods for online learning and
stochastic optimization. In COLT.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What’s in a translation rule? In HLTNAACL.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training
of context-rich syntactic translation models.
In
COLING-ACL.
Aria Haghighi, John Blitzer, John DeNero, and Dan
Klein. 2009. Better word alignments with supervised
ITG models. In ACL-IJCNLP.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In ACL.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In ACL.
Zhifei Li and Sanjeev Khudanpur. 2008. A scalable
decoder for parsing-based machine translation with
equivalent language model state maintenance. In ACL
SSST.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Alignment by agreement. In HLT-NAACL.
David J.C. MacKay. 2003. Information theory, inference, and learning algorithms. Cambridge Univ Press.
Daniel Marcu and Daniel Wong. 2002. A phrase-based,
joint probability model for statistical machine translation. In EMNLP.
Jan Niehues and Stephan Vogel. 2008. Discriminative
word alignment via alignment matrix modeling. In
ACL Workshop on Statistical Machine Translation.
David A. Smith and Jason Eisner. 2008. Dependency
parsing by belief propagation. In EMNLP.
Ben Taskar, Simon Lacoste-Julien, and Dan Klein. 2005.
A discriminative matching approach to word alignment. In EMNLP.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377–404.
Hao Zhang and Daniel Gildea. 2005. Stochastic lexicalized inversion transduction grammar for alignment. In
ACL.

Hao Zhang, Chris Quirk, Robert C. Moore, and
Daniel Gildea. 2008. Bayesian learning of noncompositional phrases with synchronous parsing. In
ACL:HLT.

