Approximate Factoring for A∗ Search
Aria Haghighi, John DeNero, Dan Klein
Computer Science Division
University of California Berkeley
{aria42, denero, klein}@cs.berkeley.edu

Abstract
We present a novel method for creating A∗ estimates for structured search problems. In our approach, we project a complex model onto multiple
simpler models for which exact inference is efﬁcient. We use an optimization framework to estimate parameters for these projections in a way
which bounds the true costs. Similar to Klein and
Manning (2003), we then combine completion estimates from the simpler models to guide search
in the original complex model. We apply our approach to bitext parsing and lexicalized parsing,
demonstrating its effectiveness in these domains.

1

Introduction

Inference tasks in NLP often involve searching for
an optimal output from a large set of structured outputs. For many complex models, selecting the highest scoring output for a given observation is slow or
even intractable. One general technique to increase
efﬁciency while preserving optimality is A∗ search
(Hart et al., 1968); however, successfully using A∗
search is challenging in practice. The design of admissible (or nearly admissible) heuristics which are
both effective (close to actual completion costs) and
also efﬁcient to compute is a difﬁcult, open problem in most domains. As a result, most work on
search has focused on non-optimal methods, such
as beam search or pruning based on approximate
models (Collins, 1999), though in certain cases admissible heuristics are known (Och and Ney, 2000;
Zhang and Gildea, 2006). For example, Klein and
Manning (2003) show a class of projection-based A∗
estimates, but their application is limited to models
which have a very restrictive kind of score decomposition. In this work, we broaden their projectionbased technique to give A∗ estimates for models
which do not factor in this restricted way.

Like Klein and Manning (2003), we focus on
search problems where there are multiple projections or “views” of the structure, for example lexical
parsing, in which trees can be projected onto either
their CFG backbone or their lexical attachments. We
use general optimization techniques (Boyd and Vandenberghe, 2005) to approximately factor a model
over these projections. Solutions to the projected
problems yield heuristics for the original model.
This approach is ﬂexible, providing either admissible or nearly admissible heuristics, depending on the
details of the optimization problem solved. Furthermore, our approach allows a modeler explicit control
over the trade-off between the tightness of a heuristic and its degree of inadmissibility (if any). We describe our technique in general and then apply it to
two concrete NLP search tasks: bitext parsing and
lexicalized monolingual parsing.

2

General Approach

Many inference problems in NLP can be solved
with agenda-based methods, in which we incrementally build hypotheses for larger items by combining
smaller ones with some local conﬁgurational structure. We can formalize such tasks as graph search
problems, where states encapsulate partial hypotheses and edges combine or extend them locally.1 For
example, in HMM decoding, the states are anchored
labels, e.g. VBD[5], and edges correspond to hidden
transitions, e.g. VBD[5] → DT[6].
The search problem is to ﬁnd a minimal cost path
from the start state to a goal state, where the path
cost is the sum of the costs of the edges in the path.
1

In most complex tasks, we will in fact have a hypergraph,
but the extension is trivial and not worth the added notation.

a' → b'

a' → b'
a→b

a
a
a

→

b
b
b

a

b→c

→

b

a
b
b

→

b
c
c

a

→

b

→
b' → c'

a
a

a
ba
a

b
b

→

→

b' → c'

a→b

2.0

3.0

b→c

3.0

4.0

b
cb
b

a' → b'

b' → c'

a→b

2.0

3.0

b→c

3.0

5.0

Original Cost Matrix

Original Cost Matrix

c(a' → b') c(a' → b')

c(a' → b') c(a' → b')

1.0

b
b

→

c
c

c(a → b) 1.0
c(b → c) 2.0

Local Conﬁgurations

(a)

2.0

2.0

3.0

c(a → b) 1.0

1.0

3.0

4.0

c(b → c) 2.0

2.0

2.0

3.0

3.0

4.0

Factored Cost Matrix

Factored Cost Matrix

(b)

(c)

Figure 1: Example cost factoring: In (a), each cell of the matrix is a local conﬁguration composed of two projections (the row and
column of the cell). In (b), the top matrix is an example cost matrix, which speciﬁes the cost of each local conﬁguration. The
bottom matrix represents our factored estimates, where each entry is the sum of conﬁguration projections. For this example, the
actual cost matrix can be decomposed exactly into two projections. In (c), the top cost matrix cannot be exactly decomposed along
two dimensions. Our factored cost matrix has the property that each factored cost estimate is below the actual conﬁguration cost.
Although our factorization is no longer tight, it still can be used to produce an admissible heuristic.

For probabilistic inference problems, the cost of an
edge is typically a negative log probability which depends only on some local conﬁguration type. For
instance, in PCFG parsing,1 the (hyper)edges reference anchored spans X[i, j], but the edge costs de1
pend only on the local rule type X → Y Z. We will
1
1
use a to refer to a local conﬁguration and use c(a)
to refer to its cost. Because edge costs are sensitive only to local conﬁgurations, the cost of a path
is a c(a). A∗ search requires a heuristic function,
which is an estimate h(s) of the completion cost, the
cost of a best path from state s to a goal.
In this work, following Klein and Manning
(2003), we consider problems with projections or
“views,” which deﬁne mappings to simpler state and
conﬁguration spaces. For instance, suppose that we
are using an HMM to jointly model part-of-speech
(POS) and named-entity-recognition (NER) tagging.
There might be one projection onto the NER component and another onto the POS component. Formally, a projection π is a mapping from states to
some coarser domain. A state projection induces
projections of edges and of the entire graph π(G).
We are particularly interested in search problems
with multiple projections {π1 , . . . , π } where each
projection, πi , has the following properties: its state
projections induce well-deﬁned projections of the
local conﬁgurations πi (a) used for scoring, and the
projected search problem admits a simpler inference. For instance, the POS projection in our NERPOS HMM is a simpler HMM, though the gains
from this method are greater when inference in the
projections have lower asymptotic complexity than

the original problem (see sections 3 and 4).
In deﬁning projections, we have not yet dealt with
the projected scoring function. Suppose that the
cost of local conﬁgurations decomposes along projections as well. In this case,
ci (a) , ∀a ∈ A

c (a) =

(1)

i=1

where A is the set of local conﬁgurations and ci (a)
represents the cost of conﬁguration a under projection πi . A toy example of such a cost decomposition in the context of a Markov process over two-part
states is shown in ﬁgure 1(b), where the costs of the
joint transitions equal the sum of costs of their projections. Under the strong assumption of equation
(1), Klein and Manning (2003) give an admissible
A∗ bound. They note that the cost of a path decomposes as a sum of projected path costs. Hence, the
following is an admissible additive heuristic (Felner
et al., 2004),
h∗ (s)
i

h(s) =

(2)

i=1

where h∗ (s) denote the optimal completion costs in
i
the projected search graph πi (G). That is, the completion cost of a state bounds the sum of the completion costs in each projection.
In virtually all cases, however, conﬁguration costs
will not decompose over projections, nor would we
expect them to. For instance, in our joint POS-NER
task, this assumption requires that the POS and NER

transitions and observations be generated independently. This independence assumption undermines
the motivation for assuming a joint model. In the
central contribution of this work, we exploit the projection structure of our search problem without making any assumption about cost decomposition.
Rather than assuming decomposition, we propose
to ﬁnd scores φ for the projected conﬁgurations
which are pointwise admissible:

We think of our unknown factored costs as a block
vector φ = [φ1 , .., φ ], where vector φi is composed
of the factored costs, φi (a), for each conﬁguration
a ∈ A. We can then ﬁnd admissible factored costs
by solving the following optimization problem,
minimize γ

(4)

φ

such that, γa = c(a) −

φi (a), ∀a ∈ A
i=1

φi (a) ≤ c(a), ∀a ∈ A

γa ≥ 0, ∀a ∈ A

(3)

i=1

Here, φi (a) represents a factored projection cost of
πi (a), the πi projection of conﬁguration a. Given
pointwise admissible φi ’s we can again apply the
heuristic recipe of equation (2). An example of
factored projection costs are shown in ﬁgure 1(c),
where no exact decomposition exists, but a pointwise admissible lower bound is easy to ﬁnd.
Claim. If a set of factored projection costs
{φ1 , . . . , φ } satisfy pointwise admissibility, then
the heuristic from (2) is an admissible A∗ heuristic.
Proof. Assume a1 , . . . , ak are conﬁgurations used
to optimally reach the goal from state s. Then,
h∗ (s) =

k
X

c(aj ) ≥

j=1

=

φi (aj )

j=1 i=1

k
X X
i=1

k
XX

j=1

!
φi (aj ) ≥

X

h∗ (s) = h(s)
i

i=1

The ﬁrst inequality follows from pointwise admissibility. The second inequality follows because each
inner sum is a completion cost for projected problem
πi and therefore h∗ (s) lower bounds it. Intuitively,
i
we can see two sources of slack in such projection
heuristics. First, there may be slack in the pointwise
admissible scores. Second, the best paths in the projections will be overly optimistic because they have
been decoupled (see ﬁgure 5 for an example of decoupled best paths in projections).
2.1

Finding Factored Projections for
Non-Factored Costs

We can ﬁnd factored costs φi (a) which are pointwise admissible by solving an optimization problem.

We can think of each γa as the amount by which
the cost of conﬁguration a exceeds the factored projection estimates (the pointwise A∗ gap). Requiring
γa ≥ 0 insures pointwise admissibility. Minimizing the norm of the γa variables encourages tighter
bounds; indeed if γ = 0, the solution corresponds
to an exact factoring of the search problem. In the
case where we minimize the 1-norm or ∞-norm, the
problem above reduces to a linear program, which
can be solved efﬁciently for a large number of variables and constraints.2
Viewing our procedure decision-theoretically, by
minimizing the norm of the pointwise gaps we are
effectively choosing a loss function which decomposes along conﬁguration types and takes the form
of the norm (i.e. linear or squared losses). A complete investigation of the alternatives is beyond the
scope of this work, but it is worth pointing out that
in the end we will care only about the gap on entire
structures, not conﬁgurations, and individual conﬁguration factored costs need not even be pointwise admissible for the overall heuristic to be admissible.
Notice that the number of constraints is |A|, the
number of possible local conﬁgurations. For many
search problems, enumerating the possible conﬁgurations is not feasible, and therefore neither is solving an optimization problem with all of these constraints. We deal with this situation in applying our
technique to lexicalized parsing models (section 4).
Sometimes, we might be willing to trade search
optimality for efﬁciency. In our approach, we can
explicitly make this trade-off by designing an alternative optimization problem which allows for slack
2

We used the MOSEK package (Andersen and Andersen,
2000).

in the admissibility constraints. We solve the following soft version of problem (4):
minimize γ + + C γ −

(5)

φ

such that, γa = c(a) −

φi (a), ∀a ∈ A
i=1

where γ + = max{0, γ} and γ − = max{0, −γ}
represent the componentwise positive and negative
−
elements of γ respectively. Each γa > 0 represents
a conﬁguration where our factored projection estimate is not pointwise admissible. Since this situation may result in our heuristic becoming inadmissible if used in the projected completion costs, we
more heavily penalize overestimating the cost by the
constant C.
2.2

Bounding Search Error

In the case where we allow pointwise inadmissibil−
ity, i.e. variables γa , we can bound our search er−
−
ror. Suppose γmax = maxa∈A γa and that L∗ is
the length of the longest optimal solution for the
−
original problem. Then, h(s) ≤ h∗ (s) + L∗ γmax ,
∀s ∈ S. This -admissible heuristic (Ghallab and
−
Allard, 1982) bounds our search error by L∗ γmax .3

3

Bitext Parsing

In bitext parsing, one jointly infers a synchronous
phrase structure tree over a sentence ws and its
translation wt (Melamed et al., 2004; Wu, 1997).
Bitext parsing is a natural candidate task for our
approximate factoring technique. A synchronous
tree projects monolingual phrase structure trees onto
each sentence. However, the costs assigned by
a weighted synchronous grammar (WSG) G do
not typically factor into independent monolingual
WCFGs. We can, however, produce a useful surrogate: a pair of monolingual WCFGs with structures
projected by G and weights that, when combined,
underestimate the costs of G.
Parsing optimally relative to a synchronous grammar using a dynamic program requires time O(n6 )
in the length of the sentence (Wu, 1997). This high
degree of complexity makes exhaustive bitext parsing infeasible for all but the shortest sentences. In
3

This bound may be very loose if L is large.

contrast, monolingual CFG parsing requires time
O(n3 ) in the length of the sentence.
3.1

A∗ Parsing

Alternatively, we can search for an optimal parse
guided by a heuristic. The states in A∗ bitext parsing are rooted bispans, denoted X [i, j] :: Y [k, l].
States represent a joint parse over subspans [i, j] of
ws and [k, l] of wt rooted by the nonterminals X and
Y respectively.
Given a WSG G, the algorithm prioritizes a state
(or edge) e by the sum of its inside cost βG (e) (the
negative log of its inside probability) and its outside
estimate h(e), or completion cost.4 We are guaranteed the optimal parse if our heuristic h(e) is never
greater than αG (e), the true outside cost of e.
We now consider a heuristic combining the completion costs of the monolingual projections of G,
and guarantee admissibility by enforcing point-wise
admissibility. Each state e = X [i, j] :: Y [k, l]
projects a pair of monolingual rooted spans. The
heuristic we propose sums independent outside costs
of these spans in each monolingual projection.
h(e) = αs (X [i, j]) + αt (Y [k, l])
These monolingual outside scores are computed relative to a pair of monolingual WCFG grammars Gs
and Gt given by splitting each synchronous rule
r=

X(s)
Y(t)

→

αβ
γδ

into its components πs (r) = X → αβ and πt (r) =
Y → γδ and weighting them via optimized φs (r) and
φt (r), respectively.5
To learn pointwise admissible costs for the monolingual grammars, we formulate the following optimization problem:6
minimize γ
γ,φs ,φt

1

such that, γr = c(r) − [φs (r) + φt (r)]
for all synchronous rules r ∈ G
φs ≥ 0, φt ≥ 0, γ ≥ 0
4

All inside and outside costs are Viterbi, not summed.
Note that we need only parse each sentence (monolingually) once to compute the outside probabilities for every span.
6
The stated objective is merely one reasonable choice
among many possibilities which require pointwise admissibility and encourage tight estimates.
5

Cost under Gs

j

i

Ta

rge

t

k

So

≤

j

i

l

u

Cost under G

Cost under Gt

rce

Monolingual completions
scored by factored model

Ta

rge

t

k

So

≤
l

u

j

i

rce

Synchronized completion
scored by factored model

Ta

rge

t

k

So

(a)

„

NP(s)
NP(t)

(s)

«

!

S

l

u

(s)

NN1 NNS2
(t)
(t)
NNS2 de NN1

→

NP

rce

NN
la
ns

Synchronized completion
scored by original model

Figure 2: The gap between the heuristic (left) and true completion cost (right) comes from relaxing the synchronized problem
to independent subproblems and slack in the factored models.

a
Tr

(b)

NNS
NP

NNS RB
VB
s
n
s
me
em meti ork
t
o
ys
w
s
s

tio

sistemas
de

NN traduccion
funcionan
a
veces

Figure 2 diagrams the two bounds that enforce the
admissibility of h(e). For any outside cost αG (e),
there is a corresponding optimal completion structure o under G, which is an outer shell of a synchronous tree. o projects monolingual completions
os and ot which have well-deﬁned costs cs (os ) and
ct (ot ) under Gs and Gt respectively. Their sum
cs (os ) + ct (ot ) will underestimate αG (e) by pointwise admissibility.
Furthermore, the heuristic we compute underestimates this sum. Recall that the monolingual outside
score αs (X [i, j]) is the minimal costs for any completion of the edge. Hence, αs (X [i, j]) ≤ cs (os )
and αt (X [k, l]) ≤ ct (ot ). Admissibility follows.
3.2

Experiments

We demonstrate our technique using the synchronous grammar formalism of tree-to-tree transducers (Knight and Graehl, 2004). In each weighted
rule, an aligned pair of nonterminals generates two
ordered lists of children. The non-terminals in each
list must align one-to-one to the non-terminals in the
other, while the terminals are placed freely on either
side. Figure 3(a) shows an example rule.
Following Galley et al. (2004), we learn a grammar by projecting English syntax onto a foreign language via word-level alignments, as in ﬁgure 3(b).7
We parsed 1200 English-Spanish sentences using
a grammar learned from 40,000 sentence pairs of
the English-Spanish Europarl corpus.8 Figure 4(a)
shows that A∗ expands substantially fewer states
while searching for the optimal parse with our op7

The bilingual corpus consists of translation pairs with ﬁxed
English parses and word alignments. Rules were scored by their
relative frequencies.
8
Rare words were replaced with their parts of speech to limit
the memory consumption of the parser.

Figure 3: (a) A tree-to-tree transducer rule. (b) An example
training sentence pair that yields rule (a).

timization heuristic. The exhaustive curve shows
edge expansions using the null heuristic. The intermediate result, labeled English only, used only
the English monolingual outside score as a heuristic. Similar results using only Spanish demonstrate
that both projections contribute to parsing efﬁciency.
All three curves in ﬁgure 4 represent running times
for ﬁnding the optimal parse.
Zhang and Gildea (2006) offer a different heuristic for A∗ parsing of ITG grammars that provides a
forward estimate of the cost of aligning the unparsed
words in both sentences. We cannot directly apply
this technique to our grammar because tree-to-tree
transducers only align non-terminals. Instead, we
can augment our synchronous grammar model to include a lexical alignment component, then employ
both heuristics. We learned the following two-stage
generative model: a tree-to-tree transducer generates
trees whose leaves are parts of speech. Then, the
words of each sentence are generated, either jointly
from aligned parts of speech or independently given
a null alignment. The cost of a complete parse under this new model decomposes into the cost of the
synchronous tree over parts of speech and the cost
of generating the lexical items.
Given such a model, both our optimization heuristic and the lexical heuristic of Zhang and Gildea
(2006) can be computed independently. Crucially,
the sum of these heuristics is still admissible. Results appear in ﬁgure 4(b). Both heuristics (lexical and optimization) alone improve parsing performance, but their sum opt+lex substantially improves
upon either one.

200

100

Exhaustive
Exhaustive
English Only Only
English
Optimization
Optimization

150
100

50
0

(b)
50
0

5

200

Avg. Edges Popped
(in thousands)

150

Avg. Edges Popped
(in thousands)

200

Avg. Edges Popped
(in thousands)

(a)

Avg. Edges Popped
(in thousands)

200

150
100

100

50

50

0

57

79

11
9

13
11

15
13

15

Sentence length
Sentence length

Exhaustive
Exhaustive
Lexical Lexical
Optimization
Optimization
Opt+Lex
Opt+Lex

150

0
5

57

13
79
911
11
Sentence lengthlength
Sentence

15
13

15

Figure 4: (a) Parsing efﬁciency results with optimization heuristics show that both component projections constrain the problem.
(b) Including a lexical model and corresponding heuristic further increases parsing efﬁciency.

4

Collins (1999), though simpler.10

Lexicalized Parsing

We next apply our technique to lexicalized parsing (Charniak, 1997; Collins, 1999). In lexicalized parsing, the local conﬁgurations are lexicalized
rules of the form X[h, t] → Y [h , t ] Z[h, t], where
h, t, h , and t are the head word, head tag, argument word, and argument tag, respectively. We
will use r = X → Y Z to refer to the CFG backbone of a lexicalized rule. As in Klein and Manning (2003), we view each lexicalized rule, , as
having a CFG projection, πc ( ) = r, and a dependency projection, πd ( ) = (h, t, h , t )(see ﬁgure 5).9 Broadly, the CFG projection encodes constituency structure, while the dependency projection
encodes lexical selection, and both projections are
asymptotically more efﬁcient than the original problem. Klein and Manning (2003) present a factored
model where the CFG and dependency projections
are generated independently (though with compatible bracketing):

4.1

Choosing Constraints and Handling
Unseen Dependencies

This model is broadly representative of the successful lexicalized models of Charniak (1997) and

Ideally we would like to be able to solve the optimization problem in (4) for this task. Unfortunately, exhaustively listing all possible conﬁgurations (lexical rules) yields an impractical number of
constraints. We therefore solve a relaxed problem in
which we enforce the constraints for only a subset
of the possible conﬁgurations, A ⊆ A. Once we
start dropping constraints, we can no longer guarantee pointwise admissibility, and therefore there is no
reason not to also allow penalized violations of the
constraints we do list, so we solve (5) instead.
To generate the set of enforced constraints, we
ﬁrst include all conﬁgurations observed in the gold
training trees. We then sample novel conﬁgurations
by choosing (X, h, t) from the training distribution
and then using the model to generate the rest of the
conﬁguration. In our experiments, we ended up with
434,329 observed conﬁgurations, and sampled the
same number of novel conﬁgurations. Our penalty
multiplier C was 10.
Even if we supplement our training set with many
sample conﬁgurations, we will still see new projected dependency conﬁgurations at test time. It is
therefore necessary to generalize scores from training conﬁgurations to unseen ones. We enrich our
procedure by expressing the projected conﬁguration
costs as linear functions of features. Speciﬁcally, we
deﬁne feature vectors fc (r) and fd (h, t, h t ) over
the CFG and dependency projections, and intro-

9
We assume information about the distance and direction of
the dependency is encoded in the dependency tuple, but we omit
it from the notation for compactness.

10
All probability distributions for the non-factored model are
estimated by Witten-Bell smoothing (Witten and Bell, 1991)
where conditioning lexical items are backed off ﬁrst.

P (Y [h, t]Z[h , t ] | X[h, t]) =

(6)

P (Y Z|X)P (h , t |t, h)
In this work, we explore the following non-factored
model, which allows correlations between the CFG
and dependency projections:
P (Y [h, t]Z[h , t ] | X[h, t]) = P (Y Z|X, t, h) (7)
P (t |t, Z, h , h) P (h |t , t, Z, h , h)

S

reopened-VBD

NPS

@@@hhhhh
@@@
¥
hhhh
@
@ @@
h
¥

VPS

PPN P

These-DT

stocks-NNS

VBD

3
3

33


NPN P

S, reopened-VBD

@
@@@4hhhhhhh
4
@@@@
h
4

$$

$ $
$ $


These

stocks

reopened-VBD







NPS , stocks-NNS

ADVPS , eventually-RB

VPS , reopened-VBD

4
4


eventually-RB

reopened-VBD

DT

NNS

RB

VBD

eventually

reopened

These

stocks

eventually

reopened

¨¨rr
¨
r

DT

NNS

NPP P

These

stocks

RB

reopened

eventually
Best Projected CFG Cost: 4.1
(a)

Actual Cost: 18.7
CFG Projection Cost : 6.9
Dep. Projection Cost: 11.1
(c)

Best Projected Dep. Cost: 9.5
(b)

Figure 5: Lexicalized parsing projections. The ﬁgure in (a) is the optimal CFG projection solution and the ﬁgure in (b) is the
optimal dependency projection solution. The tree in (c) is the optimal solution for the original problem. Note that the sum of the
CFG and dependency projections is a lower bound (albeit a fairly tight one) on actual solution cost.

duce corresponding weight vectors wc and wd . The
weight vectors are learned by solving the following
optimization problem:
minimize γ +
γ,wc ,wd

2

+ C γ−

2

(8)

such that, wc ≥ 0, wd ≥ 0
T
T
γ = c( ) − [wc fc (r) + wd fd (h, t, h , t )]

for = (r, h, t, h , t ) ∈ A
Our CFG feature vector has only indicator features
for the speciﬁc rule. However, our dependency feature vector consists of an indicator feature of the tuple (h, t, h , t ) (including direction), an indicator of
the part-of-speech type (t, t ) (also including direction), as well as a bias feature.
4.2

Experimental Results

We tested our approximate projection heuristic on
two lexicalized parsing models. The ﬁrst is the factored model of Klein and Manning (2003), given
by equation (6), and the second is the non-factored
model described in equation (7). Both models
use the same parent-annotated head-binarized CFG
backbone and a basic dependency projection which
models direction, but not distance or valence.11
In each case, we compared A∗ using our approximate projection heuristics to exhaustive search. We
measure efﬁciency in terms of the number of expanded hypotheses (edges popped); see ﬁgure 6.12
In both settings, the factored A∗ approach substantially outperforms exhaustive search. For the fac11

The CFG and dependency projections correspond to the
PCFG-PA and DEP-BASIC settings in Klein and Manning
(2003).
12
All models are trained on section 2 through 21 of the English Penn treebank, and tested on section 23.

tored model of Klein and Manning (2003), we can
also compare our reconstructed bound to the known
tight bound which would result from solving the
pointwise admissible problem in (4) with all constraints. As ﬁgure 6 shows, the exact factored
heuristic does outperform our approximate factored
heuristic, primarily because of many looser, backedoff cost estimates for unseen dependency tuples. For
the non-factored model, we compared our approximate factored heuristic to one which only bounds the
CFG projection as suggested by Klein and Manning
(2003). They suggest,
φc (r) =

min

∈A:πc ( )=r

c( )

where we obtain factored CFG costs by minimizing
over dependency projections. As ﬁgure 6 illustrates,
this CFG only heuristic is substantially less efﬁcient
than our heuristic which bounds both projections.
Since our heuristic is no longer guaranteed to be
admissible, we evaluated its effect on search in several ways. The ﬁrst is to check for search errors,
where the model-optimal parse is not found. In the
case of the factored model, we can ﬁnd the optimal
parse using the exact factored heuristic and compare
it to the parse found by our learned heuristic. In our
test set, the approximate projection heuristic failed
to return the model optimal parse in less than 1% of
sentences. Of these search errors, none of the costs
were more than 0.1% greater than the model optimal
cost in negative log-likelihood. For the non-factored
model, the model optimal parse is known only for
shorter sentences which can be parsed exhaustively.
For these sentences up to length 15, there were no
search errors. We can also check for violations of
pointwise admissibility for conﬁgurations encoun-

Avg.
(in

50
0
5

10

15
20
25
30
Sentence length

35

40

200
Exhaustive

150

Approx. Factored
Exact Factored

100

(b)
50

Avg. Edges Popped
(in thousands)

(a)

Avg. Edges Popped
(in thousands)

200

0

Exhaustive

150

CFG Only
Approx. Factored

100
50
0

5

10

15
20
25
30
Sentence length

35

40

5

10

15
20
25
30
Sentence length

35

40

tered during search. For both the factored and nonfactored model, less than 2% of the conﬁgurations
scored by the approximate projection heuristic during search violated pointwise admissibility.
While this is a paper about inference, we also
measured the accuracy in the standard way, on sentences of length up to 40, using EVALB. The factored model with the approximate projection heuristic achieves an F1 of 82.2, matching the performance
with the exact factored heuristic, though slower. The
non-factored model, using the approximate projection heuristic, achieves an F1 of 83.8 on the test set,
which is slightly better than the factored model.13
We note that the CFG and dependency projections
are as similar as possible across models, so the increase in accuracy is likely due in part to the nonfactored model’s coupling of CFG and dependency
projections.

5

Conclusion

We have presented a technique for creating A∗ estimates for inference in complex models. Our technique can be used to generate provably admissible
estimates when all search transitions can be enumerated, and an effective heuristic even for problems
where all transitions cannot be efﬁciently enumerated. In the future, we plan to investigate alternative objective functions and error-driven methods for
learning heuristic bounds.
Acknowledgments We would like to thank the
anonymous reviewers for their comments. This
work is supported by a DHS fellowship to the ﬁrst
13

Since we cannot exhaustively parse with this model, we
cannot compare our F1 to an exact search method.

Avg. Edges Popped
(in thousands)

Figure 6: Edges popped by exhaustive versus factored A∗ search. The chart in (a) is using the factored lexicalized model from
200
Klein and Manning (2003). The chart in (b) is using the non-factored lexicalized model described in section 4.
150

Exhaustive

author and a Microsoft new faculty fellowship to the
Approx. Factored
third author. 100
Exact Factored

References 50
E. D. Andersen and K. D. Andersen. 2000. The MOSEK in0
terior point optimizer 10 linear programming.30In H. Frenk
for 15
5
20
25
35
40
et al., editor, High Performance Optimization. Kluwer AcaSentence length
demic Publishers.
Stephen Boyd and Lieven Vandenberghe. 2005. Convex Optimization. Cambridge University Press.
Eugene Charniak. 1997. Statistical parsing with a context-free
grammar and word statistics. In National Conference on Artiﬁcial Intelligence.
Michael Collins. 1999. Head-driven statistical models for natural language parsing.
Ariel Felner, Richard Korf, and Sarit Hanan. 2004. Additive
pattern database heuristics. JAIR.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What’s in a translation rule? In HLT-NAACL.
Malik Ghallab and Dennis G. Allard. 1982. A∗ - an efﬁcient
near admissible heuristic search algorithm. In IJCAI.
P. Hart, N. Nilsson, and B. Raphael. 1968. A formal basis for
the heuristic determination of minimum cost paths. In IEEE
Transactions on Systems Science and Cybernetics. IEEE.
Dan Klein and Christopher D. Manning. 2003. Factored A*
search for models over sequences and trees. In IJCAI.
Kevin Knight and Jonathan Graehl. 2004. Training tree transducers. In HLT-NAACL.
I. Dan Melamed, Giorgio Satta, and Ben Wellington. 2004.
Generalized multitext grammars. In ACL.
F. J. Och and H. Ney. 2000. Improved statistical alignment
models. In ACL.
Ian H. Witten and Timothy C. Bell. 1991. The zero-frequency
problem: Estimating the probabilities of novel events in
adaptive text compression. IEEE.
Dekai Wu. 1997. Stochastic inversion transduction grammars
and bilingual parsing of parallel corpora. Comput. Linguist.
Hao Zhang and Daniel Gildea. 2006. Efﬁcient search for inversion transduction grammar. In EMNLP.

