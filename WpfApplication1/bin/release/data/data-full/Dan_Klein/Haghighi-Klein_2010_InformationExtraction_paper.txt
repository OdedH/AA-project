An Entity-Level Approach to Information Extraction
Aria Haghighi
UC Berkeley, CS Division
aria42@cs.berkeley.edu

Dan Klein
UC Berkeley, CS Division
klein@cs.berkeley.edu

Template

Abstract
We present a generative model of
template-ﬁlling in which coreference
resolution and role assignment are jointly
determined. Underlying template roles
ﬁrst generate abstract entities, which in
turn generate concrete textual mentions.
On the standard corporate acquisitions
dataset, joint resolution in our entity-level
model reduces error over a mention-level
discriminative approach by up to 20%.

1

Introduction

Template-ﬁlling information extraction (IE) systems must merge information across multiple sentences to identify all role ﬁllers of interest. For
instance, in the MUC4 terrorism event extraction task, the entity ﬁlling the individual perpetrator role often occurs multiple times, variously as
proper, nominal, or pronominal mentions. However, most template-ﬁlling systems (Freitag and
McCallum, 2000; Patwardhan and Riloff, 2007)
assign roles to individual textual mentions using
only local context as evidence, leaving aggregation for post-processing. While prior work has
acknowledged that coreference resolution and discourse analysis are integral to accurate role identiﬁcation, to our knowledge no model has been proposed which jointly models these phenomena.
In this work, we describe an entity-centered approach to template-ﬁlling IE problems. Our model
jointly merges surface mentions into underlying
entities (coreference resolution) and assigns roles
to those discovered entities. In the generative process proposed here, document entities are generated for each template role, along with a set of
non-template entities. These entities then generate
mentions in a process sensitive to both lexical and
structural properties of the mention. Our model
outperforms a discriminative mention-level baseline. Moreover, since our model is generative, it

(a)

SELLER

BUSINESS

ACQUIRED

PURCHASER

CSR Limited

Oil and Gas

Delhi Fund

Esso Inc.

Document

(b)

[S CSR] has said that [S it] has sold [S its] [B oil
interests] held in [A Delhi Fund]. [P Esso Inc.] did not
disclose how much [P they] paid for [A Dehli].

Figure 1: Example of Corporate Acquisitions role-ﬁlling
task. In (a), an example template specifying the entities playing each domain role. In (b), an example document with
coreferent mentions sharing the same role label. Note that
pronoun mentions provide direct clues to entity roles.

can naturally incorporate unannotated data, which
further increases accuracy.

2

Problem Setting

Figure 1(a) shows an example template-ﬁlling
task from the corporate acquisitions domain (Freitag, 1998).1 We have a template of K roles
(PURCHASER, AMOUNT, etc.) and we must identify which entity (if any) ﬁlls each role (CSR Limited, etc.). Often such problems are modeled at the
mention level, directly labeling individual mentions as in Figure 1(b). Indeed, in this data set,
the mention-level perspective is evident in the gold
annotations, which ignore pronominal references.
However, roles in this domain appear in several locations throughout the document, with pronominal
mentions often carrying the critical information
for template ﬁlling. Therefore, Section 3 presents
a model in which entities are explicitly modeled,
naturally merging information across all mention
types and explicitly representing latent structure
very much like the entity-level template structure
from Figure 1(a).
1
In Freitag (1998), some of these ﬁelds are split in two to
distinguish a full versus abbreviated name, but we ignore this
distinction. Also we ignore the status ﬁeld as it doesn’t apply
to entities and its meaning is not consistent.

Role Entity Parameters

Purchaser Role

r

fr

θr

[bought: 0.02,
GOV-NSUBJ obtained:0.015,
acquired: 0.01,...]
[Inc.: 0.02,
Corp.:0.015,
HEAD-NAM
Ltd.: 0.01,...]
[investment: 0.02,
MOD-POSS subsidiary:0.015,
holding: 0.01,...]

[2: 0.18,
3:0.12,
1: 0.09,...]
[1: 0.19,
2:0.14,
0: 0.08,...]
[2: 0.02,
1:0.015,
0: 0.01,...]

Other Entity Parameters

R2 ....

R1

....

RK
Role
Priors

Document
Role
Entites

Other
Entities

E2 ....

E1

....

φ
EK

Purchaser Entity

r

Lr

HEAD-NAM

Google, GOOG

HEAD-NOM

company

MOD-NN

search, giant

MOD-POSS

acquisition

1

Entity Indicators

Z1

Z2

Z3

...........

Zn

r

M2

M3

...........

Mn

w

HEAD-NAM

Mentions

M1

Purchaser Mention
Google

GOV-NSUBJ

bought

Figure 2: Graphical model depiction of our generative model described in Section 3. Sample values are illustrated for key
parameters and latent variables.

3

Model

We describe our generative model for a document,
which has many similarities to the coreferenceonly model of Haghighi and Klein (2010), but
which integrally models template role-ﬁllers. We
brieﬂy describe the key abstractions of our model.
Mentions: A mention is an observed textual
reference to a latent real-world entity. Mentions
are associated with nodes in a parse tree and are
typically realized as NPs. There are three basic forms of mentions: proper (NAM), nominal
(NOM), and pronominal (PRO). Each mention M
is represented as collection of key-value pairs.
The keys are called properties and the values are
words. The set of properties utilized here, denoted R, are the same as in Haghighi and Klein
(2010) and consist of the mention head, its dependencies, and its governor. See Figure 2 for a concrete example. Mention types are trivially determined from mention head POS tag. All mention
properties and their values are observed.
Entities: An entity is a speciﬁc individual or
object in the world. Entities are always latent in
text. Where a mention has a single word for each
property, an entity has a list of signature words.
Formally, entities are mappings from properties
r ∈ R to lists Lr of “canonical” words which that
entity uses for that property.
Roles: The elements we have described so far
are standard in many coreference systems. Our
model performs role-ﬁlling by assuming that each
entity is drawn from an underlying role. These

roles include the K template roles as well as ‘junk’
roles to represent entities which do not ﬁll a template role (see Section 5.2). Each role R is represented as a mapping between properties r and
pairs of multinomials (θr , fr ). θr is a unigram distribution of words for property r that are semantically licensed for the role (e.g. being the subject of “acquired” for the ACQUIRED role). fr is a
“fertility” distribution over the integers that characterizes entity list lengths. Together, these distributions control the lists Lr for entities which instantiate the role.
We ﬁrst present a broad sketch of our model’s
components and then detail each in a subsequent
section. We temporarily assume that all mentions belong to a template role-ﬁlling entity; we
lift this restriction in Section 5.2. First, a semantic component generates a sequence of entities E = (E1 , . . . , EK ), where each Ei is generated from a corresponding role Ri . We use
R = (R1 , . . . , RK ) to denote the vector of template role parameters. Note that this work assumes
that there is a one-to-one mapping between entities
and roles; in particular, at most one entity can ﬁll
each role. This assumption is appropriate for the
domain considered here.
Once entities have been generated, a discourse component generates which entities will be
evoked in each of the n mention positions. We
represent these choices using entity indicators denoted by Z = (Z1 , . . . , Zn ). This component utilizes a learned global prior φ over roles. The Zi in-

dicators take values in 1, . . . , K indicating the entity number (and thereby the role) underlying the
ith mention position. Finally, a mention generation component renders each mention conditioned
on the underlying entity and role. Formally:
P (E, Z, M|R, φ) =
K

P (Ei |Ri )

[Semantic, Sec. 3.1]

i=1





n

P (Zj |Z<j , φ)



[Discourse, Sec. 3.2]

j=1



restricted to antecedent mention positions j which
occur earlier in the same sentence or in the previous sentence.5
3.3

Mention Generation

Once the entity indicator has been drawn, we generate words associated with mention conditioned
on the underlying entity E and role R. For each
mention property r associated with the mention,
a word w is drawn utilizing E’s word list Lr as
well as the multinomials (fr , θr ) from role R. The
word w is drawn according to,



n

P (Mj |EZj , RZj )



[Mention, Sec. 3.3]

P (w|E, R)=(1 − αr )

j=1

3.1

Semantic Component

Each role R generates an entity E as follows: for
each mention property r, a word list, Lr , is drawn
by ﬁrst generating a list length from the corresponding fr distribution in R.2 This list is then
populated by an independent draw from R’s unigram distribution θr . Formally, for each r ∈ R, an
entity word list is drawn according to,3
P (Lr |R) = P (len(Lr )|fr )

P (w|θr )

1 [w ∈ Lr ]
+ αr P (w|θr )
len(Lr )

For each property r, there is a hyper-parameter αr
which interpolates between selecting a word uniformly from the entity list Lr and drawing from
the underlying role distribution θr . Intuitively, a
small αr indicates that an entity prefers to re-use a
small number of words for property r. This is typically the case for proper and nominal heads as well
as modiﬁers. At the other extreme, setting αr to 1
indicates the property isn’t particular to the entity
itself, but rather always drawn from the underlying
role distribution. We set αr to 1 for pronoun heads
as well as for the governor of the head properties.

w∈Lr

3.2

Discourse Component

The discourse component draws the entity indicator Zj for the jth mention according to,
P (Zj |Z<j , φ) =

P (Zj |φ), if non-pronominal
j δZj (Zj )P (j |j), o.w.

When the jth mention is non-pronominal, we draw
Zj from φ, a global prior over the K roles. When
Mj is a pronoun, we ﬁrst draw an antecedent mention position j , such that j < j, and then we set
Zj = Zj . The antecedent position is selected according to the distribution,
P (j |j) ∝ exp{−γ T REE D IST(j , j)}
where T REE D IST(j ,j) represents the tree distance
between the parse nodes for Mj and Mj .4 Mass is
2
There is one exception: the sizes of the proper and nominal head property lists are jointly generated, but their word
lists are still independently populated.
3
While, in principle, this process can yield word lists with
duplicate words, we constrain the model during inference to
not allow that to occur.
4
Sentence parse trees are merged into a right-branching
document parse tree. This allows us to extend tree distance to
inter-sentence nodes.

4

Learning and Inference

Since we will make use of unannotated data (see
Section 5), we utilize a variational EM algorithm
to learn parameters R and φ. The E-Step requires the posterior P (E, Z|R, M, φ), which is
intractable to compute exactly. We approximate
it using a surrogate variational distribution of the
following factored form:


n

K

Q(E, Z) =

qi (Ei ) 
i=1

rj (Zj )
j=1

Each rj (Zj ) is a distribution over the entity indicator for mention Mj , which approximates the
true posterior of Zj . Similarly, qi (Ei ) approximates the posterior over entity Ei which is associated with role Ri . As is standard, we iteratively
update each component distribution to minimize
KL-divergence, ﬁxing all other distributions:
qi ← arg min KL(Q(E, Z)|P (E, Z|M, R, φ)
qi

∝ exp{EQ/qi ln P (E, Z|M, R, φ))}
5

The sole parameter γ is ﬁxed at 0.1.

INDEP
JOINT
JOINT+PRO

Ment Acc.
60.0
64.6
68.2

Ent. Acc.
43.7
54.2
57.8

Table 1: Results on corporate acquisition tasks with given
role mention boundaries. We report mention role accuracy
and entity role accuracy (correctly labeling all entity mentions).

For example, the update for a non-pronominal
entity indicator component rj (·) is given by: 6
ln rj (z) ∝ EQ/rj ln P (E, Z, M|R, φ)
∝ Eqz ln (P (z|φ)P (Mj |Ez , Rz ))
= ln P (z|φ) + Eqz ln P (Mj |Ez , Rz )
A similar update is performed on pronominal entity indicator distributions, which we omit here for
space. The update for variational entity distribution is given by:
ln qi (ei ) ∝ EQ/qi ln P (E, Z, M|R, φ)

∝ E{rj } ln P (ei |Ri )



P (Mj |ei , Ri )
j:Zj =i

= ln P (ei |Ri ) +

rj (i) ln P (Mj |ei , Ri )
j

It is intractable to enumerate all possible entities
ei (each consisting of several sets of words). We
instead limit the support of qi (ei ) to several sampled entities. We obtain entity samples by sampling mention entity indicators according to rj .
For a given sample, we assume that Ei consists
of the non-pronominal head words and modiﬁers
of mentions such that Zj has sampled value i.
During the E-Step, we perform 5 iterations of
updating each variational factor, which results in
an approximate posterior distribution. Using expectations from this approximate posterior, our MStep is relatively straightforward. The role parameters Ri are computed from the qi (ei ) and rj (z)
distributions, and the global role prior φ from the
non-pronominal components of rj (z).

5

Experiments

We present results on the corporate acquisitions
task, which consists of 600 annotated documents
split into a 300/300 train/test split. We use 50
training documents as a development set. In all
6
For simplicity of exposition, we omit terms where Mj is
an antecedent to a pronoun.

documents, proper and (usually) nominal mentions are annotated with roles, while pronouns are
not. We preprocess each document identically to
Haghighi and Klein (2010): we sentence-segment
using the OpenNLP toolkit, parse sentences with
the Berkeley Parser (Petrov et al., 2006), and extract mention properties from parse trees and the
Stanford Dependency Extractor (de Marneffe et
al., 2006).
5.1

Gold Role Boundaries

We ﬁrst consider the simpliﬁed task where role
mention boundaries are given. We map each labeled token span in training and test data to a parse
tree node that shares the same head. In this setting, the role-ﬁlling task is a collective classiﬁcation problem, since we know each mention is ﬁlling some role.
As our baseline, INDEP, we built a maximum entropy model which independently classiﬁes each mention’s role. It uses features as similar
as possible to the generative model (and more), including the head word, typed dependencies of the
head, various tree features, governing word, and
several conjunctions of these features as well as
coarser versions of lexicalized features. This system yields 60.0 mention labeling accuracy (see Table 1). The primary difﬁculty in classiﬁcation is
the disambiguation amongst the acquired, seller,
and purchaser roles, which have similar internal
structure, and differ primarily in their semantic
contexts. Our entity-centered model, JOINT in Table 1, has no latent variables at training time in this
setting, since each role maps to a unique entity.
This model yields 64.6, outperforming INDEP.7
During development, we noted that often the
most direct evidence of the role of an entity was
associated with pronoun usage (see the ﬁrst “it”
in Figure 1). Training our model with pronominal
mentions, whose roles are latent variables at training time, improves accuracy to 68.2.8
5.2

Full Task

We now consider the more difﬁcult setting where
role mention boundaries are not provided at test
time. In this setting, we automatically extract
mentions from a parse tree using a heuristic ap7

We use the mode of the variational posteriors rj (Zj ) to
make predictions (see Section 4).
8
While this approach incorrectly assumes that all pronouns have antecedents amongst our given mentions, this did
not appear to degrade performance.

ROLE ID

6

OVERALL

P
INDEP
JOINT+PRO
BEST

R

F1

P

R

79.0
80.3
80.1

65.5
69.2
70.1

71.6
74.3
74.8

48.6
53.4
57.3

40.3
46.4
49.2

44.0
49.7
52.9

Conclusion

F1

Table 2: Results on corporate acquisitions data where mention boundaries are not provided. Systems must determine
which mentions are template role-ﬁllers as well as label them.
ROLE ID only evaluates the binary decision of whether a
mention is a template role-ﬁller or not. OVERALL includes
correctly labeling mentions. Our BEST system, see Section 5, adds extra unannotated data to our JOINT+PRO system.

proach. Our mention extraction procedure yields
95% recall over annotated role mentions and 45%
precision.9 Using extracted mentions as input, our
task is to label some subset of the mentions with
template roles. Since systems can label mentions
as non-role bearing, only recall is critical to mention extraction. To adapt INDEP to this setting, we
ﬁrst use a binary classiﬁer trained to distinguish
role-bearing mentions. The baseline then classiﬁes mentions which pass this ﬁrst phase as before.
We add ‘junk’ roles to our model to ﬂexibly model
entities that do not correspond to annotated template roles. During training, extracted mentions
which are not matched in the labeled data have
posteriors which are constrained to be amongst the
‘junk’ roles.
We ﬁrst evaluate role identiﬁcation (ROLE ID in
Table 2), the task of identifying mentions which
play some role in the template. The binary classiﬁer for INDEP yields 71.6 F1 . Our JOINT+PRO
system yields 74.3. On the task of identifying and
correctly labeling role mentions, our model outperforms INDEP as well (OVERALL in Table 2). As
our model is generative, it is straightforward to utilize totally unannotated data. We added 700 fully
unannotated documents from the mergers and acquisitions portion of the Reuters 21857 corpus.
Training JOINT+PRO on this data as well as our
original training data yields the best performance
(BEST in Table 2).10
To our knowledge, the best previously published results on this dataset are from Siefkes
(2008), who report 45.9 weighted F1 . Our BEST
system evaluated in their slightly stricter way
yields 51.1.
9
Following Patwardhan and Riloff (2009), we match extracted mentions to labeled spans if the head of the mention
matches the labeled span.
10
We scaled expected counts from the unlabeled data so
that they did not overwhelm those from our (partially) labeled
data.

We have presented a joint generative model of
coreference resolution and role-ﬁlling information
extraction. This model makes role decisions at
the entity, rather than at the mention level. This
approach naturally aggregates information across
multiple mentions, incorporates unannotated data,
and yields strong performance.
Acknowledgements: This project is funded in
part by the Ofﬁce of Naval Research under MURI
Grant No. N000140911081.

References
M. C. de Marneffe, B. Maccartney, and C. D. Manning. 2006. Generating typed dependency parses
from phrase structure parses. In LREC.
Dayne Freitag and Andrew McCallum. 2000. Information extraction with hmm structures learned by
stochastic optimization. In Association for the Advancement of Artiﬁcial Intelligence (AAAI).
Dayne Freitag. 1998. Machine learning for information extraction in informal domains.
A. Haghighi and D. Klein. 2010. Coreference resolution in a modular, entity-centered model. In North
American Association of Computational Linguistics
(NAACL).
S. Patwardhan and E. Riloff. 2007. Effective information extraction with semantic afﬁnity patterns and
relevant regions. In Joint Conference on Empirical
Methods in Natural Language Processing.
S. Patwardhan and E Riloff. 2009. A uniﬁed model of
phrasal and sentential evidence for information extraction. In Empirical Methods in Natural Language
Processing (EMNLP).
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and
interpretable tree annotation. In Proceedings of
the 21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 433–440,
Sydney, Australia, July. Association for Computational Linguistics.
Christian Siefkes. 2008. An Incrementally Trainable Statistical Approach to Information Extraction:
Based on Token Classiﬁcation and Rich Context
Model. VDM Verlag, Saarbr¨ cken, Germany, Geru
many.

