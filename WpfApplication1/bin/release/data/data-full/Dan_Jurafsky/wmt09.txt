Disambiguating “DE” for Chinese-English Machine Translation
Pi-Chuan Chang, Dan Jurafsky, and Christopher D. Manning
Computer Science Department, Stanford University
Stanford, CA 94305
pichuan,jurafsky,manning@stanford.edu
Abstract
Linking constructions involving dሇ (DE) are ubiquitous in Chinese, and can be translated into English in many different ways. This is a major source
of machine translation error, even when syntaxsensitive translation models are used. This paper
explores how getting more information about the
syntactic, semantic, and discourse context of uses
of dሇ (DE) can facilitate producing an appropriate
English translation strategy. We describe a ﬁnergrained classiﬁcation of dሇ (DE) constructions in
Chinese NPs, construct a corpus of annotated examples, and then train a log-linear classiﬁer, which
contains linguistically inspired features. We use the
DE classiﬁer to preprocess MT data by explicitly
labeling dሇ (DE) constructions, as well as reordering phrases, and show that our approach provides
signiﬁcant BLEU point gains on MT02 (+1.24),
MT03 (+0.88) and MT05 (+1.49) on a phrasedbased system. The improvement persists when a hierarchical reordering model is applied.

1

Introduction

Machine translation (MT) from Chinese to English has been a difﬁcult problem: structural differences between Chinese and English, such as
the different orderings of head nouns and relative clauses, cause BLEU scores to be consistently lower than for other difﬁcult language pairs
like Arabic-English. Many of these structural
differences are related to the ubiquitous Chinese
dሇ(DE) construction, used for a wide range of
noun modiﬁcation constructions (both single word
and clausal) and other uses. Part of the solution
to dealing with these ordering issues is hierarchical decoding, such as the Hiero system (Chiang,
2005), a method motivated by dሇ(DE) examples
like the one in Figure 1. In this case, the translation goal is to rotate the noun head and the preceding relative clause around dሇ(DE), so that we can
translate to “[one of few countries] dሇ [have diplomatic relations with North Korea]”. Hiero can
learn this kind of lexicalized synchronous grammar rule.
But use of hierarchical decoders has not solved
the DE construction translation problem. We analyzed the errors of three state-of-the-art systems

(the 3 DARPA GALE phase 2 teams’ systems),
and even though all three use some kind of hierarchical system, we found many remaining errors
related to reordering. One is shown here:
d੄d‫ ޞ‬dδd୛ dٍdࠣdνd҅
dሇ dϔdࣣ
local a
bad reputation DE middle school
Reference: ‘a local middle school with a bad reputation’
Team 1: ‘a bad reputation of the local secondary school’
Team 2: ‘the local a bad reputation secondary school’
Team 3: ‘a local stigma secondary schools’

None of the teams reordered “bad reputation”
and “middle school” around the dሇ. We argue that
this is because it is not sufﬁcient to have a formalism which supports phrasal reordering, but it
is also necessary to have sufﬁcient linguistic modeling that the system knows when and how much
to rearrange.
An alternative way of dealing with structural
differences is to reorder source language sentences
to minimize structural divergence with the target
language, (Xia and McCord, 2004; Collins et al.,
2005; Wang et al., 2007). For example Wang et
al. (2007) introduced a set of rules to decide if
a dሇ(DE) construction should be reordered or not
before translating to English:
• For DNPs (consisting of“XP+DEG”):
– Reorder if XP is PP or LCP;
– Reorder if XP is a non-pronominal NP
• For CPs (typically formed by “IP+DEC”):
– Reorder to align with the “that+clause”
structure of English.
Although this and previous reordering work has
led to signiﬁcant improvements, errors still remain. Indeed, Wang et al. (2007) found that the
precision of their NP rules is only about 54.6% on
a small human-judged set.
One possible reason the dሇ(DE) construction remains unsolved is that previous work has paid insufﬁcient attention to the many ways the dሇ(DE)
construction can be translated and the rich structural cues to the translation. Wang et al. (2007),
for example, characterized dሇ(DE) into only two

dဉd༬
d೤ dξ
d‫׏‬dᱻ
dണ
dᩥdБ
dሇ
dळdಚ
d‫ދ‬dए
Aozhou
shi yu
Beihan
you bangjiao
DE shaoshu guojia
Australia is
with North Korea have diplomatic relations that few
countries
dǺAustralia is one of the few countries that have diplomatic relations with North Korea.dǻ

dϥdδ
zhiyi
one of

dˊ
.
.

Figure 1: An example of the DE construction from (Chiang, 2005)

classes. But our investigation shows that there are
many strategies for translating Chinese [A dሇ B]
phrases into English, including the patterns in Table 1, only some involving reversal.
Notice that the presence of reordering is only
one part of the rich structure of these examples.
Some reorderings are relative clauses, while others
involve prepositional phrases, but not all prepositional phrase uses involve reorderings. These examples suggest that capturing ﬁner-grained translation patterns could help achieve higher accuracy
both in reordering and in lexical choice.
In this work, we propose to use a statistical classiﬁer trained on various features to predict for a
given Chinese dሇ(DE) construction both whether
it will reorder in English and which construction
it will translate to in English. We suggest that the
necessary classiﬁcatory features can be extracted
from Chinese, rather than English. The dሇ(DE)
in Chinese has a uniﬁed meaning of ‘noun modiﬁcation’, and the choice of reordering and construction realization are mainly a consequence of
facts of English noun modiﬁcation. Nevertheless,
most of the features that determine the choice of
a felicitous translation are available in the Chinese source. Noun modiﬁcation realization has
been widely studied in English (e.g., (Rosenbach,
2003)), and many of the important determinative
properties (e.g., topicality, animacy, prototypicality) can be detected working in the source language.
We ﬁrst present some corpus analysis characterizing different DE constructions based on how
they get translated into English (Section 2). We
then train a classiﬁer to label DEs into the 5 different categories that we deﬁne (Section 3). The
ﬁne-grained DEs, together with reordering, are
then used as input to a statistical MT system (Section 4). We ﬁnd that classifying DEs into ﬁnergrained tokens helps MT performance, usually at
least twice as much as just doing phrasal reordering.

2

DE classiﬁcation

The Chinese character DE serves many different
purposes. According to the Chinese Treebank tag-

ging guidelines (Xia, 2000), the character can be
tagged as DEC, DEG, DEV, SP, DER, or AS. Similar to (Wang et al., 2007), we only consider the
majority case when the phrase with dሇ(DE) is a
noun phrase modiﬁer. The DEs in NPs have a
part-of-speech tag of DEC (a complementizer or
a nominalizer) or DEG (a genitive marker or an
associative marker).
2.1

Class Deﬁnition

The way we categorize the DEs is based on their
behavior when translated into English. This is implicitly done in the work of Wang et al. (2007)
where they use rules to decide if a certain DE and
the words next to it will need to be reordered. In
this work, we categorize DEs into ﬁner-grained
categories. For a Chinese noun phrase [A dሇ B],
we categorize it into one of these ﬁve classes:
1. A B
In this category, A in the Chinese side is translated as a pre-modiﬁer of B. In most of the
cases A is an adjective form, like Example 1.1
in Table 1 or the possessive adjective example in Example 1.2. Compound nouns where
A becomes a pre-modiﬁer of B also ﬁt in this
category (Example 1.3).
2. B preposition A
There are several cases that get translated into
the form B preposition A. For example, the ofgenitive in Example 2.1 in Table 1.
Example 2.2 shows cases where the Chinese
A gets translated into a prepositional phrase
that expresses location.
When A becomes a gerund phrase and an object of a preposition, it is also categorized in
the B preposition A category (Example 2.3).
3. A ’s B
In this class, the English translation is an explicit s-genitive case, as in Example 3.1. This
class occurs much less often but is still interesting because of the difference from the
of-genitive.
4. relative clause
We include the obvious relative clause cases
like Example 4.1 where a relative clause is

introduced by a relative pronoun. We also include reduced relative clauses like Example
4.2 in this class.
5. A preposition B
This class is another small one. The English
translations that fall into this class usually
have some number, percentage or level word
in the Chinese A.
Some NPs are translated into a hybrid of these categories, or just don’t ﬁt into one of the ﬁve categories, for instance, involving an adjectival premodiﬁer and a relative clause. In those cases, they
are put into an “other” category.1
2.2

Data annotation of DE classes

In order to train a classiﬁer and test its performance, we use the Chinese Treebank 6.0
(LDC2007T36) and the English Chinese Translation Treebank 1.0 (LDC2007T02). The word
alignment data (LDC2006E93) is also used to
align the English and Chinese words between
LDC2007T36 and LDC2007T02. The overlapping part of the three datasets are a subset of CTB6
ﬁles 1 to 325. After preprocessing those three
sets of data, we have 3253 pairs of Chinese sentences and their translations. In those sentences,
we use the gold-standard Chinese tree structure to
get 3412 Chinese DEs in noun phrases that we
want to annotate. Among the 3412 DEs, 530 of
them are in the “other” category and are not used
in the classiﬁer training and evaluation. The statistics of the ﬁve classes are:
1.
2.
3.
4.
5.

3

A B: 693 (24.05%)
B preposition A: 1381 (47.92%)
A ’s B: 91 (3.16%)
relative clause: 669 (23.21%)
A preposition B: 48 (1.66%)

Log-linear DE classiﬁer

In order to see how well we can categorize DEs in
noun phrases into one of the ﬁve classes, we train a
log-linear classiﬁer to classify each DE according
to features extracted from its surrounding context.
Since we want the training and testing conditions
to match, when we extract features for the classiﬁer, we don’t use gold-standard parses. Instead,
we use a parser trained on CTB6 excluding ﬁles
1-325. We then use this parser to parse the 3253
1 The

“other” category contains many mixed cases that
could be difﬁcult Chinese patterns to translate. We will leave
this for future work.

baseline
DEPOS
+A-pattern
+POS-ngram
+Lexical
+SemClass
+Topicality

5-class Acc. (%)
54.8
67.9
72.1
74.9
75.1
75.4

2-class Acc. (%)
76.0
71.0
83.7
84.9
86.5
86.7
86.9

Table 2: 5-class and 2-class classiﬁcation accuracy. “baseline” is the heuristic rules in (Wang et al., 2007). Others are
various features added to the log-linear classiﬁer.

Chinese sentences with the DE annotation and extract parse-related features from there.
3.1

Experimental setting

For the classiﬁcation experiment, we exclude the
“other” class and only use the 2882 examples that
fall into the ﬁve pre-deﬁned classes. To evaluate the classiﬁcation performance and understand
what features are useful, we compute the accuracy
by averaging ﬁve 10-fold cross-validations.2
As a baseline, we use the rules introduced in
Wang et al. (2007) to decide if the DEs require reordering or not. However, since their rules only
decide if there is reordering in an NP with DE,
their classiﬁcation result only has two classes. So,
in order to compare our classiﬁer’s performance
with the rules in Wang et al. (2007), we have to
map our ﬁve-class results into two classes. We
mapped our ﬁve-class results into two classes. So
we mapped B preposition A and relative clause
into the class “reordered”, and the other three
classes into “not-reordered”.
3.2

Feature Engineering

To understand which features are useful for DE
classiﬁcation, we list our feature engineering steps
and results in Table 2. In Table 2, the 5-class accuracy is deﬁned by:
(number of correctly labeled DEs)
(number of all DEs)

× 100

The 2-class accuracy is deﬁned similarly, but it
is evaluated on the 2-class “reordered” and “notreordered” after mapping from the 5 classes.
The DEs we are classifying are within an NP;
we refer to them as [A dሇ B]NP . A includes all
the words in the NP before dሇ; B includes all the
words in the NP after dሇ. To illustrate, we will use
the following NP:
[[dᱻd‫ ދ‬dഡ d࠵]A dሇ [dஃdᤨ dनdᣳd‫]ދ‬B ]NP
2 We evaluate the classiﬁer performance using crossvalidations to get the best setting for the classiﬁer. The proof
of efﬁcacy of the DE classiﬁer is MT performance on independent data in Section 4.

1.
1.1.
1.2.
1.3.
2.
2.1.
2.2.
2.3.

A B
dѓdᥒ(excellent)/dሇ(DE)/d‫ޞ‬dᄮ(geographical)/dോdх(qualiﬁcation) → “excellent geographical qualiﬁcations”
d୅dр(our)/dሇ(DE)/d᫘d឵(ﬁnancial)/dᲰdᰔ(risks) → “our ﬁnancial risks”
dᤜd೙(trade)/dሇ(DE)/dІd៧dપ(complement) → “trade complement”
B preposition A
dஃdᤨ(investment)/dᄔdࠏ(environment)/dሇ(DE)/dಅd‫(ܐ‬improvement) → “the improvement of the investment environment”
d঑d೗d‫(؛‬Chongming county)/dԬ(inside)/dሇ(DE)/d‫ת‬dѬ(organization) → “organizations inside Chongming county”
dδ(one)/dϑ(measure word)/dᠷdठ(observe)/dϔd‫(ދ‬China)/dেd‫(ޡ‬market)/dሇ(DE)/dलdल(small)/dፑd‫(ح‬window)
→ “a small window for watching over Chinese markets”
A ’s B
d‫ދ‬dए(nation)/dሇ(DE)/dࣷdᠷ(macro)/dᎫdᄮ(management) → “the nation ’s macro management”
relative clause
dϔd‫(ދ‬China)/dνdᕜ(cannot)/dᅴdД(produce)/dᓨ(and)/d‫(؞‬but)/d੟(very)/d᱇dᠳ(need)/dሇ(DE)/dᙯd‫(ڳ‬medicine)
→ “medicine that cannot be produced by China but is urgently needed”
d࠮d۵(foreign business)/dஃdᤨ(invest)/dъdχ(enterprise)/dᚊd੧(acquire)/dሇ(DE)/dОdຜd৆(RMB)/dᤛd๙(loan)
→ “the loans in RMB acquired by foreign-invested enterprises”
A preposition B
d‫ݻ‬d‫ן‬d࠰dη(more than 40 million)/dᒼdԈ(US dollar)/dሇ(DE)/dДd‫(ڳ‬product) → more than 40 million US dollars in products

3.
3.1.
4.
4.1.
4.2.
5.
5.1.

Table 1: Examples for the 5 DE classes

to show examples of each feature. The parse structure of the NP is listed in Figure 2.

(NP
(NP (NR dᱻd‫))ދ‬
(CP
(IP
(VP
(ADVP (AD dഡ))
(VP (VA d࠵))))
(DEC dሇ))
(NP (NN dஃdᤨ) (NN dनdᣳd‫))))))ދ‬
Figure 2: The parse tree of the Chinese NP.

A-pattern:

Chinese syntactic patterns
appearing before dሇ
Secondly, we want to incorporate the rules in
(Wang et al., 2007) as features in the log-linear
classiﬁer. We added features for certain indicative
patterns in the parse tree (listed in Table 3).
1. A is ADJP:
true if A+DE is a DNP which is in the form of “ADJP+DEG”.
2. A is QP:
true if A+DE is a DNP which is in the form of “QP+DEG”.
3. A is pronoun:
true if A+DE is a DNP which is in the form of “NP+DEG”, and
the NP is a pronoun.
4. A ends with VA:
true if A+DE is a CP which is in the form of “IP+DEC”, and
the IP ends with a VP that’s either just a VA or a VP preceded
by a ADVP.

Table 3: A-pattern features
DEPOS:

part-of-speech tag of DE

Since the part-of-speech tag of DE indicates its
syntactic function, it is the ﬁrst obvious feature
to add. The NP in Figure 2 will have the feature “DEC”. This basic feature will be referred to
as DEPOS. Note that since we are only classifying
DEs in NPs, ideally the part-of-speech tag of DE
will either be DEC or DEG as described in Section
2. However, since we are using automatic parses
instead of gold-standard ones, the DEPOS feature
might have other values than just DEC and DEG.
From Table 2, we can see that with this simple feature, the 5-class accuracy is low but at least better
than simply guessing the majority class (47.92%).
The 2-class accuracy is still lower than using the
heuristic rules in (Wang et al., 2007), which is reasonable because their rules encode more information than just the POS tags of DEs.

Features 1–3 are inspired by the rules in (Wang
et al., 2007), and the fourth rule is based on the
observation that even though the predicative adjective VA acts as a verb, it actually corresponds to
adjectives in English as described in (Xia, 2000).3
We call these four features A-pattern. Our example NP in Figure 2 will have the fourth feature
“A ends with VA” in Table 3, but not the other
three features. In Table 2 we can see that after
adding A-pattern, the 2-class accuracy is already
much higher than the baseline. We attribute this
to the fourth rule and also to the fact that the classiﬁer can learn weights for each feature.4
3 Quote from (Xia, 2000): “VA roughly corresponds to adjectives in English and stative verbs in the literature on Chinese grammar.”
4 We also tried extending a rule-based 2-class classiﬁer
with the fourth rule. The accuracy is 83.48%, only slightly
lower than using the same features in a log-linear classiﬁer.

POS-ngram:

unigrams and bigrams of POS tags
The POS-ngram feature adds all unigrams and bigrams in A and B. Since A and B have different
inﬂuences on the choice of DE class, we distinguish their ngrams into two sets of features. We
also include the bigram pair across DE which gets
another feature name for itself. The example NP
in Figure 2 will have these features (we use b to
indicate boundaries):
• POS unigrams in A: “NR”, “AD”, “VA”
• POS bigrams in A: “b-NR”, “NR-AD”, “ADVA”, “VA-b”
• cross-DE POS bigram: “VA-NN”
• POS unigram in B: “NN”
• POS bigrams in B: “b-NN”, “NN-NN”, “NNb”
The part-of-speech ngram features add 4.24%
accuracy to the 5-class classiﬁer.
Lexical:

lexical features
In addition to part-of-speech features, we also
tried to use features from the words themselves.
But since using full word identity resulted in a
sparsity issue,5 we take the one-character sufﬁx of
each word and extract sufﬁx unigram and bigram
features from them. The argument for using sufﬁxes is that it often captures the larger category of
the word (Tseng et al., 2005). For example, dϔ
d‫( ދ‬China) and dᱻd‫( ދ‬Korea) share the same sufﬁx
d‫ ,ދ‬which means “country”. These sufﬁx ngram
features will result in these features for the NP in
Figure 2:
• sufﬁx unigrams: “d‫“ ,”ދ‬dഡ”, “d࠵”, “dሇ”,
“dᤨ”, “d‫”ދ‬
• sufﬁx bigrams: “b-d‫“ ,”ދ‬d‫-ދ‬dഡ”, “dഡ-d࠵”,
“d࠵-dሇ”, “dሇ-dᤨ”, “dᤨ-d‫“ ,”ދ‬d‫-ދ‬b”
Other than the sufﬁx ngram, we also add three
other lexical features: ﬁrst, if the word before DE
is a noun, we add a feature that is the conjunction of POS and sufﬁx unigram. Secondly, an
“NR only” feature will ﬁre when A only consists of
one or more NRs. Thirdly, we normalize different
forms of “percentage” representation, and add a
feature if they exist. This includes words that start
with “dህdթdϥ” or ends with the percentage sign
“%”. The ﬁrst two features are inspired by the fact
that a noun and its type can help decide “B prep A”
versus “A B”. Here we use the sufﬁx of the noun
5 The

accuracy is worse when we tried using the word
identity instead of the sufﬁx.

and the NR (proper noun) tag to help capture its
animacy, which is useful in choosing between the
s-genitive (the boy’s mother) and the of-genitive
(the mother of the boy) in English (Rosenbach,
2003). The third feature is added because many of
the cases in the “A preposition B” class have a percentage number in A. We call these sets of features
Lexical. Together they provide 2.73% accuracy improvement over the previous setting.
SemClass:

semantic class of words
We also use a Chinese thesaurus, CiLin, to look up
the semantic classes of the words in [A dሇ B] and
use them as features. CiLin is a Chinese thesaurus
published in 1984 (Mei et al., 1984). CiLin is organized in a conceptual hierarchy with ﬁve levels.
We use the level-1 tags which includes 12 categories.6 This feature ﬁres when a word we look up
has one level-1 tag in CiLin. This kind of feature
is referred to as SemClass in Table 2. For the example in Figure 2, two words have a single level1 tag: “dഡ”(most) has a level-1 tag K7 and “dஃ
dᤨ”(investment) has a level-1 tag H8 . “dᱻd‫ ”ދ‬and
“dनdᣳd‫ ”ދ‬are not listed in CiLin, and “d࠵” has
multiple entries. Therefore, the SemClass features
are: (i) before DE: “K”; (ii) after DE: “H”
Topicality:

re-occurrence of nouns
The last feature we add is a Topicality feature,
which is also useful for disambiguating s-genitive
and of-genitive. We approximate the feature by
caching the nouns in the previous two sentences,
and ﬁre a topicality feature when the noun appears
in the cache. Take this NP in MT06 as an example:
“d‫׬‬dᱻ dξ d‫׏‬dᱻ dሇ dࡕd᧿ dкd៨dᯰ”
For this NP, all words before DE and after DE
appeared in the previous sentence. Therefore the
topicality features “cache-before-DE” and “cacheafter-DE” both ﬁre.
After all the feature engineering above, the
best accuracy on the 5-class classiﬁer we have
is 75.4%, which maps into a 2-class accuracy of
86.9%. Comparing the 2-class accuracy to the
(Wang et al., 2007) baseline, we have a 10.9%
absolute improvement. The 5-class accuracy and
confusion matrix is listed in Table 4.
“A preposition B” is a small category and is the
most confusing. “A ’s B” also has lower accuracy,
and is mostly confused with “B preposition A”.
6 We

also tried adding more levels but it did not help.
is the category d֭dᢣ (auxiliary) in CiLin.
8 H is the category d༰d֬ (activities) in CiLin.
7K

real →
A ’s B
AB
A prep. B
B prep. A
rel. clause
Total
Accuracy(%)

A ’s B
168
48
0
239
0
455
36.92

AB
36
2473
18
691
247
3465
71.37

A prep. B
0
73
46
95
26
240
19.17

B prep. A
110
227
23
5915
630
6905
85.66

rel. clause
0
216
11
852
2266
3345
67.74

Table 4: The confusion matrix for 5-class DE classiﬁcation

This could be due to the fact that there are some
cases where the translation is correct both ways,
but also could be because the features we added
have not captured the difference well enough.

4

Machine Translation Experiments

4.1

Experimental Setting

For our MT experiments, we used a reimplementation of Moses (Koehn et al., 2003), a
state-of-the-art phrase-based system. The alignment is done by the Berkeley word aligner (Liang
et al., 2006) and then we symmetrized the word
alignment using the grow-diag heuristic. For features, we incorporate Moses’ standard eight features as well as the lexicalized reordering model.
Parameter tuning is done with Minimum Error
Rate Training (MERT) (Och, 2003). The tuning set for MERT is the NIST MT06 data set,
which includes 1664 sentences. We evaluate the
result with MT02 (878 sentences), MT03 (919
sentences), and MT05 (1082 sentences).
Our MT training corpus contains 1,560,071 sentence pairs from various parallel corpora from
LDC.9 There are 12,259,997 words on the English
side. Chinese word segmentation is done by the
Stanford Chinese segmenter (Chang et al., 2008).
After segmentation, there are 11,061,792 words
on the Chinese side. We use a 5-gram language
model trained on the Xinhua and AFP sections of
the Gigaword corpus (LDC2007T40) and also the
English side of all the LDC parallel data permissible under the NIST08 rules. Documents of Gigaword released during the epochs of MT02, MT03,
MT05, and MT06 were removed.
To run the DE classiﬁer, we also need to parse
the Chinese texts. We use the Stanford Chinese
parser (Levy and Manning, 2003) to parse the Chinese side of the MT training data and the tuning
and test sets.
9 LDC2002E18,

LDC2003E07,
LDC2003E14,
LDC2005E83, LDC2005T06, LDC2006E26, LDC2006E85,
LDC2002L27 and LDC2005T34

4.2

Baseline Experiments

We have two different settings as baseline experiments. The ﬁrst is without reordering or DE annotation on the Chinese side; we simply align the
parallel texts, extract phrases and tune parameters.
This experiment is referred to as BASELINE. Also,
we reorder the training data, the tuning and the
test sets with the NP rules in (Wang et al., 2007)
and compare our results with this second baseline
(WANG-NP).
The NP reordering preprocessing (WANG-NP)
showed consistent improvement in Table 5 on all
test sets, with BLEU point gains ranging from
0.15 to 0.40. This conﬁrms that having reordering around DEs in NP helps Chinese-English MT.
4.3

Experiments with 5-class DE annotation

We use the best setting of the DE classiﬁer described in Section 3 to annotate DEs in NPs in the
MT training data as well as the NIST tuning and
test sets.10 If a DE is in an NP, we use the annotation of dሇAB , dሇAsB , dሇBprepA , dሇrelc , or dሇAprepB
to replace the original DE character. Once we have
the DEs labeled, we preprocess the Chinese sentences by reordering them.11 Note that not all DEs
in the Chinese data are in NPs, therefore not all
DEs are annotated with the extra labels. Table
6 lists the statistics of the DE classes in the MT
training data.
class of dሇ(DE)
dሇAB
dሇAprepB
dሇAsB
dሇBprepA
dሇrelc
dሇ (unlabeled)
total number of dሇ

counts
112, 099
2, 426
3, 430
248, 862
95, 134
14, 056
476, 007

percentage
23.55%
0.51%
0.72%
52.28%
19.99%
2.95%
100%

Table 6: The number of different DE classes labeled for the
MT training data.

After this preprocessing, we restart the whole
MT pipeline – align the preprocessed data, extract
phrases, run MERT and evaluate. This setting is
referred to as DE-Annotated in Table 5.
4.4

Hierarchical Phrase Reordering Model

To demonstrate that the technique presented here
is effective even with a hierarchical decoder, we
10 The

DE classiﬁer used to annotate the MT experiment
was trained on all the available data described in Section 2.2.
11 Reordering is applied on DNP and CP for reasons described in Wang et al. (2007). We reorder only when the dሇ
is labeled as dሇBprepA or dሇrelc .

BASELINE
WANG-NP
DE-Annotated
BASELINE+Hier
DE-Annotated+Hier

BASELINE
WANG-NP
DE-Annotated

BLEU
MT06(tune)
MT02
MT03
32.39
32.51
32.75
32.75(+0.36) 32.66(+0.15) 33.15(+0.40)
33.39(+1.00) 33.75(+1.24) 33.63(+0.88)
32.96
33.10
32.93
33.96(+1.00) 34.33(+1.23) 33.88(+0.95)
Translation Error Rate (TER)
MT06(tune)
MT02
MT03
61.10
63.11
62.09
59.78(−1.32) 62.58(−0.53) 61.36(−0.73)
58.21(−2.89) 61.17(−1.94) 60.27(−1.82)

MT05
31.42
31.68(+0.26)
32.91(+1.49)
32.23
33.01(+0.77)
MT05
64.06
62.35(−1.71)
60.78(−3.28)

Table 5: MT experiments of different settings on various NIST MT evaluation datasets. We used both the BLEU and TER
metrics for evaluation. All differences between DE-Annotated and BASELINE are signiﬁcant at the level of 0.05 with the
approximate randomization test in (Riezler and Maxwell, 2005)

conduct additional experiments with a hierarchical phrase reordering model introduced by Galley
and Manning (2008). The hierarchical phrase reordering model can handle the key examples often used to motivated syntax-based systems; therefore we think it is valuable to see if the DE annotation can still improve on top of that. In Table 5, BASELINE+Hier gives consistent BLEU improvement over BASELINE. Using DE annotation
on top of the hierarchical phrase reordering models (DE-Annotated+Hier) provides extra gain over
BASELINE+Hier. This shows the DE annotation can
help a hierarchical system. We think similar improvements are likely to occur with other hierarchical systems.

5
5.1

Analysis
Statistics on the Preprocessed Data

Since our approach DE-Annotated and one of the
baselines (WANG-NP) are both preprocessing Chinese sentences, knowing what percentage of the
sentences are altered will be one useful indicator
of how different the systems are from the baseline.
In our test sets, MT02 has 591 out of 878 sentences
(67.3%) that have DEs under NPs; for MT03 it is
619 out of 919 sentences (67.4%); for MT05 it is
746 out of 1082 sentences (68.9%). This shows
that our preprocessing affects the majority of the
sentences and thus it is not surprising that preprocessing based on the DE construction can make a
signiﬁcant difference.
5.2

shows an example that contains a DE construction
that translates into a relative clause in English.12
The automatic parse tree of the sentence is listed
in Figure 3. The reordered sentences of WANG-NP
and DE-Annotated appear on the top and bottom in
Figure 4. For this example, both systems decide
to reorder, but DE-Annotated had the extra information that this dሇ is a dሇrelc . In Figure 4 we can
see that in WANG-NP, “dሇ” is being translated as
“for”, and the translation afterwards is not grammatically correct. On the other hand, the bottom
of Figure 4 shows that with the DE-Annotated preprocessing, now “dሇrelc ” is translated into “which
was” and well connected with the later translation.
This shows that disambiguating dሇ helps in choosing a better English translation.
(IP
(NP (NN dຈdЋd‫))ي‬
(VP
(ADVP (AD dട))
(VP (VV d‫צ‬d֭)
(IP
(VP (VV dᙖdய)
(NP
(QP (CD dδ)
(CLP (M dш)))
(CP
(IP
(VP (VV dᩐ)
(NP
(NP (NN dষdѕ)
(CC d‫)ڔ‬
(NN dসdᓠ) (NN dթd࣒))
(ADJP (JJ dਿd၇))
(NP (NN d‫أ‬dन)))))
(DEC dሇ))
(NP (NN dीdχ) (NN dಅdᱦ) (NN dಲd඲)))))))
(PU dˊ))

Example: how DE annotation affects
translation

Figure 3: The parse tree of the Chinese sentence in Table 7.

Our approach DE-Annotated reorders the Chinese
sentence, which is similar to the approach proposed by Wang et al. (2007) (WANG-NP). However, our focus is on the annotation on DEs and
how this can improve translation quality. Table 7

12 In this example, all four references agreed on the relative
clause translation. Sometimes DE constructions have multiple appropriate translations, which is one of the reasons why
certain classes are more confusable in Table 4.

Chinese
Ref 1
Ref 2
Ref 3
Ref 4

dຈdЋd‫ ي‬dട d‫צ‬d֭ dᙖdய [dδ dш dᩐ dষdѕ d‫ ڔ‬dসdᓠ dթd࣒ dਿd၇ d‫أ‬dन]A dሇ [dीdχ dಅdᱦ dಲd඲]B dˊ
biagi had assisted in drafting [an employment reform plan]B [that was strongly opposed by the labor
union and the leftists]A .
biagi had helped in drafting [a labor reform proposal]B [that provoked strong protests from labor unions
and the leftists]A .
biagi once helped drafting [an employment reform scheme]B [that was been strongly opposed by the
trade unions and the left - wing]A .
biagi used to assisted to draft [an employment reform plan]B [which is violently opposed by the trade
union and leftest]A .

Table 7: A Chinese example from MT02 that contains a DE construction that translates into a relative clause in English. The
[]A []B is hand-labeled to indicate the approximate translation alignment between the Chinese sentence and English references.

biagi had helped draft a reform plan for employment is strongly opposed by trade unions and left - wing activists .

relc

biagi had helped draft a reform plan for employment , which was strongly opposed by trade unions and left - wing activists

Figure 4: The top translation is from WANG-NP of the Chinese sentence in Table 7. The bottom one is from DE-Annotated.
In this example, both systems reordered the NP, but DE-Annotated has an annotation on the dሇ.

6

Conclusion

In this paper, we presented a classiﬁcation of Chinese dሇ(DE) constructions in NPs according to
how they are translated into English. We applied
this DE classiﬁer to the Chinese sentences of MT
data, and we also reordered the constructions that
required reordering to better match their English
translations. The MT experiments showed our preprocessing gave signiﬁcant BLEU and TER score
gains over the baselines. Based on our classiﬁcation and MT experiments, we found that not only
do we have better rules for deciding what to reorder, but the syntactic, semantic, and discourse
information that we capture in the Chinese sentence allows us to give hints to the MT system
which allows better translations to be chosen.

Acknowledgments
The authors would like to thank Michel Galley
and Daniel Cer for useful discussions and technical help, and Spence Green for his comments on
an earlier draft of the paper. This work is funded
by a Stanford Graduate Fellowship to the ﬁrst author and gift funding from Google for the project
“Translating Chinese Correctly”.

References
Pi-Chuan Chang, Michel Galley, and Christopher D.
Manning. 2008. Optimizing Chinese word segmentation for machine translation performance. In Proceedings of the Third Workshop on Statistical Ma-

chine Translation, pages 224–232, Columbus, Ohio,
June. Association for Computational Linguistics.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Proceedings of ACL, pages 263–270, Ann Arbor, Michigan, June. Association for Computational Linguistics.
Michael Collins, Philipp Koehn, and Ivona Kuˇ erov´ .
c
a
2005. Clause restructuring for statistical machine
translation. In ACL ’05: Proceedings of ACL, pages
531–540, Morristown, NJ, USA. Association for
Computational Linguistics.
Michel Galley and Christopher D. Manning. 2008. A
simple and effective hierarchical phrase reordering
model. In Proceedings of EMNLP, pages 847–855,
Honolulu, Hawaii, October. Association for Computational Linguistics.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
of NAACL-HLT.
Roger Levy and Christopher Manning. 2003. Is it
harder to parse Chinese, or the Chinese treebank?
In Proceedings of ACL, pages 439–446, Morristown,
NJ, USA. Association for Computational Linguistics.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Alignment by agreement. In Proceedings of HLT-NAACL,
pages 104–111, New York City, USA, June. Association for Computational Linguistics.
Jia-ju Mei, Yi-Ming Zheng, Yun-Qi Gao, and HungXiang Yin. 1984. TongYiCi CiLin. Shanghai: the
Commercial Press.
Franz Josef Och. 2003. Minimum error rate training
for statistical machine translation. In ACL.

Stefan Riezler and John T. Maxwell. 2005. On some
pitfalls in automatic evaluation and signiﬁcance testing for MT. In Proceedings of the ACL Workshop
on Intrinsic and Extrinsic Evaluation Measures for
Machine Translation and/or Summarization, pages
57–64, Ann Arbor, Michigan, June. Association for
Computational Linguistics.
Anette Rosenbach. 2003. Aspects of iconicity and
economy in the choice between the s-genitive and
the of-genitive in English. Topics in English Linguistics, 43:379–412.
Huihsin Tseng, Dan Jurafsky, and Christopher D. Manning. 2005. Morphological features help pos tagging of unknown words across language varieties.
In Proc. of the Fourth SIGHAN Workshop on Chinese Language Processing.
Chao Wang, Michael Collins, and Philipp Koehn.
2007. Chinese syntactic reordering for statistical
machine translation. In Proceedings of EMNLPCoNLL, pages 737–745, Prague, Czech Republic,
June. Association for Computational Linguistics.
Fei Xia and Michael McCord. 2004. Improving
a statistical MT system with automatically learned
rewrite patterns. In Proceedings of Coling 2004,
pages 508–514, Geneva, Switzerland, Aug 23–Aug
27. COLING.
Fei Xia. 2000. The part-of-speech tagging guidelines
for the Penn Chinese Treebank (3.0).

