Who Leads Whom: Topical Lead-Lag Analysis across
corpora

Xiaolin Shi, Ramesh Nallapati, Jure Lescovec, Dan McFarland, Dan Jurafsky
Stanford University
Stanford, CA 94305
{shixl,nmramesh,jure,dmcfarland,jurafsky}@stanford.edu

Abstract
Understanding the lead/lag of communities in the context of a given topic is an interesting
problem in computational social science. In this work, we study the particular problem of
whether research grants lead publications or vice versa. We propose simple but general
techniques for lead/lag estimation, based on LDA and time series analysis, that work on
any unlabeled textual corpora with temporal information. We perform our analysis on
about half a million Computer Science research paper abstracts and 20,000 successful grant
proposal abstracts that represent the entire ﬁeld of Computer Science in the time span of
1991-2008. Our analysis, besides revealing interesting patterns, ﬁnds that the lead/lag of
research papers with respect to research grants is highly topic speciﬁc.

1

Introduction

In the last two decades we have witnessed the emergence of enormous electronic recorded data
such as online news, blogs, friendship communities and academic publications which could inform
our understanding of social problems on a scale never seen before. Recent advances in modeling
the mechanics of information propagation within and across these social media enables scholars to
begin such large scale study of social dynamics in earnest.
Early work on tracking trends within or across corpora made use of rich meta data such as hyperlinks, authorship, citations or use short and distinctive phrases, etc. [3, 5]. However, in many other
types of textural corpora, they do not have such explicit evidences, through which we can trace the
propagation of information across them. For example, based on academic data such as publications and grant proposals with information of only abstracts and publishing years, can we answer
questions such as: in a given ﬁeld of study, does research get published after grants are awarded
or do grants get awarded after research is published? Can we analyze quantitatively the lead/lag of
research grants with respect to academic publications on various topics without information such as
authorship or citations?
These questions are of general interest to social scientists since they could be posed in a variety of
domains. For example, one might want to know which community among bloggers, professional
news, or social networking sites, are the ﬁrst to pick up the news on a speciﬁc event [3]. Similarly, in
the scientiﬁc communities, it is important to know which research university or lab leads the others
on speciﬁc ﬁelds of study.
In this paper, we study the lead/lag of topics in computer science between two corpora of NSF
grants and ISI publications from 1991 to 2008. The main contribution of this work is that we
propose an approach based on LDA and time series analysis to compute the topic-speciﬁc lead/lag
across corpora based on purely textual and time-stamp information. An additional complexity in
our dataset is that each document can discuss multiple topics, and therefore one needs to decompose
each document into its topics before we analyze them. The techniques we propose are simple and
intuitive and we hope they generalize well to other corpora with similar attributes.
1

0.012

Fraction of Load

0.011

0.010
0.009

0.008
0.007

2.5

0.006
1990 1992 1994 1996 1998 2000 2002 2004 2006 2008

2.0
1.5

user interact interfac virtual graphic
peopl augment usabl assist realiti
ISI
NSF

2.5

Normalized Fraction of Load

0.013

Human-computer interaction

Normalized Fraction of Load

user interact interfac virtual graphic
peopl augment usabl assist realiti
ISI Load
NSF Load

1.0
0.5
0.0
0.5
1.0
1.5

2.0
1990 1992 1994 1996 1998 2000 2002 2004 2006 2008

(a) Raw loading

(b) Normalized loading

2.0
1.5

user interact interfac virtual graphic
peopl augment usabl assist realiti
ISI (Lag -3 Years)
NSF

1.0
0.5
0.0
0.5
1.0
1.5
2.0
1990

1995

2000

2005

2010

(c) Normalized loading after alignment

Figure 1: Estimation of lead/lag by aligning topic loadings.

2

Methodology

Since our corpus is unlabeled, we ﬁrst need a model that learns the topics discussed in a corpus. For
this purpose, we use LDA [2], a popular unsupervised model that learns from a corpus of documents,
a prespeciﬁed number of topics, K, where each topic is represented as βk = (βk,1 , · · · , βk,V ), a
multinomial distribution over the vocabulary of size V . In addition, for each document d, LDA also
learns its soft labels in terms of these topics, represented as θd = (θd,1 , · · · , θd,K ), a multinomial
distribution over topics.
We ﬁrst train the LDA model on the union of both corpora. Then, for each corpus c, we compute its
topic-loading with respect to year y on topic k, as:
(k)
lc (y) =

and d∈c) θd,k
d:t(d)=y and d∈c) 1

d:t(d)=y

(1)
(k)

where t(d) is the timestamp of the document d. In other words, lc (y) represents the expected
proportion of documents that discuss topic k in corpus c, in year y. Intuitively, this quantity is the
popularity of the topic in the corpus in a given year.
We hypothesize that a corpus c1 lags corpus c2 on topic k by τ years, if the patterns of rise and fall
of “popularity” of the topic in corpus c2 as we vary year y is “similar” to that in corpus c1 y +τ years
hence. Accordingly, we shift the time series of topic loadings of one corpus to the past and future
until we ﬁnd the best alignment, with the topic loadings of the other corpus. The optimal alignment
value will give us the estimated lag of corpus c1 .
(k)

(k)

Mathematically, we compare the time-series data of year-wise topical loadings lc1 (y) and lc2 (y)
from the two corpora c1 and c2 using cross-correlation as follows:
ˆ(k) (y + τ )ˆ(k) (y)
lc1
lc2

Corr(k) 2 (τ ) =
c1 ,c

(2)

y
(k)

(k)

where ˆc2 (y) is the normalized lc2 (y) obtained after substracting its mean over all years and dividl
ing by its standard deviation, and τ is the lag of corpus c1 with respect to corpus c2 . The normalization is done to ﬁlter corpus speciﬁc characteristics1 and make the two plots more comparable with
each other.
Thus, Corr(k) 2 (τ ) captures the normalized cross-correlation between the two time series data
c1 ,c
(k)

(k)

lc1 (y) and lc2 (y) as a function of lag τ of the corpus c1 . We now compute the actual lag τ ∗ (k) of
corpus c1 on topic k as the value of lag that maximizes the normalized cross-correlation, as follows:
τ ∗ (k) = arg max Corr(k) 2 (τ )
c1 ,c
∗

τ

(3)

Thus, τ (k) is the time lag of corpus c1 at which the two time series best align with each other.
An example of the normalization of the topic loadings time series and their aligment process is
illustrated in Figure 1.
1

e.g.: a topic could have lower mean loading in one corpus compared to the other.

2

2015

Cross-correlation is only one of several metrics available to align time series data. We also experimented with the L2 norm between the two normalized time series as an alternative, but we found
that it had very high correlation with cross-correlation. Hence we do not report it in this paper.

3

Datasets

We focused our analysis in the area of Computer Science. From the ISI Dataset2 consisting of most
academic journal publications since 1960’s, we extracted abstracts from Computer Science publications based on the “Field” labels, which resulted in about 471,000 documents. A vast majority
of the these documents are uniformly distributed in the timespan between 1991 and 2008. We also
have successful grant proposals data from NSF3 whose awards are mostly from year 1990 to 2009.
We extracted all Computer Science abstracts from this dataset using the NSF program names, which
resulted in about 17,000 abstracts.
The ISI dataset is much larger than the NSF dataset, but we decided not to subsample the dataset
because subsampling could introduce artifacts that might distort the time series plots. For the purposes of our experiments, we assume that the two datasets represent the true state of the research
and grants worlds in Computer Science.

4

Experiments and Results

We performed all our experiments using a parallelized implementation of David Blei’s LDA code4 ,
as described in [4]. We ran LDA models for various number of topics on an 8-core Intel Xeon
2.4GHz machine with Linux kernel and 72G RAM. For the union of ISI and NSF datasets consisting
of about half a million documents, a 150-topic model ﬁnished in under 24 hours, wall clock time.
Figure 2 shows the aligned time series plots for four representative Computer Science topics discovered by the LDA model. The plots reveal some interesting trends. In both ISI and NSF datasets,
topics such as “Security and cryptography” and “Mobile networks” show increase in popularity in
recent times, while topics such as “Data structures” and “Neural networks” show decrease, which
agree with our general understanding of the Computer Science ﬁeld. We speculate that the more
recent smaller spurt (the second peak in Figure 2(d)) in popularity of “Neural networks” may have
to do with their reincarnation as “Deep belief networks”.
Although not displayed in this paper, we found some interesting topics such as “Kernel machines”5
where ISI and NSF exhibit opposite trends of popularity. In addition, there are some topics that
contain loadings in one of the corpora but not the other6 .
On the topic of “Security and Cryptography”, ISI leads by 2 years, which effectively means that on
an average, research papers are published 2 years earlier than the award of grants on this topic. On
the topics of “Mobile computing” and “Data structures”, award of grants and research publications
happen almost simultaneously. On the topic of “Neural networks”, on an average, NSF awards
grants 3 years before papers are published, which perhaps means that NSF has had high conﬁdence
on this ﬁeld. Among the 150 topics we get from LDA, there are 49 topics in which ISI leads NSF,
67 topics in which ISI lags NSF, and 34 topics having about the same pace in ISI and NSF.

5

Conclusions and Future Work

In this paper, we present an LDA based approach for analyzing the topical lead-lag relationships
across corpora with only textural and temporal information. There are other LDA extensions for
temporal data such as [1, 6], but we decided to use basic LDA mainly because parallelization of
LDA is well understood [4], and is therefore scalable, while it is less clear for the other models.
Apart from scalability, the methodologies we adopted are standard and intuitive. Moreover, our
approach is very robust: we ﬁnd that even when we vary the number of topics in the LDA model,
the lead/lag values for the same topic (which we matched by inspection) tally across the models.
2

http://www.isiknowledge.com
http://www.nsf.gov
4
http://www.cs.princeton.edu/ blei/lda-c/
5
In this topic, we found that there is increasing loading with time in publications, but the opposite in grants!
6
Topics that discuss past experience and post doctoral fellowships, etc. are speciﬁc to NSF.
3

3

(a) Security & cryptography
2.0

2.0

Normalized Fraction of Load

Normalized Fraction of Load

2.5

(b) Mobile networks

group secur kei attack signatur
authent ident member encrypt privaci
ISI (Lag -2 Years)
NSF

1.5
1.0
0.5
0.0
0.5
1.0
1.5
1990

1995

2000

2005

1.5
1.0
0.5
0.0
0.5
1.0

1.5
1990 1992 1994 1996 1998 2000 2002 2004 2006 2008

2010

(c) Data structures

1.5
1.0

(d) Neural networks

construct tree length minimum binari
string root sort span insert
ISI (Lag 0 Years)
NSF

2.5

Normalized Fraction of Load

Normalized Fraction of Load

2.0

network node mobil rout wireless
multicast broadcast hoc host station
ISI (Lag 0 Years)
NSF

0.5
0.0
0.5
1.0
1.5
2.0
2.5
1990 1992 1994 1996 1998 2000 2002 2004 2006 2008

2.0
1.5

network neural train weight layer
artifici recurr hidden ann multilay
ISI (Lag 3 Years)
NSF

1.0
0.5
0.0
0.5
1.0
1.5
2.0
1985

1990

1995

2000

2005

2010

Figure 2: Aligned time series plots and lead/lag estimates for four different CS topics. The words listed on the
top of the curve are the top rankings words in the respective topics as found by LDA. The titles for the topics
are manually assigned. The values next to the ISI legend in all the plots indicates the number of years ISI lags
NSF by. Negative values indicate lead of ISI.

As part of our future work, in order to get higher accuracy in estimating the lead-lag relationships,
we plan to link the grants and publications data using authorship information. We would do a
lead/lag analysis based on word usage patterns within a given topic to justify the results based
LDA. In addition, we also plan to run LDA on representative subsamples of the data and check the
robustness of results and resolve errors introduced by the coverage of data. Finally, we will try to
understand the causality of the lead/lag relationships between NSF grants and ISI publications.

References
[1] D. Blei and J. Lafferty. Dynamic topic models. In Proceedings of the 23rd International Conference on Machine Learning, 2006.
[2] David Blei and Andrew Ng and Michael Jordan. Latent Dirichlet Allocation. Journal of Machine
Learning Research, 2003.
[3] Jure Leskovec, Lars Backstrom, and Jon Kleinberg. Meme-tracking and the dynamics of the
news cycle. In KDD ’09: Proceedings of the 15th ACM SIGKDD international conference on
Knowledge discovery and data mining, pages 497–506, New York, NY, USA, 2009. ACM.
[4] Ramesh Nallapati, William Cohen, and John Lafferty. Parallelized variational EM for latent
dirichlet allocation: An experimental evaluation of speed and scalability. In ICDM workshop on
high performance data mining, 2007.
[5] Jie Tang and Jing Zhang. Modeling the evolution of associated data. Data & Knowledge Engineering, 69(9):965 – 978, 2010.
[6] Xuerui Wang and Andrew McCallum. Topics over time: A non-markov continuous-time model
of topical trends. In Conference on Knowledge Discovery and Data Mining, 2006.

4

