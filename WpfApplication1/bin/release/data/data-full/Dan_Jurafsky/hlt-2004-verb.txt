Shallow Semantic Parsing using Support Vector Machines ∗
Sameer Pradhan, Wayne Ward,
Kadri Hacioglu, James H. Martin
Center for Spoken Language Research,
University of Colorado, Boulder, CO 80303

Dan Jurafsky
Department of Linguistics
Stanford University
Stanford, CA 94305

{spradhan,whw,hacioglu,martin}@cslr.colorado.edu

jurafsky@stanford.edu

Abstract
In this paper, we propose a machine learning algorithm for shallow semantic parsing, extending the work of Gildea and Jurafsky (2002),
Surdeanu et al. (2003) and others. Our algorithm is based on Support Vector Machines
which we show give an improvement in performance over earlier classiﬁers. We show performance improvements through a number of new
features and measure their ability to generalize to a new test set drawn from the AQUAINT
corpus.

1

Introduction

Automatic, accurate and wide-coverage techniques that
can annotate naturally occurring text with semantic argument structure can play a key role in NLP applications
such as Information Extraction, Question Answering and
Summarization. Shallow semantic parsing – the process
of assigning a simple W HO did W HAT to W HOM, W HEN,
W HERE, W HY, H OW, etc. structure to sentences in text,
is the process of producing such a markup. When presented with a sentence, a parser should, for each predicate
in the sentence, identify and label the predicate’s semantic arguments. This process entails identifying groups of
words in a sentence that represent these semantic arguments and assigning speciﬁc labels to them.
In recent work, a number of researchers have cast this
problem as a tagging problem and have applied various supervised machine learning techniques to it (Gildea
and Jurafsky (2000, 2002); Blaheta and Charniak (2000);
Gildea and Palmer (2002); Surdeanu et al. (2003); Gildea
and Hockenmaier (2003); Chen and Rambow (2003);
Fleischman and Hovy (2003); Hacioglu and Ward (2003);
Thompson et al. (2003); Pradhan et al. (2003)). In this
∗
This research was partially supported by the ARDA
AQUAINT program via contract OCG4423B and by the NSF
via grant IS-9978025

paper, we report on a series of experiments exploring this
approach.
For the initial experiments, we adopted the approach
described by Gildea and Jurafsky (2002) (G&J) and evaluated a series of modiﬁcations to improve its performance. In the experiments reported here, we ﬁrst replaced their statistical classiﬁcation algorithm with one
that uses Support Vector Machines and then added to the
existing feature set. We evaluate results using both handcorrected TreeBank syntactic parses, and actual parses
from the Charniak parser.

2

Semantic Annotation and Corpora

We will be reporting on results using PropBank1 (Kingsbury et al., 2002), a 300k-word corpus in which predicate argument relations are marked for part of the verbs
in the Wall Street Journal (WSJ) part of the Penn TreeBank (Marcus et al., 1994). The arguments of a verb are
labeled A RG 0 to A RG 5, where A RG 0 is the P ROTO AGENT (usually the subject of a transitive verb) A RG 1
is the P ROTO -PATIENT (usually its direct object), etc.
PropBank attempts to treat semantically related verbs
consistently. In addition to these C ORE A RGUMENTS,
additional A DJUNCTIVE A RGUMENTS, referred to as
A RG Ms are also marked. Some examples are A RG ML OC, for locatives, and A RG M-T MP, for temporals. Figure 1 shows the syntax tree representation along with the
argument labels for an example structure extracted from
the PropBank corpus.
Most of the experiments in this paper, unless speciﬁed otherwise, are performed on the July 2002 release
of PropBank. A larger, cleaner, completely adjudicated
version of PropBank was made available in Feb 2004.
We will also report some ﬁnal best performance numbers
on this corpus. PropBank was constructed by assigning
semantic arguments to constituents of the hand-corrected
TreeBank parses. The data comprise several sections of
the WSJ, and we follow the standard convention of using
1

http://www.cis.upenn.edu/˜ace/

• Predicate – The predicate itself is used as a feature.
• Path – The syntactic path through the parse tree
from the parse constituent to the predicate being
classiﬁed. For example, in Figure 1, the path from
A RG 0 – “He” to the predicate talked, is represented
with the string NP↑S↓VP↓VBD. ↑ and ↓ represent
upward and downward movement in the tree respectively.
• Phrase Type – This is the syntactic category (NP,
PP, S, etc.) of the phrase/constituent corresponding
to the semantic argument.
• Position – This is a binary feature identifying
whether the phrase is before or after the predicate.
• Voice – Whether the predicate is realized as an active or passive construction.
• Head Word – The syntactic head of the phrase. This
is calculated using a head word table described by
(Magerman, 1994) and modiﬁed by (Collins, 1999,
Appendix. A).
• Sub-categorization – This is the phrase structure rule expanding the predicate’s parent node
in the parse tree. For example, in Figure 1, the
sub-categorization for the predicate talked is
VP→VBD-PP.

Section-23 data as the test set. Section-02 to Section21 were used for training. In the July 2002 release, the
training set comprises about 51,000 sentences, instantiating about 132,000 arguments, and the test set comprises
2,700 sentences instantiating about 7,000 arguments. The
Feb 2004 release training set comprises about 85,000 sentences instantiating about 250,000 arguments and the test
set comprises 5,000 sentences instantiating about 12,000
arguments.
[ARG0 He] [predicate talked] for [ARGM−TMP about
20 minutes].
S
(hhhh
(
h
( ((
VP
NP
((((hhhh
PRP
VBD
PP
(
h
( ((hhh
He
talked
IN
NP
ARG0 predicate
(((hhhhh
f or ((
about 20 minutes
NULL
ARGM − TMP
Figure 1: Syntax tree for a sentence illustrating the PropBank tags.

3

Problem Description

The problem of shallow semantic parsing can be viewed
as three different tasks.
Argument Identiﬁcation – This is the process of identifying parsed constituents in the sentence that represent
semantic arguments of a given predicate.
Argument Classiﬁcation – Given constituents known to
represent arguments of a predicate, assign the appropriate argument labels to them.
Argument Identiﬁcation and Classiﬁcation – A combination of the above two tasks.
Each node in the parse tree can be classiﬁed as either
one that represents a semantic argument (i.e., a NON N ULL node) or one that does not represent any semantic argument (i.e., a N ULL node). The NON -N ULL nodes
can then be further classiﬁed into the set of argument labels. For example, in the tree of Figure 1, the node IN
that encompasses “for” is a N ULL node because it does
not correspond to a semantic argument. The node NP
that encompasses “about 20 minutes” is a NON -N ULL
node, since it does correspond to a semantic argument
– A RG M-T MP.

4

Baseline Features

Our baseline system uses the same set of features introduced by G&J. Some of the features, viz., predicate,
voice and verb sub-categorization are shared by all the
nodes in the tree. All the others change with the constituent under consideration.

5

Classiﬁer and Implementation

We formulate the parsing problem as a multi-class classiﬁcation problem and use a Support Vector Machine
(SVM) classiﬁer. Since SVMs are binary classiﬁers, we
have to convert the multi-class problem into a number of
binary-class problems. We use the O NE vs A LL (OVA)
formalism, which involves training n binary classiﬁers
for a n-class problem.
Since the training time taken by SVMs scales exponentially with the number of examples, and about 80% of the
nodes in a syntactic tree have N ULL argument labels, we
found it efﬁcient to divide the training process into two
stages, while maintaining the same accuracy:
1. Filter out the nodes that have a very high probability of being N ULL. A binary N ULL vs NON -N ULL
classiﬁer is trained on the entire dataset. A sigmoid
function is ﬁtted to the raw scores to convert the
scores to probabilities as described by (Platt, 2000).
2. The remaining training data is used to train OVA
classiﬁers, one of which is the N ULL-NON -N ULL
classiﬁer.
With this strategy only one classiﬁer (N ULL vs NON N ULL) has to be trained on all of the data. The remaining
OVA classiﬁers are trained on the nodes passed by the
ﬁlter (approximately 20% of the total), resulting in a considerable savings in training time.

In the testing stage, we do not perform any ﬁltering
of N ULL nodes. All the nodes are classiﬁed directly
as N ULL or one of the arguments using the classiﬁer
trained in step 2 above. We observe no signiﬁcant performance improvement even if we ﬁlter the most likely
N ULL nodes in a ﬁrst pass.
For our experiments, we used TinySVM2 along with
YamCha3 (Kudo and Matsumoto, 2000)
(Kudo and Matsumoto, 2001) as the SVM training and
test software. The system uses a polynomial kernel with
degree 2; the cost per unit violation of the margin, C=1;
and, tolerance of the termination criterion, e=0.001.

signiﬁcant improvements are marked with an ∗ . In this
system, the overlap-removal decisions are taken independently of each other.

Baseline
No Overlaps

Baseline System Performance

Table 1 shows the baseline performance numbers on the
three tasks mentioned earlier; these results are based on
syntactic features computed from hand-corrected TreeBank hence (LDC hand-corrected) parses.
For the argument identiﬁcation and the combined identiﬁcation and classiﬁcation tasks, we report the precision
(P), recall (R) and the F1 4 scores, and for the argument
classiﬁcation task we report the classiﬁcation accuracy
(A). This test set and all test sets, unless noted otherwise
are Section-23 of PropBank.
Classes

Task

A LL
A RGs

Id.
Classiﬁcation
Id. + Classiﬁcation
Id.
Classiﬁcation
Id. + Classiﬁcation

C ORE
A RGs

P
(%)
90.9
83.3
94.7
88.4

R
(%)
89.8
78.5
90.1
84.1

F1
90.4
80.8
92.3
86.2

A
(%)
87.9

91.4

Table 1: Baseline performance on all three tasks using
hand-corrected parses.

7

System Improvements

7.1

Disallowing Overlaps

The system as described above might label two constituents NON -N ULL even if they overlap in words. This
is a problem since overlapping arguments are not allowed
in PropBank. Among the overlapping constituents we retain the one for which the SVM has the highest conﬁdence, and label the others N ULL. The probabilities obtained by applying the sigmoid function to the raw SVM
scores are used as the measure of conﬁdence. Table 2
shows the performance of the parser on the task of identifying and labeling semantic arguments using the handcorrected parses. On all the system improvements, we
perform a χ2 test of signiﬁcance at p = 0.05, and all the
2
3
4

http://cl.aist-nara.ac.jp/˜talus-Au/software/TinySVM/
http://cl.aist-nara.ac.jp/˜taku-Au/software/yamcha/

F1 =

2P R
P +R

R
(%)
78.5
78.1

F1

∗

80.8
81.6

Table 2: Improvements on the task of argument identiﬁcation and classiﬁcation after disallowing overlapping
constituents.
7.2

6

P
(%)
83.3
85.4

New Features

We tested several new features. Two were obtained from
the literature – named entities in constituents and head
word part of speech. Other are novel features.
1. Named Entities in Constituents – Following
Surdeanu et al. (2003), we tagged 7 named entities (P ERSON , O RGANIZATION , L OCATION ,
P ERCENT, M ONEY, T IME , DATE) using IdentiFinder (Bikel et al., 1999) and added them as 7
binary features.
2. Head Word POS – Surdeanu et al. (2003) showed
that using the part of speech (POS) of the head word
gave a signiﬁcant performance boost to their system.
Following that, we experimented with the addition
of this feature to our system.
3. Verb Clustering – Since our training data is relatively limited, any real world test set will contain predicates that have not been seen in training.
In these cases, we can beneﬁt from some information about the predicate by using predicate cluster as a feature. The verbs were clustered into 64
classes using the probabilistic co-occurrence model
of Hofmann and Puzicha (1998). The clustering algorithm uses a database of verb-direct-object relations extracted by Lin (1998). We then use the verb
class of the current predicate as a feature.
4. Partial Path – For the argument identiﬁcation task,
path is the most salient feature. However, it is also
the most data sparse feature. To overcome this problem, we tried generalizing the path by adding a new
feature that contains only the part of the path from
the constituent to the lowest common ancestor of the
predicate and the constituent, which we call “PartialPath”.
5. Verb Sense Information – The arguments that a
predicate can take depend on the word sense of the
predicate. Each predicate tagged in the PropBank
corpus is assigned a separate set of arguments depending on the sense in which it is used. Table 3

illustrates the argument sets for the predicate talk.
Depending on the sense of the predicate talk, either
A RG 1 or A RG 2 can identify the hearer. Absence of
this information can be potentially confusing to the
learning mechanism.
Talk

sense 1: speak
Tag
Description
A RG 0
Talker
A RG 1
Subject
A RG 2
Hearer

sense 2: persuade/dissuade
Tag
Description
A RG 0
Talker
A RG 1
Talked to
A RG 2
Secondary action

Table 3: Argument labels associated with the two senses
of predicate talk in PropBank corpus.
We added the oracle sense information extracted
from PropBank, to our features by treating each
sense of a predicate as a distinct predicate.
6. Head Word of Prepositional Phrases – Many adjunctive arguments, such as temporals and locatives,
occur as prepositional phrases in a sentence, and
it is often the case that the head words of those
phrases, which are always prepositions, are not very
discriminative, eg., “in the city”, “in a few minutes”,
both share the same head word “in” and neither
contain a named entity, but the former is A RG ML OC, whereas the latter is A RG M-T MP. Therefore,
we tried replacing the head word of a prepositional
phrase, with that of the ﬁrst noun phrase inside the
prepositional phrase. We retained the preposition information by appending it to the phrase type, eg.,
“PP-in” instead of “PP”.
7. First and Last Word/POS in Constituent – Some
arguments tend to contain discriminative ﬁrst and
last words so we tried using them along with their
part of speech as four new features.
8. Ordinal constituent position – In order to avoid
false positives of the type where constituents far
away from the predicate are spuriously identiﬁed as
arguments, we added this feature which is a concatenation of the constituent type and its ordinal position
from the predicate.
9. Constituent tree distance – This is a ﬁner way of
specifying the already present position feature.
10. Constituent relative features – These are nine features representing the phrase type, head word and
head word part of speech of the parent, and left and
right siblings of the constituent in focus. These were
added on the intuition that encoding the tree context
this way might add robustness and improve generalization.

11. Temporal cue words – There are several temporal
cue words that are not captured by the named entity
tagger and were considered for addition as a binary
feature indicating their presence.
12. Dynamic class context – In the task of argument
classiﬁcation, these are dynamic features that represent the hypotheses of at most previous two nodes
belonging to the same tree as the node being classiﬁed.

8

Feature Performance

Table 4 shows the effect each feature has on the argument classiﬁcation and argument identiﬁcation tasks,
when added individually to the baseline. Addition of
named entities improves the F1 score for adjunctive arguments A RG M-L OC from 59% to ∗ 68% and A RG MT MP from 78.8% to ∗ 83.4%. But, since these arguments
are small in number compared to the core arguments, the
overall accuracy does not show a signiﬁcant improvement. We found that adding this feature to the N ULL vs
NON -N ULL classiﬁer degraded its performance. It also
shows the contribution of replacing the head word and the
head word POS separately in the feature where the head
of a prepositional phrase is replaced by the head word
of the noun phrase inside it. Apparently, a combination
of relative features seem to have a signiﬁcant improvement on either or both the classiﬁcation and identiﬁcation
tasks, and so do the ﬁrst and last words in the constituent.
Features

Class
Acc.

Baseline
+ Named entities
+ Head POS
+ Verb cluster
+ Partial path
+ Verb sense
+ Noun head PP (only POS)
+ Noun head PP (only head)
+ Noun head PP (both)
+ First word in constituent
+ Last word in constituent
+ First POS in constituent
+ Last POS in constituent
+ Ordinal const. pos. concat.
+ Const. tree distance
+ Parent constituent
+ Parent head
+ Parent head POS
+ Right sibling constituent
+ Right sibling head
+ Right sibling head POS
+ Left sibling constituent
+ Left sibling head
+ Left sibling head POS
+ Temporal cue words
+ Dynamic class context

87.9
88.1
∗
88.6
88.1
88.2
88.1
∗
88.6
∗
89.8
∗
89.9
∗
89.0
∗
89.4
88.4
88.3
87.7
88.0
87.9
85.8
∗
88.5
87.9
87.9
88.1
∗
88.6
86.9
∗
88.8
∗
88.6
88.4

A RGUMENT ID
P
93.7
94.4
94.1
93.3
93.7
94.4
94.0
94.7
94.4
93.8
94.4
93.6
93.7
93.7
94.2
94.2
94.3
94.0
94.4
94.1
93.6
93.9
93.5
-

R
88.9
90.1
89.0
88.9
89.5
90.0
89.4
90.5
91.1
89.4
90.6
89.1
89.2
89.5
90.2
90.5
90.3
89.9
89.9
89.9
89.6
86.1
89.3
-

F1
91.3
∗
92.2
91.5
91.1
91.5
∗
92.2
91.7
∗
92.6
∗
92.7
91.6
∗
92.5
91.3
91.4
91.5
∗
92.2
∗
92.3
∗
92.3
91.9
∗
92.1
92.0
91.6
89.9
91.4
-

Table 4: Effect of each feature on the argument identiﬁcation and classiﬁcation tasks when added to the baseline
system.

We tried two other ways of generalizing the head word:
i) adding the head word cluster as a feature, and ii) replacing the head word with a named entity if it belonged to
any of the seven named entities mentioned earlier. Neither method showed any improvement. We also tried generalizing the path feature by i) compressing sequences of
identical labels, and ii) removing the direction in the path,
but none showed any improvement on the baseline.
8.1

Argument Sequence Information

In order to improve the performance of their statistical argument tagger, G&J used the fact that a predicate is likely
to instantiate a certain set of arguments. We use a similar
strategy, with some additional constraints: i) argument
ordering information is retained, and ii) the predicate is
considered as an argument and is part of the sequence.
We achieve this by training a trigram language model on
the argument sequences, so unlike G&J, we can also estimate the probability of argument sets not seen in the
training data. We ﬁrst convert the raw SVM scores to
probabilities using a sigmoid function. Then, for each
sentence being parsed, we generate an argument lattice
using the n-best hypotheses for each node in the syntax tree. We then perform a Viterbi search through the
lattice using the probabilities assigned by the sigmoid
as the observation probabilities, along with the language
model probabilities, to ﬁnd the maximum likelihood path
through the lattice, such that each node is either assigned
a value belonging to the P ROP BANK A RGUMENTs, or
N ULL.
C ORE A RGs/
Hand-corrected parses
Baseline w/o overlaps
Common predicate
Speciﬁc predicate lemma

P
(%)
90.0
90.8
90.5

R
(%)
86.1
86.3
87.4

task of identifying and assigning semantic arguments,
given hand-corrected parses, whereas the accuracy of the
A DJUNCTIVE A RGUMENTS slightly deteriorated. This
seems to be logical considering the fact that the A DJUNC TIVE A RGUMENTS are not linguistically constrained in
any way as to their position in the sequence of arguments, or even the quantity. We therefore decided to
use this strategy only for the C ORE A RGUMENTS. Although, there was an increase in F1 score when the language model probabilities were jointly estimated over all
the predicates, this improvement is not statistically significant. However, estimating the same using speciﬁc predicate lemmas, showed a signiﬁcant improvement in accuracy. The performance improvement is shown in Table 5.

9

Best System Performance

The best system is trained by ﬁrst ﬁltering the most
likely nulls using the best N ULL vs NON -N ULL classiﬁer trained using all the features whose argument identiﬁcation F1 score is marked in bold in Table 4, and then
training a O NE vs A LL classiﬁer using the data remaining after performing the ﬁltering and using the features
that contribute positively to the classiﬁcation task – ones
whose accuracies are marked in bold in Table 4. Table 6
shows the performance of this system.
Classes

A LL
A RGs

88.0
88.5
88.9

Table 5: Improvements on the task of argument identiﬁcation and tagging after performing a search through the
argument lattice.
The search is constrained in such a way that no two
NON -N ULL nodes overlap with each other. To simplify
the search, we allowed only N ULL assignments to nodes
having a N ULL likelihood above a threshold. While training the language model, we can either use the actual predicate to estimate the transition probabilities in and out
of the predicate, or we can perform a joint estimation
over all the predicates. We implemented both cases considering two best hypotheses, which always includes a
N ULL (we add N ULL to the list if it is not among the
top two). On performing the search, we found that the
overall performance improvement was not much different than that obtained by resolving overlaps as mentioned
earlier. However, we found that there was an improvement in the C ORE A RGUMENT accuracy on the combined

Id.
Classiﬁcation
Id. + Classiﬁcation
Id.
Classiﬁcation
Id. + Classiﬁcation

C ORE
A RGs

F1

∗

Task

Hand-corrected parses
P
R
F1
A
(%)
(%)
(%)
95.2
92.5
93.8
91.0
88.9
84.6
86.7
96.2
93.0
94.6
93.9
90.5
87.4
88.9

Table 6: Best system performance on all tasks using
hand-corrected parses.

10

Using Automatic Parses

Thus far, we have reported results using hand-corrected
parses. In real-word applications, the system will have
to extract features from an automatically generated
parse. To evaluate this scenario, we used the Charniak
parser (Chaniak, 2001) to generate parses for PropBank
training and test data. We lemmatized the predicate using
the XTAG morphology database5 (Daniel et al., 1992).
Table 7 shows the performance degradation when
automatically generated parses are used.

11

Using Latest PropBank Data

Owing to the Feb 2004 release of much more and completely adjudicated PropBank data, we have a chance to
5

ftp://ftp.cis.upenn.edu/pub/xtag/morph-1.5/morph1.5.tar.gz

Classes

A LL
A RGs
C ORE
A RGs

Task

Id.
Classiﬁcation
Id. + Classiﬁcation
Id.
Classiﬁcation
Id. + Classiﬁcation

P
(%)
89.3
84.0
92.0
86.4

Automatic parses
R
F1
A
(%)
(%)
82.9
86.0
90.0
75.3
79.4
83.3
87.4
90.5
78.4
82.2

Table 7: Performance degradation when using automatic
parses instead of hand-corrected ones.
report our performance numbers on this data set. Table 8
shows the same information as in previous Tables 6 and
7, but generated using the new data. Owing to time limitations, we could not get the results on the argument identiﬁcation task and the combined argument identiﬁcation
and classiﬁcation task using automatic parses.
A LL A RGs

Task

H AND

Id.
Classiﬁcation
Id. + Classiﬁcation
Classiﬁcation

AUTOMATIC

P
(%)
96.2
89.9
-

R
(%)
95.8
89.0
-

F1
96.0
89.4
-

90.1

In analyzing the performance of the system, it is useful
to estimate the relative contribution of the various feature
sets used. Table 9 shows the argument classiﬁcation accuracies for combinations of features on the training and
test data, using hand-corrected parses, for all PropBank
arguments.

All
All except P ath
All except P hrase T ype
All except HW and HW -P OS
All except All P hrases
All except P redicate
All except HW and F W and LW -P OS
P ath, P redicate
P ath, P hrase T ype
Head W ord
P ath

All
All except HW
All except P redicate

P
(%)
95.2
95.1
94.5

R
(%)
92.5
92.3
91.9

F1
93.8
93.7
93.2

93.0

Feature Analysis

Features

Features

A
(%)

Table 8: Best system performance on all tasks using
hand-corrected parses using the latest PropBank data.

12

information hurts performance signiﬁcantly, so does the
removal of a family of features, eg., all phrase types, or
the head word (HW), ﬁrst word (FW) and last word (LW)
information. The lower part of the table shows the performance of some feature combinations by themselves.
Table 10 shows the feature salience on the task of argument identiﬁcation. One important observation we can
make here is that the path feature is the most salient feature in the task of argument identiﬁcation, whereas it is
the least salient in the task of argument classiﬁcation. We
could not provide the numbers for argument identiﬁcation performance upon removal of the path feature since
that made the SVM training prohibitively slow, indicating
that the SVM had a very hard time separating the N ULL
class from the NON -N ULL class.

Accuracy
(%)
91.0
90.8
90.8
90.7
∗
83.6
∗
82.4
∗
75.1
74.4
47.2
37.7
28.0

Table 9: Performance of various feature combinations on
the task of argument classiﬁcation.
In the upper part of Table 9 we see the degradation in
performance by leaving out one feature or a feature family at a time. After the addition of all the new features,
it is the case that removal of no individual feature except
predicate degrades the classiﬁcation performance signiﬁcantly, as there are some other features that provide complimentary information. However, removal of predicate

Table 10: Performance of various feature combinations
on the task of argument identiﬁcation

13

Comparing Performance with Other
Systems

We compare our system against 4 other shallow semantic
parsers in the literature. In comparing systems, results are
reported for all the three types of tasks mentioned earlier.
13.1

Description of the Systems

The Gildea and Palmer (G&P) System.
The Gildea and Palmer (2002) system uses the same
features and the same classiﬁcation mechanism used by
G&J. These results are reported on the December 2001
release of PropBank.
The Surdeanu et al. System.
Surdeanu et al. (2003) report results on two systems
using a decision tree classiﬁer. One that uses exactly the
same features as the G&J system. We call this “Surdeanu
System I.” They then show improved performance of another system – “Surdeanu System II,” which uses some
additional features. These results are are reported on the
July 2002 release of PropBank.
The Gildea and Hockenmaier (G&H) System
The Gildea and Hockenmaier (2003) system uses features extracted from Combinatory Categorial Grammar
(CCG) corresponding to the features that were used by
G&J and G&P systems. CCG is a form of dependency
grammar and is hoped to capture long distance relationships better than a phrase structure grammar. The features are combined using the same algorithm as in G&J

and G&P. They use a slightly newer – November 2002 release of PropBank. We will refer to this as “G&H System
I”.

granularity, and parse accuracy. It can be seen that the
SVM System performs signiﬁcantly better than all the
other systems on all PropBank arguments.

The Chen and Rambow (C&R) System
Chen and Rambow report on two different systems,
also using a decision tree classiﬁer. The ﬁrst “C&R System I” uses surface syntactic features much like the G&P
system. The second “C&R System II” uses additional
syntactic and semantic representations that are extracted
from a Tree Adjoining Grammar (TAG) – another grammar formalism that better captures the syntactic properties of natural languages.
Classiﬁer

Accuracy
(%)
88
79
77

SVM
Decision Tree (Surdeanu et al., 2003)
Gildea and Palmer (2002)

Table 11: Argument classiﬁcation using same features
but different classiﬁers.
13.2

Classes
A LL
A RGs

13.5

Table 12 compares the results of the task of identifying the parse constituents that represent semantic arguments. As expected, the performance degrades considerably when we extract features from an automatic parse as
opposed to a hand-corrected parse. This indicates that the
syntactic parser performance directly inﬂuences the argument boundary identiﬁcation performance. This could be
attributed to the fact that the two features, viz., Path and
Head Word that have been seen to be good discriminators
of the semantically salient nodes in the syntax tree, are
derived from the syntax tree.

A LL
A RGs

System
SVM
Surdeanu System II
Surdeanu System I

P
95
85

Hand
R
92
84

F1
94
89
85

P
89
-

Automatic
R
F1
83
86
-

Table 12: Argument identiﬁcation
13.4

Argument Classiﬁcation

Table 13 compares the argument classiﬁcation accuracies
of various systems, and at various levels of classiﬁcation

Automatic
Accuracy
90
74
90.5
-

Argument Identiﬁcation and Classiﬁcation

Table 14 shows the results for the task where the system
ﬁrst identiﬁes candidate argument boundaries and then
labels them with the most likely argument. This is the
hardest of the three tasks outlined earlier. SVM does a
very good job of generalizing in both stages of processing.
Classes

Argument Identiﬁcation (N ULL vs NON -N ULL)

Hand
Accuracy
91
77
84
79
93.9
93.5
92.4

Table 13: Argument classiﬁcation

Comparing Classiﬁers

Classes

SVM
G&P
Surdeanu System II
Surdeanu System I
SVM
C&R System II
C&R System I

C ORE
A RGs

Since two systems, in addition to ours, report results using the same set of features on the same data, we can
directly assess the inﬂuence of the classiﬁers. G&P system estimates the posterior probabilities using several different feature sets and interpolate the estimates, while
Surdeanu et al. (2003) use a decision tree classiﬁer. Table 11 shows a comparison between the three systems for
the task of argument classiﬁcation.
13.3

System

A LL
A RGs
C ORE
A RGs

System
SVM
G&H System I
G&P
SVM System
G&H System I
C&R System II

P
89
76
71
90
82
-

Hand
R
85
68
64
87
79
-

F1
87
72
67
89
80
-

P
84
71
58
86
76
65

Automatic
R
F1
75
79
63
67
50
54
78
82
73
75
75
70

Table 14: Identiﬁcation and classiﬁcation

14

Generalization to a New Text Source

Thus far, in all experiments our unseen test data was
selected from the same source as the training data.
In order to see how well the features generalize to
texts drawn from a similar source, we used the classiﬁer
trained on PropBank training data to test data drawn from
the AQUAINT corpus (LDC, 2002). We annotated 400
sentences from the AQUAINT corpus with PropBank
arguments. This is a collection of text from the New
York Times Inc., Associated Press Inc., and Xinhua
News Service (PropBank by comparison is drawn from
Wall Street Journal). The results are shown in Table 15.
Task
A LL
A RGs
C ORE
A RGs

Id.
Classiﬁcation
Id. + Classiﬁcation
Id.
Classiﬁcation
Id. + Classiﬁcation

P
(%)
75.8
65.2
88.4
75.2

R
(%)
71.4
61.5
74.4
63.3

F1
73.5
63.3
80.8
68.7

A
(%)
83.8
84.0
-

Table 15: Performance on the AQUAINT test set.
There is a signiﬁcant drop in the precision and recall
numbers for the AQUAINT test set (compared to the pre-

cision and recall numbers for the PropBank test set which
were 82% and 73% respectively). One possible reason
for the drop in performance is relative coverage of the
features on the two test sets. The head word, path and
predicate features all have a large number of possible values and could contribute to lower coverage when moving
from one domain to another. Also, being more speciﬁc
they might not transfer well across domains.
Features
P redicate, P ath
P redicate, Head W ord
Cluster, P ath
Cluster, Head W ord
P ath
Head W ord

Arguments
(%)
87.60
48.90
96.31
83.85
99.13
93.02

non-Arguments
(%)
2.91
26.55
4.99
60.14
15.15
90.59

Table 16: Feature Coverage on PropBank test set using
parser trained on PropBank training set.
Features
P redicate, P ath
P redicate, Head W ord
Cluster, P ath
Cluster, Head W ord
P ath
Head W ord

Arguments
(%)
62.11
30.26
87.19
65.82
96.50
84.65

non-Arguments
(%)
4.66
17.41
10.68
45.43
29.26
83.54

Table 17: Coverage of features on AQUAINT test set using parser trained on PropBank training set.
Table 16 shows the coverage for features on the handcorrected PropBank test set. The tables show feature
coverage for constituents that were Arguments and constituents that were N ULL. About 99% of the predicates in
the AQUAINT test set were seen in the PropBank training set. Table 17 shows coverage for the same features on
the AQUAINT test set. We believe that the drop in coverage of the more predictive feature combinations explains
part of the drop in performance.

15

Conclusions

We have described an algorithm which signiﬁcantly improves the state-of-the-art in shallow semantic parsing.
Like previous work, our parser is based on a supervised
machine learning approach. Key aspects of our results
include signiﬁcant improvement via an SVM classiﬁer,
improvement from new features and a series of analytic
experiments on the contributions of the features. Adding
features that are generalizations of the more speciﬁc features seemed to help. These features were named entities, head word part of speech and verb clusters. We also
analyzed the transferability of the features to a new text
source.

We would like to thank Ralph Weischedel and Scott Miller of
BBN Inc. for letting us use their named entity tagger – IdentiFinder; Martha Palmer for providing us with the PropBank
data, Valerie Krugler for tagging the AQUAINT test set with
PropBank arguments, and all the anonymous reviewers for their
helpful comments.

References
[Bikel et al.1999] Daniel M. Bikel, Richard Schwartz, and Ralph M. Weischedel.
1999. An algorithm that learns what’s in a name. Machine Learning, 34:211–
231.
[Blaheta and Charniak2000] Don Blaheta and Eugene Charniak. 2000. Assigning
function tags to parsed text. In NAACL, pages 234–240.
[Chaniak2001] Eugene Chaniak. 2001. Immediate-head parsing for language
models. In ACL-01.
[Chen and Rambow2003] John Chen and Owen Rambow. 2003. Use of deep
linguistics features for the recognition and labeling of semantic arguments.
EMNLP-03.
[Collins1999] Michael John Collins. 1999. Head-driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, University of Pennsylvania,
Philadelphia.
[Daniel et al.1992] K. Daniel, Y. Schabes, M. Zaidel, and D. Egedi. 1992. A freely
available wide coverage morphological analyzer for English. In COLING-92.
[Fleischman and Hovy2003] Michael Fleischman and Eduard Hovy. 2003. A
maximum entropy approach to framenet tagging. In HLT-03.
[Gildea and Hockenmaier2003] Dan Gildea and Julia Hockenmaier. 2003. Identifying semantic roles using combinatory categorial grammar. In EMNLP-03.
[Gildea and Jurafsky2000] Daniel Gildea and Daniel Jurafsky. 2000. Automatic
labeling of semantic roles. In ACL-00, pages 512–520.
[Gildea and Jurafsky2002] Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational Linguistics, 28(3):245–288.
[Gildea and Palmer2002] Daniel Gildea and Martha Palmer. 2002. The necessity
of syntactic parsing for predicate argument recognition. In ACL-02.
[Hacioglu and Ward2003] Kadri Hacioglu and Wayne Ward. 2003. Target word
detection and semantic role chunking using support vector machines. In HLT03.
[Hofmann and Puzicha1998] Thomas Hofmann and Jan Puzicha. 1998. Statistical
models for co-occurrence data. Memo, MIT AI Laboratory.
[Kingsbury et al.2002] Paul Kingsbury, Martha Palmer, and Mitch Marcus. 2002.
Adding semantic annotation to the Penn Treebank. In HLT-02.
[Kudo and Matsumoto2000] Taku Kudo and Yuji Matsumoto. 2000. Use of support vector learning for chunk identiﬁcation. In CoNLL-00.
[Kudo and Matsumoto2001] Taku Kudo and Yuji Matsumoto. 2001. Chunking
with support vector machines. In NAACL-01.
[LDC2002] LDC. 2002. The AQUAINT Corpus of English News Text, Catalog
no. LDC2002t31.
[Lin1998] Dekang Lin. 1998. Automatic retrieval and clustering of similar words.
In COLING-98.
[Magerman1994] David Magerman. 1994. Natural Language Parsing as Statistical Pattern Recognition. Ph.D. thesis, Stanford University, CA.
[Marcus et al.1994] Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz,
Robert MacIntyre, Ann Bies, Mark Ferguson, Karen Katz, and Britta Schasberger. 1994. The Penn TreeBank: Annotating predicate argument structure.
[Platt2000] John Platt. 2000. Probabilities for support vector machines. In
A. Smola, P. Bartlett, B. Scolkopf, and D. Schuurmans, editors, Advances in
Large Margin Classiﬁers. MIT press.
[Pradhan et al.2003] Sameer Pradhan, Kadri Hacioglu, Wayne Ward, James Martin, and Dan Jurafsky. 2003. Semantic role parsing: Adding semantic structure to unstructured text. In ICDM-03.
[Surdeanu et al.2003] Mihai Surdeanu, Sanda Harabagiu, John Williams, and Paul
Aarseth. 2003. Using predicate-argument structures for information extraction. In ACL-03.
[Thompson et al.2003] Cynthia A. Thompson, Roger Levy, and Christopher D.
Manning. 2003. A generative model for semantic role labeling. In ECML-03.

