D ROPOUT T RAINING AS A DAPTIVE R EGULARIZATION
S TEFAN WAGER , S IDA WANG , AND P ERCY L IANG
D ROPOUT T RAINING

S TANFORD U NIVERSITY

D ROPOUT FOR G ENERALIZED L INEAR M ODELS

T HE D ROPOUT R EGULARIZER

Dropout acts as a label-independent regularizer

Level surfaces of the regularizer are shown in
blue; likelihood surfaces are black. Dropout acts
as an L2 penalty applied after scaling X by the
root inverse diagonal Fisher information.

n

ˆ
βDROP = argmin

(i)

β; x , y

β

(i)

+ R(β; xi )

.

i=1

In a generalized linear model (GLM),

A second-order expansion of A gives us
p

1
2 2
βj x j .
R(β; x) ≈ A (β · x)
2
j=1

(β; x, y) = −y β · x + A (β · x) .
We can write x as ξ x, where ξ = 0 or 1/(1 − δ)
˜
and is a component-wise product. The dropout
loss becomes

For a probabilistic model of the form
ˆ
P y x =f β·x ,
dropping out a feature is equivalent to setting it
to 0. Writing for the loss (i.e., negative loglikelihood),

Eξ [ (β; ξ

x, y)]

= −Eξ [y β · (ξ

x)] + Eξ [A (β · (ξ

= −y β · x + Eξ [A (β · (ξ

ˆ
βDROP = argminβ

x))]

E

(i)

β; x , y
˜

(i)

,

1
log. reg.: R (β; X) =
2

R(β; x) = Eξ [A (β · (ξ

2
βj
j

x))] − A (β · x) .

L2 regularization

2
xij
i

2
βj x2 pi (1 − pi )
ˆ
ij ˆ

q

where R(·) is the dropout regularizer

0
with prob. δ
(i)
xj /(1 − δ) with prob. 1 − δ

=

1
lin. reg.: R (β; X) =
2
q

x))]

i=1

where

1
R (β; X) = β diag (X V X) β,
2
where V is diagonal with Vii = A (β · x).
q

= (β; x, y) + R(β; x),

n

(i)
xj
˜

This leads to a quadratic dropout penalty

i,j

ˆ
Here, pi = σ(β · xi ) is the ith prediction.
ˆ

R is always non-negative because A is convex.

Intuition: For logistic regression, dropout privileges rare features and conﬁdent predictions.

D ROPOUT AND A DA G RAD

S EMI - SUPERVISED D ROPOUT

S EMI - SUPERVISED R ESULTS : S ENTIMENT C LASSIFICATION

Stochastic gradient descent uses the update rule

If we have m unlabeled datapoints {x∗ }, we can
j
use them to learn a better adaptive regularizer


n
m
n
∗
∗ 

R (β) =
R (β; xi ) +
R β; xj
n + αm i=1
j=1

ˆ
(βt ).
x t , yt

This is equivalent to solving a linearized L2 penalized problem:
ˆ
βt+1 = argmin

For the examples below, we split the full dataset
into 3 folds of equal size: training, test, and unlabeled. K is the number of classes

ˆ + gt · (β − βt )
ˆ

xt , yt (βt )

β

1
ˆ
+
β − βt
2ηt

2
2

.

We could use a dropout-like penalty instead
ˆ
βt+1 = argmin

ˆ
ˆ
(βt ) + gt · (β − βt )+
x t , yt

β

1
ˆ
β − βt
2

t
2

diag

xi ,yi

ˆ
βi

ˆ
β − βt

Dataset
CoNLL
20news
RCV14
R21578
TDT2

K
5
20
4
65
30

L2
91.46
76.55
94.76
90.67
97.34

Drop
91.81
79.07
94.79
91.24
97.54

+Unlabeled
92.02
80.47
95.16
90.30
97.89

i=1

The result is closely related to diagonal AdaGrad
(Duchi et al., 2010).

This table is from our follow up paper with
Mengqiu Wang and Chris Manning (EMNLP,
2013), which also extends our results to linearchain conditional random ﬁelds.

dropout+unlabeled
dropout
L2

0.89

Methods
MNB-Uni
MNB-Bi
Vect.Sent
LogReg-Bi
NBSVM-Bi
Drop-Uni
Drop-Bi

0.88

0.87

0.86

0.85

.

IMDB sentiment classiﬁcation dataset (Maas et al,
2011). Highly polar reviews for both training and
test (25k each). 50k unlabeled reviews (not all polarized). We used logistic regression with dropout
on unigram/bigram features. Semi-supervised
dropout improves on state-of-the-art results.

0.9

accuracy

ˆ
ˆ
βt+1 = βt − ηt gt , where gt =

Dropout regularization

0

10000
20000
30000
size of unlabeled data

40000

Unigram features, with 10k labeled examples.

Labeled
83.62
86.63
88.33
90.13
91.22
87.78
91.31

+Unlabeled
84.13
86.98
88.89
–
–
89.52
91.98

References. Multinomial naive Bayes (MNB): Su
et al., 2011; Word vectors (Vect.Sent): Maas et al,
2011; Naive Bayes SVM (NBSVM): Wang & Manning, 2012.

