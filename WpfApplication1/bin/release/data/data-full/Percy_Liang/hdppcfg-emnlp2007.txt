The Inﬁnite PCFG using Hierarchical Dirichlet Processes
Percy Liang
Slav Petrov
Michael I. Jordan
Dan Klein
Computer Science Division, EECS Department
University of California at Berkeley
Berkeley, CA 94720
{pliang, petrov, jordan, klein}@cs.berkeley.edu

Abstract
We present a nonparametric Bayesian model
of tree structures based on the hierarchical
Dirichlet process (HDP). Our HDP-PCFG
model allows the complexity of the grammar
to grow as more training data is available.
In addition to presenting a fully Bayesian
model for the PCFG, we also develop an efﬁcient variational inference procedure. On
synthetic data, we recover the correct grammar without having to specify its complexity in advance. We also show that our techniques can be applied to full-scale parsing
applications by demonstrating its effectiveness in learning state-split grammars.

1

Introduction

Probabilistic context-free grammars (PCFGs) have
been a core modeling technique for many aspects of linguistic structure, particularly syntactic phrase structure in treebank parsing (Charniak,
1996; Collins, 1999). An important question when
learning PCFGs is how many grammar symbols
to allocate to the learning algorithm based on the
amount of available data.
The question of “how many clusters (symbols)?”
has been tackled in the Bayesian nonparametrics
literature via Dirichlet process (DP) mixture models (Antoniak, 1974). DP mixture models have since
been extended to hierarchical Dirichlet processes
(HDPs) and HDP-HMMs (Teh et al., 2006; Beal et
al., 2002) and applied to many different types of
clustering/induction problems in NLP (Johnson et
al., 2006; Goldwater et al., 2006).
In this paper, we present the hierarchical Dirichlet process PCFG (HDP-PCFG), a nonparametric

Bayesian model of syntactic tree structures based
on Dirichlet processes. Speciﬁcally, an HDP-PCFG
is deﬁned to have an inﬁnite number of symbols;
the Dirichlet process (DP) prior penalizes the use
of more symbols than are supported by the training
data. Note that “nonparametric” does not mean “no
parameters”; rather, it means that the effective number of parameters can grow adaptively as the amount
of data increases, which is a desirable property of a
learning algorithm.
As models increase in complexity, so does the uncertainty over parameter estimates. In this regime,
point estimates are unreliable since they do not take
into account the fact that there are different amounts
of uncertainty in the various components of the parameters. The HDP-PCFG is a Bayesian model
which naturally handles this uncertainty. We present
an efﬁcient variational inference algorithm for the
HDP-PCFG based on a structured mean-ﬁeld approximation of the true posterior over parameters.
The algorithm is similar in form to EM and thus inherits its simplicity, modularity, and efﬁciency. Unlike EM, however, the algorithm is able to take the
uncertainty of parameters into account and thus incorporate the DP prior.
Finally, we develop an extension of the HDPPCFG for grammar reﬁnement (HDP-PCFG-GR).
Since treebanks generally consist of coarselylabeled context-free tree structures, the maximumlikelihood treebank grammar is typically a poor
model as it makes overly strong independence assumptions. As a result, many generative approaches
to parsing construct reﬁnements of the treebank
grammar which are more suitable for the modeling task. Lexical methods split each pre-terminal
symbol into many subsymbols, one for each word,
and then focus on smoothing sparse lexical statis-

tics (Collins, 1999; Charniak, 2000). Unlexicalized
methods reﬁne the grammar in a more conservative
fashion, splitting each non-terminal or pre-terminal
symbol into a much smaller number of subsymbols
(Klein and Manning, 2003; Matsuzaki et al., 2005;
Petrov et al., 2006). We apply our HDP-PCFG-GR
model to automatically learn the number of subsymbols for each symbol.

2

Models based on Dirichlet processes

In document clustering, for example, each data
point xi is a document represented by its termfrequency vector. Each component (cluster) z
has multinomial parameters φz which speciﬁes a
distribution F (·; φz ) over words. It is customary to use a conjugate Dirichlet prior G0 =
Dirichlet(α , . . . , α ) over the multinomial parameters, which can be interpreted as adding α − 1 pseudocounts for each word.
2.2

At the heart of the HDP-PCFG is the Dirichlet process (DP) mixture model (Antoniak, 1974), which is
the nonparametric Bayesian counterpart to the classical ﬁnite mixture model. In order to build up an
understanding of the HDP-PCFG, we ﬁrst review
the Bayesian treatment of the ﬁnite mixture model
(Section 2.1). We then consider the DP mixture
model (Section 2.2) and use it as a building block
for developing nonparametric structured versions of
the HMM (Section 2.3) and PCFG (Section 2.4).
Our presentation highlights the similarities between
these models so that each step along this progression
reﬂects only the key differences.
2.1

Bayesian ﬁnite mixture model

We begin by describing the Bayesian ﬁnite mixture
model to establish basic notation that will carry over
the more complex models we consider later.
Bayesian ﬁnite mixture model
β ∼ Dirichlet(α, . . . , α) [draw component probabilities]
For each component z ∈ {1, . . . , K}:
−φz ∼ G0
[draw component parameters]
For each data point i ∈ {1, . . . , n}:
−zi ∼ Multinomial(β)
−xi ∼ F (·; φzi )

[choose component]
[generate data point]

The model has K components whose prior distribution is speciﬁed by β = (β1 , . . . , βK ). The
Dirichlet hyperparameter α controls how uniform
this distribution is: as α increases, it becomes increasingly likely that the components have equal
probability. For each mixture component z ∈
{1, . . . , K}, the parameters of the component φz are
drawn from some prior G0 . Given the model parameters (β, φ), the data points are generated i.i.d. by
ﬁrst choosing a component and then generating from
a data model F parameterized by that component.

DP mixture model

We now consider the extension of the Bayesian ﬁnite
mixture model to a nonparametric Bayesian mixture
model based on the Dirichlet process. We focus
on the stick-breaking representation (Sethuraman,
1994) of the Dirichlet process instead of the stochastic process deﬁnition (Ferguson, 1973) or the Chinese restaurant process (Pitman, 2002). The stickbreaking representation captures the DP prior most
explicitly and allows us to extend the ﬁnite mixture
model with minimal changes. Later, it will enable us
to readily deﬁne structured models in a form similar
to their classical versions. Furthermore, an efﬁcient
variational inference algorithm can be developed in
this representation (Section 2.6).
The key difference between the Bayesian ﬁnite
mixture model and the DP mixture model is that
the latter has a countably inﬁnite number of mixture
components while the former has a predeﬁned K.
Note that if we have an inﬁnite number of mixture
components, it no longer makes sense to consider
a symmetric prior over the component probabilities;
the prior over component probabilities must decay in
some way. The stick-breaking distribution achieves
this as follows. We write β ∼ GEM(α) to mean
that β = (β1 , β2 , . . . ) is distributed according to the
stick-breaking distribution. Here, the concentration
parameter α controls the number of effective components. To draw β ∼ GEM(α), we ﬁrst generate
a countably inﬁnite collection of stick-breaking proportions u1 , u2 , . . . , where each uz ∼ Beta(1, α).
The stick-breaking weights β are then deﬁned in
terms of the stick proportions:
(1 − uz ).

βz = uz

(1)

z <z

The procedure for generating β can be viewed as
iteratively breaking off remaining portions of a unit-

0

β1

β2

β3 ...

1

a new set of stick-breaking weights β are generated
according based on β:
β ∼ DP(α , β),

(2)

Figure 1: A sample β ∼ GEM(1).
length stick (Figure 1). The component probabilities
{βz } will decay exponentially in expectation, but
there is always some probability of getting a smaller
component before a larger one. The parameter α determines the decay of these probabilities: a larger α
implies a slower decay and thus more components.
Given the component probabilities, the rest of the
DP mixture model is identical to the ﬁnite mixture
model:

where the distribution of DP can be characterized
in terms of the following ﬁnite partition property:
for all partitions of the positive integers into sets
A1 , . . . , Am ,
(β (A1 ), . . . , β (Am ))

(3)

∼ Dirichlet α β(A1 ), . . . , α β(Am ) ,
where β(A) = k∈A βk .1 The resulting β is another distribution over the positive integers whose
similarity to β is controlled by a concentration parameter α .

DP mixture model
β ∼ GEM(α)
[draw component probabilities]
For each component z ∈ {1, 2, . . . }:
−φz ∼ G0
[draw component parameters]
For each data point i ∈ {1, . . . , n}:
−zi ∼ Multinomial(β)
−xi ∼ F (·; φzi )

2.3

[choose component]
[generate data point]

HDP-HMM

The next stop on the way to the HDP-PCFG is the
HDP hidden Markov model (HDP-HMM) (Beal et
al., 2002; Teh et al., 2006). An HMM consists of a
set of hidden states, where each state can be thought
of as a mixture component. The parameters of the
mixture component are the emission and transition
parameters. The main aspect that distinguishes it
from a ﬂat ﬁnite mixture model is that the transition parameters themselves must specify a distribution over next states. Hence, we have not just one
top-level mixture model over states, but also a collection of mixture models, one for each state.
In developing a nonparametric version of the
HMM in which the number of states is inﬁnite, we
need to ensure that the transition mixture models
of each state share a common inventory of possible
next states. We can achieve this by tying these mixture models together using the hierarchical Dirichlet
process (HDP) (Teh et al., 2006). The stick-breaking
representation of an HDP is deﬁned as follows: ﬁrst,
the top-level stick-breaking weights β are drawn according to the stick-breaking prior as before. Then,

HDP-HMM
β ∼ GEM(α)
[draw top-level state weights]
For each state z ∈ {1, 2, . . . }:
−φE ∼ Dirichlet(γ)
[draw emission parameters]
z
−φT ∼ DP(α , β)
[draw transition parameters]
z
For each time step i ∈ {1, . . . , n}:
−xi ∼ F (·; φE )
[emit current observation]
zi
−zi+1 ∼ Multinomial(φTi )
[choose next state]
z

Each state z is associated with emission parameters φE . In addition, each z is also associated
z
with transition parameters φT , which specify a disz
tribution over next states. These transition parameters are drawn from a DP centered on the top-level
stick-breaking weights β according to Equations (2)
and (3). Assume that z1 is always ﬁxed to a special
S TART state, so we do not need to generate it.
2.4

HDP-PCFG

We now present the HDP-PCFG, which is the focus
of this paper. For simplicity, we consider Chomsky
normal form (CNF) grammars, which has two types
of rules: emissions and binary productions. We consider each grammar symbol as a mixture component
whose parameters are the rule probabilities for that
symbol. In general, we do not know the appropriate
number of grammar symbols, so our strategy is to
let the number of grammar symbols be inﬁnite and
place a DP prior over grammar symbols.
1
Note that this property is a speciﬁc instance of the general
stochastic process deﬁnition of Dirichlet processes.

HDP-PCFG
β ∼ GEM(α)
[draw top-level symbol weights]
For each grammar symbol z ∈ {1, 2, . . . }:
−φT ∼ Dirichlet(αT )
[draw rule type parameters]
z
−φE ∼ Dirichlet(αE )
[draw emission parameters]
z
−φB ∼ DP(αB , ββ T )
[draw binary production parameters]
z
For each node i in the parse tree:
−ti ∼ Multinomial(φTi )
z
−If ti = E MISSION:
−−xi ∼ Multinomial(φE )
zi
−If ti = B INARY-P RODUCTION:
−−(zL(i) , zR(i) ) ∼ Multinomial(φB )
zi

z1
φB
z
φT
z

β

z2

z3

x2

x3

[choose rule type]

φE
z

[emit terminal symbol]

∞

z

[generate children symbols]

Figure 2: The deﬁnition and graphical model of the HDP-PCFG. Since parse trees have unknown structure,
there is no convenient way of representing them in the visual language of traditional graphical models.
Instead, we show a simple ﬁxed example tree. Node 1 has two children, 2 and 3, each of which has one
observed terminal child. We use L(i) and R(i) to denote the left and right children of node i.

In the HMM, the transition parameters of a state
specify a distribution over single next states; similarly, the binary production parameters of a grammar symbol must specify a distribution over pairs
of grammar symbols for its children. We adapt the
HDP machinery to tie these binary production distributions together. The key difference is that now we
must tie distributions over pairs of grammar symbols together via distributions over single grammar
symbols.
Another difference is that in the HMM, at each
time step, both a transition and a emission are made,
whereas in the PCFG either a binary production or
an emission is chosen. Therefore, each grammar
symbol must also have a distribution over the type
of rule to apply. In a CNF PCFG, there are only
two types of rules, but this can be easily generalized
to include unary productions, which we use for our
parsing experiments.
To summarize, the parameters of each grammar
symbol z consists of (1) a distribution over a ﬁnite
number of rule types φT , (2) an emission distribuz
tion φE over terminal symbols, and (3) a binary proz
duction distribution φB over pairs of children gramz
mar symbols. Figure 2 describes the model in detail.
Figure 3 shows the generation of the binary production distributions φB . We draw φB from a DP
z
z
centered on ββ T , which is the product distribution
over pairs of symbols. The result is a doubly-inﬁnite
matrix where most of the probability mass is con-

β ∼ GEM(α)
state

left child state

ββ T
right child state

left child state

φB
z

∼ DP(ββ T )
right child state

Figure 3: The generation of binary production probabilities given the top-level symbol probabilities β.
First, β is drawn from the stick-breaking prior, as
in any DP-based model (a). Next, the outer-product
ββ T is formed, resulting in a doubly-inﬁnite matrix
matrix (b). We use this as the base distribution for
generating the binary production distribution from a
DP centered on ββ T (c).
centrated in the upper left, just like the top-level distribution ββ T .
Note that we have replaced the general

G0 and F (φE ) pair with Dirichlet(αE ) and
zi
Multinomial(φE ) to specialize to natural language,
zi
but there is no difﬁculty in working with parse
trees with arbitrary non-multinomial observations
or more sophisticated word models.
In many natural language applications, there is
a hard distinction between pre-terminal symbols
(those that only emit a word) and non-terminal symbols (those that only rewrite as two non-terminal or
pre-terminal symbols). This can be accomplished
by letting αT = (0, 0), which forces a draw φT to
z
assign probability 1 to one rule type.
An alternative deﬁnition of an HDP-PCFG would
be as follows: for each symbol z, draw a distribution
over left child symbols lz ∼ DP(β) and an independent distribution over right child symbols rz ∼
DP(β). Then deﬁne the binary production distribuT
tion as their cross-product φB = lz rz . This also
z
yields a distribution over symbol pairs and hence deﬁnes a different type of nonparametric PCFG. This
model is simpler and does not require any additional
machinery beyond the HDP-HMM. However, the
modeling assumptions imposed by this alternative
are unappealing as they assume the left child and
right child are independent given the parent, which
is certainly not the case in natural language.

subsymbols. The former can be handled through
a ﬁnite Dirichlet distribution since all symbols are
known and observed, but the latter must be handled
with the Dirichlet process machinery, since the number of subsymbols is unknown.

2.5

We present an inference algorithm for the HDPPCFG model described in Section 2.4, which can
also be adapted to the HDP-PCFG-GR model with
a bit more bookkeeping. Most previous inference
algorithms for DP-based models involve sampling
(Escobar and West, 1995; Teh et al., 2006). However, we chose to use variational inference (Blei
and Jordan, 2005), which provides a fast deterministic alternative to sampling, hence avoiding issues
of diagnosing convergence and aggregating samples.
Furthermore, our variational inference algorithm establishes a strong link with past work on PCFG reﬁnement and induction, which has traditionally employed the EM algorithm.
In EM, the E-step involves a dynamic program
that exploits the Markov structure of the parse tree,
and the M-step involves computing ratios based on
expected counts extracted from the E-step. Our variational algorithm resembles the EM algorithm in
form, but the ratios in the M-step are replaced with
weights that reﬂect the uncertainty in parameter es-

HDP-PCFG for grammar reﬁnement

An important motivation for the HDP-PCFG is that
of reﬁning an existing treebank grammar to alleviate unrealistic independence assumptions and to
improve parsing accuracy. In this scenario, the set
of symbols is known, but we do not know how
many subsymbols to allocate per symbol. We introduce the HDP-PCFG for grammar reﬁnement
(HDP-PCFG-GR), an extension of the HDP-PCFG,
for this task.
The essential difference is that now we have a
collection of HDP-PCFG models for each symbol
s ∈ S, each one operating at the subsymbol level.
While these HDP-PCFGs are independent in the
prior, they are coupled through their interactions in
the parse trees. For completeness, we have also included unary productions, which are essentially the
PCFG counterpart of transitions in HMMs. Finally,
since each node i in the parse tree involves a symbolsubsymbol pair (si , zi ), each subsymbol needs to
specify a distribution over both child symbols and

HDP-PCFG for grammar reﬁnement (HDP-PCFG-GR)
For each symbol s ∈ S:
−β s ∼ GEM(α)
[draw subsymbol weights]
−For each subsymbol z ∈ {1, 2, . . . }:
−−φT ∼ Dirichlet(αT )
[draw rule type parameters]
sz
−−φE ∼ Dirichlet(αE (s))
[draw emission parameters]
sz
−−φu ∼ Dirichlet(αu )
[unary symbol productions]
sz
−−φb ∼ Dirichlet(αb )
[binary symbol productions]
sz
−−For each child symbol s ∈ S:
−−−φU ∼ DP(αU , β s )
[unary subsymbol prod.]
szs
−−For each pair of children symbols (s , s ) ∈ S × S:
[binary subsymbol]
−−−φB s ∼ DP(αB , β s β T )
szs
s
For each node i in the parse tree:
−ti ∼ Multinomial(φTi zi )
[choose rule type]
s
−If ti = E MISSION:
−−xi ∼ Multinomial(φE zi )
[emit terminal symbol]
si
−If ti = U NARY-P RODUCTION:
−−sL(i) ∼ Multinomial(φui zi )
[generate child symbol]
s
−−zL(i) ∼ Multinomial(φUi zi sL(i) )
[child subsymbol]
s
−If ti = B INARY-P RODUCTION:
−−(sL(i) , sR(i) ) ∼ Mult(φsi zi )
[children symbols]
−−(zL(i) , zR(i) ) ∼ Mult(φB zi sL(i) sR(i) )
[subsymbols]
si

2.6

Variational inference

φB
z

for z > K. While the posterior grammar does have
an inﬁnite number of symbols, the exponential decay of the DP prior ensures that most of the probability mass is contained in the ﬁrst few symbols (Ishwaran and James, 2001).2 While our variational approximation q is truncated, the actual PCFG model
is not. As K increases, our approximation improves.

z1

φT
z

β

z2

φE
z

z3

∞

z

2.8

Figure 4: We approximate the true posterior p over
parameters θ and latent parse trees z using a structured mean-ﬁeld distribution q, in which the distribution over parameters are completely factorized but
the distribution over parse trees is unconstrained.

timates. Because of this procedural similarity, our
method is able to exploit the desirable properties of
EM such as simplicity, modularity, and efﬁciency.
2.7

Structured mean-ﬁeld approximation

We denote parameters of the HDP-PCFG as θ =
(β, φ), where β denotes the top-level symbol probabilities and φ denotes the rule probabilities. The
hidden variables of the model are the training parse
trees z. We denote the observed sentences as x.
The goal of Bayesian inference is to compute the
posterior distribution p(θ, z | x). The central idea
behind variational inference is to approximate this
intractable posterior with a tractable approximation.
In particular, we want to ﬁnd the best distribution q ∗
as deﬁned by
def

q ∗ = argmin KL(q(θ, z)||p(θ, z | x)),

(4)

q∈Q

where Q is a tractable subset of distributions. We
use a structured mean-ﬁeld approximation, meaning
that we only consider distributions that factorize as
follows (Figure 4):
K
def

Q =

q(φT )q(φE )q(φB ) .
z
z
z

q(z)q(β)

(5)

z=1

We further restrict q(φT ), q(φE ), q(φB ) to be
z
z
z
Dirichlet distributions, but allow q(z) to be any
multinomial distribution. We constrain q(β) to be a
degenerate distribution truncated at K; i.e., βz = 0

Coordinate-wise ascent

The optimization problem deﬁned by Equation (4)
is intractable and nonconvex, but we can use a simple coordinate-ascent algorithm that iteratively optimizes each factor of q in turn while holding the
others ﬁxed. The algorithm turns out to be similar in
form to EM for an ordinary PCFG: optimizing q(z)
is the analogue of the E-step, and optimizing q(φ)
is the analogue of the M-step; however, optimizing
q(β) has no analogue in EM. We summarize each
of these updates below (see (Liang et al., 2007) for
complete derivations).
Parse trees q(z): The distribution over parse trees
q(z) can be summarized by the expected sufﬁcient statistics (rule counts), which we denote as
C(z → zl zr ) for binary productions and C(z →
x) for emissions. We can compute these expected
counts using dynamic programming as in the E-step
of EM.
While the classical E-step uses the current rule
probabilities φ, our mean-ﬁeld approximation involves an entire distribution q(φ). Fortunately, we
can still handle this case by replacing each rule probability with a weight that summarizes the uncertainty over the rule probability as represented by q.
We deﬁne this weight in the sequel.
It is a common perception that Bayesian inference
is slow because one needs to compute integrals. Our
mean-ﬁeld inference algorithm is a counterexample:
because we can represent uncertainty over rule probabilities with single numbers, much of the existing
PCFG machinery based on EM can be modularly
imported into the Bayesian framework.
Rule probabilities q(φ): For an ordinary PCFG,
the M-step simply involves taking ratios of expected
2
In particular, the variational distance between the stickbreaking distribution and the truncated version decreases exponentially as the truncation level K increases.

counts:

2

φB (zl , zr ) =
z

C(z → zl zr )
.
C(z → ∗ ∗)

(6)

exp(Ψ(x))
x

1.5

For the variational HDP-PCFG, the optimal q(φ) is
given by the standard posterior update for Dirichlet
distributions:3

1

0.5

q(φB ) = Dirichlet(φB ; αB ββ T + C(z)),
z
z

(7)

where C(z) is the matrix of counts of rules with lefthand side z. These distributions can then be summarized with multinomial weights which are the only
necessary quantities for updating q(z) in the next iteration:
B
Wz (zl , zr )

def

=

exp Eq [log φB (zl , zr )]
z

(8)

0
0

0.5

1

1.5

2

x

Figure 5: The exp(Ψ(·)) function, which is used in
computing the multinomial weights for mean-ﬁeld
inference. It has the effect of reducing a larger fraction of small counts than large counts.

)+αB β

=

zl β zr )
eΨ(C(z→zl zr
, (9)
B)
eΨ(C(z→∗ ∗)+α

where Ψ(·) is the digamma function. The emission
parameters can be deﬁned similarly. Inspection of
Equations (6) and (9) reveals that the only difference
between the maximum likelihood and the mean-ﬁeld
update is that the latter applies the exp(Ψ(·)) function to the counts (Figure 5).
When the truncation K is large, αB βzl βzr is near
0 for most right-hand sides (zl , zr ), so exp(Ψ(·)) has
the effect of downweighting counts. Since this subtraction affects large counts more than small counts,
there is a rich-get-richer effect: rules that have already have large counts will be preferred.
Speciﬁcally, consider a set of rules with the same
left-hand side. The weights for all these rules only
differ in the numerator (Equation (9)), so applying
exp(Ψ(·)) creates a local preference for right-hand
sides with larger counts. Also note that the rule
weights are not normalized; they always sum to at
most one and are equal to one exactly when q(φ) is
degenerate. This lack of normalization gives an extra degree of freedom not present in maximum likelihood estimation: it creates a global preference for
left-hand sides that have larger total counts.
Top-level symbol probabilities q(β): Recall that
we restrict q(β) = δβ ∗ (β), so optimizing β is
equivalent to ﬁnding a single best β ∗ . Unlike q(φ)
3

Because we have truncated the top-level symbol weights,
the DP prior on φB reduces to a ﬁnite Dirichlet distribution.
z

and q(z), there is no closed form expression for
the optimal β ∗ , and the objective function (Equation (4)) is not convex in β ∗ . Nonetheless, we can
apply a standard gradient projection method (Bertsekas, 1999) to improve β ∗ to a local maxima.
The part of the objective function in Equation (4)
that depends on β ∗ is as follows:
L(β ∗ ) = log GEM(β ∗ ; α)+

(10)

K

Eq [log Dirichlet(φB ; αB β ∗ β ∗T )]
z
z=1

See Liang et al. (2007) for the derivation of the gradient. In practice, this optimization has very little effect on performance. We suspect that this is because
the objective function is dominated by p(x | z) and
p(z | φ), while the contribution of p(φ | β) is minor.

3

Experiments

We now present an empirical evaluation of the HDPPCFG(-GR) model and variational inference techniques. We ﬁrst give an illustrative example of the
ability of the HDP-PCFG to recover a known grammar and then present the results of experiments on
large-scale treebank parsing.
3.1

Recovering a synthetic grammar

In this section, we show that the HDP-PCFG-GR
can recover a simple grammar while a standard

(a)

0.25

1

2

PCFG fails to do so because it has no built-in control over grammar complexity. From the grammar in
Figure 6, we generated 2000 trees. The two terminal
symbols always have the same subscript, but we collapsed Xi to X in the training data. We trained the
HDP-PCFG-GR, with truncation K = 20, for both
S and X for 100 iterations. We set all hyperparameters to 1.
Figure 7 shows that the HDP-PCFG-GR recovers
the original grammar, which contains only 4 subsymbols, leaving the other 16 subsymbols unused.
The standard PCFG allocates all the subsymbols to
ﬁt the exact co-occurrence statistics of left and right
terminals.
Recall that a rule weight, as deﬁned in Equation (9), is analogous to a rule probability for standard PCFGs. We say a rule is effective if its weight
is at least 10−6 and its left hand-side has posterior
is also at least 10−6 . In general, rules with weight
smaller than 10−6 can be safely pruned without affect parsing accuracy. The standard PCFG uses all
20 subsymbols of both S and X to explain the data,
resulting in 8320 effective rules; in contrast, the
HDP-PCFG uses only 4 subsymbols for X and 1 for
S, resulting in only 68 effective rules. If the threshold is relaxed from 10−6 to 10−3 , then only 20 rules
are effective, which corresponds exactly to the true
grammar.
Parsing the Penn Treebank

In this section, we show that our variational HDPPCFG can scale up to real-world data sets. We ran
experiments on the Wall Street Journal (WSJ) portion of the Penn Treebank. We trained on sections
2–21, used section 24 for tuning hyperparameters,
and tested on section 22.
We binarize the trees in the treebank as follows:
for each non-terminal node with symbol X, we in-

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

subsymbol

standard PCFG

(b)

Figure 6: (a) A synthetic grammar with a uniform
distribution over rules. (b) The grammar generates
trees of the form shown on the right.

3.2

0.25
posterior

→ X1 X1 | X2 X2 | X3 X3 | X4 X4
S
→ a1 | b1 | c1 | d1
→ a2 | b2 | c2 | d2
Xi
Xi
→ a3 | b3 | c3 | d3
{ai , bi , ci , di } {ai , bi , ci , di }
→ a4 | b4 | c4 | d4

posterior

S
X1
X2
X3
X4

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

subsymbol

HDP-PCFG

Figure 7: The posteriors over the subsymbols of the
standard PCFG is roughly uniform, whereas the posteriors of the HDP-PCFG is concentrated on four
subsymbols, which is the true number of symbols
in the grammar.

troduce a right-branching cascade of new nodes with
symbol X. The end result is that each node has at
most two children. To cope with unknown words,
we replace any word appearing fewer than 5 times
in the training set with one of 50 unknown word tokens derived from 10 word-form features.
Our goal is to learn a reﬁned grammar, where each
symbol in the training set is split into K subsymbols. We compare an ordinary PCFG estimated with
maximum likelihood (Matsuzaki et al., 2005) and
the HDP-PCFG estimated using the variational inference algorithm described in Section 2.6.
To parse new sentences with a grammar, we compute the posterior distribution over rules at each span
and extract the tree with the maximum expected correct number of rules (Petrov and Klein, 2007).
3.2.1

Hyperparameters

There are six hyperparameters in the HDP-PCFGGR model, which we set in the following manner:
α = 1, αT = 1 (uniform distribution over unaries versus binaries), αE = 1 (uniform distribution
over terminal words), αu (s) = αb (s) = N1 , where
(s)
N (s) is the number of different unary (binary) righthand sides of rules with left-hand side s in the treebank grammar. The two most important hyperparameters are αU and αB , which govern the sparsity
of the right-hand side for unary and binary rules.
We set αU = αB although more performance could
probably be gained by tuning these individually. It
turns out that there is not a single αB that works for
all truncation levels, as shown in Table 1.
If the top-level distribution β is uniform, the value
of αB corresponding to a uniform prior over pairs of
children subsymbols is K 2 . Interestingly, the optimal αB appears to be superlinear but subquadratic

truncation K
best αB
uniform αB

2
16
4

4
12
16

8
20
64

12
28
144

16
48
256

20
80
400

Table 1: For each truncation level, we report the αB
that yielded the highest F1 score on the development
set.
K
1
2
4
8
12
16
20

PCFG
F1
Size
60.47 2558
69.53 3788
75.98 3141
74.32 4262
70.99 7297
66.99 19616
64.44 27593

PCFG (smoothed)
F1
Size
60.36
2597
69.38
4614
77.11
12436
79.26
120598
78.8
160403
79.2
261444
79.27
369699

HDP-PCFG
F1
Size
60.5
2557
71.08
4264
77.17
9710
79.15 50629
78.94 86386
78.24 131377
77.81 202767

Table 2: Shows development F1 and grammar sizes
(the number of effective rules) as we increase the
truncation K.

in K. We used these values of αB in the following
experiments.
3.2.2

Results

The regime in which Bayesian inference is most
important is when training data is scarce relative to
the complexity of the model. We train on just section 2 of the Penn Treebank. Table 2 shows how
the HDP-PCFG-GR can produce compact grammars
that guard against overﬁtting. Without smoothing,
ordinary PCFGs trained using EM improve as K increases but start to overﬁt around K = 4. Simple
add-1.01 smoothing prevents overﬁtting but at the
cost of a sharp increase in grammar sizes. The HDPPCFG obtains comparable performance with a much
smaller number of rules.
We also trained on sections 2–21 to demonstrate that our methods can scale up and achieve
broadly comparable results to existing state-of-theart parsers. When using a truncation level of K =
16, the standard PCFG with smoothing obtains an
F1 score of 88.36 using 706157 effective rules while
the HDP-PCFG-GR obtains an F1 score of 87.08 using 428375 effective rules. We expect to see greater
beneﬁts from the HDP-PCFG with a larger truncation level.

4

Related work

The question of how to select the appropriate grammar complexity has been studied in earlier work.
It is well known that more complex models necessarily have higher likelihood and thus a penalty
must be imposed for more complex grammars. Examples of such penalized likelihood procedures include Stolcke and Omohundro (1994), which used
an asymptotic Bayesian model selection criterion
and Petrov et al. (2006), which used a split-merge
algorithm which procedurally determines when to
switch between grammars of various complexities.
These techniques are model selection techniques
that use heuristics to choose among competing statistical models; in contrast, the HDP-PCFG relies on
the Bayesian formalism to provide implicit control
over model complexity within the framework of a
single probabilistic model.
Johnson et al. (2006) also explored nonparametric grammars, but they do not give an inference algorithm for recursive grammars, e.g., grammars including rules of the form A → BC and B → DA.
Recursion is a crucial aspect of PCFGs and our
inference algorithm does handle it. Finkel et al.
(2007) independently developed another nonparametric model of grammars. Though their model is
also based on hierarchical Dirichlet processes and is
similar to ours, they present a different inference algorithm which is based on sampling. Kurihara and
Sato (2004) and Kurihara and Sato (2006) applied
variational inference to PCFGs. Their algorithm is
similar to ours, but they did not consider nonparametric models.

5

Conclusion

We have presented the HDP-PCFG, a nonparametric
Bayesian model for PCFGs, along with an efﬁcient
variational inference algorithm. While our primary
contribution is the elucidation of the model and algorithm, we have also explored some important empirical properties of the HDP-PCFG and also demonstrated the potential of variational HDP-PCFGs on a
full-scale parsing task.

References
C. E. Antoniak. 1974. Mixtures of Dirichlet processes
with applications to Bayesian nonparametric problems. Annals of Statistics, 2:1152–1174.
M. Beal, Z. Ghahramani, and C. Rasmussen. 2002. The
inﬁnite hidden Markov model. In Advances in Neural
Information Processing Systems (NIPS), pages 577–
584.
D. Bertsekas. 1999. Nonlinear programming.
D. Blei and M. I. Jordan. 2005. Variational inference for
Dirichlet process mixtures. Bayesian Analysis, 1:121–
144.
E. Charniak. 1996. Tree-bank grammars. In Association
for the Advancement of Artiﬁcial Intelligence (AAAI).
E. Charniak. 2000. A maximum-entropy-inspired parser.
In North American Association for Computational
Linguistics (NAACL), pages 132–139.
M. Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, University
of Pennsylvania.
M. D. Escobar and M. West. 1995. Bayesian density
estimation and inference using mixtures. Journal of
the American Statistical Association, 90:577–588.
T. S. Ferguson. 1973. A Bayesian analysis of some nonparametric problems. Annals of Statistics, 1:209–230.
J. R. Finkel, T. Grenager, and C. Manning. 2007. The
inﬁnite tree. In Association for Computational Linguistics (ACL).
S. Goldwater, T. Grifﬁths, and M. Johnson. 2006. Contextual dependencies in unsupervised word segmentation. In International Conference on Computational
Linguistics and Association for Computational Linguistics (COLING/ACL).
H. Ishwaran and L. F. James. 2001. Gibbs sampling
methods for stick-breaking priors. Journal of the
American Statistical Association, 96:161–173.
M. Johnson, T. Grifﬁths, and S. Goldwater. 2006. Adaptor grammars: A framework for specifying compositional nonparametric Bayesian models. In Advances
in Neural Information Processing Systems (NIPS).
D. Klein and C. Manning. 2003. Accurate unlexicalized
parsing. In Association for Computational Linguistics
(ACL), pages 423–430.
K. Kurihara and T. Sato. 2004. An application of the
variational Bayesian approach to probabilistic contextfree grammars. In International Joint Conference on
Natural Language Processing Workshop Beyond Shallow Analyses.
K. Kurihara and T. Sato. 2006. Variational Bayesian
grammar induction for natural language. In International Colloquium on Grammatical Inference.
P. Liang, S. Petrov, M. I. Jordan, and D. Klein.
2007. Nonparametric PCFGs using Dirichlet processes. Technical report, Department of Statistics,
University of California at Berkeley.

T. Matsuzaki, Y. Miyao, and J. Tsujii. 2005. Probabilistic CFG with latent annotations. In Association for
Computational Linguistics (ACL).
S. Petrov and D. Klein. 2007. Learning and inference
for hierarchically split PCFGs. In Human Language
Technology and North American Association for Computational Linguistics (HLT/NAACL).
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning accurate, compact, and interpretable tree annotation. In International Conference on Computational Linguistics and Association for Computational
Linguistics (COLING/ACL).
J. Pitman. 2002. Combinatorial stochastic processes.
Technical Report 621, Department of Statistics, University of California at Berkeley.
J. Sethuraman. 1994. A constructive deﬁnition of Dirichlet priors. Statistica Sinica, 4:639–650.
A. Stolcke and S. Omohundro. 1994. Inducing probabilistic grammars by Bayesian model merging. In
Grammatical Inference and Applications.
Y. W. Teh, M. I. Jordan, M. Beal, and D. Blei. 2006.
Hierarchical Dirichlet processes. Journal of the American Statistical Association, 101:1566–1581.

