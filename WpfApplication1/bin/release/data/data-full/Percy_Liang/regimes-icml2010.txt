On the Interaction between Norm and Dimensionality:
Multiple Regimes in Learning

Percy Liang
pliang@cs.berkeley.edu
Computer Science Division, University of California, Berkeley, CA 94720, USA
Nati Srebro
Toyota Technological Institute at Chicago, Chicago, IL 60637, USA

Abstract
A learning problem might have several measures of complexity (e.g., norm and dimensionality) that aﬀect the generalization
error.
What is the interaction between
these complexities? Dimension-free learning theory bounds and parametric asymptotic analyses each provide a partial picture of the full learning curve. In this paper, we use high-dimensional asymptotics
on two classical problems—mean estimation
and linear regression—to explore the learning curve more completely. We show that
these curves exhibit multiple regimes, where
in each regime, the excess risk is controlled
by a subset of the problem complexities.

nati@ttic.edu

What we really want to understand is the true behavior of the excess risk En (B, d) as a function of
the sample size n, norm B, and dimensionality d.
In this paper, we analyze the excess risk for two
classical problems—mean estimation (Section 2) and
linear regression (Section 3)—by performing a highdimensional asymptotic analysis. In particular, we allow the complexity (B, d) of the problem to grow with
the sample size n, so that the excess risk En (B, d)
˜ ˜
converges to a non-vanishing asymptotic limit E(B, d),
˜
˜
where B and d are the rescaled complexities. We then
˜
˜
study this limiting function as B and d vary to see
the interaction between norm and dimensionality. We
show how the excess risk can have multiple regimes,
where in each regime, the excess risk is controlled by a
subset of the relevant complexities. Furthermore, we
ﬁnd that the transitions between regimes are smooth,
even asymptotically.

1. Introduction
Most analyses of learning algorithms proceed by identifying a measure of complexity of the learning problem
and then provide either bounds or asymptotic expressions for the generalization error (risk) in terms of that
complexity. For instance, for linear models, bounds
based on Rademacher complexity (Bartlett & Mendelson, 2001), covering numbers (Pollard, 1984), or online
learning (Cesa-Bianchi & Lugosi, 2006) depend on the
norm (in relation to the variance of data and the noise)
and not the dimensionality. On the other hand, classical parametric asymptotic analyses (van der Vaart,
1998; Liang et al., 2010) provide answers that depend
only on the dimensionality and not the norm. There
seems to be some tension here: If the sample complexity depends asymptotically only on the dimensionality,
how can it be bounded in terms of only the norm?
Appearing in Proceedings of the 27 th International Conference on Machine Learning, Haifa, Israel, 2010. Copyright
2010 by the author(s)/owner(s).

Notation For a vector v ∈ Rd , we write v ⊗ = vv .
Let Xn = Op (1) denote that the sequence of random variables (Xn )n≥1 is bounded in probability, that
is, for every > 0, there exists M < ∞ such that
supn P (Xn > M ) ≤ . We write Xn = Op (Yn )
P

n
to mean Xn = Op (1). Let Xn − 0 denote con→
Y
vergence in probability, that is, for every
> 0,
limn→∞ P (|Xn − X| > ) = 0. When we use big-O
notation, only universal constants are hidden, never
parameters of the learning problem.

2. Constrained Mean Estimation
A classical problem in statistics is estimating the mean
of a multivariate Gaussian from i.i.d. samples. We consider a variant of this problem where the norm of mean
vector is constrained to a Euclidean ball. Even in this
simple problem, we will see that two learning regimes
emerge: a random regime controlled by the norm and a
unregularized regime controlled by the dimensionality.

On the Interaction between Norm and Dimensionality: Multiple Regimes in Learning

2.1. Setup
The mean estimation problem is deﬁned as follows: Let
µ∗ ∈ Rd be the unknown mean vector that we wish to
estimate, and let B = µ∗ 2 denote its norm. We
obtain n i.i.d. training points: X (i) ∼ N (µ∗ , σ 2 Id×d )
for i = 1, . . . , n. Let the tuple Ψ = (B, d, σ 2 ) speciﬁes an instance of the mean estimation problem, which
includes the norm B, dimensionality d, and data variance σ 2 .
Given a vector µ ∈ Rd , we measure its generalization
error (risk) using squared loss:
def

(µ) = EX∼N (µ∗ ,σ2 I) (X − µ)2 .

(1)

Deﬁne the following estimator which minimizes the
empirical risk subject to a norm constraint:
n
def

µn = argmin
ˆ
µ

2 =B

(X

(i)

2

− µ) .

(2)

i=1

Our goal is to study the excess risk of µn , deﬁned as
ˆ
follows:
def
En (Ψ) = (ˆn ) − (µ∗ ).
µ
(3)
2.2. Preliminary Analysis
We ﬁrst derive a closed form solution for the estimator
µn . Consider the Lagrangian of the constrained optiˆ
n
mization problem in (2): L(µ, λ) = i=1 (X (i) − µ)2 +
2
λ µ . Diﬀerentiating L with respect to µ and setting
¯
n
1
X
¯
it to zero, we get µ = 1+λ , where X = n i=1 X (i) is
the empirical mean. To satisfy the constraint µ 2 =
¯
B, we must have µn = B X2 . The estimator µn is
ˆ
ˆ
¯
X
¯
just the unconstrained estimator X projected onto the
radius-B sphere.
Next, we decompose the risk in (1) into two orthogonal
parts: (µ) = E[(X−µ∗ )2 ]+(µ−µ∗ )2 = dσ 2 +(µ−µ∗ )2 .
Plugging the derived expressions for µn and (µ) into
ˆ
(3) yields the following expression for the excess risk:
En (Ψ) =

¯
BX
∗
¯ 2 −µ
X

2

.

(4)

2
¯
Note that X is distributed as N (µ∗ , σ ).
n

2.3. Asymptotic Analysis
To analyze the excess risk En (Ψ), we turn to asymptotics to simplify the form of En (Ψ). In particular, we
2
consider a sequence of problems Ψn = (Bn , dn , σn ) so
that the excess risk En (Ψn ) converges to some non˜
vanishing quantity E(Ψ) as n → ∞. The allowed sequences Ψn are given in the following deﬁnition:

Deﬁnition 1 (Limiting Problem Speciﬁcation). A se2
2
quence of mean estimation problems Ψn = (Bn , dn , σn )
˜ = (B 2 , d, σ 2 ) if
˜ ˜˜
has a limit Ψ
2
˜
Bn → B 2 ,

2
dn σn
˜
→ d,
n

2
σn → σ 2
˜

(5)

as n → ∞.
˜
Intuitively, the limiting problem speciﬁcation Ψ captures the essence of the mean estimation problem. The
following proposition gives a precise handle of the excess risk in this limit:
Proposition 1. Suppose a sequence of mean estima2
˜
tion problems Ψn = (Bn , dn , σn ) has a limit Ψ =
˜˜
˜ d, σ 2 ). Then the excess risk (3) has the following
(B,
asymptotic limit:
def
P
En = En (Ψn ) − E(Ψ),
→ ˜

where the asymptotic excess risk is

1
˜
˜
E(Ψ) = 4B 2 sin2  arctan
2


˜
d 
.
˜
B2

(6)

(7)

˜
Note that E(Ψ) is a non-random function; this is because in high dimensions, the excess risk concentrates.
Before proving the proposition, let us establish some
˜
intuitions about the regimes that E(Ψ) exhibit by vary˜
ing Ψ:
˜
˜
• Random Regime (B 2
d): When the rescaled di˜
mensionality d is large, the arctan term tends to π ;
2
1
also, sin2 ( π ) = 2 , so the asymptotic excess risk is
4
˜
˜
E 2B 2 . In this regime, the norm B dominates the
˜
excess risk and the dimensionality d is irrelevant.
Geometrically, the estimator µn essentially proˆ
duces a random point on a (dn − 1)-dimensional
sphere, whose squared distance from µ∗ concenn
2
trates around 2Bn .
˜
˜
• Unregularized Regime (d
B 2 ):
When
the rescaled dimensionality is small, then
1
4 sin2 ( 2 arctan(x))
x2 , so the excess risk is
˜ Here, the dimensionality d dominates the
˜
E
d.
˜ is irrelevant.
excess risk and the norm B
This regime is very closely related to parametric
asymptotics. The maximum likelihood estimator
2
σn
¯
Xn has excess risk exactly n · χ2n , where χ2n ded
d
notes a χ2 random variable with dn degrees of free2
dom, which has mean dn . When Bn is large, the
¯
sphere looks locally ﬂat, so the projection of Xn
onto its surface simply removes an insigniﬁcant degree of freedom.

On the Interaction between Norm and Dimensionality: Multiple Regimes in Learning
P

˜
d

{

→
Vn −

excess risk

1.28
0.64

P ˜
→
Un − B

0.32
0.16

√

En

¯
Xn

µn
ˆ

θn

actual (En )

0.08

{

µ∗
n

asymptotic (E)
upper bound (E B )

0.04

101

102

103

104

n
Figure 1. On constrained mean estimation: Log-log plot
2
2
of the excess risk for dn = 1000, Bn = 1, σn = 1. The
2
˜
corresponding limiting values are B 2 = Bn = 1 and
2
d n σn
˜
d = n = 1000 . One can clearly see the two regimes
n
marked by their slopes (0 and −1, respectively): In the ran˜
dom regime, the norm B controls the excess risk, while in
˜
the unregularized regime, the dimensionality d does. The
˜ ˜
bound E B = min{2B 2 , d} represents the ends of E quite accurately, but misses the smooth transition in the middle.

Based on the previous discussion, we can actually
stitch together the following upper bound,
˜ def
˜ ˜
˜
E (Ψ) = min 2B 2 , d ≥ E(Ψ),
B

(8)

Figure 2. Geometric depiction of the excess risk En (used
in the proof of Proposition 1). The constrained estimator
¯
µn is obtained by projecting Xn down to the radius-Bn
ˆ
sphere. The key is that in high dimensions, the two random
components Un and Vn both concentrate.

We can obtain En using basic trigonometry. First,
n
compute the angle θn = arctan Un . Bisecting θn
V
√
and converting angles back to lengths yields En =
2Bn sin( 1 θn ). Putting everything together, we have
2
2
En = 4Bn sin2

1
arctan
2

Un
Vn

.

(9)

Now we compute the limits of Un and Vn . First, Un
includes the small deviation along the ﬁrst component:
2
P ˜
σn
) − B. Vn includes the deviations
→
Un = Bn +Op (
n

which clearly marks out the two regimes.
We can see how well the asymptotics represent the
actual excess risk En for ﬁnite n by plotting the learning curve (Figure 1), increasing n while keeping Ψn
ﬁxed. Note that increasing n decreases the rescaled
˜
dimensionality d. From Figure 1, we see that while
B
the bound E matches the asymptotic E at the ends,
there is a noticeable gap when transitioning between
the two regimes. In particular, the bound is piecewise
linear whereas asymptotic curve is smooth, tracking
the empirical excess risk much more closely. We note
that the transition is smooth even in the asymptotic
limit; this is due to the smoothness of the loss function.
Proof of Proposition 1. Without loss of generality, assume µ∗ = (Bn , 0, . . . , 0) because the problem is ron
tationally invariant.
First, let us decompose the pre-projected estimator
¯
Xn into two components: (1) the component in the
def
¯
subspace of µ∗ , which has length Un = |Xn1 |, and
n
∗
(2) the component orthogonal to µn , which has length
def
dn ¯ 2
X . Figure 2 depicts the setup: the
Vn =
j=2

nj

excess risk En we want to compute is denoted geometrically.

along the other d − 1 components, which amounts to
2
P
σn 2
˜
Vn =
χ
−
→ d. Since En depends on Un and
n

dn −1

Vn only via smooth trigonometric functions (9), we
can apply the continuous mapping theorem to obtain
P
En − E as desired.
→

3. Regularized Linear Regression
We now turn to norm-regularized linear regression. We
ﬁrst analyze a componentwise estimator (which treats
each parameter separately), showing that even in this
simple case, the asymptotic excess risk exhibits three
regimes, not two as in mean estimation. For the full
least squares estimator, we use a combination of upper
bounds to hypothesize the existence of four regimes.
3.1. Setup
We assume that data are generated as follows: X ∼
N (0, Σd×d ) and Y = X, β ∗ + W , where β ∗ ∈ Rd
is the true parameter and W ∼ N (0, σ 2 ) is independent noise. Let pβ ∗ (X, Y ) denote the resulting distribution. We consider a linear regression problem to be
fully speciﬁed by the tuple Ψ = (Σ, β ∗ , σ 2 ).
Given a predictor β ∈ Rd , we are interested in its

On the Interaction between Norm and Dimensionality: Multiple Regimes in Learning

generalization error (risk), which averages the squared
loss over test points:
def

(β) = E(X,Y )∼pβ∗ [(Y − X, β )2 ].

(10)

Given n i.i.d. training points {(X (i) , Y (i) )}n drawn
i=1
from pβ ∗ , deﬁne the regularized least-squares estimator:
1
ˆλ def
βn = argmin
β∈Rd n

n

Y (i) − X (i) , β

2

+λ β

2

,

i=1

(11)
where λ ≥ 0 is the regularization parameter. We are
interested in analyzing the excess risk of this estimator:
def

λ
ˆλ
En (Ψ) = (βn ) − (β ∗ ).

(12)

We also consider the excess risk of an oracle estimator
which chooses the optimal λ (in a data-dependent way)
to minimize the excess risk:
def

∗
λ
En (Ψ) = inf En (Ψ).
λ≥0

(13)

Assumption 1 (Diagonal Covariance). Assume that
the covariance matrix of the data is diagonal, that is,
2
2
Σ = diag(τ1 , . . . , τd ).
This assumption is made without loss of generality for
the estimators we have deﬁned so far, which are rotationally invariant. This is not true for the componentwise estimator that we will introduce later.
3.2. Preliminary Analysis
ˆ def 1 n
Deﬁne the empirical covariance Σ = n i=1 X (i)⊗
ˆ def 1 n
and let S = n i=1 X (i) W (i) . First, we solve (11) in
the standard way by diﬀerentiating and setting the
ˆλ
ˆ
ˆ
ˆ
result to zero to get βn = (Σ + λI)−1 (Σβ ∗ + S).
Next, since the noise W is independent of X, we can
2
rewrite (10) as (β) = σ + tr{Σ(β − β ∗ )⊗ }. Apn
plying these two derived expressions to (12), we get
λ
ˆλ
En (Ψ) = tr{Σ(βn − β ∗ )⊗ }.
Using some algebra, we can write the parameter error
ˆλ
ˆ
ˆ
ˆ
as βn − β ∗ = −λ(Σ + λI)−1 β ∗ + (Σ + λI)−1 S. Putting
the last two equations together yields:
λ
ˆ
ˆ
ˆ
En (Ψ) = tr{Σ[λ(Σ+λI)−1 β ∗ −(Σ+λI)−1 S]⊗ }. (14)

Unregularized Estimator Before we consider regularization, let us comment on the unregularized estimator (when λ = 0). In this case, the excess risk
0
0
ˆ ˆ ˆ
En (Ψ) in (14) simpliﬁes to En (Ψ) = tr{Σ−1 S ⊗ Σ−1 Σ}.
0
We can compute the expectation of En (Ψ) in closed

form. First, conditioned on X (1) , . . . , X (n) , we can
ˆ
integrate out the W (1) , . . . , W (n) in S by indepenˆ⊗ | X (1) , . . . , X (n) ] = Σ σ2 .
ˆ
dence; this yields E[S
n
ˆ
Next, the inverse covariance matrix Σ−1 has an inverse
1
ˆ
Wishart distribution (Σ−1 ∼ W −1 ( n Σ, n)), which has
−1
nΣ
mean n−d−1 . Putting everything together, we obtain
0
E[En (Ψ)] =

dσ 2
n−d−1 .

It is interesting to note that the excess risk does not
depend on β ∗ and Σ, but only on the dimensionality
d. The norm of β ∗ does not play a role at all because
the unregularized estimator is shift-invariant. The covariance of the data Σ does not play a role due to the
following intuition: the larger Σ is, the easier it is to
estimate β, but the harder it is to predict. The two
forces cancel out exactly.
3.3. Componentwise Estimator Asymptotics
In this section, we introduce and analyze a simple estimator that still provides additional insight into the
interaction between norm and dimensionality. Deﬁne
the componentwise least-squares estimator, which estiˆ
mates each component of β ∈ Rd separately, as follows:
ˆλ
βj =

τj βj + Sj
ˆ2 ∗ ˆ
τj + λ
ˆ2

∀j = 1, . . . , d,

(15)

where τj = Σjj .
ˆ2 ˆ
The componentwise estimator consistently estimates
β ∗ regardless of whether Σ is diagonal. When Σ is diagonal, the excess risk is just the sum across the components, where component involves a one-dimensional
regression problem. Without regularization, the expected excess risk of the componentwise estimator is
dσ 2
n−2 . Note this is smaller than the excess risk of the
2

dσ
full unregularized estimator, which is n−d−1 . We eﬀectively gain an eﬀective sample of size d−1 by exploiting
knowledge of the eigenstructure of Σ.

In this section, we analyze the excess risk of the componentwise estimator using asymptotics. The lack
of covariance structure simpliﬁes the math considerably. Consider a sequence of regression problems
2
∗
2
Ψn = (τn , βn , σn ). We do not yet commit to a particular scaling, but we do impose the following constraints:
Assumption 2 (Constraints on Limiting Problem
d σ2
Speciﬁcation). Assume that lim supn nn n < ∞ and
dn
∗
lim supn j=1 |βnj |τnj < ∞.
We now derive the asymptotic excess risk of the oracle
componentwise estimator:
Proposition 2. Consider a sequence of regression
2
∗
2
problems Ψn = (τn , βn , σn ) satisfying Assumption 2.

On the Interaction between Norm and Dimensionality: Multiple Regimes in Learning
λ
the randomness in En . Using these variables, deﬁne:

Deﬁne
dn
def

λ
En =

j=1

∗2 2
2 4
λ2 βnj τnj
σn τnj
+
.
2
2
(τnj + λ)2 n(τnj + λ)2

dn

(16)

λ
Fn (R)

=
j=1

∗2 2
∗
3
λ2 βnj τnj − 2λβnj σn τnj Rj1 +
2
(τnj (1 + Rj2 ) + λ)2

2
σn 4 ˆ
n τnj Hnj

dn

Suppose that there exists a function E λ such that
λ
∗ def
supλ≥0 |En − E λ | → 0. Then the excess risk En =
∗
En (Ψn ) of the oracle componentwise estimator (13)
has the following asymptotic limit E ∗ :
def

P

def

∗
λ
En = inf En − inf E λ = E ∗ .
→
λ≥0

(17)

λ≥0

λ
For a ﬁxed λ, we will show that the excess risk En
converges to a non-random asymptotic excess risk E λ ,
λ
using En as an intermediate quantity that intuitively
λ
removes the randomness from En . The concentration
λ
λ
of En around En must be established.

What is new in regression is the minimization over λ.
λ P
→
To establish inf λ≥0 En − inf λ≥0 E λ (i.e., switching
inf with lim), we need some sort of uniformity over λ,
which occupies most of the following proof.

Gλ (R) =
n
j=1

P

P

λ
λ
Now, we need to show that supλ≥0 |En − En | − 0,
→
λ
λ
i.e., that the residual |En − En | goes to zero at a rate
that does not depend on λ. Specializing (14) to the
componentwise estimator and expanding yields:

dn

j=1

2
∗2
∗ ˆ
ˆ2
τnj (λ2 βnj − 2λβnj Snj + Snj )
.
(ˆnj + λ)2
τ2

(18)

ˆ
1
Snj
−2
),
σn τnj = Op (n
ˆ2
def nSnj
ˆ
Hnj = σ2 τ 2 − 1 =
n nj

def
ˆ
For each j = 1, . . . , dn , let Rnj1 =
def
ˆ
Rnj2 =

2
τnj −τnj
ˆ2
2
τnj

1

= Op (n− 2 ), and

Op (1). Importantly, these variables (1) do not depend
on the problem speciﬁcation Ψn and (2) capture all

.

(20)

ˆ
Let An be the event that Rn ∞ ≤ 1 . On event An ,
2
λ ˆ
Lemma 1 below will show that
Fn (Rn ) 1 ≤ M for
some constant M independent of Ψn and λ. Note:
norms on the matrices are element-wise.
1
Lemma 1. For all R ∈ Rdn ×2 such that R ∞ ≤ 2
λ
and λ ≥ 0, we have
Fn (R) 1 ≤ M , where M is a
constant independent of Ψn and λ.
def

2
Proof. Let Qnj = τnj (1 + Rj2 ) + λ. For each j, we
λ
∂Fn (R)

∂Rj2

2
σn 4
n τnj ).

∗
→
Proof of Proposition 2. To prove that En − E ∗ , it
P
λ
λ
suﬃces to show that supλ≥0 |En −En | − 0. If we have
→
that, the proposition can be established as follows: Let
λ
λn ∈ argminλ≥0 En and λ∗ ∈ argminλ≥0 E λ . Note
∗
∗
λn
that En = En and E ∗ = E λ . For any > 0, we have
λn
λ∗
λ∗
λ∗
λ
λ
En ≤ En
En
E ≤ E λn 2 En n 2 En n
2
2
for suﬃciently large n with high probability, where
∗
a
b denote |a − b| ≤ . This ensures |E ∗ − En | ≤ .

2
(τnj (1 + Rj2 ) + λ)2

λ
λ
λ
We have constructed Fn and Gλ so that En = Fn (0)
n
λ
λ ˆ
λ ˆ
and En = Fn (Rn ) + Gn (Rn ), which can be veriλ
ﬁed with some algebra. Intuitively, Fn (0) captures
the non-random problem-dependent part of the excess
ˆ
ˆ
risk; Rn and Gλ (Rn ) contribute the random problemn
independent part.

have

λ
En =

,

(19)

variance

squared bias

2
σn 4
n τnj

2
∗2 2
∗
3
= −2Q−3 τnj (λ2 βnj τnj −2λβnj σn τnj Rj1 +
nj

1 2
Using the fact that Qnj is larger than 2 τnj and

λ, we obtain |

λ
∂Fn (R)
∂Rj2 |

∗2 2
∗
≤ 4βnj τnj + 8βnj σn τnj + 16

2
σn
n .

∂F λ (R)

∗
n
Similarly, we can bound | ∂Rj1 | ≤ 4βnj σn τnj . Sum
the right-hand sides over j = 1, . . . , dn . By Assumption 2, this sum is bounded above by a quantity independent of Ψn and λ.
λ ˆ
λ
By the mean value theorem, Fn (Rn ) − Fn (0) =
λ
ˆ
ˆ
Fn (cRn ) Rn for some c ∈ [0, 1], where, abusing noˆ
tation, Rn is treated as a vector. Applying the lemma
with H¨lder’s inequality and taking a sup over λ yields
o
λ ˆ
λ
ˆ
supλ≥0 |Fn (R) − Fn (0)| ≤ M Rn ∞ . Note this is all
still conditioned on An .

ˆ
Now we want to bound supλ≥0 |Gλ (Rn )|. It sufn
ﬁces to take λ = 0, which is where Gλ attains
n
ˆ
its maximum value. Simplifying, we get G0 (Rn ) =
n
2
σn
n

ˆ
Hnj
dn
ˆ
j=1 (1+Rnj2 )2 .

Note that each summand con-

verges in distribution to a χ2 distribution minus 1
(which has mean zero), independent of Ψn and λ.
To ﬁnish the proof, ﬁx > 0. With high probability,
we can take n large enough (in a way that does not
ˆ
depend on Ψn or λ) such that (1) R ∞ < 2M and
event An holds by applying a standard tail bound plus

On the Interaction between Norm and Dimensionality: Multiple Regimes in Learning

ˆ
a union bound over the 2dn = O(n) elements of Rn ;
ˆ
Hnj
dn
ˆ
j=1 (1+Rnj2 )2 |

≤

˜
2d

7.8e-1

by the law of large

λ ˆ
λ
This ensures that both supλ≥0 |Fn (R) − Fn (0)| ≤ 2
λ
λ ˆ
and supλ≥0 |Gn (R)| ≤ 2 , implying that supλ≥0 |En −
λ
En | ≤ .

1.6e-1

excess risk

and (2) | d1
n
numbers.

3.1e-2
6.3e-3

actual (En )

1.3e-3

asymptotic (E ∗ )

2.5e-4

lower bound ( 1 E B )
4
upper bound (E B )

3.4. Learning Regimes
To get some concrete intuition for Proposition 2, let
us specialize the problem speciﬁcation:
Assumption 3 (Two-Part Regression Structure). Let
the
true
parameter
vector
be
∗
βn = (1, 0, . . . , 0) ∈ Rdn and the data covari2
2
Cn
Cn
2
ance be Σ = diag(Bn , dn −1 , . . . , dn −1 ) ∈ Rdn ×dn for
2
2
some Bn , Cn > 0.
2
The idea is that Bn captures the squared norm of the
signal in the data (which exists only on the ﬁrst com2
ponent), and Cn captures the squared norm of irrele∗
vant components. The norm of βn can always taken to
be one without loss of generality, since the true measure of complexity is the product of the norm of the
predictor with the norm of the data.

Deﬁnition
2
(Limiting Problem Speciﬁcation). A sequence of linear regression prob2
2
2
lems Ψn = (Bn , Cn , dn , σn ) converges to a limit
˜ = (B 2 , C 2 , d, σ 2 ) if
˜ ˜ ˜˜
Ψ
2
2 2
2
˜ ˆ
˜ C σ
˜ dn σn → d, σ 2 → σ 2 (21)
Bn → B 2 , n n → C 2 ,
˜
n
n
n

102

104

n
Figure 3. Componentwise estimator for linear regression:
Log-log plot of the learning curve for d = 100, B 2 = 1, C 2 =
10, σ 2 = 100. There are three regimes, each characterized
by a diﬀerent slope (0 corresponding to a constant excess
risk in the random regime, − 1 corresponding to a rate of
2
1
1
√
in the regularized regime, and −1 corresponding to n
n
in the unregularized regime).
∗
Proposition 2 is the limit of the excess risk En of the
oracle estimator. The following proposition sheds light
into the multi-regime structure of E ∗ :
Proposition 3 (Bounds on Regimes). Let

˜2
def
˜
˜ 2C , d .
E B = min B 2 ,
˜ ˜
B d

Note that we allow both the dimensionality dn and the
2
squared norm Cn of the irrelevant components tend to
2
∞. The presence of Cn will create a new intermediate
learning regime.
Specializing the asymptotic excess risk from (16) to
this problem speciﬁcation:
C4

2
dn
n
2
2 4
σn (dn −1)2
Bn λ 2
σn Bn
λ
En =
+
+
,
2
2
2
(Bn + λ)2
n(Bn + λ)2 j=2 n( Cn + λ)2
dn −1

which can be shown to converge uniformly across λ ≥ 0
to
˜
C4
˜
B 2 λ2
˜
λ
E =
+ ˜2 d
.
(22)
C
˜ 2 + λ)2
(B
( ˜ + λ)2
d

def

squared bias = U

λ

def

variance = V λ

Recall that we are ultimately interested in the excess
risk of the oracle estimator E ∗ = inf λ≥0 E λ , which by

(23)

The asymptotic excess risk E ∗ of the oracle componentwise estimator deﬁned in (15) is bounded by E B to
within a factor of four:
1 B
E ≤ E ∗ ≤ E B.
4

as n → ∞.

106

(24)

We can plot the learning curve as a relationship between the sample size and excess risk, for a ﬁxed speciﬁcation of the regression problem. Figure 3 shows the
actual excess risk En , the asymptotic excess risk E ∗ ,
1
˜
˜
and bounds 4 E B , E B . Note that C 2 and d scale inversely with n, so that the three regimes scale as 1,
1
√ , and 1 , respectively.
n
n
The bound (23) indicates three regimes corresponding
to each of the three terms:
• Random Regime: In this regime, λ should be large
so that the variance V λ → 0 and the squared bias
˜
U λ → B 2 (see (22)). This corresponds to simply
ˆ
guessing βn = 0. Only the squared norm of the
˜ 2 controls the excess risk.
signal B
• Regularized Regime: In this new regime, λ must be
optimized to balance the squared bias U λ and variance V λ terms. The squared norm of the irrelevant

On the Interaction between Norm and Dimensionality: Multiple Regimes in Learning
7.8e-1
1.6e-1

1
Now we show the lower bound 4 E B ≤ E ∗ . Using the
1
1
fact that a ≥ b ≥ 0 implies (a+b)2 ≥ 4a2 , we have the
following relations:

7.8e-1
1.6e-1

3.1e-2

3.1e-2

E∗

E ∗ 6.3e-3
dn = 30
dn = 100
dn = 300

1.3e-3
2.5e-4

˜
˜
(i) λ > B 2 implies U λ ≥ 1 B 2 ,
4
˜
(ii) λ ≤ B 2 implies U λ ≥ 1 λ2 ,

2
Cn = 10

6.3e-3

2
Cn = 30

1.3e-3

2
Cn = 100

4

2.5e-4
10

2

10

4

10

6

10

2

10

4

10

6

n

n

(iv) λ ≤

2
(b) Varying norm Cn

(a) Varying dimensionality dn

(iii) λ >

Figure 4. Log-log plots of asymptotic excess risk E ∗ (same
default parameters as in Figure 3), where we study the im2
pact of varying the norm Cn and dimensionality dn . Varying the dimensionality dn aﬀects both the regularized and
2
unregularized regimes (a); varying the squared norm Cn
only aﬀects the regularized regime (b).

˜
components C 2 dominates the excess risk, favorably
˜
˜
scaled down by large B and d. The implied de1
pendence of the excess risk on n is √n .
• Unregularized Regime: λ should be small so that
˜
˜
U λ → 0 and V λ → d. Here, the dimensionality d
controls the excess risk, yielding an excess risk of
1
order n , independent of the norm.
Figure 4 shows the impact of varying the problem parameters on the various regimes. As one would expect
from (23), changing the norm C 2 only aﬀects the intermediate regime, whereas changing the dimensionality d aﬀects both the variance and the intermediate
regime.
Proof of Proposition 3. We ﬁrst prove the upper
bound E ∗ ≤ E B . To do this, we need to show that E ∗
(deﬁned in terms of (22)) is upper bounded by each of
the three terms in (23).
˜
To get E ∗ ≤ B 2 , take λ → ∞ (inﬁnite regularization).
In this limit, the squared bias term U λ dominates and
˜
converges to B 2 .

˜
C2
˜
d
˜
C2
˜
d

implies V λ ≥
λ

implies V ≥

˜
C4

˜
1 d
4 λ2

, and

1 ˜
4 d.

Take any λ ≥ 0. The plan is to construct Lλ out of
two lower bounds on U λ and V λ , respectively, based
on which of the above relations are satisﬁed. In doing
so, we ensure that Lλ ≤ E λ . We will also show that
1 B
≤ minλ ≥0 Lλ ≤ Lλ . Since this holds for all
4E
1
λ ≥ 0, we will have that 4 E B ≤ inf λ≥0 E λ = E ∗ .
˜
Now we consider the four cases for λ: If λ > B 2 (i) and
˜
1 ˜2
C2
λ
λ > d (iii), we have inf λ L = 4 B with λ → ∞. If
˜
˜
˜
˜
˜ 2 (i) and λ ≤ C 2 (iv), inf λ Lλ = 1 B 2 + 1 d. If
λ>B
˜
λ ≤ B 2 (ii) and λ >

˜
d
˜
C2
˜
d

(iii), inf λ Lλ =

4
4
˜
1 2C 2
√ , with
4 ˜ ˜
B d

˜˜
√
λ = B C , based on an earlier derivation. Finally, if
2

˜
d

˜
λ ≤ B 2 (ii) and λ ≤
λ = 0.

˜
C2
˜
d

(iv), inf λ Lλ =

1 ˜
4d

with

The regularized regime does not always exist. For ex˜
ample if the dimensionality is relatively small (d ≤
˜2
2C
B
˜ ), then based on E in (23), the excess risk of the
B2
˜2
√
regularized regime ( 2C ) will be larger than the geo˜
B

˜
d

metric average of the excess risks of the other regimes
˜ ˜
(B d). In this case, we jump directly from the random regime to the unregularized regime.
3.5. Full Estimator
So far, we have analyzed the componentwise estimator. This section oﬀers a partial characterization of
the learning curve for the full estimator (11).

(25)

˜
Clearly, E ∗ ≤ B 2 by taking λ → ∞ (the random
∗
˜
regime), and E ≤ d by taking λ = 0 (the unregularized regime). To analyze the intermediate regime, deﬁne the constrained estimator to be one which chooses
ˆλ
λ so that βn ≤ 1, and λn denote this λ. Let E C
be the corresponding asymptotic excess risk and note
that the oracle asymptotic excess risk E ∗ ≤ E C .

˜
Finally, to show that E ∗ ≤ d, simply set λ = 0 (corre˜
sponding to no regularization), to get that E λ = d.

Having not yet been able to derive an exact asymptotic
form for E C , we instead oﬀer some speculations based
on upper bounds for stochastic optimization (online
ˆsgd
learning). Let βn be the estimator obtained by running one pass of stochastic gradient descent over the

To show E ∗ ≤

˜
2C 2
√ ,
˜ d
B ˜

observe that E λ = U λ +V λ , where
˜
C4

2

λ
d
U λ ≤ B 2 and V λ ≤ λ˜ (22). Optimize the sum of the
2
˜
two bounds with respect to λ:
˜
C4

∗

E ≤ inf

λ≥0

λ2
˜
+ d2
˜
λ
B2

=

˜
2C 2
,
˜ ˜
B d

˜˜
√
which is attained by setting λ2 = B C .
2

˜
d

On the Interaction between Norm and Dimensionality: Multiple Regimes in Learning

training data. Then in expectation over the sample we
have
sgd
En ≤ 2

2
2
Bn + C n
+2
n

2

2
2
2
(Bn + Cn )σn
.
n

This follows from Srebro et al. (2010), which is based
on Theorem 1 of Shalev-Shwartz (2007). While this
bound holds only for stochastic gradient descent, we
strongly suspect that it also holds for the constrained
estimator (the regularized empirical risk minimizer).
Putting together all the pieces yields the following
coarse approximate form to the risk:
EC

˜
min B 2 , O

˜
C2
˜
+C
σ2
˜

˜
,d .

(26)

We emphasize that (26) is purely speculative. Nevertheless, it can help us understand what might change
from the componentwise analysis. Comparing (26)
˜
√
with (23), we see an additional factor of BC d that
˜
might correspond to the beneﬁt of specializing to a
diagonal covariance. We also notice the additional ad˜2
1
ditive term C2 , which behaves as n and is the relevant
σ
˜
term when σ 2 is large. This additional additive term
˜
gives rise to a fourth regime. While the componentwise
estimator has three regimes, (26) suggests the full estimator has four regimes, with two regimes controlled
only by the norm, independent of the dimensionality.
Further work is necessary to conﬁrm these hypotheses.

4. Discussion
Our broad goal is to obtain an accurate picture of the
learning curve. There are a plethora of approaches in
the literature that tackle pieces of the curve. Classical parametric asymptotics, a dominant approach in
statistics, let the sample size n → ∞ while ﬁxing
the problem speciﬁcation Ψ. Hence, they consider
the limit of the learning curve where the excess risk
P
En − 0. These analyses thus focus on the local ﬂuc→
tuations of estimators around a limiting value. As a
result, norm constraints do not enter into the asymptotic risk, even with considering higher-order asymptotics (e.g., Liang et al. (2010)).
On the other hand, ﬁnite sample complexity bounds
(e.g., Bartlett & Mendelson (2001)) provide statements for any sample size n and problem speciﬁcation
Ψn . These focus on controlling structural complexities. Thus, they are well suited for handling norm
constraints and typically yield dimension-free results.
However, these are only upper bounds and can be far
from being tight.

Both analyses provide complementary but incomplete
views of the learning curve. In this paper, we used
high-dimensional asymptotics to obtain an asymptotically exact analysis also when En is away from zero, albeit for simple problems. The key is that as Ψn grows
with n, the appropriate ratios between sample size and
complexity are maintained, while still allowing us reap
the beneﬁts of asymptotics, namely concentration.
Such ideas have been around in statistics since Kolmogorov’s work in the 1960s, and more recently have
played an important role in high-dimensional sparse
settings (e.g., Wainwright (2009)). Related ideas can
also be found in statistical physics approaches for
studying learning curves (Haussler et al., 1994).
The particular focus in this paper has been on understanding how multiple problem complexities interact to generate multiple regimes in learning curves.
We have so far characterized the regimes for two
problems—mean estimation and componentwise linear
regression–as a starting point. We hope future work
will help shed light on learning curves in more general
settings.

References
Bartlett, P. L. and Mendelson, S. Rademacher and Gaussian complexities: Risk bounds and structural results.
In Computational Learning Theory, pp. 224–240, 2001.
Cesa-Bianchi, N. and Lugosi, G. Prediction, learning, and
games. Cambridge University Press, Cambridge, UK,
2006.
Haussler, D., Kearns, M., Seung, H. S., and Tishby, N. Rigorous learning curve bounds from statistical mechanics.
In Computational Learning Theory, pp. 76–87, 1994.
Liang, P., Bach, F., Bouchard, G., and Jordan, M. I.
Asymptotically optimal regularization in smooth parametric models. In Advances in Neural Information Processing Systems (NIPS), Cambridge, MA, 2010. MIT
Press.
Pollard, D. Convergence of Stochastic Processes. SpringerVerlag, 1984.
Shalev-Shwartz, S. Online Learning: Theory, Algorithms,
and Applications. PhD thesis, The Hebrew University of
Jerusalem, 2007.
Srebro, N., Sridharan, K., and Tewari, A. Stochastic optimization and online learning with smooth loss functions.
Technical report, TTI Chicago, 2010.
van der Vaart, A. W. Asymptotic Statistics. Cambridge
University Press, Cambridge, UK, 1998.
Wainwright, M. J. Sharp thresholds for noisy and highdimensional recovery of sparsity using 1 -constrained
quadratic programming (lasso). IEEE Transactions on
Information Theory, 55:2183–2202, 2009.

