Adaptivity and Optimism: An Improved Exponentiated Gradient Algorithm

Jacob Steinhardt
Percy Liang
Stanford University, 353 Serra Street, Stanford, CA 94305 USA

JSTEINHARDT @ CS . STANFORD . EDU
PLIANG @ CS . STANFORD . EDU

Abstract

D∞

We present an adaptive variant of the exponentiated gradient algorithm. Leveraging the optimistic learning framework of Rakhlin & Sridharan (2012), we obtain regret bounds that in the
learning from experts setting depend on the variance and path length of the best expert, improving on results by Hazan & Kale (2008) and Chiang et al. (2012), and resolving an open problem
posed by Kale (2012). Our techniques naturally
extend to matrix-valued loss functions, where we
present an adaptive matrix exponentiated gradient algorithm. To obtain the optimal regret bound
in the matrix case, we generalize the Follow-theRegularized-Leader algorithm to vector-valued
payoffs, which may be of independent interest.

Chiang et al. (2012)

V∞

maxi Di

maxi Vi

Di∗

Vi∗

this work

The exponentiated gradient (EG) algorithm is a powerful
tool for performing online learning in the presence of many
irrelevant features (Kivinen & Warmuth, 1997; Littlestone,
1988). EG is often used in the “learning from experts” setting, in which it is also known as the weighted majority
algorithm (Littlestone & Warmuth, 1989). In this setting,
EG entertains regret bounds of the form
2
∞,

Si∗
Cesa-Bianchi et al. (2007)

Figure 1. Summary of possible regret bounds with references to
algorithms known to achieve these bounds. An arrow A → B
indicates that A is a strictly better bound than B. Our algorithm
simultaneously improves upon several existing results. D represents the path length, V the variance, and S the second moment;
these quantities are deﬁned formally in Section 3, Equation 24.
Even in situations where Di∗ is Θ(1), both maxi Di and Vi∗
(and hence all other entries in the lattice) can be Θ(T ).

wt+1,i ∝ wt,i (1 − ηzt,i ) rather than the usual EG update of
wt+1,i ∝ wt,i exp(−ηzt,i ). This algorithm cannot be cast
in the mirror descent framework with a ﬁxed regularizer,
yet it achieves an improved regret bound of

T

log(n)
+η
zt
η
t=1

maxi Si

Hazan & Kale (2008)

1. Introduction

Regret ≤

S∞
Kivinen & Warmuth (1997)

T

(1)

where η is the step size, zt is the vector of losses, and n
is the number of experts. Such bounds (as well as slightly
stronger bounds based on local norms) can be obtained under the mirror descent framework, a general tool that gives
rise to many other online learning algorithms (see ShalevShwartz (2011) for a survey).
In contrast, Cesa-Bianchi et al. (2007) present a variant of this algorithm based on a multiplicative update of
Proceedings of the 31 st International Conference on Machine
Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s).

Regret ≤

log(n)
2
+η
zt,i∗ .
η
t=1

(2)

Comparing the regret bounds (2) and (1), note that (2) is in
terms of the best expert i∗ instead of a maximum over all
experts. This latter bound can be much stronger; we show
√
in Proposition 2.2 that there is in fact a Θ( T ) separation
of the worst-case regret in the setting where the best expert has loss identically equal to zero. Other differences
between these two types of updates are discussed in Arora
et al. (2012).
The fact that an algorithm achieving a better regret bound
cannot be cast in the mirror descent framework is a bit unsettling. Does this mean we should abandon mirror descent

Adaptivity and Optimism: An Improved Exponentiated Gradient Algorithm

as the gold standard for online learning, despite theorems
asserting its optimality (Srebro et al., 2011)? We answer
this question in the negative: the (1 − ηzt,i ) update can be
understood as a form of adaptive mirror descent (Orabona
et al., 2013), where the regularizer changes in each round
t in response to previously observed vectors z1:t . We obtain a natural interpretation of the update as performing a
second-order correction to the gradient.
Examining (2) more closely, we see that this corrected update should perform well when the best expert i∗ incurs
losses consistently close to zero; then the second term in
T
2
the regret is t=1 zt,i∗ ≈ 0. However, this assumption
may be unrealistic, and many authors have recently considered variance bounds that depend only on the deviation of zt from its average, or path-length bounds in terms
of zt − zt−1 (Hazan & Kale, 2008; Chiang et al., 2012;
Yang et al., 2013). Rakhlin & Sridharan (2012) present
an optimistic learning framework that yields such bounds
for any mirror descent algorithm. However, the updates in
Hazan & Kale (2008) are not mirror descent updates (for
any ﬁxed regularizer), and their bounds are incomparable
to the bounds obtained via optimistic learning.
In the learning from experts setting, we subsume all the previously mentioned bounds by obtaining a bound in terms of
the path length of the best expert:
T

Regret ≤

log(n)
+η
(zt,i∗ − zt−1,i∗ )2 .
η
t=1

(3)

Obtaining such a bound is posed as an open problem in
Kale (2012). We achieve such a regret bound (Equation 23)
by applying Rakhlin’s updates in the context of an adaptive
mirror descent algorithm, thus obtaining an adaptive optimistic exponentiated gradient algorithm. When the path
length is not known and η must be determined adaptively,
our bounds weaken slightly but are still strong enough to
answer the problem in Kale (2012), as well as to subsume
all of the previously mentioned bounds in the adaptive step
size setting.
Finally, we extend all these results to the matrix setting,
where the learner plays a positive semideﬁnite matrix Wt
with trace 1 (in analogy with the simplex). This setting has
been extensively studied (Tsuda et al., 2005; Arora & Kale,
2007) and is important in obtaining online and approximation bounds for various combinatorial optimization problems (Arora & Kale, 2007; Hazan et al., 2012). As far as
we are aware, the best known results in this setting are of
the form (1). Using the machinery so far developed, all
of our results extend naturally to the matrix setting. However, for the variance bound we need a new analysis tool:
a variant of FTRL for vector-valued losses ordered relative
to some cone K.
In summary, the main contributions of this paper are:

• An interpretation of the multiplicative weights update
of Cesa-Bianchi et al. (2007) as exponentiated gradient with an adaptive regularizer (Section 2).
• An improved exponentiated gradient algorithm obtaining best-known variance and path-length bounds
(Section 3).
• An adaptive matrix exponentiated gradient algorithm
attaining similar bounds (Section 4).
• A generalization of Follow-the-Regularized-Leader to
vector-valued loss functions (Lemma 4.3).
Related work. There is a rich literature on using adaptive
updates to obtain better regret bounds for online learning.
A common setting is adaptive learning of a quadratic regularizer, as in the AROW (Crammer et al., 2009), AdaGrad
(Duchi et al., 2011), and online preconditioning (Streeter
& McMahan, 2010) algorithms. Other work includes
dimension-free exponentiated gradient (Orabona, 2013),
whitened perceptron (Cesa-Bianchi et al., 2005), and online adaptation of the step size (Hazan et al., 2007). The
non-stationary setting was explored by Vaits et al. (2013),
and McMahan & Streeter (2010) obtain regret bounds relative to a family of regularizers. More recently, many of
these algorithms have been uniﬁed into a single framework
by Orabona et al. (2013). To our knowledge, adaptively
regularized exponentiated gradient has not been explicitly
explored, though many variants on the basic multiplicative updates have been proposed (Cesa-Bianchi et al., 2007;
Hazan & Kale, 2008; Chiang et al., 2012), which can be interpreted in our framework as making implicit use of an
adaptive regularizer.
In addition to the variants on exponentiated gradient discussed above, Auer & Warmuth (1998) and Herbster &
Warmuth (1998) have studied the case where the best expert can change over time. Finally, Sabato et al. (2012)
consider a generalization of the Winnow algorithm (Littlestone, 1988), which corresponds to exponentiated gradient
with a hinge-like loss, and provide a careful analysis of the
regret that is more precise than the mirror descent analysis.

2. A Tale of Two Updates
Our point of departure is the two different types of multiplicative updates mentioned in the introduction. For simplicity we will consider the setting of learning from expert
advice.1 In this setting there are n experts, and the learner
maintains a probability distribution wt ∈ ∆n over the experts. In each round t = 1, . . . , T , the learner plays wt ,
a vector zt ∈ [−1, 1]n is revealed, and the learner incurs
1
The general setting follows a nearly identical analysis and is
covered in the supplementary material.

Adaptivity and Optimism: An Improved Exponentiated Gradient Algorithm

Name
EG (MW1)
MW2
Variation-MW
Optimistic MW
AEG-Path
AMEG-Path

Update
βt+1 = βt − ηzt
βt+1,i = βt,i + log(1 − ηzt,i )
βt+1,i = βt,i − ηzt,i − 4η 2 (zt,i − mt,i )2
t−1
mt = 1 s=1 zs
t
βt+1,i = βt,i − ηzt,i
βt+1,i = βt,i − ηzt,i − η 2 (zt,i − zt−1,i )2
Bt+1 = Bt − ηZt − η 2 (Zt − Zt−1 )2

Prediction
exp(βt )
exp(βt )

Source
(Kivinen & Warmuth, 1997)
(Cesa-Bianchi et al., 2007)

exp(βt )

(Hazan & Kale, 2008)

exp(βt − ηzt−1 )
exp(βt − ηzt−1 )
exp(Bt − ηZt−1 )

(Chiang et al., 2012)
this work
this work

Table 1. An overview of known adaptive exponentiated gradient algorithms. The AEG-Path updates incorporate components of both
the Variation-MW and Optimistic MW algorithms, and are motivated by interpreting MW2 in terms of adaptive mirror descent. The
AMEG-Path updates extend AEG-Path to the matrix case (which had previously only been done for MW1).

loss wt zt . The learner’s goal is to minimize the regret
supu∈∆n Regret(u), where
T
def

T

wt zt −

Regret(u) =

t=1

u zt .

(4)

t=1

1
The learner starts by playing w1 , where w1,i = n for 1 ≤
i ≤ n. On subsequent iterations, we consider two types of
updates for the weight vector wt , as shown in (MW1) and
(MW2) below:

wt+1,i ∝ wt,i exp(−ηzt,i )

(MW1)

wt+1,i ∝ wt,i (1 − ηzt,i ),

(MW2)

where η is the step size. The regret bounds for each of
(MW1) and (MW2) are well-known (see Shalev-Shwartz
(2011) and Cesa-Bianchi et al. (2007) respectively) but we
include them for completeness.
Theorem 2.1. For any 0 < η ≤ 1 and zt ∞ ≤ 1, the
2
updates (MW1) and (MW2) obtain respective regret bounds
of
n

Regret(u) ≤

log(n)
+η
η
i=1
n

Regret(u) ≤

that (MW2) achieves asymptotically constant (as a function
of T ) regret for any quasi-realizable sequence. In contrast,
√
(MW1) can suffer Ω( T ) regret:
Proposition 2.2. For any step size η and T , there is a
quasi-realizable loss sequence (zt )T and a vector u ∈
t=1
∆n such that the updates (MW1) result in Regret(u) =
√
Ω( T ).
The proof is given in the supplementary material, but the
main idea is that (MW1) will have trouble distinguishing
between an expert whose loss is always zero and an expert
whose loss alternates between 1 and −1. This establishes
that the apparent separation between (MW1) and (MW2) is
real and not an artifact of the analysis. We remark that this
separation does not exist when all losses are non-negative.
In this case both (MW1) and (MW2) enjoy O(1) regret (as
a function of T ).
Finally, note that (MW2) cannot be realized as mirror descent for any ﬁxed regularizer. This is because, for any mirror descent algorithm, the prediction on round t + 1 must
t
be a function of s=1 zs , which is not the case for (MW2).

T
2
wt,i zt,i

(5)

t=1
T

log(n)
2
+η
ui
zt,i
η
t=1
i=1

(6)

Adaptive mirror descent However, not all is lost, as
we will obtain (MW2) in terms of an adaptive regularizer
ψt (w). The mirror descent predictions for an adaptive regularizer are given by
t−1

To understand why (6) may be a better bound than (5), suppose that the best expert has loss identically equal to zero.2
Then the optimal u places all mass on that expert, and (6)
reduces to log(n) = 2 log(n) for η = 1 .
η
2
More formally, deﬁne a sequence of losses zt to be quasirealizable if one of the experts i∗ has identically zero loss
and all other experts have non-negative cumulative loss, i.e.
T
t=1 zt,i ≥ 0. It is apparent by the preceding paragraph
2

Of course, if we knew that this was the case ahead of time,
there would be far better algorithms; we use this scenario purely
for illustrative purposes.

wt =

∗
ψt

(θt ) ,

def

θt = −η

zs ,

(7)

s=1
def

where ψ ∗ (x) = supw {w x − ψ(w)} is the Fenchel conjugate of ψ. We provide general properties of Fenchel
conjugates as well as several calculations of interest in the
supplementary material. See Orabona et al. (2013) for a
more complete exposition on adaptive mirror descent, and
Shalev-Shwartz (2011) for a general survey.
We can cast (MW2) in the adaptive mirror descent framework, as detailed in Proposition 2.3 below. As we will ex-

Adaptivity and Optimism: An Improved Exponentiated Gradient Algorithm

plain in the next section, these updates have a natural interpretation as “pushing the regret into the regularizer”.
def

Proposition 2.3. Deﬁne βt,i =
let

t−1
s=1

log(1 − ηzs,i ) and

n
def

ui log(ui ) + u (θt − βt ).

ψt (u) =

(8)

t−1

and if mt = 1 s=1 zs , we obtain variance bounds. We
t
illustrate geometrically in Figure 2 how optimistic updates
can improve the regret bound.
We combine optimistic learning (Rakhlin & Sridharan,
2012) with adaptive regularization (Orabona et al., 2013)
to yield Algorithm 1.

i=1

Then adaptive mirror descent with regularizer ψt corresponds exactly to the updates (MW2). The corresponding
regret bound is
Regret(u) ≤

∗
ψ1 (θ1 ) + ψT +1 (u)
η
n

≤

(9)
T

log(n)
2
+η
ui
zt,i .
η
t=1
i=1

(10)

Algorithm 1 Adaptive Optimistic Mirror Descent
Given: convex regularizers ψt and hints mt
Initialize θ1 = 0
for t = 1 to T do
∗
Choose wt = ψt (θt − ηmt )
Observe zt and suffer loss wt zt
Update θt+1 = θt − ηzt
end for
The regret bound for Algorithm 1 is given in Theorem 3.1:

Proof. By standard properties of Fenchel conjugates, we
have
∗
ψt (θt ) = arg min ψt (w) − w θt

(11)

w∈∆n

Theorem 3.1. Suppose that for all t, ψt is convex and satisﬁes the loss-bounding property:
∗
∗
ψt+1 (θt − ηzt ) ≤ ψt (θt − ηmt ) − ηwt (zt − mt ). (13)

n

wi log(wi ) − w βt .

= arg min
w∈∆n

(12)

Then
Regret(u) ≤

i=1

∗
ψ1 (θ1 ) + ψT +1 (u)
.
η

(14)

t−1

From here we see that wt,i ∝ exp(βt,i ) = s=1 (1−ηzs,i ),
so that wt,i does indeed correspond to (MW2).
We omit the proof of the regret bound; it follows straightforwardly from the machinery in the next section (see
Proposition 3.3).
Proposition 2.3 says we can obtain bounds that depend on
2
the the average squared loss zt,i∗ of the best expert i∗ (u
∗
places all its mass on i ). But intuitively, we would like
to not suffer much regret even if zt,i∗ is large so long as
its variation is small. We turn to this issue in the next section.

Proof. The proof is a relatively straightforward combina∗
tion of known results. First note that ψt is convex and that
∗
∗
∗
wt = ψt (θt − ηmt ). Thus, ψt (θt ) ≥ ψt (θt − ηmt ) +
ηwt mt . Then, by deﬁnition of the Fenchel conjugate together with telescoping sums, we have, for any u,
u θT +1 − ψT +1 (u)
∗
≤ ψT +1 (θT +1 )
T
∗
= ψ1 (θ1 ) +

∗
∗
ψt+1 (θt+1 ) − ψt (θt )
t=1
T

3. Adaptive Optimistic Learning
In the previous section, we saw how to obtain regret bounds
that depend on the best expert i∗ , but involve the second
moment. Next, we show how to use the idea of optimistic
learning (Rakhlin & Sridharan, 2012) to obtain results that
depend on variance or path length.
In the optimistic learning framework, we are given a sequence of “hints” mt of what zt might be. Then rather than
choosing wt based on the negative cumulative gradients θt ,
we choose wt based on a preemptive update θt − ηmt . The
resulting regret bounds thus depend on the error in the hints
(zt − mt ) rather than zt . If mt = 0, we recover vanilla mirror descent; if mt = zt−1 , we obtain path-length bounds;

∗
≤ ψ1 (θ1 ) +

∗
∗
ψt+1 (θt+1 ) − ψt (θt − ηmt ) − ηwt mt .
t=1

By the conditions of the theorem, the sum is termwise upper bounded by −ηwt zt and we have
T
∗
wt zt ≤ ψ1 (θ1 ) + ψT +1 (u).

u θT +1 + η

(15)

t=1

Expanding θT +1 as −η

T
t=1 zt

completes the proof.

The key intuition, also spelled out by Orabona et al. (2013),
is that, if we make ψt+1 − ψt large enough to “swallow
the regret” on round t, then we obtain bounds that depend

Adaptivity and Optimism: An Improved Exponentiated Gradient Algorithm
ψ ∗ (θt − ηzt )

ψ ∗ (θt − ηzt )
ψ ∗ (θt − ηmt ) − ηwt (zt − mt )

∗

ψ ∗ (θt − ηmt )

ψ ∗ (θt ) − ηwt zt

∗

ψ (θt )

ψ (θt )

Figure 2. Illustration of how optimistic updates affect the regret bound. For a ﬁxed regularizer ψ ∗ , the increase in regret is bounded
above by ψ ∗ (θt+1 ) − ψ ∗ (θt ) − ηwt zt . Normally wt = ψ ∗ (θt ), so that the bound is equal to the gap between ψ ∗ and its tangent line,
as illustrated on the left. For optimistic updates we instead take wt = ψ ∗ (θt − ηmt ), which replaces the tangent line by the dashed
line on the right. This dashed line can be bounded by the tangent line at θt − ηmt , depicted as the solid line on the right.

on the regularizer ψT +1 (u), rather than typical bounds that
depend on Bregman divergences between θt and θt+1 .3
Regularization based on corrections While Theorem 3.1 deals with general sequences of regularizers ψt ,
for our purposes we will only need to consider regularizers
of a special form:
t−1

ψt (w) = ψ(w) − w

β1 − η 2

def

Regret(u) ≤
as ,

t−1

Deﬁne ψt (w) = ψ(w) − w [β1 − η 2 s=1 as ]. Note that
∗
ψt (w) = ψ(w) − w (βt − θt ) and hence ψt (x) = ψ ∗ (x +
(βt − θt )). Then, looking at the condition of Theorem 3.1,
∗
∗
we have ψt+1 (θt − ηzt ) = ψt+1 (θt+1 ) = ψ ∗ (βt+1 ) and
∗
∗
ψt (θt − ηmt ) = ψ (βt − ηmt ), so that the conditions on ψ
and at in this corollary match those on ψt in Theorem 3.1.
The corresponding regret bound is

(16)

∗
ψ1 (θ1 ) + ψT +1 (u)
η

=

s=1

where ψ is a ﬁxed regularizer and at is a sequence of corrections. This choice of regularizer yields the more specialized Algorithm 2, which can be interpreted as performing
second-order corrections to the typical gradient updates.

=

ψ ∗ (β1 ) + ψ(u) − u β1
+ ηu
η

T
t=1

ψ ∗ (β1 ) + ψ(u) + u [−β1 + η 2
η

at ]

T

at ,
t=1

as was to be shown.
Algorithm 2 Adaptive Optimistic Mirror Descent (specialized to corrections)
Given: convex regularizer ψ, corrections at and hints mt
Initialize β1 arbitrarily
for t = 1 to T do
Choose wt = ψ ∗ (βt − ηmt )
Observe zt and suffer loss wt zt
Update βt+1 = βt − ηzt − η 2 at
end for
Corollary 3.2. Suppose ψ is convex and at is such that
ψ ∗ (βt −ηzt −η 2 at ) ≤ ψ ∗ (βt −ηmt ) − ηwt (zt −mt ). Then
Regret(u) ≤

ψ ∗ (β1 ) + ψ(u) − u β1
+ ηu
η

T

at .
t=1

(17)
Proof. The proof essentially consists of translating into the
language of Theorem 3.1 and making use of the property
that the Fenchel conjugate of w → ψ(w) − w c is x →
ψ ∗ (x + c).
3
The typical Bregman divergence bound can be recovered by
setting ψt+1 (w) to ψt (w) + Dψ∗ (θt+1 θt ).

To give some intuition for the condition in Corollary 3.2,
note that wt = ψ ∗ (βt − ηmt ), and so ψ ∗ (βt − ηzt ) ≈
ψ ∗ (βt − ηmt ) − ηwt (zt − mt ). Since ψ ∗ is convex, we
actually have ψ ∗ (βt − ηzt ) ≥ ψ ∗ (βt − ηmt ) − ηwt (zt −
mt ), so we can view the subtraction of η 2 at as a secondorder correction that ﬂips the sign of the inequality. The η 2
coefﬁcient in front of at is motivated by the fact that the
second-order term in the Taylor expansion of ψ ∗ (βt − ηzt )
is of order η 2 , and so for the η 2 at term to cancel this out
we need at to be of constant order.
Adaptive step size. The exposition so far assumes a ﬁxed
step size η, and the subsequent bounds we present will assume that the optimal value of η is known. In practice, it
is rarely the case that we know this optimal value in advance, and it is thus necessary to choose η adaptively. We
ignore this issue in the main text, but an adaptive scheme
following Cesa-Bianchi et al. (2007) is provided in the supplementary material for the interested reader. We note that,
for the adaptive case, our regret bound is slightly worse and
corresponds to the maxi Di entry in Figure 1.
Application to exponentiated gradient. Using the adaptive optimistic mirror descent framework, we can now ob-

Adaptivity and Optimism: An Improved Exponentiated Gradient Algorithm

tain an adaptive exponentiated gradient algorithm that incorporates hints mt . The algorithm is obtained from Aln
gorithm 2 by setting ψ(w) = i=1 wi log(wi ) and at,i =
(zt,i − mt,i )2 . This choice of correction at makes intuitive
sense, as it will downweight experts i for whom the hints
mt,i are inaccurate.
Proposition 3.3 (Adaptive Exponentiated Gradient). Consider the updates given by β1,i = 0 and βt+1,i = βt,i −
ηzt,i − η 2 (zt,i − mt,i )2 , with prediction wt,i ∝ exp(βt,i −
ηmt,i ). Then, assuming zt ∞ ≤ 1, mt ∞ ≤ 1 and
1
0 < η ≤ 4 , we have for all u ∈ ∆n :
n

veriﬁed the condition of Corollary 3.2, we obtain a regret
∗
n
bound of ψ (0)+ψ(u) + η i=1 u at . Finally, we note that
η
n
ψ ∗ (0) = log(n), ψ(u) = i=1 ui log(ui ) ≤ 0, and at,i =
(zt,i − mt,i )2 , which completes the proof.
Comparison to (MW2). For mt = 0 we obtain the same
regret bound (6) that was obtained for the update (MW2).
Interestingly, the two updates are essentially the same to
second order:
2
βt+1,i = βt,i − ηzt,i − η 2 zt,i

Proof. Corollary 3.2 reduces the proof to straightforward
n
computation. Note that, for ψ(w) =
i=1 wi log(wi )
and w constrained to the simplex ∆n , ψ ∗ (β) =
n
log( i=1 exp(βi )) and ψ ∗ (βt − ηmt ) is equal to wt as
deﬁned in the proposition. The updates above thus correspond to Algorithm 2 and so it sufﬁces to check that the
main condition of Corollary 3.2 is satisﬁed with at,i =
(zt,i − mt,i )2 . This follows from the calculation:

Since −x − x2 ≤ log(1 − x) when |x|≤ 1 , we can
2
think of the adaptive EG updates as a second-order underapproximation to (MW2) when mt = 0. The regret bound
(6) for (MW2) can be obtained by a near-identical calculation to the one in Proposition 3.3.
Variance bound.
a variance bound

log(n)
+ η(2Vi∗ + 6),
η

T

exp(βt,i − ηzt,i − η 2 (zt,i − mt,i )2 ))

def

(zt,i − zi )2 ,
¯

Vi =

def

z =
¯

t=1

i=1
n

exp(βt,i − ηmt,i ) exp(−η(zt,i − mt,i )
i=1

− η 2 (zt,i − mt,i )2 ))
n

exp(βt,i − ηmt,i )(1 − η(zt,i − mt,i )))

≤ log(

t−1
s=1 zs ,

1
t

By setting mt =

Regret ≤

n

= log(

(20)

we obtain

(21)

where i∗ is the best expert and

ψ ∗ (βt − ηzt − η 2 at )
= log(

βt+1,i = βt,i + log(1 − ηzt,i ).

versus

T

log(n)
Regret(u) ≤
+η
ui
(zt,i − mt,i )2 . (18)
η
t=1
i=1

(19)

i=1
n

1
T

T

zt

(22)

t=1

is the variance of expert i. This improves the result
in Hazan & Kale (2008), who obtain a regret based on
maxn Vi rather than Vi∗ .4
i=1
The choice of mt corresponds to running an auxiliary instance of Follow-the-Regularized-Leader (Shalev-Shwartz,
2011) to minimize the regret bound (18), an idea ﬁrst introduced by Rakhlin & Sridharan (2012). The details are
given in the supplementary material.

exp(βt,i − ηmt,i )

= log(
i=1

n

−η

exp(βt,i − ηmt,i )(zt,i − mt,i ))

Path-length bound. For mt = zt−1 we obtain the algorithm AEG-Path given in Table 1 and achieve the bound

i=1
n

≤ log(

Regret ≤
exp(βt,i − ηmt,i ))

i=1
n
i=1

−η

exp(βt,i − ηmt,i )(zt,i − mt,i )
n
i=1 exp(βt,i − ηmt,i )

= ψ ∗ (βt − ηmt ) − η ψ ∗ (βt − ηmt ) (zt − mt ).
The two inequalities we made use of were exp(−x−x2 ) ≤
1
1 − x for |x|≤ 2 and log(x − y) ≤ log(x) − y/x. Having

log(n)
+ ηDi∗ ,
η

T
def

(zt,i − zt−1,i )2 .

Di =

t=1

(23)
This is called a path-length bound because Di can be
thought of as the path length (squared) of the losses for expert i. This improves upon the algorithm and bound given
in Chiang et al. (2012), where Di is replaced with the quandef
T
2
tity D∞ =
t=1 zt − zt−1 ∞ , which is always larger
4
Actually, their bound is slightly better than that, but the exact
bound is difﬁcult to state concisely.

Adaptivity and Optimism: An Improved Exponentiated Gradient Algorithm

than Di∗ . We note that Di∗ ≤ 4Vi∗ + 2, so path-length
bounds subsume variance bounds.

Z can have negative eigenvalues). See Warmuth & Kuzmin
(2006) for more on this interpretation.

The path-length bound obtained above resolves a problem
posed by Kale (2012), who asked whether it is possible to
obtain bounds in terms of Di∗ .

We start by extending the adaptive EG algorithm (Proposition 3.3) to the matrix setting:

Comparison of bounds. Recall the deﬁnitions of Di ,
D∞ , and Vi , and further deﬁne V∞ , Si , and S∞ :
def

Di =

def

Vi =

def

Si =

T
2
t=1 (zt,i − zt−1,i )
T
¯ 2
t=1 (zt − zi )
T
2
t=1 zt,i

def

D∞ =

def

V∞ =

def

S∞ =

T
t=1
T
t=1
T
t=1

zt − zt−1
zt − z
¯

2
∞

2
∞

zt 2
∞

(24)
Figure 1 shows the 3 × 3 grid of potential regret bounds,
summarizing the relevant results. The original exponentiated gradient algorithm has regret in terms of S∞ , while the
adaptive algorithm proposed by Cesa-Bianchi et al. (2007)
obtains regret in terms of the smaller quantity Si∗ . Hazan &
Kale (2008) obtain a bound based on maxn Vi , and Chii=1
ang et al. (2012) obtain a bound based on D∞ . All three
of these latter bounds are incomparable, but our AEG-Path
algorithm obtains a bound in terms of Di∗ , which is strictly
better than all of the above. We note that in some cases,
slightly better bounds can be obtained in terms of the behavior of the learner (see e.g. Section 1.2 of Hazan & Kale
(2008)), but we omit these results for brevity and because
the behavior of the learner is not known ahead of time.

4. Extension to Matrices
We now extend our results to the matrix setting, where
the learner chooses a positive semideﬁnite matrix W with
tr(W ) = 1. The ﬂexibility of Corollary 3.2 makes the
extension to this case straightforward; essentially the only
n
change is replacing the regularizer i=1 wi log(wi ) with
n
tr(W log(W )) = i=1 λi log(λi ), where (λi )n are the
i=1
eigenvalues of W .
Setup. On each round the learner chooses a matrix Wt
with Wt
0 and tr(Wt ) = 1, and a matrix of losses Zt
is revealed; Zt is assumed to be symmetric and to satisfy
Zt op ≤ 1, where · op is the operator norm (maximum
singular value). The loss in round t is tr(Wt Zt ). Note
that we can embed the vector setting in the matrix setting
via wt → diag(wt ), zt → diag(zt ), where diag(v) is the
diagonal matrix V with Vii = vi .
To give some intuition, the constraint that tr(W ) = 1
means that W can be written as a convex combination
n
i=1 pi vi vi of unit vectors. The inner product tr(W Z)
n
can then be written as i=1 pi · (vi Zvi ). Thus an equivalent game would be for the learner to (stochastically) pick
a vector v and receive payoff v Zv. Here the stochasticity
of the choices is crucial because v Zv is not convex (since

Proposition 4.1 (Adaptive matrix exponentiated gradient).
For any sequence of matrices Mt , consider the updates
given by B1 = 0 and Bt+1 = Bt − ηZt − η 2 (Zt − Mt )2 ,
exp(Bt −ηMt )
with prediction Wt = tr(exp(Bt −ηMt )) . For 0 < η ≤ 1 ,
4
Zt op ≤ 1, and Mt op ≤ 1, we have
n

Regret(U ) ≤
for all U

log(n)
+η
tr(U (Zt − Mt )2 )
η
i=1

(25)

0 with tr(U ) = 1.

The main additional tool we need is the Golden-Thompson
inequality tr(exp(A+B)) ≤ tr(exp(A) exp(B)) (Golden,
1965; Thompson, 1965). Otherwise, the proof proceeds as
in Proposition 3.3, so we leave the details for the supplementary material.
Path-length and variance bounds. By setting Mt to
Zt−1 as before, we obtain the algorithm AMEG-Path in
Table 1 and achieve the following path-length bound:
T

Regret(U ) ≤

log(n)
tr(U (Zt − Zt−1 )2 ). (26)
+η
η
t=1

We now turn our attention to the variance bound. The
path length bound already implies a variance bound, but
deriving a variance bound directly provides additional insight as well as better constants. Mimicking Rakhlin &
t−1
Sridharan (2012), we would like to set Mt to 1 s=1 Zs
t
and then interpret this choice of Mt as playing Followthe-Regularized-Leader (FTRL) to minimize the sum in
(25). In previous applications this has been straightforward, but here, due to the adaptivity of the regularizer, the
sum (25) is a function of U , which is not known in advance. We address this issue with Lemmas 4.2 and 4.3 below. Lemma 4.3 establishes that there is an optimal value
M ∗ for Mt that is independent of U . Lemma 4.3 provides
a way of attaining the optimum; the lemma is fairly general
and may be useful in obtaining variance bounds for other
adaptive regularizers.
def

1
Lemma 4.2. For any δ ≥ 0, deﬁne M ∗ = T +δ
Then, for any symmetric matrix M , we have
T

δ(M ∗ )2 +

Zt .

T

(Zt − M ∗ )2
t=1

T
t=1

δ(M )2 +

(Zt − M )2 .
t=1

The proof is in the supplementary material. We remark that
the proof is almost purely algebraic, and only relies on the
property that D2 0 for any symmetric matrix D.

Adaptivity and Optimism: An Improved Exponentiated Gradient Algorithm
T
1
¯
Setting δ to 0, we see that Z = T t=1 Zt is the optimal (ﬁxed) value of Mt for any U
0. We now have a
¯
target value Z for the Mt , but we cannot simply apply the
standard FTRL Lemma, since we need a result of the form
T

T

¯
(Zt − Z)2 + αI,

(Zt − Mt )2
t=1

(27)

t=1

which cannot be straightforwardly expressed as a regret
bound (the αI term is meant to be the matrix equivalent
of a small constant α). We deal with this by deriving a generalization of the FTRL algorithm, which we call FTRL-K.
This algorithm has vector-valued losses and obtains regret
relative to a partial ordering deﬁned by a cone K.5
An important notion is that of a global minimizer. For a
function f : X → V where V is a vector space and a cone
K ⊂ V , we say that x is a global minimizer of f relative to
K if f (x) ≤K f (y) for all y ∈ X ; that is, x+K contains the
image of f . Intuitively, K must contain all the directions in
which f can vary relative to f (x).
Lemma 4.3 (FTRL-K). Suppose that for all 1 ≤ t ≤
T + 1, there exists a global minimizer Mt of ψ(M ) +
t−1
s=1 fs (M ). Then for all M ,
T

ft (Mt ) − ft (M ) ≤K ψ(M ) − ψ(M1 )
t =1

(28)

T

ft (Mt ) − ft (Mt+1 ).

+
t=1

Taking ψ(M ) = M 2 , ft (M ) = (Zt − M )2 , and K the
cone of PSD matrices, we obtain the following corollary:
t−1
Corollary 4.4. Suppose that we choose Mt = 1 s=1 Zs .
t
Then, assuming Zt op ≤ 1 for all t, we have
T

T

(Zt − Mt )2
t=1

¯ def
for Z =

1
T

¯
(Zt − Z)2 + 6I,

2

(29)

t=1
T
t=1

Zt .

Both proofs can be found in the supplementary material.
Combining Proposition 4.1 with Corollary 4.4 gives the desired variance bound:
1
Corollary 4.5. For 0 < η ≤ 4 and Zt op ≤ 1, setting
t−1
1
Mt = t s=1 Zs achieves a bound of
T

Regret(U ) ≤

log(n)
¯
+η 2
tr(U (Zt − Z)2 ) + 6 .
η
t=1

We remark that by optimizing the proof of Corollary 4.4,
we can replace the constants 2 with 1 + for any > 0.
5

Recall that for a cone K satisfying K ∩ (−K) = {0}, we
deﬁne the partial order x ≤K y iff y − x ∈ K. Common choices
of K are the positive orthant and the positive semideﬁnite cone.

5. Discussion
We have presented an adaptive exponentiated gradient algorithm, which attains regret bounded by the variance and
path length of the best expert in hindsight. To achieve these
bounds, we relied on the synergy of adaptivity and optimism, allowing us to use “hints” for immediate prediction,
and adaptively performing a second-order correction to the
gradient updates based on the accuracy of the hints. A remaining open problem is to adaptively tune the step size to
achieve asymptotically optimal regret.
Recently, Duchi et al. (2011) proposed AdaGrad, an adaptive subgradient algorithm. A major difference is that they
update their regularizer by a large multiplicative amount
in each round, whereas our regularizer changes by a small
additive second-order term η 2 ut . We also obtain different
regret bounds; at a high level, AdaGrad can be expected to
perform well when the optimal predictor is dense but the
gradient updates are sparse. In contrast, our algorithm will
perform well when the optimal predictor is sparse but the
gradient updates are dense.
Our FTRL-K lemma (Lemma 4.3) is closely related to
Blackwell approachability (Blackwell, 1956); see Perchet
(2013) for a recent survey. As far as we can tell, the conditions in Lemma 4.3 are not equivalent to Blackwell approachability; they are (intuitively) stronger but have the
advantage of offering a potentially tighter analysis, as in
Corollary 4.4. Abernethy et al. (2011) recently provided
a very elegant connection between Blackwell approachability and regret minimization; our algorithm is, however,
different from theirs. We note that the global minimizer criterion is essentially a lower bound on the curvature of the
cumulative regularized loss near its optimum. We could
thus imagine adding to the regularizer term until the criterion held, if necessary.
Finally, we think the general idea of “pushing the regret
into the regularizer”, as in Theorem 3.1 and in earlier work
(Orabona, 2013; Orabona et al., 2013), is quite interesting, as it allows us to obtain regret bounds in terms of the
best expert rather than the learner. It should be the case
T
that any time our regret involves a sum t=1 zt − mt 2 t ,
w
where · wt is a local norm, we can instead obtain a bound
T
on Regret(u) involving t=1 zt − mt 2 , as long as ψ ∗
u
is well-behaved (perhaps having a bounded third derivative). Precisely characterizing these conditions, and obtaining such local norm results for cases beyond the entropy
and von-Neumann (matrix) entropy, is an interesting direction of future work.
Acknowledgments. We thank Paul Christiano and
Jonathan Huggins for helpful discussions, as well as the
anonymous reviewers for providing a thorough review of
the paper and several helpful comments. The ﬁrst author
was supported by the Hertz Foundation.

Adaptivity and Optimism: An Improved Exponentiated Gradient Algorithm

References
Abernethy, Jacob, Bartlett, Peter L, and Hazan, Elad. Blackwell
approachability and no-regret learning are equivalent. JMLR:
Workshop and Conference Proceedings (COLT), 19:27–46,
2011.
Arora, Sanjeev and Kale, Satyen. A combinatorial, primal-dual
approach to semideﬁnite programs. In Proceedings of the
thirty-ninth annual ACM symposium on Theory of computing,
pp. 227–236. ACM, 2007.

Kivinen, Jyrki and Warmuth, Manfred K. Exponentiated gradient
versus gradient descent for linear predictors. Information and
Computation, 132(1):1–63, 1997.
Littlestone, Nick. Learning quickly when irrelevant attributes
abound: A new linear-threshold algorithm. Machine learning,
2(4):285–318, 1988.
Littlestone, Nick and Warmuth, Manfred K. The weighted majority algorithm. In Foundations of Computer Science, 30th
Annual Symposium on, pp. 256–261. IEEE, 1989.

Arora, Sanjeev, Hazan, Elad, and Kale, Satyen. The multiplicative
weights update method: a meta-algorithm and applications.
Theory of Computing, 8(1):121–164, 2012.

McMahan, H Brendan and Streeter, Matthew. Adaptive bound
optimization for online convex optimization. arXiv preprint
arXiv:1002.4908, 2010.

Auer, Peter and Warmuth, Manfred K. Tracking the best disjunction. Machine Learning, 32(2):127–150, 1998.

Orabona, Francesco. Dimension-free exponentiated gradient.
In Advances in Neural Information Processing Systems, pp.
1806–1814, 2013.

Blackwell, David. An analog of the minimax theorem for vector
payoffs. Paciﬁc Journal of Mathematics, 6(1):1–8, 1956.
Cesa-Bianchi, Nicol` , Conconi, Alex, and Gentile, Claudio. A
o
second-order perceptron algorithm. SIAM Journal on Computing, 34(3):640–668, 2005.
Cesa-Bianchi, Nicolo, Mansour, Yishay, and Stoltz, Gilles. Improved second-order bounds for prediction with expert advice.
Machine Learning, 66(2-3):321–352, 2007.
Chiang, Chao-Kai, Yang, Tianbao, Lee, Chia-Jung, Mahdavi,
Mehrdad, Lu, Chi-Jen, Jin, Rong, and Zhu, Shenghuo. Online optimization with gradual variations. Journal of Machine
Learning Research, 2012.

Orabona, Francesco, Crammer, Koby, and Cesa-Bianchi, Nicolo.
A generalized online mirror descent with applications to classiﬁcation and regression. arXiv preprint arXiv:1304.2994, 2013.
Perchet, Vianney. Approachability, regret and calibration; implications and equivalences. arXiv preprint arXiv:1301.2663,
2013.
Rakhlin, Alexander and Sridharan, Karthik. Online learning with
predictable sequences. arXiv preprint arXiv:1208.3728, 2012.
Sabato, Sivan, Shalev-Shwartz, Shai, Srebro, Nathan, Hsu,
Daniel, and Zhang, Tong. Learning sparse low-threshold linear
classiﬁers. arXiv preprint arXiv:1212.3276, 2012.

Crammer, Koby, Kulesza, Alex, and Dredze, Mark. Adaptive regularization of weight vectors. Machine Learning, pp. 1–33,
2009.

Shalev-Shwartz, Shai. Online learning and online convex optimization. Foundations and Trends in Machine Learning, 4(2):
107–194, 2011.

Duchi, John, Hazan, Elad, and Singer, Yoram. Adaptive subgradient methods for online learning and stochastic optimization.
The Journal of Machine Learning Research, pp. 2121–2159,
2011.

Srebro, Nati, Sridharan, Karthik, and Tewari, Ambuj. On the universality of online mirror descent. In Advances in Neural Information Processing Systems, pp. 2645–2653, 2011.

Golden, Sidney. Lower bounds for the helmholtz function. Physical Review, 137(4B):B1127, 1965.
Hazan, Elad. The convex optimization approach to regret minimization. Optimization for machine learning, pp. 287, 2011.
Hazan, Elad and Kale, Satyen. Extracting certainty from uncertainty: Regret bounded by variation in costs. In Proceedings of
the Twenty First Annual Conference on Computational Learning Theory, 2008.
Hazan, Elad, Rakhlin, Alexander, and Bartlett, Peter L. Adaptive
online gradient descent. In Advances in Neural Information
Processing Systems, pp. 65–72, 2007.
Hazan, Elad, Kale, Satyen, and Shalev-Shwartz, Shai. Nearoptimal algorithms for online matrix prediction. arXiv preprint
arXiv:1204.0136, 2012.
Herbster, Mark and Warmuth, Manfred K. Tracking the best expert. Machine Learning, 32(2):151–178, 1998.
Kale, Satyen. Commentary on “online optimization with gradual
variations”. Journal of Machine Learning Research, pp. 6–24,
2012.

Streeter, Matthew and McMahan, H Brendan. Less regret via
online conditioning. arXiv preprint arXiv:1002.4862, 2010.
Thompson, Colin J. Inequality with applications in statistical mechanics. Journal of Mathematical Physics, 6:1812, 1965.
Tsuda, Koji, R¨ tsch, Gunnar, and Warmuth, Manfred K. Matrix
a
exponentiated gradient updates for on-line learning and bregman projection. In Journal of Machine Learning Research, pp.
995–1018, 2005.
Vaits, Nina, Moroshko, Edward, and Crammer, Koby. Secondorder non-stationary online learning for regression. arXiv
preprint arXiv:1303.0140, 2013.
Warmuth, Manfred K and Kuzmin, Dima. Online variance minimization. In Learning Theory, pp. 514–528. Springer, 2006.
Yang, Tianbao, Mahdavi, Mehrdad, Jin, Rong, and Zhu,
Shenghuo. Regret bounded by gradual variation for online convex optimization. Machine Learning, pp. 1–41, 2013.

Adaptivity and Optimism: An Improved Exponentiated Gradient Algorithm

Appendix
This appendix contains several pieces of exposition that were removed from the main text due to space constraints. First,
in Appendix A, we provide several properties of Fenchel conjugates, which we hope will serve as a useful reference. In
Appendix B, we provide proofs for results that were stated without proof in the main text. In Appendix C, we re-prove one
of these results in the vector case for the convenience of readers who do not wish to read through matrix manipulations. In
Appendix D, we generalize the exponentiated gradient results from the simplex case to the unconstrained case. Finally, in
Appendix E, we show how to adaptively control the step size η in our algorithms to obtain regret almost as good as if the
optimal η were known in advance.

A. Properties of Fenchel Conjugates
Throughout this paper we make extensive use of properties of Fenchel conjugates. We provide them here for reference. In
all cases we assume that ψ is a convex function. We assume that the argument w to ψ is constrained to lie in some convex
set S.
A.1. General Properties
def

Deﬁnition. The Fenchel conjugate ψ ∗ (β) of a function ψ(w) is deﬁned as ψ ∗ (β) = supw∈S w β − ψ(w).
Gradient. Let w be the maximizing vector in the preceding deﬁnition. Then w is a subgradient of ψ ∗ at β. If ψ ∗ is
differentiable then
ψ ∗ (β) = arg max w β − ψ(w),

(30)

w∈S

and in particular

ψ ∗ (β) ∈ S for all β.

∗
Translations. For any vector c, deﬁne ψc (w) to be ψ(w) − w c. Then ψc (β) = ψ ∗ (β + c).

A.2. Calculations (vector case)
Simplex. Let S = ∆n and ψ(w) = i=1 wi log(wi ). This choice of ψ is also called the negative entropy (as well as,
n
somewhat confusingly, an entropic regularizer). Then we have ψ ∗ (β) = log( i=1 exp(βi )) and ψ ∗ (β)i = nexp(βi ) j ) .
exp(β
j=1

To see the latter, we note that applying the KKT conditions to w β − ψ(w) implies that the maximizer (and hence the
gradient ψ ∗ (β)) satisﬁes βi = log(wi ) + 1 + λ for some scalar λ, hence wi ∝ exp(βi ), and so ψ ∗ (βi ) = wi =
exp(βi )
∗
exp(βj ) . Computing ψ (β) now only involves evaluating w β − ψ(w) at its maximizing value, yielding (where we
j

deﬁne Zβ as

j

exp(βj ))
n

ψ ∗ (β) =

βi
i=1
n

=
i=1

exp(βi )
exp(βi )
− log(exp(βi )/Zβ )
Zβ
Zβ

(31)

exp(βi )
log(Zβ )
Zβ

(32)

= log(Zβ ),

(33)

which completes the calculation.
Non-negative orthant. If instead S is the non-negative orthant and we now take ψ(w) =
n
ψ ∗ (β) = i=1 exp(βi ) and ψ ∗ (β)i = exp(βi ).

n
i=1

wi log(wi ), we will have

To see this, again apply the KKT conditions to w β − ψ(w), which imply that the maximizing value of w satisﬁes

Adaptivity and Optimism: An Improved Exponentiated Gradient Algorithm

βi = log(wi ), and hence

ψ ∗ (β)i = wi = exp(βi ). Evaluating w β − ψ(w) at this point yields
n

ψ ∗ (β) =

[βi exp(βi ) − βi exp(βi ) + exp(βi )]

(34)

exp(βi ),

(35)

i=1
n

=
i=1

thus completing the calculation.
A.3. Calculations (matrix case)
Trace constrained. Let S = {W | W 0, tr(W ) = 1} and let ψ(W ) = tr(W log(W )). This choice of ψ is called the
exp(B)
von-Neumann entropy. We have ψ ∗ (B) = log(tr(exp(B))) and ψ ∗ (B) = tr(exp(B)) . Note that in this case ψ ∗ (B) is
deﬁned as supW ∈S tr(W B) − tr(W log(W )).
To calculate ψ ∗ , note that the KKT conditions yield B = log(W ) + (1 + λ)I for the maximizing value of W . Thus
exp(B)
W ∝ exp(B) and hence ψ ∗ (B) = W = tr(exp(B)) . Deﬁning ZB to be tr(exp(B)) and plugging back in yields
ψ ∗ (B) = tr(B exp(B))/ZB − tr(exp(B)[B − log(ZB )I])/ZB

(36)

= tr(exp(B)) log(ZB )/ZB

(37)

= log(ZB ),

(38)

which completes the calculations for the trace-constrained case.
Trace unconstrained. Let S = {W | W
and ψ ∗ (B) = exp(B).

0} and let ψ(W ) = tr(W log(W ) − W ). We have ψ ∗ (B) = tr(exp(B))

To calculate ψ ∗ , note that the KKT conditions yield B = log(W ) and hence
in to ψ ∗ yields

ψ ∗ (B) = W = exp(B). Plugging back

ψ ∗ (B) = tr(B exp(B)) − tr(exp(B) log(exp(B)) − exp(B))
= tr(exp(B)),

(39)
(40)

which completes the calculations for the unconstrained case.

B. Deferred Proofs
In this section we prove all results stated in the main text that were deferred to the supplementary material.
Proof of Proposition 2.2. We will construct two sequences (zt )T such that exponentiated gradient with any ﬁxed step
t=1
√
size η will perform poorly (Ω( T )) on at least one of them. Our constructed sequences will involve n = 2 experts. In
T
both sequences, the ﬁrst expert has zt,1 = 0 for all t, and zt,2 will satisfy t=1 zt,2 ≥ 0 to ensure quasi-realizability.
T

Sequence 1. The second expert has loss zt,2 = (−1)t−1 . Then t=1 zt,2 is either 0 or 1 depending on the parity
1
1
of T , and in particular is non-negative. On odd-numbered rounds, wt =
, and on even-numbered rounds,
2
2
wt =

1
1+exp(−η)

1
1+exp(η)

. Assume that η ≤ 1. The total loss (and hence regret) of the learner is then at least
T
2

k=1

1
T
1
−
=
2 1 + exp(η)
2
≥
≥

T
2

1
1
−
2 1 + exp(η)

(41)

1
1
−
2 2 + 2η

(42)

1 T
η.
4 2

(43)

Adaptivity and Optimism: An Improved Exponentiated Gradient Algorithm

So, for any η ≤ 1, there is a quasi-realizable sequence with regret at least 1 T η. Since (41) can be seen to be an
4 2
increasing function of η, we have a lower bound of 1 T min(η, 1). The point is that for large η, the learner will pay
4 2
heavily because it switches around too much.
Sequence 2. On the other hand, we consider the sequence given by zt,2 = 1 for all t. Then wt,2 =

1
1+exp((t−1)η) , which
1
T, η . The point is

1
1
1
for t ≤ η is at least 1+e . Therefore, the regret of the learner on this sequence is at least 1+e min
that for small η, the learner will pay heavily because it can’t decrease the weight on expert 2 fast enough.
√
√
Combining these together, we see that the ﬁrst sequence inﬂicts a regret of Ω( T ) whenever η ≥ 1/ T , whereas the
√
√
T
second sequence inﬂicts a regret of Ω(1/ T ) whenever η ≤ 1/ √ . Since one of these two conditions on η must always
be satisﬁed, one of these sequences will always inﬂict regret Ω(1/ T ), thus proving the proposition.

Proof of Proposition 4.1. As noted in the main text, the proof parallels Proposition 3.3, with the main new tool being the
Golden-Thompson inequality, which says that tr(exp(A + B)) ≤ tr(exp(A) exp(B)) (Golden, 1965; Thompson, 1965).
When ψ(W ) = tr(W log(W )) and W is constrained to have trace 1, we have ψ ∗ (B) = log(tr(exp(B))) and ψ ∗ (B) =
exp(B)
ψ ∗ (Bt − ηMt ) matches Wt as given in the proposition. So, again, we are performing an instance of
tr(exp(B)) , so that
Algorithm 2 and it sufﬁces to check that the condition of Corollary 3.2 is satisﬁed for At = (Zt − Mt )2 . To do so, we use
1
the Golden-Thompson inequality together with the fact that −X − X 2 log(I − X) for − 1 I X
2
2 I. We have
ψ ∗ (Bt − ηZt − η 2 At )
= log(tr(exp(Bt − ηZt − η 2 (Zt − Mt )2 )))
≤ log(tr(exp(Bt − ηMt ) exp(−η(Zt − Mt ) − η 2 (Zt − Mt )2 )))
≤ log(tr(exp(Bt − ηMt )(I − η(Zt − Mt ))))
= log(tr(exp(Bt − ηMt )) − η tr(exp(Bt − ηMt )(Zt − Mt )))
≤ log(tr(exp(Bt − ηMt ))) − η
= ψ ∗ (Bt − ηMt ) − η

tr(exp(Bt − ηMt )(Zt − Mt ))
tr(exp(Bt − ηMt ))

ψ ∗ (Bt − ηMt ), Zt − Mt .
∗

T

This veriﬁes the condition of Corollary 3.2, so that we have a regret bound of ψ (0)+ψ(U ) + η t=1 tr(U At ). Finally,
η
noting that ψ ∗ (0) = log(n), ψ(U ) = tr(U log(U )) ≤ 0, and At = (Zt − Mt )2 completes the proof.
Proof of Lemma 4.2. Write M = M ∗ + D. Then we have
T
2

(Zt − M )2

δ(M ) +

(44)

t =1
T

= δ(M ∗ + D)2 +

(Zt − M ∗ − D)2

(45)

t=1
T

= δ(M ∗ )2 +

T

(Zt − M ∗ )2 + δM ∗ +
t=1

T

(M ∗ − Zt ) D + D δM ∗ +
t=1

(M ∗ − Zt ) + (T + δ)D2

(46)

t=1

T

= δ(M ∗ )2 +

(Zt − M ∗ )2 + (T + δ)D2

(47)

(Zt − M ∗ )2 ,

(48)

t=1
T

δ(M ∗ )2 +
t=1

Adaptivity and Optimism: An Improved Exponentiated Gradient Algorithm

which completes the lemma.
Proof of Lemma 4.3. The proof is structurally identical to the vector case (see Hazan (2011) for a proof of the vector case).
We will prove the lemma by induction on T . Note that the lemma is equivalent to showing that
T

T

ft (Mt+1 ) ≤K ψ(M ) +

ψ(M1 ) +
t=1

ft (M )

(49)

t=1

for all M . In the base case T = 0, we have
ψ(M1 ) ≤K ψ(M ),

(50)

which follows from the fact that M1 is a global minimizer of ψ and hence ψ(M1 ) ≤K ψ(M ) for all M . For the inductive
step, suppose that
T −1

T −1

ft (Mt+1 ) ≤K ψ(M ) +
t=1

ft (M )

(51)

t=1

for all M , and invoke this for the particular choice M = MT +1 . Then we have
T −1

T

ψ(M1 ) +

ft (Mt+1 ) = ψ(M1 ) +
t=1

ft (Mt+1 ) + fT (MT +1 )

(52)

t=1
T −1

≤K ψ(MT +1 ) +

ft (MT +1 ) + fT (MT +1 )

(53)

t=1
T

= ψ(MT +1 ) +

ft (MT +1 )

(54)

t=1
T

≤K ψ(M ) +

ft (M )

(55)

t=1

for all M , where we use the fact that MT +1 is a global minimizer of ψ(M ) +
completes the induction and hence the proof.

T
t=1

ft (M ) for the last inequality. This

1 2
2
Proof of Corollary 4.4. The key tool is the matrix Young’s inequality: AB + BA
γ A + γB for all symmetric A, B
√
√
2
and all γ > 0. (This follows immediately upon expanding (A/ γ − γB)
0.) We then note that, by Lemma 4.2, Mt
obeys Lemma 4.3 with ψ(M ) = M 2 , ft (M ) = (M − Zt )2 , and K the cone of positive semideﬁnite matrices. Therefore:
T

T

¯
(Zt − Mt )2 − (Zt − Z)2

¯
Z2 +

t =1

(Zt − Mt )2 − (Zt − Mt+1 )2

(56)

2
Zt (Mt+1 −Mt ) + (Mt+1 −Mt )Zt + Mt2 −Mt+1

(57)

t=1
T

¯
= Z2 +
t=1

T
2
2
¯
= Z 2 + M1 − MT +1 +

[Zt (Mt+1 − Mt ) + (Mt+1 − Mt )Zt ]

(58)

1
[Zt (Zt − Mt ) + (Zt − Mt )Zt ]
t+1

(59)

t=1
T
2
2
¯
= Z 2 + M1 − MT +1 +
t=1

(since Mt+1 =

1
t+1 Zt

+

t
t+1 Mt )

Adaptivity and Optimism: An Improved Exponentiated Gradient Algorithm
T

I+
t=1

2
Zt
+ γ(Zt − Mt )2
γ(t + 1)2

(60)

T

I+

I
+γ
(Zt − Mt )2 .
γ
t=1

(61)

2
2
(For the second-to-last inequality, note that M1 = 0 and hence M1 − MT +1
T

T

1
1−γ

(Zt − Mt )2 ≤
t=1

0.) Re-arranging yields

1+γ
¯
I+
(Zt − Z)2
γ
t=1

.

(62)

1
Setting γ to 2 gives the desired result. Note that by instead setting γ to 2 , we can replace the constants 2 and 6 by 1 +
6
and for any ≤ 1.

C. Improved Variance Bound
We claimed in Section 3 that we could obtain a regret bound in terms of 2Vi + 6 by using the optimistic prediction based
t−1
on mt = 1 s=1 zs . The following proposition establishes this. Its proof is essentially the same as that of Corollary 4.5,
t
and in fact is implied by Corollary 4.5. The only purpose of this section is to keep proofs accessible to readers who prefer
not to read through algebraic manipulations of matrices.
Proposition C.1. Suppose that we choose mt,i =
have

1
t

t−1
s=1 zs,i

and that zs

∞≤

1. Then for all i and all 0 <

≤ 1 we

T

T

(zt,i − m∗ )2 + 6.
i

(zt,i − mt,i )2 ≤ 2

Proof. Note that mt,i is the minimizer of m2 +
i
have

(63)

t=1

t=1

t−1
s=1 (zs,i

− mi )2 . Therefore, by the FTRL Lemma (Hazan, 2011), we

T

T

(zt,i − mt,i )2 − (zt,i − m∗ )2 ≤ (m∗ )2 +
i
i
t=1

(zt,i − mt,i )2 − (zt,i − mt+1,i )2

(64)

2zt,i (mt+1,i − mt,i ) + m2 − m2
t,i
t+1,i

(65)

t=1
T

= (m∗ )2 +
i
t=1

T

=

(m∗ )2
i

+

m2
1,i

−

m2 +1,i
T

+
t=1

T

≤1+
t=1

2
zt,i (zt,i − mt,i )
t+1

2
zt,i
+ γ(zt,i − mt,i )2
γ(t + 1)2

(66)

(67)

T

≤1+

1
+γ
(zt,i − mt,i )2 .
γ
t=1

(68)

Re-arranging yields
T

(zt,i − mt,i )2 ≤
t=1

Setting γ to

1
2

then yields the desired result.

1
1−γ

T

1+γ
+
(zt,i − m∗ )2
i
γ
t=1

.

(69)

Adaptivity and Optimism: An Improved Exponentiated Gradient Algorithm

D. Bounds for Exponentiated Gradient in the Unconstrained Case
The main text contained an analysis of adaptive versions of the exponentiated gradient and matrix exponentiated gradient
algorithms. However, this analysis was for the case that the weights were constrained to the simplex (or that tr(W ) = 1 in
the case of matrices). In Section 2 we promised to include an analysis of these algorithms in the unconstrained case, and
we do so here. Note that this “unconstrained case” still has the constraint w ≥ 0 (or W 0 for matrices), although this is
not a serious limitation since we can split w into its positive and negative components (see Kivinen & Warmuth (1997) for
details).
The updates and proofs are almost identical. The major difference is in the initialization, where to obtain good bounds we
need to initialize β1,i to − log(n) rather than 0 (in the matrix case, we need to initialize B1 to − log(n)I). The complete
algorithms are shown below:
Exponentiated Gradient:
β1,i = − log(n)

(70)

wt,i = exp(βt,i − ηmt,i )
βt+1,i = βt,i − ηzt,i − η 2 (zt,i − mt,i )2
Matrix Exponentiated Gradient:
B1 = − log(n)I

(71)

Wt = exp(Bt − ηMt )
Bt+1 = Bt − ηZt − η 2 (Zt − Mt )2
We have the following regret bounds in the vector and matrix cases:
Proposition D.1. For zt
achieve the bound

∞≤

Regret(u) ≤
Proposition D.2. For Zt
(71) achieve the bound

op ≤

1, mt

∞≤

1, and 0 < η ≤

1 + (log(n) − 1) u

1

+

η
1, Mt

n
i=1

1
4,

the unconstrained exponentiated gradient updates (70)

ui log(ui )

n

T

(zt,i − mt,i )2 .

ui

+η
i=1

(72)

t=1

1
1, and 0 < η ≤ 4 , the unconstrained matrix exponentiated gradient updates

op ≤

T

Regret(U ) ≤

1 + (log(n)−1) tr(U ) + tr(U log(U ))
+η
tr(U (Zt − Mt )2 ).
η
t=1

(73)

The proofs are basically identical to the proofs of Propositions 3.3 and 4.1, but we include them for completeness.
n

Proof of Proposition D.1. We note that, for ψ(w) = i=1 wi log(wi )−wi and w constrained to be non-negative, ψ ∗ (β) =
n
ψ ∗ (βt − ηmt ) is equal to wt as deﬁned in the proposition. It therefore sufﬁces to check that the
i=1 exp(βi ) and
condition of Corollary 3.2 is satisﬁed with at,i = (zt,i − mt,i )2 . We have
n

ψ ∗ (βt − ηzt − η 2 at ) =

exp(βt,i − ηzt,i − η 2 (zt,i − mt,i )2 )

(74)

exp(βt,i − ηmt,i ) exp(−η(zt,i − mt,i ) − η 2 (zt,i − mt,i )2 )

(75)

exp(βt,i − ηmt,i )(1 − η(zt,i − mt,i ))

(76)

i=1
n

=
i=1
n

≤
i=1

Adaptivity and Optimism: An Improved Exponentiated Gradient Algorithm
n

n

exp(βt,i − ηmt,i ) − η

=
i=1

exp(βt,i − ηmt,i )(zt,i − mt,i )

(77)

i=1

= ψ ∗ (βt − ηmt ) − η ψ ∗ (βt − ηmt ) (zt − mt ).

(78)

The one inequality we made use of was exp(−x − x2 ) ≤ 1 − x for |x|< 1 . This veriﬁes the condition of Corollary 3.2,
2
∗

n

n

yielding a regret bound of ψ (β1 )+ψ(u)−u β1 + η i=1 u at . Finally, we note that ψ ∗ (β1 ) = i=1 exp(− log(n)) = 1,
η
n
ψ(u) − u β1 = i=1 ui log(ui ) + (log(n) − 1)ui , and at,i = (zt,i − mt,i )2 , which completes the proof.
Proof of Proposition D.2. When ψ(W ) = tr(W log(W ) − W ) and W is constrained to be positive semideﬁnite, we have
ψ ∗ (B) = log(tr(exp(B))) and ψ ∗ (B) = exp(B), so that ψ ∗ (Bt − ηMt ) matches Wt as given in the proposition.
So, again, it sufﬁces to check that the condition of Corollary 3.2 is satisﬁed for At = (Zt − Mt )2 . To do so, we need to
make use of the Golden-Thompson inequality tr(exp(A + B)) tr(exp(A) exp(B)) (Golden, 1965; Thompson, 1965),
1
together with the fact that −X − X 2 log(I − X) for − 1 I X
2
2 I. We then have
ψ ∗ (Bt − ηZt − η 2 At ) = tr(exp(Bt − ηZt − η 2 (Zt − Mt )2 ))

(79)
2

2

≤ tr(exp(Bt − ηMt ) exp(−η(Zt − Mt ) − η (Zt − Mt ) ))

(80)

≤ tr(exp(Bt − ηMt )(I − η(Zt − Mt )))

(81)

= tr(exp(Bt − ηMt )) − η tr(exp(Bt − ηMt )(Zt − Mt ))

(82)

∗

= ψ (Bt − ηMt ) − η

∗

ψ (Bt − ηMt ), Zt − Mt .
∗

(83)
T

This veriﬁes the condition of Corollary 3.2, so that we have a regret bound of ψ (B1 )+ψ(U )−tr(B1 U ) + η t=1 tr(U At ).
η
1
Finally, noting that ψ ∗ (B1 ) = tr( n I) = 1, ψ(U ) − tr(B1 U ) = tr(U log(U )) + (log(n) − 1) tr(U ), and At = (Zt − Mt )2
completes the proof.

E. Adaptive Step Size
In this section we show how to obtain an adaptive version of Algorithm 2, which relies on the standard doubling trick. The
adaptive algorithm is given as Algorithm 3. The regret bound of this procedure when applied to learning from experts is
worse than in the non-adaptive case, depending (in the language of Figure 1 and (24)) on maxi Di rather than Di∗ (in other
words, the maximum path length of any expert rather than the path length of the best expert).
The algorithm basically calls Algorithm 2 repeatedly with different step sizes, halving the step size every time the regret
exceeds a certain bound. For this algorithm we require a bound B on the inner product term u zt and a bound C on the
regularizer term in the regret bound. Cesa-Bianchi et al. (2007) proposed an adaptive step size scheme in the learning from
experts setting that does not require knowledge of B. It would be interesting to apply the same ideas here, but we have not
tried to do so, although the exposition given below follows Section 3.1 of the same paper.
The regret of Algorithm 3 is bounded in the following theorem:
Theorem E.1. Let ut ∈ arg minu u
regret of Algorithm 3 is bounded as

t
s=1 zs

and let Qt = ut

Regret ≤ B 1 + log

Q
B

t
s=1

as . Let Q = max B, maxT Qt . Then the
t=1

+ 10

CQ.

(84)

Proof. First note η is monotonically non-increasing across rounds, and decays by a factor of 2 every time it changes. We
can group the rounds based on what value of η was used in that round; in this way, Algorithm 3 is equivalent to running
several sub-algorithms, each of which is an instance of Algorithm 2. The total regret is then bounded above by the sum of
the regrets of these individual algorithms.
Now consider the rounds when η is equal to 2−j
ut−1

t−1
s=1

j+1

as ≤ 4

C
B.

Let tj be the ﬁnal such round. By construction, we must have

B, or else we would have already decreased η by the next factor of 2. Let Regretj denote the regret

Adaptivity and Optimism: An Improved Exponentiated Gradient Algorithm

Algorithm 3 Adaptive Step Size Mirror Descent
Given: convex regularizer ψ, corrections at , hints mt , and β
Let B be any bound on maxT u zt
t=1
Let C be any upper bound on ψ ∗ (β1 ) + ψ(u) − u β1
C
Q, η, t ← B, B , 1
while there are rounds remaining do
βt ← β
C
while Q ≥ η do
2

Choose wt = ψ ∗ (βt − ηt mt )
Observe zt and suffer loss wt zt
2
Update βt+1 = βt − ηt zt − ηt at
t
Let ut ∈ arg minu u
s=1 zs
t
Q ← max(Q, ut
as )
s=1
t←t+1
end while
η←η
2
end while

of the sub-algorithm on this set of rounds. Note that it is bounded above by B plus the regret on all but the last of these
rounds. Then we have
t−1

C
+ ηut−1
as
η
s=1
√
√
= B + 2j CB + 2−j 4j+1 CB
√
= B + 5 · 2j CB

Regretj ≤ B +

≤B+5
Note that

Qtj ≥ 2

CQtj .

(85)
(86)
(87)
(88)

Qtj−1 by construction. Then we have
Regret ≤

Regretj

(89)

j

≤

B+5

CQtj

(90)

j

≤ B 1 + log
as was to be shown.

Q
B

+ 10

CQ,

(91)

