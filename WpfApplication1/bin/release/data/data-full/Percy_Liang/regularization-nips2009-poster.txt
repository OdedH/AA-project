Asymptotically Optimal Regularization for Smooth Parametric Models
Percy Liang

Francis Bach

Guillaume Bouchard

Michael I. Jordan

UC Berkeley

´
INRIA - Ecole Normale Sup´rieure
e

Xerox Research Centre Europe

UC Berkeley

Motivation

Part 1 (oracle regularizer)

Many regularizers used in machine learning:
• Penalize norms of parameter vector (L2 , L1 , block norms, etc.)
• Regularize discriminative model with generative model
• Multi-task learning: shrink related tasks towards each other
• Semi-supervised learning: entropy reg., posterior reg., Gen. Expect. Criteria
Questions:
• Given a regularizer, how well does it perform?
• What is the best regularizer?

Main theorem:

Setup:
Ln (λ) = Ln (0) + L(λ) · n−2 + · · ·

Discriminative model: pθ (y | x)
Generative model: pθ (x, y)
Past asymptotic analysis [Liang & Jordan, ’08]:
If model well-speciﬁed, generative better (provides more stability)
If model mis-speciﬁed, discriminative better (achieves lower risk)

Asymptotic risk (simpliﬁed version):
r (variance reduction)
¨
1 2
L(λ) = λ r
˙
2
def

2

˙ (loss)
θ∞

− λtr(¨)
r

r (regularizer bias)
˙

Oracle regularizer (solve for λ):
λ∗ = argminλ L(λ) =

tr(¨)
r
r 2
˙

∗

L(λ ) =

Theorem formalizes our intuitions:
r: asymptotic misspeciﬁcation
˙
r: extra Fisher information provided by x
¨

2

tr(¨)
r
−2 r 2
˙

(Note that optimal regularization λ∗ could be negative!)

Setup
Loss function:
(z; θ)
model parameters θ ∈ Rd
1
2
Example (linear regression): ((x, y); θ) = 2 (y − θ x)
Regularizer:
λ
regularization parameters λ ∈ Rk
Rn (λ, θ) (e.g., = n r(θ))
1
Example (L2 regularization): r(θ) = 2 θ 2
Training:
i.i.d. ∗
Training data: Z1 , . . . , Zn ∼ p
ˆ =
Estimator: θλ def argmin 1
n

θ n

Zi = (Xi , Yi )
n
i=1

(Zi ; θ) + Rn (λ, θ)

Evaluation:
ˆλ
Expected risk: Ln (λ) def EZ1 ,...,Zn ∼p∗ EZ∼p∗ [ (Z; θn )]
=
Assumptions:
• Loss function is smooth (not necessarily squared loss or includes log p∗ )
• Regularizer Rn is smooth
Coverage of our analysis:
• Included: linear regression, logistic regression; L2 regularization
• Excluded: SVMs; L1 regularization

Outline of approach
Wishful thinking: ﬁnd reg. parameters λ that minimize the expected risk
argminλ Ln (λ)
Part 1:
Problem: Ln (λ) is very complicated, can’t be minimized directly
Solution: minimize Taylor approximation of Ln (λ)
Signiﬁcance: provides insight into loss-regularizer interactions
Part 2:
Problem: Unimplementable since best λ depends on p∗
(through θ∞ = argminθ EZ∼p∗ [ (Z, θ)])
ˆ0 (preliminary unregularized estimate) for θ∞
Solution: plugin θn
Signiﬁcance: get practical algorithm

Example (ridge regression): r(θ) = 1 θ 2
2
Regularizer bias: r = θ∞
˙
⇒
Variance reduction: r = tr(I) = d
¨

Leverage both and analyze in our framework:
Loss (x, y; θ) = − log pθ (y | x) [discriminative]
n
λ
Regularizer Rn (λ, θ) = − n2 i=1 log pθ (x, y) [generative]

∗

Oracle regularizer: λ =

Part 2 (plugin regularizer)

d
θ∞

2

Example: Multitask learning
Setup:
K
Loss (x, y; θ) = k=1 (yk − xk θk )2 (K linear regression tasks)
1
Regularizer: r(θ; Λ) = 2 θ (Λ ⊗ I)θ (shrink similar tasks towards each other)

Oracle regularizer:
λ∗ = f (θ∞ ) [depends on θ∞ , not implementable]
Plugin regularizer:
ˆ
ˆ0
ˆ0
λn = f (θn ) [plug unregularized estimate θn in for θ∞ ]

ˆ
ˆn ˆn
Plugin regularizer: Λn = d · ((Θ0 ) Θ0 )−1 , where Θ = (θ1 , . . . , θK ) ∈ Rd×K
• Intuition: shrink tasks more if they are close according to Θ0
ˆn

Plugin algorithm (motivated by oracle analysis):

Experiment: predict binding aﬃnity of MHC-I molecules
5 tasks, one for each molecule; 20 features

1
ˆ0 = argminθ n n (Zi , θ) [unregularized]
θn
i=1
ˆ0
ˆ
λn = f (θn ) [compute regularization parameter adaptively]
ˆ
ˆ
ˆλn = argminθ 1 n (Zi , θ) + λn r(θ) [plugin]
θn
i=1
n

Analysis:
ˆ
Re-analyze new regularizer λn r(θ) in our framework
ˆ
ˆλn is L(λ∗ ) − f˙ r
Result: expected risk of θn
˙

Example: Stein’s paradox
Question:
Given X1 , . . . , Xn ∼ N (θ∞ , Id×d ) [all independent]
ˆ
ˆn (minimizes L(θn ) = E θn − θ∞ 2 )?
ˆ
What is the best estimator θ
ˆml = X = 1 n Xi Not optimal!
¯
Maximum likelihood: θn
i=1
n
Stein paradox (1961):
ˆjs
James-Stein estimator: θn = (1 −

d−2
¯
)X
¯
n||X||2

ˆjs
ˆml
Surprising result: L(θn ) < L(θn ) for all θ∞ , d ≥ 3
Relationship to our work:
With r(θ) = 1 θ 2 , plugin estimator ⇒ James-Stein estimator
2

• Allow setting K 2 regularization parameters Λ, not just one

Three regularizers:
diag CV: independent regularization
uniform CV: ﬁxed sharing
plugin CV: sharing determined by plugin

17
16

test risk

Regularization is important to prevent overﬁtting (in theory and practice)

Example: Hybrid generative/discriminative learning

Cross-validate regularizer strength

15

"unregularized"
"diag CV"
"uniform CV"
"plugin CV"

14
13
200

300
500
800 1000
number of training points (n)

1500

Conclusion
Summary:
• Minimize Taylor expansion of expected risk ⇒ oracle regularizer
• Yields simple algorithm based on plugin regularizer
Asymptotic analysis:
• Oﬀers a new perspective to risk bounds (more common in machine learning)
• Get exact higher-order term (not just bound)
• Advantage: can compare diﬀerent regularizers

