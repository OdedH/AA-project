Tensor Factorization via Matrix Factorization

arXiv:1501.07320v1 [cs.LG] 29 Jan 2015

Volodymyr Kuleshov∗

Arun Tejasvi Chaganty∗
Department of Computer Science
Stanford University
Stanford, CA 94305

Abstract

knowledge bases [2], topic modeling [3], community
detection [4], learning graphical models [5, 6]. The
last three fall into a class of procedures based on the
method of moments for latent-variable models, which
are notable because they provide guarantees of consistent parameter estimation [7].

Tensor factorization arises in many machine
learning applications, such knowledge base
modeling and parameter estimation in latent
variable models. However, numerical methods for tensor factorization have not reached
the level of maturity of matrix factorization
methods. In this paper, we propose a new
method for CP tensor factorization that uses
random projections to reduce the problem
to simultaneous matrix diagonalization. Our
method is conceptually simple and also applies to non-orthogonal and asymmetric tensors of arbitrary order. We prove that a small
number random projections essentially preserves the spectral information in the tensor, allowing us to remove the dependence
on the eigengap that plagued earlier tensorto-matrix reductions. Experimentally, our
method outperforms existing tensor factorization methods on both simulated data and
two real datasets.

1

However, tensors, unlike matrices, are fraught with difﬁculties: identiﬁability is a delicate issue [8, 9, 10], and
computing Equation 1 is in general NP-hard [11, 12].
In this work, we propose a simple procedure to reduce
the problem of factorizing tensors to that of factorizing matrices. Speciﬁcally, we ﬁrst project the tenˆ
sor T onto a set of random vectors, producing a set
of matrices. Then we simultaneously diagonalize the
matrices, producing an estimate of the factors of the
original tensor. We can optionally reﬁne our estimate
by running the procedure using the estimated factors
rather than random vectors. Our approach applies to
orthogonal, non-orthogonal and asymmetric tensors of
arbitrary order.
From a practical perspective, this approach enables
us to immediately leverage mature algorithms for matrix factorization. Such algorithms often have readily
available implementations that are numerically stable
and highly optimized. In our experiments, we observed
that they contribute to improvements in accuracy and
speed over methods that deal directly with a tensor.

Introduction

Given a tensor T ∈ Rd×d×d of the following form:
k

πi ai ⊗ bi ⊗ ci + noise,

T =

Percy Liang

(1)

i=1

our goal is to estimate the factors ai , bi , ci ∈ Rd and
factor weights π ∈ Rk . In machine learning and statistics, this tensor T typically represents higher-order relationships among variables, and we would like to uncover the salient factors that explain these relationships. This problem of tensor factorization is an important problem rich with applications [1]: modeling
Appearing in Proceedings of the 18th International Conference on Artiﬁcial Intelligence and Statistics (AISTATS)
2015, San Diego, CA, USA. JMLR: W&CP volume 38.
Copyright 2015 by the authors.

From a theoretical perspective, we consider both statistical and optimization aspects of our method. Most
of our results pertain to the former: we provide guarantees on the accuracy of a solution as a function of the
noise (this noise typically comes from the statistical
estimation of T from ﬁnite data) that are comparable
to those of existing methods (Table 1). Algorithms
based on matrix diagonalization have been previously
criticized [7] to be extremely sensitive to noise due to a
dependence on the smallest diﬀerence between eigenvalues (the eigengap). We show that this dependence
can be entirely avoided using just O(log k) tensor projections chosen uniformly at random. Furthermore,
* These authors contributed equally.

Tensor Factorization via Matrix Factorization

our guarantees are independent of the algorithm used
for diagonalizing the projection matrices.
The optimization aspects of our method, on the other
hand, depend on the choice of joint diagonalization
subroutine. Most subroutines enjoy local quadratic
convergence rates [13, 14, 15] and so does our method.
With suﬃciently low noise, global convergence guarantees can be established for some joint diagonalization
algorithms [16]. More importantly, local optima are
not an issue for our method in practice, which is in
sharp contrast to some other approaches, such as expectation maximization (EM).
Finally, we show that our method obtains accuracy
improvements over alternating least squares and the
tensor power method on several synthetic and real
datasets. On a community detection task, we obtain
up to a 15% reduction in error compared to a recently
proposed approach [4], and up to an 8% reduction in
error on a crowdsourcing task [17], matching or outperforming a state-of-the-art EM-based estimator on
three of the four datasets.
Notation Let [n] = {1, . . . , n} denote the ﬁrst n positive integers. Let ei be the indicator vector which is
1 in component i and 0 in all other components. We
use ⊗ to denote the tensor product: if u, v, w ∈ Rd ,
then u ⊗ v ⊗ w ∈ Rd×d×d .1 For a third order tensor
T ∈ Rd×d×d we deﬁne vector and matrix application
as,
d

d

TPM [7]

ui − ui
˜

0

Givens [18]
ALS [19]

πmin

0
polylog(d)
√
d

SD2 [20]

0

This paper

1
d

2

?
√
πmin

+

Conv.
G
G

k/dp−1
πmin

k5
πmin (mini=j |πi −πj |)
2
V
2
2
(1−µ2 )πmin

L
G
L/G

Table 1: Comparison of tensor factorization algorithms (Section 2.1). For a tensor with noise (Equation 1) and allowed incoherence µ, we show an upper
bound on the error in the recovered factors ui − ui 2
˜
and whether the convergence is (L)ocal or (G)lobal.
The factor weights π are assumed to be normalized
( π 1 = 1). V 2 is the 2-norm of the inverse factors U −1 . Our method allows for incoherence with
a sensitivity to noise comparable to existing methods
([20, 7, 19]), and with better empirical performance.
In the orthogonal setting, our algorithm is globally
convergent for suﬃciently small .
For a vector of values π ∈ Rk , we use πmin and πmax to
denote the minimum and maximum absolute values of
the entries, respectively. Finally, we use δij to denote
the indicator function, which equals 1 when i = j and
0 otherwise.

Background

Tijk xi yj zk
i=1 j=1 k=1
d

d

d

Tlmn Xli Ymj Znk ,

T (X, Y, Z)ijk =
l=1 m=1 n=1

for vectors x, y, z ∈ Rd and matrices X, Y, Z ∈
Rd×k . The partial vector application (or projection)
T (I, I, w) of a vector w ∈ Rd returns a d × d matrix:
d
T (I, I, w)ij = k=1 Tijk wk .
We deﬁne the CP decomposition of a tensor T ∈
k
Rd×d×d as T = i=1 πi ai ⊗ bi ⊗ ci , for ai , bi , ci ∈ Rd .
The rank of T is said to be k. When ai = bi = ci = ui
for all i, and the ui ’s are orthogonal, we say T has a
k
symmetric orthogonal factorization, T = i=1 πi u⊗3 .
i
k
Projecting a tensor T = i=1 πi ai ⊗ bi ⊗ ci along w
k
produces a matrix T (I, I, w) = i=1 πi (ci w)ai ⊗ bi .
We use λi = πi (ci w) to refer to the factor weights (or
eigenvalues in the orthogonal setting) of the projected
matrix.
1

µ

2

d

T (x, y, z) =

Method

We will only consider third order tensors for the remainder of this paper, though the approach naturally extends to tensors of arbitrary order.

In this section, we establish the context for tensor factorization, method of moments for estimating latentvariable models, and simultaneous matrix diagonalization.
2.1

Tensor factorization algorithms

Existing tensor factorization methods vary in their
sensitivity to noise in the tensor, their tolerance of
non-orthogonality (as measured by the incoherence µ)
and in their convergence properties (Table 1). The
robust tensor power method (TPM, [7]) is a popular algorithm with theoretical guarantees on global
convergence. A recently-developed coordinate-descent
method for orthogonal tensor factorization based on
Givens rotations [18] is empirically more robust than
the TPM; however it is limited to the full-rank setting
and lacks a sensitivity analysis. A further limitation
of both methods is that they only work for symmetric
orthogonal tensors. Asymmetric non-orthogonal tensors could be handled by preprocessing and whitening,
but this can be a major source of errors in itself [21].
Alternating least squares (ALS) and other gradient-

Volodymyr Kuleshov∗ , Arun Tejasvi Chaganty∗ , Percy Liang

based methods [22] are simple, popular, and apply to
the non-orthogonal setting, but are known to easily
get stuck in local optima [23]. Anandkumar et al. [19]
explicitly show both local and global convergence guarantees for a slight modiﬁcation of the ALS procedure
ˆ
under certain assumptions on the tensor T .
Finally, some authors have also proposed using simultaneous diagonalization for tensor factorization: Lathauwer [23] proposed a reduction, but it requires forming a linear system of size O(d4 ) and is quite complex. Anandkumar et al. [20] performed multiple random projections, but only diagonalized two at a time
(SD2), leading to unstable results; the method also
only applies to orthogonal factors. Anandkumar et al.
[7] brieﬂy remarked that using all the projections at
once was possible but did not pursue it. In contrast,
our method, has comparable bounds to the tensor
power method in the orthogonal setting (conventionally π 1 = 1 is assumed), and the ALS method in the
non-orthogonal setting.
2.2

Parameter estimation in mixture models

Tensor factorization can be used for parameter estimation for a wide range of latent-variable models such
as Gaussian mixture models, topic models, hidden
Markov models, etc. [7]. For illustrative purposes, we
focus on the single topic model [7], deﬁned as follows:
For each of n documents, draw a latent “topic” h ∈ [k]
with probability P[h = i] = πi and three observed
words x1 , x2 , x3 ∈ {e1 , . . . , ed }, which are conditionally independent given h with P[xj = w | h = i] = uiw
for each j ∈ {1, 2, 3}. The parameter estimation task is
to output an estimate of the parameters (π, {ui }k )
i=1
(i)
(i)
(i)
given n documents {(x1 , x2 , x3 }n (importantly,
i=1
the topics are unobserved).
Traditional approaches typically use Expectation
Maximization (EM) to optimize the marginal loglikelihood, but this algorithm often gets stuck in local optima. The method of moments approach is to
cast estimation as tensor factorization: deﬁne the em(i)
(i)
(i)
n
1
ˆ
pirical tensor T = n i=1 x1 ⊗ x2 ⊗ x3 . It can be
k
ˆ
shown that T = i=1 πi ui ⊗ ui ⊗ ui + R (a reﬁnement
of Equation 1), where R ∈ Rd×d×d is the statistical
noise which goes to zero as n → ∞. A tensor factorization scheme that asymptotically recovers estimates of
(π, {ui }k ) therefore provides a consistent estimator
i=1
of the parameters.
2.3

Simultaneous diagonalization

We now brieﬂy review simultaneous matrix diagonalization, the main technical driver in our approach. In
simultaneous diagonalization, we are given a set of

symmetric matrices M1 , . . . , ML ∈ Rd×d (see Section 6
for a reduction from the asymmetric case), where each
matrix can be expressed as
Ml = U Λl U

+ Rl .

(2)

The diagonal matrix Λl ∈ Rk×k and the noise Rl
are individual to each matrix, but the non-singular
transform U ∈ Rd×k is common to all the matrices.
We also deﬁne the full-rank extensions,
¯
U= U

U⊥

Λl
¯
Λl =
0

0
,
0

(3)

where the columns of U ⊥ ∈ Rd−k×d span the orthog¯
onal subspace of U and Λl ∈ Rd×d has been appropri¯¯ ¯
ately padded with zeros. Note that U Λl U = U Λl U .
The goal is to ﬁnd an invertible transform V −1 ∈ Rd×d
such that each V −1 Ml V − is nearly diagonal. We refer to the V −1 as inverse factors. When = 0, this
problem admits a unique solution when there are at
least two matrices [24]. There are a number of objective functions for ﬁnding V [25, 13, 26], but in this
paper, we focus on a popular one that penalizes oﬀdiagonal terms:
L

oﬀ(X −1 Ml X − ),

F (X)
l=1

A2 .
ij

oﬀ(A) =
i=j

(4)
An important setting of this problem, which we refer
to as the orthogonal case, is when we know the true
factors U to be orthogonal. In this case we constrain
our optimization variable X to be orthogonal as well,
i.e. X −1 = X .
In principle, we could just diagonalize one of the matrices, say M1 (assuming its eigenvalues are distinct)
to recover U . However, when > 0, this procedure
is unreliable and simultaneous diagonalization greatly
improves on robustness to noise, as we will witness in
Section 4.
There exist several algorithms for optimizing F (X). In
this paper, we will use the Jacobi method [27, 25] for
the orthogonal case and the QRJ1D algorithm [26] for
the non-orthogonal case. Both techniques are based on
same idea of iteratively constructing X −1 via a product of simple matrices X −1 = BT · · · B2 B1 , where at
each iteration t = 1, . . . , T , we choose Bt to minimize
F (X). Typically, this can be done in closed form.
The Jacobi algorithm for the orthogonal case is a simple adaptation of the Jacobi method for diagonalizing
a single matrix. Each Bt is chosen to be a Givens rotation [27] deﬁned by two of the d axes i < j ∈ [d]:
Bt = (cos θ)(∆ii + ∆jj ) + (sin θ)(∆ij − ∆ji ) for some

Tensor Factorization via Matrix Factorization

angle θ, where ∆ij is a matrix that is 1 in the (i, j)-th
entry and 0 elsewhere. We sweep over all i < j, compute the best angle θ in closed form using the formula
proposed by Cardoso and Souloumiac [25] to obtain
Bt , and then update each Ml by Bt Ml Bt . The above
can be done in O(d3 L) time per sweep.
For the non-orthogonal case, the QRJ1D algorithm is
similar, except that Bt is chosen to be either a lower or
upper unit triangular matrix (Bt = I + a∆ij for some
a and i = j). The optimal value of a that minimizes
F (X) can also be computed in closed form (see [26] for
details). The running time per iteration is the same
as before.

3

Tensor factorization via
simultaneous matrix diagonalization

We now outline our algorithm for symmetric third order tensors. In Section 6, we describe how to generalize our method to arbitrary tensors. Observe that
⊗3
the projection of T =
along a vector w
i πi ui
⊗2
is a matrix T (I, I, w) =
πi (w ui )ui that prei
serves all the information about the factors ui (assuming the πi (w ui )’s are distinct). In principle one
can recover the ui through an eigendecomposition of
T (I, I, w). However, this method is sensitive to noise:
the error ui − ui 2 of an estimated eigenvector ui
˜
˜
depends on the reciprocal of the smallest eigengap
maxj=i 1/|λi − λj | of the projected matrix (recall that
λi = πi (w ui )), which can be large and lead to inaccurate estimates.
Instead, let us obtain the factorization of T from projections along multiple vectors w1 , w2 , · · · , wL . The
projections produce matrices of the form Ml =
⊗2
i λil ui , with λil = πi wl ui ; they have common
eigenvectors, and therefore can be simultaneously diagonalized. The advantage is, as we will show later,
that simultaneous diagonalization is sensitive to the
L
L
2
measure mini=j l=1 (λil − λjl )2 /
l=1 (λil − λjl ) ,
which averages the minimum eigengap across the matrices Ml (here, λil = πi (wl ui )).
A natural question to ask is along which vectors (wl )
should we project? In Section 4 and Section 5 we show
that (a) estimates of the inverse factors (vi ) are a good
choice (when the (vi ) are approximately orthogonal,
they are close to the factors (ui )) and that (b) random vectors do almost as well. This suggests a simple two-step method: (i) ﬁrst, we ﬁnd approximations
of the tensor factors by simultaneously diagonalizing
a small number of random projections of the tensor;
(ii) then we perform another round of simultaneous
diagonalization on projections along inverse of these
approximate factors. Algorithm 1 describes the ap-

Algorithm 1 Two-stage tensor factorization algorithm
Require: T = T + R ∈ Rd×d×d , where T has a CP
k
decomposition T = i=1 πi u⊗3 , L0 ≥ 2
i
Ensure: Estimates of factors, π , u1 , · · · , uk .
˜ ˜
˜
ˆ
1: Deﬁne M(0) ← {T (I, I, wl )}L0 with {wl }L0 are
l=1
l=1
chosen uniformly from the unit sphere S d−1 .
(0)
2: Obtain factors {˜i }k
u
and their inverse
i=1
(0)
{˜i }k from the simultaneous diagonalization of
v
i=1
M(0) .
(0)
ˆ
3: Deﬁne M(1) ← {T (I, I, vi )}k .
˜
i=1
(1)
4: return Factors {˜i }k
u
i=1 and factor weights
{˜i }k from simultaneously diagonalizing M(1) .
π i=1
proach. Its running time is O(k 2 d2 s), where s is the
number of sweeps for the simultaneous diagonalization
algorithm.

4

Perturbation analysis for orthogonal
tensor factorization

In this section, we will focus on the orthogonal setting,
returning to non-orthogonal factors in Section 5. For
ease of exposition, we restrict ourselves to symmetk
ric third-order orthogonal tensors: T = i=1 πi u⊗3 .
i
Here the inverse factors (vi ) are equivalent to the factors (ui ), and we do not distinguish between the two.
The proofs for this section can be found in Appendix
B.
Our sensitivity analysis builds on the perturbation
analysis result for the simultaneous diagonalization of
matrices in Cardoso [28].
Lemma 1 (Cardoso [28]). Let Ml = U Λl U + Rl ,
l ∈ [L], be matrices with common factors U ∈ Rd×k
¯
and diagonal Λl ∈ Rk×k . Let U ∈ Rd×d be a fullrank extension of U with columns u1 , u2 , . . . , ud and
˜
let U ∈ Rd×d be the orthogonal minimizer of the joint
diagonalization objective F (·). Then, for all uj , j ∈
˜
[k], there exists a column uj of U such that
˜
d

uj − uj
˜

2

2
Eij + o( ),

≤

(5)

i=1

where E ∈ Rd×k is
Eij

L
l=1 (λil − λjl )uj Rl ui
L
2
l=1 (λil − λjl )

(6)

when i = j and i ≤ k or j ≤ k. We deﬁne Eij = 0
when i = j and λil = 0 when i > k.
In the tensor factorization setting, we jointly diagonalize projections Ml , l = 1, 2, . . . , L of the noisy

Volodymyr Kuleshov∗ , Arun Tejasvi Chaganty∗ , Percy Liang

tensor T along vectors wl : Ml = T (I, I, wl ) =
k
⊗2
+ R(I, I, wl ), where Rl
i=1 πi (wl ui )ui
R(I, I, wl ) has unit operator norm. Cardoso’s lemma
provides bounds on the accuracy of recovering the ui
via joint diagonalization; in particular, we can further
rewrite Equation 6 in the tensor setting as:
Eij =
where pij

L
l=1 wl
L
l=1 wl

pij rij wl
pij pij wl

(πi ui − πj uj ) and rij

,

(7)

R(ui , uj , I).

Equation 7 tells us that we can control the magnitude of the Eij (and hence the error on recovering the ui ) through appropriate choice of the
projections (wl ).
Ideally, we would like to ensure that the projected eigengap, mini=j wl pij =
mini=j πi (wl ui ) − πj (wl uj ) , is bounded away from
zero for at least one Ml so that the denominator of
Equation 7 does not blow up.

wl − ul 2 = O( ), and let M ∈ Rd×d be constructed
via projection of T along w1 , . . . , wk . Let ui be esti˜
mates of the ui derived from the Ml . Then, for every
ui , there exists a ui such that
˜
ui − ui
˜

Theorem 1 (Tensor factorization with random projections). Let w1 , . . . , wL be i.i.d. Gaussian vectors,
wl ∼ N (0, I), and let the matrices Ml ∈ Rd×d be
constructed via projection of T along w1 , . . . , wL . Let
ui be estimates of the ui derived from the Ml . Let
˜
L ≥ 16 log(2d(k − 1)/δ)2 . Then, with probability at
least 1 − δ, for every ui , there exists a ui such that
˜
ui − ui
˜

where C(δ)

2

≤

2

2 π 1 πmax
C(δ)
+
2
πi
πi

O log(kd)/δ)

d
L

+ o( ),

.

The ﬁrst of the above two terms is the fundamental
ˆ
error in estimating a noisy tensor T ; the second term
is due to the concentration of random projections and
can be made arbitrarily small by increasing L.

≤

π 1 πmax
+ o( ).
2
πi

2

Note that Theorem 1 says that with O(d) random
projections, we can recover the eigenvectors ui with
almost the same precision as if we used approximate
eigenvectors, with high probability. Moreover, as L →
∞, there is no gap between the precision of the two
methods. Theorem 2 on the other hand suggests that
we can tolerate errors on the order of O( ) without signiﬁcantly aﬀecting the error in recovering ui . In prac˜
tice, we ﬁnd that using the plug-in estimates allows us
to improve accuracy with fewer random projections.

5
Random projections The ﬁrst step of Algorithm
1 projects the tensor along random directions. The
form of Equation 7 suggests that the error terms, Eij ,
should concentrate over several projections and we will
show that this is indeed the case. Consequently, the error terms will depend inversely on the mean of wl pij ,
2
2
2
pij 2 = πi + πj > πmin . Our ﬁnal result is as follows:
2

2

Perturbation analysis for
non-orthogonal tensor factorization

We now extend our results to the case when the tensor
T has a non-orthogonal symmetric CP decomposition:
k
T = i=1 πi u⊗3 , where the ui are not orthogonal and
i
k ≤ d. We parameterize the non-orthogonality using
incoherence: µ maxi=j ui uj and the norm of the inverse factor V 2 where V
U −1 . Compared to the
orthogonal setting, our bounds reveal an O
dependence on incoherence when µ ≤
this section are found in Appendix C.

1
2d .

2
V
2
1−µ2

Proofs for

We base our analysis on the perturbation result by
Afsari [24].
Lemma 2 (Afsari [24]). Let Ml = U Λl U + Rl , l ∈
[L], be matrices with common factors U ∈ Rd×k and
¯
diagonal Λl ∈ Rk×k . Let U ∈ Rd×d be a full-rank
¯
extension of U with columns u1 , u2 , . . . , ud and let V =
¯ −1 , with rows v1 , v2 , . . . , vd . Let U ∈ Rd×d be the
˜
U
minimizer of the joint diagonalization objective F (·)
˜
˜
and let V = U −1 .
Then, for all uj , j ∈ [k], there exists a column uj of
˜
˜
U such that
d

Plug-in projections The next step of our algorithm projects the tensor along the approximate factors from step 2. Intuitively, if the wl are close to the
eigenvectors ui , then wl pij = wl (πi ui −πj uj ) ≈ πi δil .
Then for each i = j, there is some projection that ensures that Eij is bounded and does not depend on the
projected eigengap mini=j π(wl ui ) − π(wl uj ) .
Theorem 2 (Tensor factorization with plug-in projections). Let w1 , . . . , wk be approximations of u1 , . . . , uk :

uj − uj
˜

2

2
Eij + o( ),

≤

(8)

i=1

where the entries of E ∈ Rd×k are bounded by
|Eij | ≤

1
1 − ρ2
ij

1
λi

2
2

+

L

1
λj

2
2

L

vi Rl vj λjl +
l=1

vi Rl vj λil
l=1

,

Tensor Factorization via Matrix Factorization

when i = j and Eij = 0 when i = j and λil = 0 when
i > k. Here λi = (λi1 , λi2 , ..., λiL ) ∈ RL and ρij =
λi λj
λi 2 λj

is the modulus of uniqueness, a measure of
2
how ill-conditioned the problem is.
In the orthogonal case, we had a dependence on the
eigengap λi − λj . Now the error crucially depends on
the modulus of uniqueness, ρij . The non-orthogonal
simultaneous diagonalization problem has a unique solution iﬀ |ρij | < 1 for all i = j [24]. In the orthogonal case, ρij = 0. It can be shown that ρij can once
again be controlled by appropriately choosing the projections (wl ).
To get a handle on the diﬃculty of the problem, let us
assume that the vectors ui are incoherent: ui uj ≤ µ
for all i = j. Intuitively, the problem is easy when
µ ≈ 0 and hard when µ ≈ 1. In the results that
1
follow, we require µ ≤ 2d .
Random projections Intuitively, random projections are isotropic and hence we expect the projections
λi and λj to be nearly orthogonal to each other. This
allows us to show that ρij ≤ O(µ), which matches our
intuitions on the diﬃculty of the problem. Our ﬁnal
result is the following:
Theorem 3 (Non-orthogonal tensor factorization
with random projections). Let w1 , . . . , wL be i.i.d. random Gaussian vectors, wl ∼ N (0, I), and let the matrices Ml ∈ Rd×d be constructed via projection of
1
T along w1 , . . . , wL . Assume incoherence µ ≤ 2d
on (ui ): ui uj ≤ µ.

50
1−µ2

Let L0

2

and let

2

L ≥ L0 log(15d(k − 1)/δ) . Then, with probability at
least 1 − δ, for every ui , there exists a ui such that
˜
uj − uj
˜

2

≤O

where C(δ)

π 1 πmax V 2
2
(1 + C(δ))
2
πmin
1 − µ2

log(kd/δ)

d
L

+ o( ),

.

Once again, the error decomposes into a fundamental
recovery error and a concentration term. Note that
the error is sensitive to the smallest factor weight,
πmin . This dependence arises from the sensitivity of
the non-orthogonal factorization method to the λi with
the smallest norm and is unavoidable.
Plug-in projections When using plug-in estimates
for the projections, two obvious choices arise: estimates of the columns of the factors, (ui ), or the rows
of the inverse, (vi ). Using estimates of (ui ) leads to
ρij ≤ O(µ), similar to what we saw with random projections. However, using estimates of (vi ) ensures that
the λi are nearly orthogonal, resulting in ρij ≈ 0! This
leads to estimates that are less sensitive to the incoherence µ.

Theorem 4 (Non-orthogonal tensor factorization
with plug-in projections). Let w1 , . . . , wk be approximations of v1 , . . . , vk : wl − vl 2 ≤ O( ), and let the
matrices Ml ∈ Rd×d be constructed via projection of
T along w1 , . . . , wk . Also assume that the ui are inco1
herent: ui uj ≤ µ ≤ 2d when j = i. Then, for every
uj , there exists a uj such that
˜
uj − uj
˜

6

2

≤O

π 1 πmax
V
2
πmin

3
2

+ o( ).

Asymmetric and higher-order
tensors

In this section, we present simple extensions to the
algorithm to asymmetric and higher order tensors.
Asymmetric tensors We use a reduction to handle asymmetric tensors. Observe that the l-th projection Ml of an asymmetric tensor has the form
Ml = i λi uil vil = U Λl V , for some diagonal (not
necessarily positive) matrix Λl and common U, V , not
necessarily orthogonal. For each Ml , deﬁne another
0
matrix Nl = M Ml
and observe that
0
l

0
Ml

Ml
0

=

1 V
2 U

V
−U

Λl
0

0
−Λl

V
U

V
−U

.

The (Nl ) are symmetric matrices with common (in
general, non-orthogonal) factors. Therefore, they can
be jointly diagonalized and from their components, we
can recover the components of the (Ml ). This reduction does not change the modulus of uniqueness of the
problem: the factor weights remain unchanged.
Higher order tensors Finally, if we have a higher
order (say fourth order) tensor T = i πi ai ⊗bi ⊗ci ⊗di
then we can ﬁrst determine the ai , bi by projecting
into matrices T (I, I, w, u) = i π(w ci )(u di )ai ⊗ bi ,
and then determine the ci , di by projecting along the
ﬁrst two components. Our bounds only depend on the
dimension of the matrices being simultaneously diagonalized, and thus this reduction does not introduce
additional error. Intuitively, we should expect that
additional modes of a tensor should provide more information and thus help estimation, not hurt it. However, note that as the tensor order increases, the noise
in the tensor will presumably increase as well.

7

Convergence properties.

The convergence of our algorithm depends on the
choice of joint diagonalization subroutine. Theoretically, the Jacobi method, the QRJ1D algorithm, and

Volodymyr Kuleshov∗ , Arun Tejasvi Chaganty∗ , Percy Liang
Orthogonal case

0.010
0.008
Error

other algorithms are guaranteed to converge to a local
minimum at a quadratic rate [27, 14, 29]. The question of global convergence is currently open [30, 25].
Empirically though, these algorithms have been found
in the literature to converge reliably to global minima
[27, 25, 30] and to corroborate this claim, we conducted
a series of experiments [16].

0.006
0.004
0.002
0.000
0

10

20

30
40
Number of projections

50

60

Non-orthogonal case

0.05

We ﬁrst examined convergence to global minima in
the orthogonal setting. In 1000 trials of the Jacobi
algorithm on random sets of matrices for various
and d = L = 15 , we found that the objective values formed a Gaussian distribution around (the best
accuracy that can be achieved). Then, on each of
our real crowdsourcing datasets, we ran our algorithm
from 1000 random starting points; in every case, the
algorithm converged to the same solution (unlike EM).
This suggests that our diagonalization algorithm is not
sensitive to local optima. To complement this empirical evidence, we also established that the Jacobi algorithm will converge to the global minimum when
is suﬃciently small and when the algorithm is initialized with the eigendecomposition of a single projection
matrix [16].
We also performed similar experiments in the nonorthogonal setting using the QRJ1D algorithm. Unlike
Jacobi, QRJ1D suﬀers from local optima, which is expected since the general CP decomposition problem is
NP-hard. However, local optima appear to only aﬀect
matrices with bad incoherence values, and in several
real world experiments (see below), non-orthogonal
methods fared better their orthogonal counterparts.

8

Experiments

In the orthogonal setting, we compare our algorithms
(OJD0, which uses random projections, and OJD1
which uses with plug-in) with the tensor power method
(TPM), alternating least squares (ALS), and with the
method of de Lathauwer [23]. In the non-orthogonal
setting, we compare de Lathauwer, alternating least
squares (ALS), non-linear least squares (NLS), and
our non-orthogonal methods (NOJD0 and NOJD1).
Random versus plug-in projections We generk
ated random tensors T = i=1 πu⊗3 + R with Gausi
sian entries in π, R and ui distributed uniformly in
the sphere S d−1 . In Figure 1, we plot the error
k
1
˜
i=1 k ui − ui 2 (averaged over 1000 trials) of using
L random projections (blue line), versus using L random projections followed by plug-in (green line). The
accuracy of random projections tends to a limit that
is immediately achieved by the plug-in projections, as
predicted by our theory. In the orthogonal setting,
plug-in reduces the total number of projected matrices

Error

0.04
0.03
0.02
Random projections
Plug-in projections

0.01
0.00
0

10

20

30
40
Number of projections

50

60

Figure 1: Comparing random vs. plug-in projections
(d = k = 10, ortho = 0.05, nonortho = 0.01)

L required to achieve the limiting error by three-fold
(20 vs. 60 when d = 10). In the non-orthogonal setting, the diﬀerence between the two regimes is much
smaller.
Synthetic accuracy experiments We generated
random tensors for various d, k, using the same procedure as above. We vary and report the average
k
1
error i=1 k ui − ui 2 across 50 trials.
˜
Our method realizes its full potential in the full-rank
non-orthogonal setting, where OJD0 and OJD1 are
up to three times more accurate than alternative methods (Figure 2, top). In the (arguably easier) undercomplete case, our methods do not achieve more than a
10% improvement, and overall, all algorithms fare similarly (Figure 4 in the supplementary material). Alternating least squares displayed very poor performance,
and we omit it from our graphs.
In the full rank setting, there is little diﬀerence in performance between our method and Lathauwer (Figure 2, bottom). In both the full and low-rank cases
(Figure 2, bottom and Figure 5 in the supplementary
material), we consistently outperform the standard approaches, ALS and NLS, by 20–50%. Although we do
not always outperform Lathauwer (a state-of-the-art
method), NOJD0 and NOJD1 are faster and much
simpler to implement.
We also tested our method on estimating the single
topic model from Section 2.2. For d = 50 and k = 10,
over 50 trials in which model parameters were generated uniformly at random in S d−1 , OJD0 and OJD1
obtained error rates of 0.05 and 0.055 respectively, followed by TPM (0.62 error), and Lathauwer (0.65
error).
We refer the readers to the supplementary material for
additional experiments on asymmetric tensors and on

Tensor Factorization via Matrix Factorization

0.16
0.14

Error

0.12
0.10
0.08

Effect of noise on algorithm performance (d=100, k=100)

Table 2: Crowdsourcing experiment results

OJD1
OJD0
TPM
Lathauwer
NOJD

Dataset
TPM
OJD
NOJD
ALS
LATH
MV+EM
Size

0.06
0.04
0.02
0.00

0.001

0.002

0.003

0.004
Noise level

0.005

0.006

0.007

d=50, k=50
0.20

Error

0.15
0.10

NOJD
NOJD1
ALS
Lathauwer
NLS

0.05
0.00

0.0001

0.0002

0.0003

0.0004

0.0005

0.0006

0.0007

0.0008

Noise level

Figure 2: Algorithm performance on full-rank synthetic tensors.
0.10
Accuracy

0.08
0.06
0.04
0.02
0.00
0.01

TPM
OJD

0.02

0.03

0.04
0.05
Recovery ratio

0.06

0.07

Figure 3: Accuracy/recovery tradeoﬀ for community
detection.
algorithm running time.
Community detection in a social network
Next, we use our method to detect communities in a
real Facebook friend network at an American university [31] using a recently developed estimator based
on the method of moments [4]. We reproduce a previously proposed methodology for assessing the performance of this estimator on our Facebook dataset [31]:
ground truth communities are deﬁned by the known
dorm, major, and high school of each student; empirical and true community membership vectors ci , ci
ˆ
are matched using a similarity threshold t > 0; for
a given threshold, we deﬁne the recovery ratio as the
number of true ci to which an empirical ci is matched
ˆ
and we deﬁne the accuracy to be the average 1 norm
distance between ci and all the ci that match to it.
ˆ
See [31] for more details. By varying t > 0, we obtain a tradeoﬀ curve between the recovery ratio and
accuracy (Figure 3). Our OJD1 method determines
the top 10 communities more accurately than TPM;
ﬁnding smaller communities was equally challenging
for both methods.

Web
82.25
82.33
83.49
83.15
83.00
83.68
2665

RTE
88.75
90.00
90.50
88.75
88.75
92.75
800

Birds
87.96
89.81
89.81
88.89
88.89
88.89
106

Dogs
84.01
84.01
84.26
84.26
84.26
83.89
807

Label prediction from crowdsourcing data
Lastly, we use our algorithm to infer the true labels
of data points within several datasets based on crowdsourcing annotations by real workers. We incorporate
our algorithm into a recently proposed estimator based
on the method of moments [17] and evaluate the resulting approach on the same datasets that were used
to validate this estimator (except one, which we could
not obtain). In addition to previously deﬁned methods, we also compare to the expectation maximization algorithm initialized with majority voting by the
workers (MV+EM). We measure the label prediction accuracy. Overall, NOJD1 outperforms all other
tensor-based methods on three out of four datasets
and results in accuracy gains of up to 1.75% (Table
2). Our orthogonal method outperforms the TPM
on every dataset but one, and in two cases even outperforms ALS and Lathauwer, even though they are
not aﬀected by whitening. Most interestingly, on two
datasets, at least one of our methods matches or outperforms the EM estimator, unlike any of the other
tensor methods.

9

Discussion

We have presented a simple and eﬃcient method for
tensor factorization based on three ideas: simultaneous matrix diagonalization, random projections, and
plugin estimates. While simultaneous diagonalization
algorithms for tensor factorization have been proposed
in the past, they have either been computationally too
expensive [23] or numerically unstable [20]. We overcome both these limitations using O(log(k)) random
projections of the tensor. Note that our use of random
projections is atypical: instead of using projections for
dimensionality reduction (e.g. [32]), we use it to reduce
the order of the tensor. Finally, we improve estimates
of the factors retrieved with random projections by
using them as plugin estimates, a common technique
in statistics to improve statistical eﬃciency [33]. Extensive empirical experiments show that our approach
results in a factorization algorithm that is both more
eﬃcient and more accurate than the state-of-the-art.

Volodymyr Kuleshov∗ , Arun Tejasvi Chaganty∗ , Percy Liang

References
[1] T. G. Kolda and B. W. Bader. Tensor decompositions and applications. SIAM review, 51(3):
455–500, 2009.

[14] A. Ziehe, P. Laskov, G. Nolte, and K. M¨ller. A
u
fast algorithm for joint diagonalization with nonorthogonal transformations and its application
to blind source separation. Journal of Machine
Learning Research (JMLR), 5:777–800, 2004.

[2] M. Nickel, V. Tresp, and H. Kriegel. A three-way
model for collective learning on multi-relational
data. In International Conference on Machine
Learning (ICML), pages 809–816, 2011.

[15] R. Vollgraf and K. Obermayer. Quadratic optimization for simultaneous matrix diagonalization.
IEEE Transactions on Signal Processing, 54(9):
3270–3278, 2006.

[3] A. Anandkumar, D. P. Foster, D. Hsu, S. M.
Kakade, and Y. Liu. Two SVDs suﬃce: Spectral decompositions for probabilistic topic modeling and latent Dirichlet allocation. In Advances in
Neural Information Processing Systems (NIPS),
2012.

[16] V. Kuleshov, A. Chaganty, and P. Liang. Simultaneous diagonalization: the asymmetric, low-rank,
and noisy settings. Technical report, arXiv, 2015.

[4] A. Anandkumar, R. Ge, D. Hsu, and S. Kakade. A
tensor spectral approach to learning mixed membership community models. In Conference on
Learning Theory (COLT), pages 867–881, 2013.
[5] Y. Halpern and D. Sontag. Unsupervised learning
of noisy-or Bayesian networks. In Uncertainty in
Artiﬁcial Intelligence (UAI), 2013.
[6] A. Chaganty and P. Liang. Estimating latentvariable graphical models using moments and
likelihoods. In International Conference on Machine Learning (ICML), 2014.
[7] A. Anandkumar, R. Ge, D. Hsu, S. M. Kakade,
and M. Telgarsky. Tensor decompositions for
learning latent variable models. Technical report,
arXiv, 2013.
[8] J. B. Kruskal. Three-way arrays: Rank and
uniqueness of trilinear decompositions, with application to arithmetic complexity and statistics.
Linear Algebra and Applications, 18:95–138, 1977.
[9] d. S. V and L. L. Tensor rank and the IllPosedness of the best Low-Rank approximation
problem. SIAM Journal on Matrix Analysis and
Applications, 30:1084–1127, 2008.
[10] J. Brachat, P. Comon, B. Mourrain, and E. Tsigaridas. Symmetric tensor decomposition. Linear
Algebra and its Applications, 433(11):1851–1872,
2010.
[11] J. Hoastad. Tensor rank is NP-complete. Journal
of Algorithms, 11(4), 1990.
[12] C. J. Hillar and L. Lim. Most tensor problems
are NP-Hard. Journal of the ACM (JACM), 60,
2013.
[13] A. Yeredor. Non-orthogonal joint diagonalization
in the least-squares sense with application in blind
source separation. IEEE Transactions on Signal
Processing, 50(7):1545–1553, 2002.

[17] Y. Zhang, X. Chen, D. Zhou, and M. I. Jordan.
Spectral methods meet EM: A provably optimal
algorithm for crowdsourcing. Technical report,
arXiv, 2014.
[18] U. Shalit and G. Chechik. Coordinate-descent for
learning orthogonal matrices through givens rotations. In International Conference on Machine
Learning (ICML), 2014.
[19] A. Anandkumar, R. Ge, and M. Janzamin. Guaranteed non-orthogonal tensor decomposition via
alternating rank-1 updates. Technical report,
arXiv, 2014.
[20] A. Anandkumar, D. Hsu, and S. M. Kakade. A
method of moments for mixture models and hidden Markov models. In Conference on Learning
Theory (COLT), 2012.
[21] S. A. Joint diagonalization: Is non-orthogonal
always preferable to orthogonal? In Computational Advances in Multi-Sensor Adaptive Processing, pages 305–308, 2009.
[22] P. Comon, X. Luciani, and A. L. D. Almeida. Tensor decompositions, alternating least squares and
other tales. Journal of Chemometrics, 23(7):393–
405, 2009.
[23] L. D. Lathauwer. A link between the canonical
decomposition in multilinear algebra and simultaneous matrix diagonalization. SIAM Journal of
Matrix Analysis and Applications, 28(3):642–666,
2006.
[24] B. Afsari. Sensitivity analysis for the problem
of matrix joint diagonalization. SIAM Journal
on Matrix Analysis and Applications, 30(3):1148–
1171, 2008.
[25] J. Cardoso and A. Souloumiac. Jacobi angles for
simultaneous diagonalization. SIAM Journal on
Matrix Analysis and Applications, 17(1):161–164,
1996.
[26] B. Afsari.
Simple LU and QR based nonorthogonal matrix joint diagonalization. In In-

Tensor Factorization via Matrix Factorization

dependent Component Analysis and Blind Signal
Separation, pages 1–7, 2006.
[27] A. Bunse-Gerstner, R. Byers, and V. Mehrmann.
Numerical methods for simultaneous diagonalization. SIAM Journal on Matrix Analysis and Applications, 14(4):927–949, 1993.
[28] J. Cardoso. Perturbation of joint diagonalizers.
Technical report, T’el’ecom Paris, 1994.
[29] A. Yeredor, A. Ziehe, and K. M¨ller. Approxiu
mate joint diagonalization using a natural gradient approach. Independent Component Analysis
and Blind Signal Separation, 1:86–96, 2004.
[30] L. D. Lathauwer, B. D. Moor, and J. Vandewalle.
Independent component analysis and (simultaneous) third-order tensor diagonalization. Signal
Processing, IEEE Transactions on, 49(10):2262–
2271, 2001.
[31] F. Huang, U. N. Niranjan, M. U. Hakeem, and
A. Anandkumar. Fast detection of overlapping
communities via online tensor methods. Technical
report, arXiv, 2013.
[32] H. N, M. P, and T. J. Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions. SIAM
Review, 53:217–288, 2011.
[33] A. W. van der Vaart. Asymptotic statistics. Cambridge University Press, 1998.
[34] B. Laurent and P. Massart. Adaptive estimation
of a quadratic functional by model selection. Annals of Statistics, 28(5):1302–1338, 2000.

Volodymyr Kuleshov∗ , Arun Tejasvi Chaganty∗ , Percy Liang

0.16
0.14

Error

0.12
0.10
0.08

Effect of noise on algorithm performance (d=100, k=100)
OJD1
OJD0
TPM
Lathauwer
NOJD

0.06
0.04
0.02
0.00

0.25

0.001

0.002

0.003

0.004
Noise level

0.005

0.006

0.007

Effect of noise on algorithm performance (d=50, k=10)

Error

0.20
0.15
0.10
0.05
0.00
0.001

0.002

0.003

0.004
0.005
Noise level

0.006

0.007

Figure 4: Algorithm performance in the orthogonal setting.
d=50, k=50
0.20

Error

0.15
0.10
0.05
0.00

0.0001

0.0002

0.0003

0.0004

0.0005

0.0006

0.0007

Noise level

d=50, k=10

0.10

Error

0.08
0.06
NOJD0
NOJD1
ALS
Lathauwer
NLS

0.04
0.02
0.00

0.0001

0.0002

0.0003

0.0004

0.0005

0.0006

0.0007

Noise level

Figure 5: Algorithm performance in the non-orthogonal setting.

A
A.1

Experiments
Synthetic experiments

Orthogonal tensors We start by generating random tensors T = i πu⊗3 + R with Gaussian entries in
i
π, R and ui distributed uniformly in the unit sphere S d−1 . We let d = 25, 50, 100 and in each case consider two
regimes: undercomplete tensors with k = 0.2d and full rank tensors, k = d. We vary and report the average error
ui − ui 2 across all eigenvectors ui and across 50 trials. In the orthogonal setting, we compare our algorithms
˜
(OJD0 uses random projections, OJD1 is with plugin) with the tensor power method (TPM), alternating least
squares (ALS), and with the method of de Lauthauwer [23]. Alternating least squares displayed very poor
performance, and we omit it from our graphs. In the undercomplete case (Figure 4, right), all algorithms fare
similarly and errors are within 10% of each other. Our method realizes its full potential in the full-rank setting,
where OJD0 and OJD1 are up to three times more accurate than alternative methods ((Figure 4, left).
Non-orthogonal tensors In the non-orthogonal setting, we compare de Lathauwer, alternating least squares
(ALS), non-linear least squares (NLS), and our non-orthogonal methods (NOJD0 and NOJD1). We follow
the same experimental setup as above and summarize our experiments in Figure 5. In the undercomplete setting,
Lathauwer’s algorithm has the highest accuracy, about a 10% more than our approach (Figure 5, right). In the
full rank setting, there is little diﬀerence in performance between our method and Lathauwer’s. In both settings,
we consistently outperform the standard approaches, ALS and NLS, by 20-50% (Figure 5, left). Although we do

Tensor Factorization via Matrix Factorization
Orthogonal tensors

0.10

OJD0
OJD1
Lathauwer

Error

0.08
0.06
0.04
0.02
0.00

0.007

0.006

0.005

0.004
0.003
Noise level

0.002

0.001

Non-orthogonal tensors
NOJD
NOJD1
ALS
Lathauwer

0.20
0.15
0.10
0.05
0.00

0.007

0.006

0.005

0.004
Noise level

0.003

0.002

0.001

Figure 6: Algorithm performance on asymmetric tensors.
Orthogonal case

10

10

OJD1
OJD0
TPM
NOJD

9

Flops

10

8

10

7

10

6

10

5

10

20

30

40

80

90

100

80

90

100

Non-orthogonal case

11

10

ALS
NOJD
NOJD1

10

10
Flops

50
60
70
Problem dimension

9

10

8

10

7

10

6

10

20

30

40

50
60
70
Problem dimension

Figure 7: Number of ﬂops performed by various algorithms.

not always outperform Lauthauwer’s state-of-the-art method, NOJD0 and NOJD1 are faster and much simpler
to implement.
Asymmetric tensors Lastly, we evaluate the extension of our algorithm to tensors of size 50 × 50 × 50 having
three distinct sets of asymmetric components (one in each mode). We ﬁnd that performance is consistent with
the symmetric setting, in both orthogonal and non-orthogonal regimes; our method outperforms is competitors
by at least 25%, and in the non-orthogonal setting, it achieves an error reduction of up to 70% over Lathauer
(Figure 6).
A.2

Algorithm running time

Figure 7 compares the running time in ﬂops of the main algorithms.
We obtain the plots in Figure 7 by calculating ﬂops as follows. The Jacobi method performs at each sweep
2dL(dk − k ) ﬂops (where L is the number of matrices); the QRJ1 non-orthogonal diagonalization algorithm
2
performs 4d3 L ﬂops per sweep. The tensor power method performs a total of Lkd3 ﬂops (where L is the number
of restarts), times the number of steps it takes to reach convergence for a given eigenvector. The ﬂop count of
Lathauwer’s method is much higher than that of other method’s: at one stage, it requires ﬁnding the SVD of a
d4 × k 2 matrix. Consequently, we do not include it in our summary.

Volodymyr Kuleshov∗ , Arun Tejasvi Chaganty∗ , Percy Liang

B

Proofs for orthogonal tensor factorization

In this section we prove perturbation bounds for our algorithm in the setting of orthogonal tensors.
k

Recall that we observe T = T + R where T = i=1 πi u⊗3 where πi are factor weights, ui ∈ Rd are orthogonal
i
unit vectors and R is, without loss of generality, symmetric with R op = 1. Our objective is to estimate π
and (ui ). Algorithm 1 does so by simultaneously diagonalizing a number of projections of T ; we make use of
projections along random vectors and along approximate factors. In this section we will show why both schemes
recover πi and (ui ) with high probability.
Setup Let M = {M1 , . . . , ML } be the projections of T along vectors w1 , . . . , wL , and M = {M1 , . . . , ML } be
d
the projections of T along w1 , . . . , wL . We have that Ml = i=1 πi (wl ui )ui ⊗ ui and that Ml = Ml + Rl , where
Rl = R(I, I, wl ). Thus, Ml are a set of simultaneously diagonalizable matrices with factors U and factor weights
¯
λil πi (wl ui ). From the discussion in Section 2, let U be a full-rank extension of U , with columns u1 , u2 , . . . ud .
Let π and u be a factorization of T returned by Algorithm 1. From Lemma 1, we have that
˜
˜
d

uj − uj
˜

2

2
Eij + o( ),

≤

(9)

i=1

for j ∈ [k] where E ∈ Rd×k has entries
Eij =


0


For notational convenience, let pij

for i = j
L
l=1 (λil −λjl )uj Rl ui
L
2
l=1 (λil −λjl )

(10)

for i = j.

(πi ui − πj uj ) so that λil − λjl = wl pij . Let rij

R(ui , uj , I) so that

uj Rl ui = R(uj , ui , wl ) = R(ui , uj , I) wl = rij wl .
The expression for Eij when j = i simpliﬁes to,
Eij =

L
l=1 wl
L
l=1 wl

pij rij wl
pij pij wl

.

(11)

In the rest of this section, we will bound Eij for diﬀerent choices of {wl }L .
l=1
B.1

Plugin projections

In Section 4 we proposed using approximate factors ui as directions to project the tensor T along. In this section,
˜
we show that doing so guarantees small errors in ui .
We begin by bounding the terms Eij .
Lemma 3 (Eij with plug-in projections). Let w1 , . . . , wk be unit-vectors approximations of the unit vectors
u1 , . . . , uk : wl − ul 2 ≤ γ (so L = k), and let M = {M1 , . . . , ML } be constructed via projection of T along
w1 , . . . , wL . If the set of matrices M is simultaneously diagonalized, then to a ﬁrst-order approximation,
Eij =

pij rij
+ O(γ).
pij 2

Proof. We have that
wl (pij ) = (ul + (wl − ul )) (πi ui − πj uj )
= πi δil − πj δjl + (wl − ul ) (πi ui − πj uj )
≤ πi δil − πj δjl + wl − ul
= πi δil − πj δjl + O(γ),

2

πi ui − πj uj

2

Tensor Factorization via Matrix Factorization

where δij = 1 if i = j and 0 otherwise.
Thus,
Eij =
=

L
l=1 wl pij rij wl
L
l=1 wl pij pij wl
L
l=1 (πi δil − πj δjl + O(γ)) rij wl
L
2
l=1 (πi δil − πj δjl + O(γ))

=

πi rij wi − πj rij wj + O(γ)
2
2
πi + πj + O(γ)

=

πi rij ui + πi (wi − ui ) rij − πj rij uj − πj (wj − uj ) rij + O(γ)
2
2
πi + πj + O(γ)

Note that (wi − ui ) rij = O(γ) and (wj − uj ) rij = O(γ), and hence both can be included in the O(γ) term.
Eij =

rij (πi ui − πj uj ) + O(γ)
.
2
2
πi + πj + O(γ)
2

Finally, recall that pij
(πi ui − πj uj ) and that pij
1
= 1 + x + o(x), we obtain
1−x
Eij =

2
2
= πi + πj . Combining this with the observation that

pij rij
+ O(γ).
pij 2

Next, we use these term-wise bounds to bound the error in ui .
Theorem 5 (Tensor factorization with plugin projections). Let w1 , . . . , wk be approximations of u1 , . . . , uk such
that wl − ul 2 ≤ γ = O( ), and let M = {M1 , . . . , ML } be constructed via projection of T along w1 , . . . , wL .
Then, for j ∈ [k],
uj − uj
˜

2

≤

2

π 1 πmax
2
πi

+ o( ).

Proof. From Equation 9, we have that,
d

uj − uj
˜

2

2
Eij ,

≤
j=1;j=i

for all j ∈ [k]. By Lemma 3, we get,
Eij =

pij rij
+ O( ),
pij 2

and thus,
d

uj − uj
˜

2

≤
i=1;i=j

pij rij
pij 2

2

+ o( ).

Volodymyr Kuleshov∗ , Arun Tejasvi Chaganty∗ , Percy Liang

Now, we must bound

d
2
i=1;i=j (pij rij ) .

We expect this the projection to mostly preserve the norm of pij because

rij are eﬀectively random vectors. Using Lemma 10 with µ = 0, we get that
2
2
2
Finally, pij 2 = πi + πj ≥ πj .
2
uj − uj
˜

2

4 π 1 πmax
2
πj

B.2

+ o( ).

2

≤

≤ 4 π 1 πmax .

+ o( )

π 1 πmax
2
πj

≤

d
2
i=1;i=j (pij rij )

Random projections

Let us now consider the case when {wl }L are random Gaussian vectors and present similar bounds.
l=1
Given Equation 11, we should expect Eij to sharply, and now show that this is indeed the case.
Lemma 4 (Concentration of error Eij ). Let w1 , . . . , wL be i.i.d. random Gaussian vectors wl ∼ N (0, I), and let
M = {M1 , . . . , ML } be constructed via projection of T along w1 , . . . , wL . If the set of matrices M is simultaneously diagonalized, then the ﬁrst-order error Eij is sharply concentrated. If L ≥ 16 log(2δ), then with probability
at least 1 − δ,
Eij ≤

pij rij
10 log(2/δ) rij
√
+
pij 2
pij
L
2

2

.

2

Proof. The numerator and denominator of Equation 11 are both distributed as the sum of χ2 variables; we show
below that they respectively concentrate about pij rij and pij 2 .
2
From Lemma 14, we have that the following hold independently with probability at least 1 − δ/2,
1
L
1
L

L

wl pij rij wl ≤ pij rij + pij

rij

3

l=1
L
2

wl pij pij wl ≥ pij

1−

l=1

log(2/δ)
L

2 log(2/δ)
√
L

Applying a union bound on both these events, we get that with probability at least 1 − δ,
L
l=1 wl pij rij wl
L
2
m=1 wm pij 2

Eij =

pij rij + pij

2

rij

≤
pij

Note that with the given condition on L,
have that

2 log(2/δ)
√
L

1−

3

log(2/δ)
L

2 log(2/δ)
√
L

.

1
< 2 . Using the property that when x ≤ 1 ,
2

1
1−

2
2

2

2 log(2/δ)
√
L

≤1+

4 log(2/δ)
√
.
L

1
1−x

≤ 1 + 2x, we

Tensor Factorization via Matrix Factorization

Consequently,
1
pij

Eij ≤
≤
≤

pij rij + pij

2
2

pij rij
pij 2
2

1+

rij

2

4 log(2/δ)
√
L

rij
pij

+6

pij rij
10 log(2/δ) rij
√
+
pij 2
pij
L
2

2

log(2/δ)
L

3

2

2
2

1+

4 log(2/δ)
√
L

log(2/δ)
L

.

2

With this term-wise bound, we can again proceed to bounding the error ui .
Theorem 6 (Tensor factorization with random projections). Let w1 , . . . , wL be i.i.d. random Gaussian vectors,
wl ∼ N (0, I), and let M = {M1 , . . . , ML } be constructed via projection of T along w1 , . . . , wL . Furthermore, let
L ≥ 16 log(2d(k − 1)/δ)2 , then, with probability at least 1 − δ,
uj − uj
˜

2

2 π 1 πmax
2
πi

2

≤

+

√
20 2 log(2d(k − 1)/δ)

d/L
πi

+ o( ).

for all j ∈ [k].
Proof. From Equation 9, we have that,
d

uj − uj
˜

2

2
Eij + o( ).

≤
i=1;i=j

By Lemma 4, with probability at least 1 − δ/(d(k − 1)),
Eij ≤

|pij rij | 10 log(2d(k − 1)/δ) rij
√
+
pij 2
pij
L
2

2

.

2

Applying a union bound over (Eij )d , we have that with probability at least 1 − δ,
j=i
d

uj − uj
˜

2

≤

2
i=1;i=j

pij rij
pij 2
2

2

+

10 log(2d(k − 1)/δ)
√
L

d

2
i=1;i=j

rij
pij

2
2

+ o( ),

2

for all j ∈ [k]. √ have used the fact that for a, b ≥ 0, (a + b)2 = a2 + 2ab + b2 ≤ a2 + (a2 + b2 ) + b2 = 2a2 + 2b2
We √
√
and a + b ≤ a + b.
Note that pij

2

=

2
2
πi + πj ≥ |πi |. In Lemma 10, we show that

d
2
i=1;i=j (pij rij )

≤ 4 π 1 πmax . Furthermore,

rij ≤ 1 by the operator norm bound on R. Thus, we get,
uj − uj
˜

C

2

≤

2

2 π 1 πmax
2
πi

+

√
20 2 log(2d(k − 1)/δ)

d/L
πi

+ o( ).

Proofs for non-orthogonal tensor factorization

In this section we extend our previous analysis to non-orthogonal tensor decomposition.

Volodymyr Kuleshov∗ , Arun Tejasvi Chaganty∗ , Percy Liang

Setup As before, let M = {M1 , . . . , ML } be the projections of T along vectors w1 , . . . , wL , and M =
d
{M1 , . . . , ML } be the projections of T along w1 , . . . , wL . We have that Ml = i=1 πi (wl ui )ui ⊗ ui and that
Ml = Ml + Rl , where Rl = R(I, I, wl ). Thus, Ml are a set of simultaneously diagonalizable matrices with
¯
factors U and factor weights λil
πi (wl ui ). Let U be the full-rank extension of U with unit-norm columns
¯
¯
u1 , u2 , . . . , ud . In this setting, however, the factor U is not orthogonal. Let V = U −1 , with rows v1 , v2 , . . . , vd .
Note that we place our incoherence assumption on the columns of U and present results in terms of the 2-norm
of V . When U is incoherent, it can be shown that V 2 ≤ 1 + O(µ). Finally, note that in the orthogonal case,
when µ = 0, the rows (vi ) and columns (ui ) are identical, and no distinction between the two need be made.
Let π and u be a factorization of T returned by Algorithm 1. From Lemma 2, we have that
˜
˜
d

uj − uj
˜

2

2
Eij ,

=
i=1

where the entries of E ∈ Rd×k are bounded by Lemma 17:
|Eij | ≤

1
1 − ρ2
ij

1
λi

2
2

+

1
λj

L

L

vi Rl vj λjl +

2
2

vi Rl vj λil

l=1

,

(12)

l=1

where λi ∈ RL is the vector of i-th factor values of Ml , i.e. λil is the i-th factor value of matrix Ml (i.e.
λil = (Λl )ii ) and ρij =

λi λj
λi 2 λj

2

, the modulus of uniqueness, is a measure of the singularity of the problem.

When λil is generated by projections, λil = πi wl ui . Let rij

R(vi , vj , I) so that

vi Rl vj = R(vi , vj , wl ) = R(vi , vj , I) wl = rij wl .
Note that rij

2

≤ vi

2

vj

2

2
2.

≤ V

Equation 12 then simpliﬁes to,
|Eij | ≤
where λi

2
2

2
= πi

L
l=1

1
1 − ρ2
ij

1
λi

2
2

1
λj

+

L

L

wl uj rij wl + |πi |

|πj |

2
2

wl ui rij wl

,

(13)

l=1

l=1

wl ui ui wl , and ρij has the following expression,
ρij =

λi λj
λi 2 λj

L
l=1

=
2

(

L
l=1

wl ui uj wl

wl ui ui wl )(

L
l=1

.

(14)

wl uj uj wl )

Observe that the terms ui interact with the factor weights λil , while the terms vi interact only with the noise
terms Rl .
In the rest of this section, we will bound Eij and ρij with diﬀerent choices of {wl }L .
l=1
C.1

Plugin projections

We now assume we have plugin estimates (wl ) that are close to the inverse factors (vl ): wl − vl
l ∈ [k]. Then,
wl ui = (vl + (wl − vl )) ui
= vl ui + ||wl − vl ||2 ·
= vl ui + O(γ).
Recall that V = U −1 , so vl ui = δil .

(wl − vl ) ui
||wl − vl ||2

2

≤ O(γ) for

Tensor Factorization via Matrix Factorization

It will be useful to keep track of λi 2 ,
2
L

λi

2
2

2
πi (wl ui )2

=
l=1

k
2
= πi

(vl ui + O(γ))2
l=1

2
= πi + O(γ).

(15)

Lemma 5 (Modulus of uniqueness for plugin projections). Let w1 , . . . , wk be approximations of v1 , . . . , vk :
wl − vl 2 ≤ O(γ) for l ∈ [k], and let M = {M1 , . . . , ML } be constructed via projection of T along w1 , . . . , wL .
Then, for i = j,
ρ2 ≤ O(γ),
ij
Proof. Let us ﬁrst bound the numerator of Equation 14.
2

L
2

(λi λj ) =

2 2
πi πj

w l ui uj w l
l=1
2

L

=

2 2
πi πj

vl ui uj vl + O(γ)
l=1

2 2
= πi πj δij + O(γ)

= O(γ).
Using Equation 15, we get that
O(γ)
(1 + O(γ))(1 + O(γ))
= O(γ).

ρ2 =
ij

where in the last line we used the fact that

1
1−x

= 1 + x + o(x).

Lemma 6 (Bound on Eij for non-orthogonal plugin projections). Let w1 , . . . , wk be approximations of v1 , . . . , vk :
wl − vl 2 ≤ O(γ) for l ∈ [k], and let M = {M1 , . . . , ML } be constructed via projection of T along w1 , . . . , wL .
1
1
2 + π2
πi
j

|Eij | ≤
where pij

|πi |

vi
vi

2

+ |πj |

vj
vj

2

V

2

pij rij + O(γ),

.

Proof. Let us bound each term within our expression for Eij (Equation (13)).
k

k

wl uj rij wl =
l=1

vl uj rij vl + O(γ)
l=1

≤ rij vj + O(γ).
Similarly,
k

wl ui rij wl ≤ rij vi + O(γ),
l=1

Volodymyr Kuleshov∗ , Arun Tejasvi Chaganty∗ , Percy Liang

From Equation (15), we have
2
2
λj 2
2

λi

= |πi |2 + O(γ)
= |πj |2 + O(γ).

From Lemma 5 we have that
ρ2 ≤ O(γ)
ij
1
1
≤
+ O(γ)
1 − ρ2
1 − O(γ)
ij
≤ 1 + O(γ).
Finally,
|Eij | ≤

1
1
2 + π2
πi
j

(|πi |vi + |πj |vj ) rij + O(γ)

≤

1
1
2 + π2
πi
j

V

2

pij rij + O(γ).

Note that the error terms depend not on ui but rather vi . This is because the projections (wl ) are chosen to be
close to the vi . Now, let us bound the error in ui .
Theorem 7 (Non-orthogonal tensor factorization with plug-in projections). Let w1 , . . . , wk be approximations
of v1 , . . . , vk : wl − vl 2 ≤ O( ) for l ∈ [k] and let M = {M1 , . . . , ML } be constructed via projection of T along
w1 , . . . , wL . Then, for all j ∈ [k],
uj − uj
˜

2

π 1 πmax
V
2
πmin

≤8

3
2

+ o( ).

Proof. From Lemma 16 we have that
d

uj − uj
˜

2

2
Eij + o( ),

≤
i=1

for j ∈ [k], where Eij is bounded in Lemma 6 as follows:
|Eij | ≤
≤

1
1
2 + π2
πi
j
2
V
2
πmin

V
2

2

pij rij + O( )

pij rij + O( ).

Consequently,
d

uj − uj
˜

2

2
Eij

≤
i=j

≤

2
2
πmin


4 
≤ 2
πmin

d

V

2

pij rij + O( )

2

+ o( )

i=j



d

V
i=j

2

2

pij rij + + o( ),

Tensor Factorization via Matrix Factorization

where we have used the fact that (a + b)2 ≤ 2(a2 + b2 ) and that

2

≤

4
2
πmin

6
2

4 π 1 πmax V
π 1 πmax
V
2
πmin

≤8

C.2

a+b≤

√

a+

√

b.

4
2,

From Lemma 10 we have, pij rij ≤ 4 π 1 πmax V
uj − uj
˜

√

3
2

+ o( )

+ o( ).

Random projections

We now study the case where the random projections, (wl ), are drawn from a standard Gaussian distribution.
First let us show that the modulus of uniqueness ρij sharply concentrates around ui uj .
Lemma 7 (Modulus of Uniqueness with random projections). Let w1 , · · · wL ∈ Rd be entries drawn i.i.d. from
the standard Normal distribution. Let L > 16 log(3/δ)2 Then, with probability at least 1 − δ,
ρij ≤ ui uj +

10 log(3/δ)
√
.
L

Proof. Observe from Equation 14 that the numerator and the denominator of ρij are essentially distributed as
a χ2 distribution (Lemma 14). Thus, with probability at least 1 − δ/3 each, the following hold,
1
L

L

wl ui uj wl ≤ ui uj + ui

1
L
2

L

log(3/δ)
L

3

2

(wl ui )2 ≥ ui

2

1−

2 log(3/δ)
√
L

(wl uj )2 ≥ uj

2

1−

2 log(3/δ)
√
L

l=1
L

l=1

.

= 1 and applying a union bound on the above three events, we get that with

ρij ≤

Under the conditions on L,

uj

l=1

1
L

Noting that ui 2 = uj
probability at least 1 − δ,

2

2 log(3/δ)
√
L

log(3/δ)
L
2 log(3/δ)
√
L

ui uj + 3
1−

.

1
≤ 1 . Applying the property that when x < 2 ,
2

1
1−

2 log(3/δ)
√
L

≤1+

4 log(3/δ)
√
< 2.
L

Finally,
ρij ≤

ui uj + 3

log(3/δ)
L

4 log(3/δ)
√
L
10 log(3/δ)
√
≤ ui uj +
.
L

≤ ui uj 1 +

1+
+3

4 log(3/δ)
√
L
log(3/δ)
×2
L

1
1−x

≤ 1 + 2x,

Volodymyr Kuleshov∗ , Arun Tejasvi Chaganty∗ , Percy Liang

Let’s now bound the inverse modulus of uniqueness.
Lemma 8 (Bounding inverse modulus of uniqueness). Let w1 , · · · wL ∈ Rd be entries drawn i.i.d. from the
standard Normal distribution. Assume incoherence µ for that the (ui ): ui uj ≤ µ for i = j. Let L0

50
(1−µ2 )

2

Let L ≥ L0 log(3/δ)2 . Then, with probability at least 1 − δ,
1
1
2 ≤
1 − ρij
1 − (ui uj )2

L0
log(3/δ) .
L

1+

Proof. From Lemma 7, we have that with probability at least 1 − δ,
ρij ≤ ui uj +

10 log(3/δ)
√
.
L

Then,
ρ2 ≤ (ui uj )2 + 2ui uj
ij

10 log(3/δ)
√
L

10 log(3/δ)
√
L

+

2

.

Given the assumptions on L, we have that L ≥ L0 log(3/δ)2 ≥ 50 log(3/δ)2 and thus
10 log(3/δ)
√
L
25 log(3/δ)
√
= (ui uj )2 +
.
L

ρ2 ≤ (ui uj )2 + 2
ij

Now, we bound

+

10 log(3/δ)
√
L

≤ 1:
2

1 10 log(3/δ)
√
2
L

1
,
1−ρ2
ij

1
1
≤
2−
1 − ρ2
1 − (ui uj )
ij

25 log(3/δ)
√
L

≤
≤

1
2

1
1 − (ui uj )2 1 −

≤

Again, given assumptions on L,

1
1 − (ui uj )2 1 −

1
1 − (ui uj )2 1 −

log(3/δ)

L0
L

1
25 log(3/δ)
√
(1−(ui uj )2 ) L

1
25 log(3/δ)
√
(1−µ2 ) L

1
1
2

log(3/δ)

.
L0
L

1
≤ 1 . Using the identity that if x < 2 ,
2

1
1
≤
1 − ρ2
1 − (ui uj )2
ij

1 + log(3/δ)

L0
L

1
1−x

≤ 1 + 2x,

.

We are now ready to bound the termwise entries of E.
Lemma 9 (Concentration of Eij ). Let w1 , . . . , wL be i.i.d. random Gaussian vectors wl ∼ N (0, I), and let
M = {M1 , . . . , ML } be constructed via projection of T along w1 , . . . , wL . Assume incoherence µ for that the
(ui ): ui uj ≤ µ for i = j. Furthermore, let L ≥ L0 log(15/δ)2 . Then, with probability at least 1 − δ,
√
pij rij
¯
1
1
πij rij 2 20 + L0 log(15/δ)
¯
√
|Eij | ≤
+ 2
+
,
2
πi
πj
1 − (ui uj )2
1 − (ui uj )2
L
where pij
¯

|πi |ui + |πj |uj and πij
¯

|πi | + |πj |.

Tensor Factorization via Matrix Factorization

Proof. Each term in Equation 13 concentrates sharply about its mean value. We bound each in turn.
First, consider λi 2 /L =
2

L
l=1 (wl

1
2
L |πi |

ui )2 . With probability at least 1 − δ/5 each, the following hold,

1
λi
L
1
λj
L
Thus, using the fact that ui

2
2

2
2

2
≥ πi ui

2
2

2
2

2
≥ πj uj

2
2

1
λi

1
+
λj

2
2

2 log(5/δ)
√
L

Given our assumption on L, it follows that
x ≤ 1 to obtain the following bound:
2
1
λi

L
1
L

L
l=1

2
2

1
L
Note that by deﬁnition, ui

2

1
λj

+

wl ui rij wl and
1
L

.

= 1,
L

Next, we bound
1 − δ/5 each,

2 log(5/δ)
√
L
2 log(5/δ)
√
1−
L

1−

≤

2
2
L
l=1

1
L

2
2

≤

1
2
πi

≤
1
2.

+

1
2
πj

2 log(5/δ)
√
L

1−

.

Thus we can use the fact that

1
1
2 + π2
πi
j

4 log(5/δ)
√
L

1+

wl uj rij wl . From Lemma 14, we have with probability at least

L

wl uj rij wl ≤ rij uj + rij

2

uj

2

3

log(5/δ)
L

wl ui rij wl ≤ rij ui + rij

2

ui

2

3

log(5/δ)
L

l=1
L

l=1

.

= 1.

1
1
≤
1 − ρ2
1 − (ui uj )2
ij

1+

L0
log(15/δ) .
L

Putting it all together, we get that with probability at least 1 − δ,
1
1 − (ui uj )2

L0
log(15/δ)
L

1+

1
1
2 + π2
πi
j

|πi |rij ui + |πj |rij uj + (|πi | + |πj |) rij
Let us deﬁne pij
¯

|πi |ui + |πj |uj and πij
¯
|Eij | ≤

1
1 − (ui uj )2

pij rij + πij rij
¯
¯

Given that L ≥ L0 log(15/δ)2 , we have that
1+

2

3

1+

4 log(5/δ)
√
L

log(5/δ)
L

.

|πi | + |πj |:

1+

2

≤ 1 + 2x when

.

Using Lemma 8, we have that with probability at least 1 − δ/5,

|Eij | ≤

1
1−x

L0
log(15/δ)
L
3

L0
L

log(5/δ)
L

1
1
2 + π2
πi
j

1+

4 log(5/δ)
√
L

.

log(15/δ) ≤ 1 and

L0
log(15/δ)
L

1+

4 log(5/δ)
√
L

4 log(5/δ)
√
L

≤ 1, thus

≤2×2
≤ 4.

Volodymyr Kuleshov∗ , Arun Tejasvi Chaganty∗ , Percy Liang

Finally, note that |πi |rij ui + |πj |rij uj ≤ (|πi | + |πj |) rij
1
1
2 + π2
πi
j

|Eij | ≤
+

1
1
2 + π2
πi
j
1
1
2 + π2
πi
j

≤

2,

giving us,

pij rij
¯
1 − (ui uj )2
πij rij 2
¯
1 − (ui uj )2

L0
log(5/δ)
4 log(5/δ)
+4 3
log(15/δ) + 2 √
L
L
L
√
pij rij
¯
πij rij 2 20 + L0 log(15/δ)
¯
√
+
.
2
1 − (ui uj )
1 − (ui uj )2
L

Finally, we bound the error in estimating uj .
Theorem 8 (Non-orthogonal tensor factorization with random projections). Let w1 , . . . , wL be i.i.d. random
Gaussian vectors, wl ∼ N (0, I), and let M = {M1 , . . . , ML } be constructed via projection of T along w1 , . . . , wL .
Assume incoherence µ ≤

1
2d
2

for both (ui ) and (vi ): ui uj ≤ µ and vi vj ≤ µ for i = j. Let L0

L ≥ L0 log(15d(k − 1)/δ) . Then, with probability at least 1 − δ and for

uj − uj
˜

where C(δ)

√
20+ L0
√
L

2

π 1 πmax
V
2
πmin

8
1 − µ2

≤

2
2

50
1−µ2

2

. Let

small enough,

√
1 + C(δ) d ,

log(15(d(k − 1))/δ).

Proof. From Lemma 16 we have that
d

uj − uj
˜

2

2
Eij + o( ),

≤
i=1

for j ∈ [k].
Using Lemma 9, we have that with probability at least 1 − δ/(d(k − 1)),

|Eij | ≤
≤
≤

1
1
2 + π2
πi
j
2

pij rij
¯
1 − (ui uj )2

2
πmin

1
1 − µ2

2
2
πmin

1
1 − µ2

where we have deﬁned C(δ)
|πi | + |πj | ≤ 2|πmax |.

√
20+ L0
√
L

+

pij rij + 2|πmin | V
¯
pij rij + 2|πmin | V
¯

2
2

√

L0 log(15(d(k − 1))/δ)
√
L
√
20 + L0 log(15(d(k − 1))/δ)
√
L

πij rij 2 20 +
¯
1 − (ui uj )2

2
2 C(δ)

,

log(15(d(k − 1))/δ) and are using the fact that ui uj ≤ µ and πij =
¯

Tensor Factorization via Matrix Factorization

Applying a union bound on all the entries of Eij , we arrive at the following bound for all j.
uj − uj
˜

2

2
Eij

≤
i=j


2
≤ 2
πmin (1 − µ2 )

2

d

2 C(δ)
2

pij rij + 2πmax V
¯


i=j


4

≤ 2
πmin (1 − µ2 )

4

≤ 2
πmin (1 − µ2 )

d



d

(¯ij rij )2 + 2πmax V
p

2
2 C(δ)

i=j

1
i=j

d

(¯ij rij )2 + 2πmax V
p


√
2

2 C(δ) d .

i=j

where we use the fact that (a + b)2 ≤ 2(a2 + b2 ).
d
2
p
j=i (¯ij rij )

By Lemma 10 we also have,
uj − uj
˜

2

≤
≤

D

4
2
πmin (1

−

4
2.

≤ 4 π 1 πmax V

2
2

4 π 1 πmax V

µ2 )

π 1 πmax
V
2
πmin

8
1 − µ2

Finally, note that πmax ≤

2
2

πmax π 1 :

√

+ 2πmax V

2
2 C(δ)

d

√
1 + C(δ) d .

Proofs of auxiliary lemmas

In this section, we prove some auxiliary results that appear as intermediate steps in the main lemmas above.
Lemma 10 (Bounding pij rij ). Let pij πi ui − πj uj ∈ Rd and rij R(vi , vj , I) ∈ Rd , where R is a tensor with
unit operator norm and where (ui ) ∈ Rd are unit vectors and (vi ) ∈ Rd form the columns of the matrix V with
bounded 2 norm. Then,
d

(pij rij )2 ≤ 4πmax π

1

4
2.

V

i=j

Proof. Firstly, note that it is trivial to bound the sum as follows,
d

d

(pij rij )2 ≤
i=j

pij

2
2

rij

2
2

i=j
2
≤ 4(d − 1)πmax V

using the properties that pij
rij 2 = R(vi , vj , I) 2 ≤ V

4
2,

πi ui − πj uj and that R has unit operator norm and thus pij

2

≤ 2πmax and

2
2.

However, we would like a tighter bound with a lower-order dependence on k. To do so, let us expand pij ,
d

d

(pij rij )2 =
i=j

((πi ui − πj uj ) rij )2
i=j
d

(πi R(vi , vj , ui ) − πj R(vi , vj , uj ))2

=
i=j
d

d
2
πj R(vi , vj , uj )2 +

=
i=j

d
2
πi R(vi , vj , ui )2 −

i=j

2πi πj R(vi , vj , ui )R(vi , vj , uj ).
i=j

Volodymyr Kuleshov∗ , Arun Tejasvi Chaganty∗ , Percy Liang

Using the assumption that R has unit norm, the latter two terms can be bounded by π
respectively.

2
2

V

4
2

and 2 πj π

1

V

4
2

d

2
˜
We now focus on the ﬁrst term, πj i=j R(vi , vj , uj )2 . Note that R(vi , vj , uj ) = R(I, vj , uj ) vi = rj vi , where
rj R(I, vj , uj ) and rj 2 ≤ V 2 by the operator norm condition on R.
˜
˜
d

(˜j vi )2 = V rj
r
˜

2
2

i=1

rj
˜

= V

2
2
4
2

2
2

V

4
2

≤ V

2
2

Put together, we get that,
d
2
(pij rij )2 ≤ πj V

4
2

+ π

+ 2πi π

1

V

4
2.

i=j
2
Finally, πi ≤ πmax π

1

and, by H¨lder’s inequality, π
o

2
2

≤ πmax π 1 , giving us,

d

(pij rij )2 ≤ 4πmax π

4
2.

V

1

i=j

E

Incoherence lemmas

In this section, we prove a couple of useful facts when dealing with incoherent vectors.
Lemma 11 (Projection on incoherent vectors). Let u1 , · · · , uk ∈ Rd be a set of incoherent unit vectors: |ui uj | ≤
µ for i = j. If µ ≤ 1/(k − 1), then, for any vector r ∈ Rd ,
k

|r ul | ≤
l=1

1 + (k − 1)µ √
1 − (k − 1)µ

k

k r

(r ul )2 ≤

2
l=1

(1 + (k − 1)µ)2
r 2.
2
1 − (k − 1)µ

Furthermore, if µ ≤ 1/2(k − 1), then the bounds simplify to,
k

√
|r ul | ≤ (1 + (k − 1)µ) 2k r

k

(r ul )2 ≤ (1 + 7(k − 1)µ) r 2 .
2

2

l=1

l=1
k

Proof. Without loss of generality, let r 2 = 1. Let r = l=1 ρl ul + ρ⊥ u⊥ , where u⊥ is a unit vector orthogonal
to u1 , · · · , uL . Let ρ = (ρ1 , · · · , ρk ) ∈ Rk be the vector of coeﬃcients.
Firstly, note that
2

k

r

2
2

=

ρl ul + ρ⊥ u⊥
l=1
k

k

k

ρ2 +
l

=
l=1

ρl ρl ul ul + ρ2
⊥
l=1 l =l
k

k

l=1

|ρl ρl |
l=1 l =l

k

≥ (1 + µ)
≥ (1 + µ)

k

ρ2 − µ
l

≥ (1 + µ)

2

k

ρ2
l
l=1
ρ 2−
2

−µ

|ρl |
l=1

µ ρ 2.
1

Tensor Factorization via Matrix Factorization

Next, we use the fact that ρ
√
k ρ 2.

2
2

2
1

≤ ρ

2
2

≤ k ρ

to get the bounds ρ

2

≤ r 2/

1 − (k − 1)µ and ρ

Now,
k

k

k

|r ul | ≤
l=1

|ρl ul ul |
l=1 l =1
k

k

≤ (1 − µ)

k

|ρl | + µ
l =1

≤ (1 − µ) ρ

|ρl |
l=1 l =1

+ µk ρ 1
1 + (k − 1)µ √
k r 2.
≤
1 − (k − 1)µ
1

In the 2-norm case,
k

2

k

k

l=1

l =1

2

(r ul ) =
l=1

ρl ul ul
k
2

≤

((1 − µ)|ρl | + µ ρ 1 )
l=1
k

= (1 − µ)2
= (1 − µ)

2

k

ρ2 + 2(1 − µ)µ ρ
l
l=1
ρ 2
2
2

1
l=1

+ 2(1 − µ)µ

ρ 2+
1
2 2

≤ (1 − µ) + 2k(1 − µ)µ + k µ
2

≤ (1 − µ + kµ)

≤ (1 + (k − 1)µ)
≤

k

|ρl | + µ2

ρ
2

l=1
2

kµ ρ
ρ

ρ

2
1

2
2

2
2

ρ

2
2

(1 + (k − 1)µ)2
r 2.
2
1 − (k − 1)µ

Let us simplify these expressions when µ ≤ 1/2(k − 1).
k

|r ul | ≤

1 + (k − 1)µ √

k r 2
1 − (k − 1)µ
1 + (k − 1)µ √
≤
k r 2
1/2
√
≤ (1 + (k − 1)µ) 2k r 2 .

l=1

Finally,
k

(r ul )2 ≤
l=1

=
=
≤

(1 + (k − 1)µ)2
r
1 − (k − 1)µ

2
2

(1 + (k − 1)µ)2 − (1 − (k − 1)µ)
1 − (k − 1)µ
3(k − 1)µ + (k − 1)2 µ2
1+
r 2
2
1 − (k − 1)µ
1+

1+

1
3(k − 1)µ + 2 (k − 1)µ
1
2

≤ (1 + 7(k − 1)µ) r 2 .
2

r

2
2

r

2
2

2
1

1

≤

Volodymyr Kuleshov∗ , Arun Tejasvi Chaganty∗ , Percy Liang

F

Concentration Inequalities

In this section, we present several concentration results that are key to our results. The χ2 tail bounds presented
in Laurent and Massart [34] play a key role and are reproduced below.
Lemma 12 (χ2 tail inequality). Let q ∼ χ2 be distributed as a chi-squared variable with k degrees of freedom.
k
k
Then, for any t > 0,
√
P(q − k > 2 kt + 2t) ≤ e−t
√
P(k − q > 2 kt) ≤ e−t .
Alternatively, we have that with probability at least 1 − δ,
q ≥k 1−

2 log(1/δ)
√
k

.

(16)

and similarly, with probability at least 1 − δ,
q ≤k 1+2

log(1/δ) 2 log(1/δ)
+
k
k

.

(17)

Proof. See Laurent and Massart [34, Lemma 1].
Lemma 13 (Gaussian quadratic forms). Let x ∼ N (0, I) ∈ Rd be a random Gaussian vector. If A is symmetric,
d
x Ax is distributed as the sum of d independent χ2 variables, i=1 λi (A)χ2 , where λi are the eigenvalues of A.
1
d

Proof. Let A = i=1 λi ui ui be the eigendecomposition of A. Then, x Ax =
d
is distributed as independent χ2 random variables. Thus, x Ax = i=1 λi χ2 .
1
1

d
i=1

λi ui xi 2 . However, ui xi

Lemma 14 (Gaussian products). Let xi ∼ N (0, I) ∈ Rd for i = 1, . . . , L be random Gaussian vectors. Let
L ≥ 4 log(1/δ). Then,
1.

L
i=1 (xi

a)2 where a ∈ Rd is distributed as a 2 χ2 . Consequently, with probability at least 1 − δ,
2 L
1
L

L

2.

L
i=1

2
2

1+2

log(1/δ) 2 log(1/δ)
+
L
L

≤ a
1
L

(xi a)2 ≤ a

2
2

1+3

log(1/δ)
L

(xi a)2 ≥ a

2
2

1−

i=1

L

i=1

2 log(1/δ)
√
L

.

xi ab xi a, b ∈ Rd and a = b is sharply concentrated around a b: with probability at least 1 − δ,
1
L

L

xi ab xi ≤ a b + a

2

b

2

2

log(1/δ) 2 log(1/δ)
+
L
L

≤a b+ a

2

b

2

3

log(1/δ)
L

i=1

.

Tensor Factorization via Matrix Factorization

Proof. The ﬁrst part follows directly from Lemma 13 and the χ2 tail bound, Lemma 12.
For the second part, let A = ab +ba . Note that xi ab xi = xi Axi . Then, by Lemma 13, xi Axi = λ1 χ2 +λ2 χ2 ,
1
1
2
ab +ba
, one of λ1 or λ2 is negative, and
where λ1 and λ2 are the eigenvalues of A. Furthermore, because A =
2
the other is positive. Without loss of generality, let λ1 > 0 > λ2 .
Applying the χ2 tail bound, Lemma 12, we get that with probability at least 1 − δ,
log(2/δ)
log(2/δ)
+2
)
L
L
2 log(2/δ)
√
|λ2 |χ2 ≥ |λ2 |(1 −
).
1
L
λ1 χ2 ≤ λ1 (1 + 2
1

Applying a union bound, we get,
1
L

L

xi ab xi ≤ λ1 (1 + 2
i=1

log(2/δ)
log(2/δ)
2 log(2/δ)
√
+2
) + λ2 (1 −
)
L
L
L
log(2/δ) 2 log(2/δ)
+
L
L

≤ (λ1 + λ2 ) + |λ1 | 2

≤ (λ1 + λ2 ) + (|λ1 | + |λ2 |) 2

G

L

xi ab xi ≤ a b + a

2

b

2

2

i=1

2 log(2/δ)
√
L

log(2/δ) 2 log(2/δ)
+
L
L

Observe that λ1 + λ2 = tr(A) = a b. Similarly, |λ1 | + |λ2 | = A
with probability at least 1 − δ,
1
L

+ |λ2 |

∗

= 2( 1 a
2

2

.

b 2 ). Thus, we ﬁnally have that

log(2/δ) 2 log(2/δ)
+
L
L

.

Perturbation bounds for joint diagonalization

In this section, we present minor extensions to the perturbation bounds of Cardoso [28] and Afsari [24] so that
they apply in the low-rank setting.
Notation Let Ml = U Λl U + Rl for l = 1, 2, . . . , L be a set of d × d matrices to be jointly diagonalized.
Λl ∈ Rk×k is a diagonal matrix, Rl ∈ Rd×d is an arbitrary unit operator norm matrix and is a scalar. In the
orthogonal setting, U ∈ Rd×k is orthogonal, while in the non-orthogonal setting U ∈ Rd×k is an arbitrary matrix
with unit operator norm. Let λil
(Λl )i be the i-th factor weight of matrix Ml . Finally, we say that a set of
matrices {M1 , · · · , ML }, Ml =

d
i=1

T
λil ui vi has joint rank k if {i |

L
l=1

|λil | > 0} = k.

Lemma 15 (Cardoso [28]). Let Ml = U Λl U + Rl , l ∈ [L], be matrices with common factors U ∈ Rd×k and
¯
˜
diagonal Λl ∈ Rk×k . Let U ∈ Rd×d be a full-rank extension of U with columns u1 , u2 , . . . , ud and let U ∈ Rd×d
be the orthogonal minimizer of the joint diagonalization objective F (·). Then, for all uj , j ∈ [k], there exists a
˜
column uj of U such that
˜
d

uj − uj
˜

2

2
Eij + o( ),

≤

(18)

i=1

where E ∈ Rd×k is
Eij

L
l=1 (λil − λjl )uj Rl ui
L
2
l=1 (λil − λjl )

when i = j and i ≤ k or j ≤ k. We deﬁne Eij = 0 when i = j and λil = 0 when i > k.

(19)

Volodymyr Kuleshov∗ , Arun Tejasvi Chaganty∗ , Percy Liang

Proof. See Cardoso [28, Proposition 1]. Note that in the low rank setting, the entries of Eij (Cardoso [28,
˜
Equation 15]) where i, j > k are not deﬁned, however, these terms only eﬀect the last d − k columns of U . The
bounds for vectors u1 , ..., uk only depend on Eij where i ∈ [d] and j ∈ [k], and these are derived in the low-rank
setting in the same way as they are derived in the full-rank proof of Cardoso [28].
We now present the corresponding perturbation bounds in Afsari [24] to the low rank setting.
Lemma 16 (Afsari [24]). Let Ml = U Λl U + Rl , l ∈ [L], be matrices with common factors U ∈ Rd×k and
¯
¯
¯
diagonal Λl ∈ Rk×k . Let U ∈ Rd×d be a full-rank extension of U with columns u1 , u2 , . . . , ud and let V = U −1 ,
˜ ∈ Rd×d be the minimizer of the joint diagonalization objective F (·) and let
with rows v1 , v2 , . . . , vd . Let V
˜
˜
U = V −1 .
˜
Then, for all uj , j ∈ [k], there exists a column uj of U such that
˜
d

uj − uj
˜

2

2
Eij + o( ),

≤

(20)

i=1

where the entries of E ∈ Rd×k satisfy the equation
−1
ηij
Eij
=
Eji
γij (1 − ρ2 ) −ρij
ij

−ρij
−1
ηij

Tij
.
Tji

when i = j and either i ≤ k or j ≤ k. When i = j, Eij = 0. The matrix T has zero on-diagonal elements, and
is deﬁned as
Tij =

for 1 ≤ j = i ≤ d

vi Rl vj λjl ,
l

and the other parameters are
γij = λi

2

λj

2,

ηij =

λi
λj

2

,

ρij =

2

λi λj
λj 2 λi

,

(λi )k = λik .

2

We deﬁne λil = 0 when i > k.
˜
Proof. In Afsari [24, Theorem 3] it is shown that V = (I + E)V + o( ), where Eij is deﬁned for i, j ∈ [d] (Afsari
[24, Equation 36]). Then,
˜
˜
U = U (I + E)−1 + o( )
˜
= U (I − E) + o( ).
Note that, once again, in the low rank setting, the entries of Eij when i, j > k are not characterized by Afsari’s
˜
results; however, these terms only eﬀect the last d − k columns of U .
Lemma 17. Let Ml = U Λl U + Rl , l ∈ [L], be matrices with common factors U ∈ Rd×k and diagonal
¯
¯
¯
Λl ∈ Rk×k . Let U ∈ Rd×d be a full-rank extension of U with columns u1 , u2 , . . . , ud and let V = U −1 , with rows
˜
˜
˜
v1 , v2 , . . . , vd . Let V ∈ Rd×d be the minimizer of the joint diagonalization objective F (·) and let U = V −1 .
˜
Then, for all uj , j ∈ [k], there exists a column uj of U such that
˜
d

uj − uj
˜

2

2
Eij + o( ),

≤

(21)

i=1

where the entries of E ∈ Rd×k are bounded by
1
|Eij | ≤
1 − ρ2
ij

1
λi

2
2

1
+
λj

L

L
2
2

vi Rl vj λjl +
l=1

vi Rl vj λil

,

l=1

when i = j and Eij = 0 when i = j and λil = 0 when i > k. Here λi = (λi1 , λi2 , ..., λiL ) ∈ RL and ρij =
is the modulus of uniqueness, a measure of how ill-conditioned the problem is.

λi λj
λi 2 λj

2

Tensor Factorization via Matrix Factorization

Proof. From Lemma 16, we have that
Eij
Eji

≤

ηij + ηji
γji (1 − ρ2 )
ij

Tij
Tji

,

where
γij = λi

2

λj

2,

ηij =

λi
λj

2

,

ρij =

2

λi λj
λj 2 λi

and the matrix T is deﬁned to be zero on the diagonal and for i = j deﬁned as
L

Tij =

for 1 ≤ j = i ≤ d

vi Rl vj λjl ,
l=1

Taking

·

to be the l1 -norm in the above expression, we have that
|Eij | ≤ |Eij | + |Eji | ≤

Since

ηij + ηji
(|Tij | + |Tji |) .
γji (1 − ρ2 )
ij

ηij + ηji
λi 2 + λj 2
1
2
2
=
=
γji
λi 2 λj 2
λi
2
2

and

L

vi Rl vj λjl ,

Tij =
l=1

the claim follows.

2
2

+

1
λj

2
2

,
2

