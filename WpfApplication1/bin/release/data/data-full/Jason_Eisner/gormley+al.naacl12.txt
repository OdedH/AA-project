Shared Components Topic Models
Matthew R. Gormley

Mark Dredze
Benjamin Van Durme
Center for Language and Speech Processing
Human Language Technology Center of Excellence
Department of Computer Science
Johns Hopkins University, Baltimore, MD
{mrg,mdredze,vandurme,jason}@cs.jhu.edu

Abstract

leads us to ﬁnd two problems with LDA and its variants mentioned above: (1) independently generated
topics and (2) overparameterized models.

With a few exceptions, extensions to latent
Dirichlet allocation (LDA) have focused on
the distribution over topics for each document.
Much less attention has been given to the underlying structure of the topics themselves. As
a result, most topic models generate topics independently from a single underlying distribution and require millions of parameters, in
the form of multinomial distributions over the
vocabulary. In this paper, we introduce the
Shared Components Topic Model (SCTM), in
which each topic is a normalized product of a
smaller number of underlying component distributions. Our model learns these component
distributions and the structure of how to combine subsets of them into topics. The SCTM
can represent topics in a much more compact
representation than LDA and achieves better
perplexity with fewer parameters.

1

Jason Eisner

Introduction

Topic models are probabilistic graphical models
meant to capture the semantic associations underlying corpora. Since the introduction of latent Dirichlet allocation (LDA) (Blei et al., 2003), these models have been extended to account for more complex
distributions over topics, such as adding supervision
(Blei and McAuliffe, 2007), non-parametric priors
(Blei et al., 2004; Teh et al., 2006), topic correlations (Li and McCallum, 2006; Mimno et al., 2007;
Blei and Lafferty, 2006) and sparsity (Williamson et
al., 2010; Eisenstein et al., 2011).
While much research has focused on modeling
distributions over topics, less focus has been given to
the makeup of the topics themselves. This emphasis

Independent Topics In the models above, the topics are modeled as independent draws from a single
underlying distribution, typically a Dirichlet. This
violates the topic modeling community’s intuition
that these distributions over words are often related.
As an example, consider a corpus that supports two
related topics, baseball and hockey. These topics
likely overlap in their allocation of mass to high
probability words (e.g. team, season, game, players), even though the two topics are unlikely to appear in the same documents. When topics are generated independently, the model does not provide a
way to capture this sharing between related topics.
Many extensions to LDA have addressed a related
issue, LDA’s inability to model topic correlation,1
by changing the distributions over topics (Blei and
Lafferty, 2006; Li and McCallum, 2006; Mimno et
al., 2007; Paisley et al., 2011). Yet, none of these
change the underlying structure of the topic’s distributions over words.
Overparameterization
Topics are most often
parameterized as multinomial distributions over
words: increasing the topics means learning new
multinomials over large vocabularies, resulting in
models consisting of millions of parameters. This
issue was partially addressed in SAGE (Eisenstein
et al., 2011) by encouraging sparsity in the topics
which are parameterized by their difference in logfrequencies from a ﬁxed background distribution.
Yet the problem of overparameterization is also tied
1

Two correlated topics, e.g. nutrition and exercise, are likely
to co-occur, but their word distributions might not overlap.

783
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 783–792,
Montr´ al, Canada, June 3-8, 2012. c 2012 Association for Computational Linguistics
e

to the number of topics, and though SAGE reduces
the number of non-zero parameters, it still requires
a vocabulary-sized parameter vector for each topic.
We present the Shared Components Topic Model
(SCTM), which addresses both of these issues by
generating each topic as a normalized product of a
smaller number of underlying components. Rather
than learning each new topic from scratch, we model
a set of underlying component distributions that
constrain topic formation. Each topic can then be
viewed as a combination of these underlying components, where in a model such as LDA, we would
say that components and topics stand in a one to one
relationship. The key advantages of the SCTM are
that it can learn and share structure between overlapping topics (e.g. baseball and hockey) and that it can
represent the same number of topics in a much more
compact representation, with far fewer parameters.
Because the topics are products of components,
we present a new training algorithm for the signiﬁcantly more complex product case which relies on a Contrastive Divergence (CD) objective.
Since SCTM topics, which are products of distributions, could be represented directly by distributions as in LDA, our goal is not necessarily to learn
better topics, but to learn models that are substantially smaller in size and generalize better to unseen
data. Experiments on two corpora show that our
model uses fewer underlying multinomials and still
achieves lower perplexity than LDA, which suggests
that these constraints could lead to better topics.

2

Shared Components Topic Models

The Shared Components Topic Model (SCTM) follows previous topic models in inducing admixture
distributions of topics that are used to generate each
document. However, here each topic multinomial
distribution over words itself results from a normalized product of shared components, each a multinomial over words. Each topic selects a subset of components. We begin with a review and then introduce
the SCTM.
Latent Dirichlet allocation (LDA) (Blei et al.,
2003) is a probabilistic topic model which deﬁnes
a generative process whereby sets of observations
are generated from latent topic distributions. In the
SCTM, we use the same generative process of topic
784

assignments as LDA, but replace the K independently generated topics (multinomials over words)
with products of C components.
Latent Dirichlet allocation generative process
For each topic k ∈ {1, . . . , K}:
φk ∼ Dir(β)
[draw distribution over words]
For each document m ∈ {1, . . . , M }:
θ m ∼ Dir(α)
[draw distribution over topics]
For each word n ∈ {1, . . . , Nm }:
zmn ∼ Mult(1, θ m )
[draw topic]
xmn ∼ φzmi
[draw word]

LDA draws each topic φk independently from a
Dirichlet. The model generates each document m
of length M , by ﬁrst sampling a distribution over
topics θ m . Then, for each word n, a topic zmn is
chosen and a word type xmn is generated from that
topic’s distribution over words φzmi .
A Product of Experts (PoE) model (Hinton,
1999) is the normalized product of the expert distributions. In the SCTM, each component (an expert) models an underlying multinomial word distribution. We let φc be the parameters of the cth
component, where φcv is the probability of the cth
component generating word v. If the structure of a
PoE included only components c ∈ C in the product, it would have the form: p(x|φ1 , . . . , φC ) =
Q
φcx
PV c∈C
Q
, where there are C components, and
v=1

c∈C

φcv

the summation in the denominator is over the vocabulary. In a PoE, each component can overrule the
others by giving low probability to some word. A
PoE can be viewed as a soft intersection of its components, whereas a mixture is a soft union.
The Beta-Bernoulli model (Grifﬁths and
Ghahramani, 2006) is a distribution over binary
matrices with a ﬁxed number of rows and columns.
It is the ﬁnite counterpart to the Indian Buffet
Process. In this work, we use the Beta-Bernoulli as
our prior for an unobserved binary matrix B with C
columns and K rows. In the SCTM, each row bk of
the matrix, a binary feature vector, deﬁnes a topic
distribution. The binary vector acts as a selector
for the structure of the PoE for that topic. The row
determines which components to include in the
product by which entries bkc are “on” (equal to 1)
in that row. Under Beta-Bernoulli prior, for each
column, a coin with weight πc is chosen. For each
entry in the column, the coin is ﬂipped to determine
if the entry is “on” or “off”. This corresponds to

the notion that some components are a priori more
likely to be included in topics.
The Beta-Bernoulli model generative process
For each component c ∈ {1, . . . , C}:
[columns]
γ
πc ∼ Beta( C , 1)
[draw probability of component c]
For each topic k ∈ {1, . . . , K}:
[rows]
bkc ∼ Bernoulli(πc )
[draw whether topic includes cth
component in its PoE]

2.1

Shared Components Topic Models

The Shared Components Topic Model generates
each document just like LDA, the only difference
is the topics are not drawn independently from a
Dirichlet prior. Instead, topics are soft intersections
of underlying components, each of which is a multinomial distribution over words. These components
are combined via a PoE model, and each topic is
constructed according to a length C binary vector
bk ; where bkc = 1 includes and bkc = 0 excludes
component c. Stacking the K vectors forms a K ×C
matrix; rows correspond to topics and columns to
components. Overlapping topics share components
in common.
Generative process SCTM’s generative process
generates topics and words, but must also generate
the binary matrix. For each of the C shared components, we generate a distribution φc over the V
words from a Dirichlet parametrized by β. Next,
we generate a K × C binary matrix using the BetaBernoulli prior. These components and the binary
matrix implicitly deﬁne the complete set of K topic
distributions, each of which is a PoE.
p(x|bk , φ) =

C
bkc
c=1 φcx
bkc
V
C
v=1
c=1 φcv

(1)

The distribution p(·|bk , φ) deﬁnes the kth topic.
Conditioned on these K topics, the remainder of the
generative process, which generates the documents,
is just like LDA.
The Shared Components Topic Model generative process
For each component c ∈ {1, . . . , C}:
φc ∼ Dir(β)
[draw distribution over words]
γ
πc ∼ Beta( C , 1)
[draw probability of component c]
For each topic k ∈ {1, . . . , K}:
bkc ∼ Bernoulli(πc )
[draw whether topic includes cth
component in its PoE]
For each document m ∈ {1, . . . , M }
θ m ∼ Dir(α)
[draw distribution over topics]
For each word n ∈ {1, . . . , Nm }
zmn ∼ Mult(1, θ m )
[draw topic]
xmn ∼ p(· |bzmn , φ) given by Eq. (1)
[draw word]

785

See Figure 1 for the graphical model.
Discussion An advantage of this formulation is the
ability to model many topics using few components.
While LDA must maintain V ×K parameters for the
topic distributions, the SCTM maintains just V × C
parameters, plus an additional K × C binary matrix.
Since C < K
V this results in many fewer parameters for the SCTM.2 Extending the number of
topics (rows) requires storing additional binary vectors, a lightweight requirement. In theory, we could
enable all 2C possible component combinations, although we expect to use far less. On the other hand,
constraining the SCTM’s topics by the components
gives less ﬂexible topics as compared to LDA. However, we ﬁnd empirically that a large number of topics can be effectively modeled with a smaller number of components.
Observe that we can reparameterize the SCTM as
LDA by assuming an identity square matrix; each
component corresponds to a topic in LDA, making
LDA a special case of the SCTM with an identity
matrix IC . Intuitively, SCTM learning could produce an LDA model where appropriate. Finally, we
can also think of the SCTM as learning the structure of many PoE models. In applications where experts abstain, the SCTM could learn in which setting
(row) each expert casts a vote.

3

Parameter Estimation

Parameter estimation infers values for model parameters φ, π, and θ from data using an unsupervised training procedure. Because exact inference
is intractable in the SCTM, we turn to approximate
methods. As is common in these models, we will
integrate out π and θ, sample latent variables Z and
B, and optimize the components φ. Our algorithm
follows the outline of the Monte Carlo EM (MCEM)
algorithm (Wei and Tanner, 1990). In the Monte
Carlo E-step, we will re-sample the latent variables
Z and B based on current model parameters φ and
observed data X. In the M-step, we will ﬁnd new
model parameters φ. Since these parameters correspond to experts in the PoE, we rely on a contrastive
divergence (CD) objective (Hinton, 2002), popular
for PoE training, rather than maximizing the data
2

The vocabulary size V could be much larger if n-grams or
relational triples are used, as opposed to unigrams.

log-likelihood. Normally, CD only estimates the parameters of the expert distributions. However, in our
model, the structure of the PoEs themselves change
based on the E-step. Since we generate multiple
samples in the E-step, we modify the CD objective
to compute the gradient for each E-step sample and
take the average to approximate the expectation under B and Z.3
3.1

E-Step

The E-step approximates an expectation under
p(B, Z|X, φ, α, γ) for latent topic assignments Z
and matrix B using Gibbs sampling. The Gibbs
sampler uses the full conditionals for both zi (7) and
bkc (12), which we derive in Appendix A. Using this
sampler, we obtain J samples of Z and B by iterating through each value of zi and bkc J times (in our
experiments, we use J=1, which appears to work as
well on this task as multiple samples). These J samples are then used in the M-step as an approximation
of the expectation of the latent variables.
3.2

M-Step

Given many samples of B and Z, the M-step optimizes the component parameters φ which cannot be
collapsed out. We utilize the standard PoE training
procedure for experts: contrastive divergence (CD).
We approximate the CD gradient as the difference of
the data distribution and the one-step reconstruction
of the data according to the current parameters. As
in Generalized EM (Dempster et al., 1977), a single
gradient step in the direction of the contrastive divergence objective is sufﬁcient for each M-step. A
key difference in our model is that we must incorporate the expectation of the PoE model structure,
which in our case is a random variable instead of a
ﬁxed observed structure. We achieve this by simply
3

CD training within MCEM is not the only possible approach. One alternative would be to compute the CD gradient
summing over all values of B and Z, effectively training the
entire model using CD. This approach prevents the normal CD
objective derivation from being simpliﬁed into a more tractable
form. Another approach would be a pure MCMC algorithm,
which sampled φ directly. While using the natural parameters
allows the sampler to mix, it is too computationally intensive to
be practical. Finally, we could train with Generalized MCEM,
where the exact gradient of the log-likelihood (or log-posterior)
is used, but this easily gets stuck in local minima. After experimenting with these and other options, we present our current
most effective estimation method.

786

computing the CD gradient for each PoE given each
of the J samples {Z, B}(j) from the E-Step, then
average the result.
Another difﬁculty arises from computing the gradient directly for the multinomial φc due to the V −1
degrees of freedom imposed by sum-to-one constraints. Therefore, we switch to the natural parameters, which obviates the need for considering
the sum-to-one constraint in the optimization, by
deﬁning φc in terms of V real valued parameters
{ξc1 , . . . , ξcV }:
φcv =

exp(ξcv )
V
t=1 exp(ξcv )

(2)

The V parameters ξcv are then used to compute φcv
for use in the E-step.
As explained above, the M-step does not maximize the data log-likelihood, but instead minimizes
contrastive divergence. Hinton (2002) explains that
maximizing data log-likelihood is equivalent to minimizing Q0 ||Q∞ , the KL divergence between the
ξ
observed data distribution, Q0 , and the model’s
equilibrium distribution, Q∞ .4 Minimizing Q0 ||Q∞
ξ
ξ
would require the computation of an intractable expectation under the equilibrium distribution. We
avoid this by instead minimizing the contrastive divergence objective,
CD(ξ|{Z, B}(j) ) = Q0 ||Q∞ − Q1 ||Q∞ ,
ξ
ξ
ξ

(3)

where Q1 is the distribution over one-step reconξ
structions of the data, X given Z, B, ξ, that are generated by a single step of Gibbs sampling.
Unlike standard applications of CD training, the
hidden variables (Z,B) are not contained within the
experts. Instead they deﬁne the structure of the PoE
model, where B indicates which experts to use in
each product (topic) and Z indicates which PoE generates each word. Unfortunately, CD training cannot
infer this structure since the CD derivation makes
use of a ﬁxed structure in the one-step reconstruction. Therefore, we have taken a MCEM approach,
ﬁrst sampling the PoE structure in the E-step, then
4

Hinton (2002) used this notation because the data distribution, Q0 , can be described as the state of a Markov chain at time
0 that was started at the data distribution. Similarly, the equilibrium distribution, Q∞ could be obtained by running the same
ξ
Markov chain to time ∞.

α
θm

πc

zmn

bkc

xmn

Algorithm 1 SCTM Training

γ

φc

Initialize parameters: ξc , bkc , zi .
while not converged do
{E-step:}
for j = 1 to J do
{Draw jth sample {Z, B}(j) }
for i = 1 to N do
Sample zi using Eq. (7)
for k = 1 to K do
for c = 1 to C do
Sample bkc using ratio in Eq. (12)
{M-step:}
for c = 1 to C do
for v = 1 to V do
Single gradient step over ξ

K
β

Nm
M

C

Figure 1: The graphical model for the SCTM.

(t+1)

ξcv

ﬁxing these samples for Z and B when computing
the one-step reconstruction of the data, X.
Contrastive Divergence Gradient We provide
the approximate derivative of the contrastive divergence objective, where Z and B are treated as
ﬁxed.5
d CD(ξ|{Z, B}(j) )
≈−
dξ

d log f (x|bz , φ)
dξ

+

d log f (x|bz , φ)
dξ

Q0

Q1
ξ

where f (x|bz , φ) = C φbzc is the numerator of
c=1 cx
p(x|bz , φ) and the derivative of its log is efﬁcient to
compute:
d log f (x|bz , φ)
dξcv

=

bzc (1 − φcv )
−bzc φcv

for x = v
for x = v

To approximate the expectation under Q1 , we hold
ξ
Z, B, ξ ﬁxed and resample the data, X, using one
step of Gibbs sampling.
3.3

Summary

Our learning algorithm can be viewed
in terms of a Q function:
Q(ξ|ξ (t) ) ≈
J
1
(j) )where we average over
j=1 CD(ξ|{Z, B}
J
J samples. The E-step computes Q(ξ|ξ (t) ). The
M-step minimizes Q with respect to ξ to obtain the
updated ξ (t+1) by performing gradient descent on
(t)
(t+1)
(t)
the Q function as ξcv
= ξcv − η · d Q(ξ|ξ ) for
dξcv
all values of c, v.
5

−

The derivative is approximate because we drop the term:

d Q1
ξ
dξ

·

d Q1 ||Q∞
ξ
ξ
d Q1
ξ

, which is ‘problematic to compute’ (Hinton,

2002). This is the standard use of CD.

787

4

(t)

= ξcv − η ·

d Q(φ|φ(t) )
dξcv

Related Models

The SCTM is closely related to the the Inﬁnite
Overlapping Mixture Model (IOMM) (Heller and
Ghahramani, 2007), yet our model differs from and,
in some ways, extends theirs. The IOMM models the geometric overlap of Gaussian clusters using PoEs, and models the structure of the PoEs with
the rows of a binary matrix. The SCTM models a
ﬁnite number of columns, where the IOMM models an inﬁnite number. The IOMM generates a row
for each data point, whereas the SCTM generates a
row for each topic. Thus, the SCTM goes beyond
the IOMM by allowing the rows to be shared among
documents and models document-speciﬁc mixtures
over the rows of the matrix.6
SAGE for topic modeling (Eisenstein et al., 2011)
can be viewed as a restricted form of the SCTM.
Consider an SCTM in which the binary matrix is restricted such that the ﬁrst column, b·,1 , consists of
all ones and the remainder forms a diagonal matrix.
If we then set the ﬁrst component, φ1 , to the corpus background distribution, and add a Laplace prior
on the natural parameters, ξcv , we have the SAGE
model. Note that by removing the restriction that
the matrix contain a diagonal, we could allow multiple components to combine in the SCTM fashion,
while incorporating SAGE’s sparsity beneﬁts.
6

The IOMM uses Metropolis-Hastings (MH) to sample the
parameters of the experts. This approach is computationally
feasible because their experts are Gaussian, unlike the SCTM
in which the experts are multinomials and the MH step too expensive.

The relation of TagLDA (Zhu et al., 2006) to
the SCTM is similar to that of SAGE and SCTM.
TagLDA has a PoE of exactly two experts: one expert for the topic, and one for the supervised wordlevel tag. Examples of tags are abstract or body,
indicating which part of a research paper the word
appears in.
Unlike the SCTM and SAGE, most prior extensions to LDA have enhanced the distribution over
topics for each document. One of the closest is hierarchical LDA (hLDA) (Blei et al., 2004) and its application to PAM (Mimno et al., 2007). Though topics are still generated independently from a Dirichlet prior, hLDA learns a tree structure underlying
the topics. Each document samples a single path
through the tree and samples words from topics
along that path. The SCTM models an orthogonal
issue to topic hierarchy: how the topics themselves
are represented as the intersection of components.
Finally, while prior work has primarily used mixtures for the sake of conjugacy, we take a fundamentally different approach to modeling the structure by
using normalized product distributions.

5

Evaluation

We compare the SCTM with LDA in terms of overall model performance (held-out perplexity) as well
as parameter usage (varying numbers of components
and topics). We select LDA as our baseline since our
model differs only in how it forms topics, which focuses evaluation on the beneﬁt of this model change.
We consider two popular data sets for comparison: NIPS: A collection of 1,617 NIPS abstracts
from 1987 to 19997 , with 77,952 tokens and 1,632
types. 20N EWS: 1,000 randomly selected articles
from the 20 Newsgroups dataset,8 with 70,011 tokens and 1,722 types. Both data sets excluded stop
words and words occurring in fewer than 10 documents. For 20N EWS, we used the standard by-date
train/test split. For NIPS, we randomly partitioned
the data by document into 75% train and 25% test.
We compare the SCTM to LDA by evaluating
the average perplexity-per-word of the held-out test
7

We follow prior work (Blei et al., 2004; Li and McCallum, 2006; Li et al., 2007) in using only the abstracts:
http://www.cs.nyu.edu/˜roweis/data.html
8
Williamson et al. (2010) created a similar subset:
http://people.csail.mit.edu/jrennie/20Newsgroups/

788

data, perplexity = 2− log2 (data|model)/N . Exact computation is intractable, so we use the left-to-right algorithm (Wallach et al., 2009) as an accurate alternative. With the topics ﬁxed, the SCTM is equivalent to LDA and requires no adaptation of the leftto-right algorithm.
We used a collapsed Gibbs sampler for training
LDA and the algorithm described above for training
the SCTM. Both were trained for 4000 iterations,
sampling topics every 10 iterations after a burn-in of
3000. The hyperparameter α was optimized as an
asymmetric Dirichlet, β as a symmetric Dirichlet,
and γ = 3.0 was ﬁxed.9 Following the observation of
Hinton (2002) that CD training beneﬁts from initializing the experts to nearly uniform distributions, we
initialize the component distributions from a symˆ
metric Dirichlet with parameter β = 1 × 106 . We use
J = 1 samples per iteration and a decaying learning
rate centered at η = 100.10 We ranged LDA from 10
to 200 topics, and the SCTM from 10 to 100 components (C). We then selected the number of SCTM
topics (K) as K ∈ {C, 2C, 3C, 4C, 5C}. For each
model, we used ﬁve random restarts, selecting the
model with the highest training data likelihood.
5.1

Results

Our goal is to demonstrate that (1) modeling topics
as products of components is an expressive alternative to generating topics independently and (2) the
SCTM can both achieve lower perplexity than LDA
and use fewer model parameters in doing so.
Topics as Products of Components Figures 3b
and 3c show the perplexity for the held-out portions
of 20N EWS and NIPS for different numbers of components C. The shaded region shows the full SCTM
perplexity range we observed for different K and
at each value of C, we label the number of topics
K (rows in the binary matrix). For each number of
components, LDA falls within the upper portion of
the shaded region. While for some (small) values of
K for the SCTM, LDA does better, the SCTM can
easily include more K (requiring few new parameters) to achieve better results. This supports our
hypothesis that topics can be comprised of the overlap between shared underlying components. More9
10

On development data the model was rather insensitive to γ.
We experimented with larger J but it had no effect.

Figure 2: SCTM binary matrix and topics from 3599 training documents of 20N EWS for C = 10, K = 20. Blue
squares are “on” (equal to 1).

10

5

6

x

1800

Perplexity

1600

40
50

60
80
100

1200

q

60
80

80
q

100
q

120

160
200

1000

q

160

180
240
300

200

240
320
400

300
400
500

800
20

q

20

600
40

120

0

40 q
20,80 40,80

1200

20,100

60 q
40,120 60,120

80 q
100 q
40,160
120
80,160
60,180
100,200
40,200
60,240 80,240
100,300
60,300 80,320

1000

40

q

140

q

80,400
100,400
100,500

800
0

100

200

300

400

500

600

# of Model Parameters (thousands)

10

700

LDA
SCTM

20
40 q
q

LDA
SCTM

q

20,40 q
10,40 20 q
21
1400 10,50
20,60

(a)
q

20
30

1400

10 q
10,20 q
11
10,30

8 10

10

q
q

Top words for topic after ablating component c=1
organization subject israel law peace deﬁne israeli
administration president year market money senior
years food center year air russian war army
opinions drive hard power support cost research price
pitt ﬁle program year center programs image division

60

# of Components

80

100

30
40
50

500

q
q

LDA
SCTM

550
q
q

40

40

80
100

q

400

80

60

120
160
200

120

q

80 q
100

180
240
300

160
240
320
400

300
20

40

60

q

200
300
400
500

80

100

# of Components

(b)

q

10,20

600 10,30

20

60

0

10 q
11 q

Perplexity

4

Top words for topic
subject organization israel return deﬁne law org
encryption chip clipper keys des escrow security law
turkish armenian armenians war turkey turks armenia
drive card disk scsi hard controller mac drives
image jpeg window display code gif color mit
jews israeli jewish arab peace land war arabs
org money back question years thing things point
christian bible church question christ christians life
administration president year market money senior
health medical center research information april
gun law state guns control bill rights states
world organization system israel state usa cwru reply
space nasa gov launch power wire ground air
space nasa gov launch power wire ground air
team game year play games season players hockey
car lines dod bike good uiuc sun cars
windows ﬁle government key jesus system program
article writes center page harvard virginia research
max output access digex int entry col line
lines people don university posting host nntp time

Perplexity

2

αk
0.306
0.031
0.025
0.102
0.071
0.018
0.074
0.106
0.011
0.055
0.063
0.160
0.042
0.038
0.079
0.158
0.136
0.122
0.017
0.380

Perplexity

15

y

k
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20

←
←
←
←
←
←
←
←
←
←
←
←
←
←
←
←
←
←
←
←

20

500
450
400
350
300

10,40 20 q
21 q
10,50
20,40
20,60

LDA
SCTM

40 q

20,80
40,80
60 q
20,100
q
40,120
60,120 80
100 q
40,160
q
60,180
40,200 80,160 120 140 q
60,240
160 q
80,240
180 q
100,200
60,300 100,300
200
80,320 100,400
80,400
100,500

0

100

200

300

q

400

# of Model Parameters (thousands)

(c)

(d)

Figure 3: Perplexity results on held-out data for 20N EWS (b) and NIPS (c) showing the results of LDA and the SCTM
for the same number of components and varying K (SCTM). For the same number of components (multinomials), the
SCTM achieves lower perplexity by combining them into more topics. Results for 20N EWS (a) and NIPS (d) showing
non-square SCTM achieves lower perplexity than LDA with a more compact model.

over, this suggests that our products (PoEs) provide
additional and complementary expressivity over just
mixtures of topics.
Model Compactness
Including an additional
topic in the SCTM only adds C binary parameters,
for an extra row in the matrix. Whereas in LDA, an
additional topic requires V (the size of the vocabulary) additional parameters to represent the multinomial. In both cases, the number of documentspeciﬁc parameters must increase as well. Figures
3a and 3d present held-out perplexity vs. number
of model parameters on 20N EWS and NIPS, excluding the case of square (C = K) binary matrices for
the SCTM. The regions show a conﬁdence interval (p = 0.05) around the smoothed ﬁt to the data,
789

LDA labels show C, and SCTM labels show C, K.
The SCTM achieves lower perplexity with fewer
model parameters, even when the increase in noncomponent parameters is taken into account. We expect that because of its smaller size the SCTM exhibits lower sample complexity, allowing for better
generalization to unseen data.
5.2

Analysis

Figure 2 gives the binary matrix and topics learned
on a larger section of 20N EWS training documents.
These topics evidence that the SCTM is able to
achieve a diversity of topics by combining various
subsets of components, and we expect that the low
perplexity achieved by the SCTM can be attributed

c=9
visual image
images cells
cortex scene
support spatial
feature vision
cues stimulus
statistics

c=4
paper units
output layer
networks
patterns unit
pattern set rule
network rules
weights training

c=2

c=1

network
networks data
learning optimal
linear vector
independent
binary natural
algorithms pca

model
information
parameters
kalman robust
matrices
likelihood
experimentally

k=16 αk =0.11
training units
paper hidden
number output
problem rule set
order unit show
present method
weights task

k=14 αk =0.07
models images
image problem
structure
analysis mixture
clustering
approach show
computational

k=8 αk =0.23
algorithm
training error
function method
performance
input
classiﬁcation
classiﬁer

k=4 αk =0.12
bayesian
results show
estimation
method based
parameters
likelihood
methods models

k=18 αk =0.07
information
analysis
component rules
signal
independent
representations
noise basis

k=3 αk =0.06
object
recognition
system objects
information
visual matching
problem based
classiﬁcation

k=12 αk =0.13
problem state
control
reinforcement
problems models
time based
decision markov
systems function

k=5 αk =0.04
object
recognition
system objects
information
visual matching
problem based
classiﬁcation

k=7 αk =0.08
data paper
networks network
output feature
features
patterns set
train introduced
unit functions

k=1 αk =0.11
model learning
system
information
parameters
networks robust
kalman rules
estimation

k=9 αk =0.02
vector feature
classiﬁcation
support vectors
kernel
regression
weight inputs
dimensionality

k=13 αk =0.05
networks
network learning
distributed
system weight
vectors property
binary point
optimal real

k=10 αk =0.09
neural neurons
analog synaptic
neuron networks
memory time
capacity model
associative
noise dynamics

k=2 αk =0.13
network input
information time
recurrent back
propagation
units
architecture
forward layer

k=19 αk =0.03
system networks
set neurons
visual phase
feature
processing
features output
associative

k=20 αk =0.02
time network
weights
activation delay
current chaotic
connected
discrete
connections

k=6 αk =0.23
neural network
paper
recognition
speech systems
based results
performance
artiﬁcial

k=17 αk =0.10
number
functions
weights function
layer
generalization
error results
loss linear size

k=11 αk =0.08
learning
networks system
recognition time
network
describes hand
context views
classiﬁcation

k=15 αk =0.12
cells neurons
visual cortex
motion response
processing
spatial cell
properties
patterns spike

Figure 4: Hasse diagram on NIPS for C = 10, K = 20 showing the top words for topics and unrepresented components (in shaded box). Notice that some topics only consist of a single component. The shaded box contains the
components that didn’t appear as a topic. For the sake of clarity, we only show arrows for the subsumption relationships between the topics, and we omit the implicit arrows between the components in the shaded box and the
topics.

to the high-level of component re-use across topics.
Topics are typically interpreted by looking at the
top-N words, whereas the top-N words of a component often do not even appear in the topics to which
it contributes. Instead, we ﬁnd that the components
contribution to a topic is typically through vetoing
words. For example, the top words of component
c=1, corresponding to the ﬁrst column of the binary
matrix in ﬁgure 2, are [subject organization posting apple mit
screen write window video port], yet only a few of these appear in topics k=1,2,3,4,5, which use it.
On the right of ﬁgure 2, we show what the topics become when we ablate component c=1 from
the matrix by setting the column to all zeros. Topic
k=2 changes from being about information security
to general politics and is identical to k=9. Topic k=3
changes from the Turkish-Armenian War to a more
general war topic. Topic k=4 changes to a less focused version of itself. In this way, we can gain further insight into the contribution of this component,
and the way in which components tend to increase
the speciﬁcity of a topic to which they are added.
790

The SCTM learns each topic as a soft intersection of its components, as represented by the binary
matrix. We can describe the overlap between topics
based on the components that they have in common.
One topic subsumes another topic when the parent
consists of a subset of the child’s components. In
this way, the binary matrix deﬁnes a Hasse diagram,
a directed acyclic graph describing all the subsumption relationships between topics. Figure 4 shows
such a Hasse diagram on the NIPS data. Several topics consist of only a single component, such as k=12
on reinforcement learning and k=8 on optimization.
These two topics combine with the component c=1
so that their overlap forms the topic k=4 on Bayesian
methods. These subsumption relationships are different from and complementary to hLDA (see §4),
which models topic co-occurrence, not component
intersection. For example, topic k=10 on connectionism and k=2 on neural networks intersect to
form k=20 which contains words that would only
appear in both of its subsuming topics, thereby explicitly modeling topic overlap.

The SCTM sometimes learns identical topics (two
rows with the same binary entries “on”) such as
k=13 and k=14 in ﬁgure 2 and k=3 and k=5 in ﬁgure 4, which is likely due to the Gibbs sampler for
the binary matrix getting stuck in a local optimum.

6

Discussion

We have presented the Shared Components Topic
Model (SCTM), in which topics are products of
underlying component distributions. This model
change learns shared topic structures—as expressed
through components—as opposed to generating
each topic independently. Reducing the number of
components yields more compact models with lower
perplexity than LDA. The two main limitations of
the current SCTM are, when restricted to a square
binary matrix (C = K), the inference procedure is
unable to recover a model with perplexity as low as
a collapsed Gibbs sampler for LDA, and the components are not consistently interpretable.
The use of components opens up interesting directions of research. For example, task speciﬁc side
information can be expressed as priors or constraints
over the components, or by adding conditioning
variables tied to the components. Additionally, tasks
beyond document modeling may beneﬁt from representing topics as products of distributions. For example, in vision, where topics are classes of objects,
the components could be features of those objects.
For selectional preference, components could correspond to semantic features that intersect to deﬁne
semantic classes (Gormley et al., 2011). We hope
new opportunities will arise as this work explores a
new research area for topic models.

Full conditional of zi
Recall that p(Z|α) is
the Dirichlet-Multinomial distribution over topic
assignments, where θ has been integrated out.
The form of this distribution is identical to the
corresponding distribution over topics in LDA.
The derivation of the full conditional of zi ∈
{1, . . . , K}, follows from the factorization in Eq. 4:
p(zi |X, Z −(i) ,B, φ, α, β, γ)
∝ p(X|Z, B, φ)p(Z|α)

p(X, Z, B, φ|α, β, γ) =

(6)

p(xi |bzi , φ)(˜ mzi
n−(i)

(7)

∝

+ αzi )

Z −(i) is the set of all topic assignments except zi .
We use the independence of each document, recalling that example i belongs to document m. In practice, we cache p(x|bz , φ) for all x, z (V × K values)
and these are shared by all zi in a sampling iteration.
Above, just as in LDA, p(Z|α) is simpliﬁed by
−(i)
−(i)
˜
proportionality to (˜ mzi + αzi ), where nmk is the
n
count of examples for document m that are assigned
topic k excluding zi ’s contribution (Heinrich, 2008).
Full conditional of bkc Recall that p(B|γ) is the
prior for a Beta-Bernoulli matrix. The full conditional distribution of a position in the binary vector
is (Grifﬁths and Ghahramani, 2006):
−(k)

p(bkc = 1|B −(kc) , γ) =

γ
+C
nc
¯
γ
K+C

(8)

−(k)

where nc
¯
is the count of topics with component
c excluding topic k, and B −(kc) is the entire matrix
except for the entry bkc .
To ﬁnd the full conditional for bkc ∈ {0, 1}, we
again start with the factorization from Eq. 4.
p(bkc |X, Z, B −(kc) , φ, α, β, γ)

(9)

∝ p(X|Z, B, φ)p(B|γ)

(10)

p(xi |bzi , φ) p(bkc |B −(kc) , γ)

∝

Appendix A: Derivation of Full Conditionals
The model’s complete data likelihood over all
variables—observed words X, latent topic assignments Z, matrix B, and component/expert distributions φ:

(5)

(11)

i:zi =k

where p(bkc |B −(kc) , γ) is given by Eq. 8,


=

V
v=1
V
v=1

ˆ
φnkv
cv

C
j=1

bkj
φjv

bkc
−||ˆ k ||1
n



−(kc)
, γ)
 p(bkc |B

p(X|Z, B, φ)p(Z|α)p(B|γ)p(φ|β) (4)

(12)

This follows from the conditional independence assumptions. It is tractable to integrate out all parameters except Z, B, φ and hyperparameters α, β, γ. 11

and where nkv is the count of words assigned topic
ˆ
k that are type v, and ||ˆ k ||1 (the L1 -norm of count
n
vector nk ) is the count of all words with topic k.
ˆ

11

For simplicity, we switch from indexing examples as xmn
to xi . In this presentation, xi is the ith example in the corpus,

791

which corresponds to some m, n pair.

References
David Blei and John Lafferty. 2006. Correlated topic
models. In Advances in Neural Information Processing Systems (NIPS), volume 18.
David Blei and Jon McAuliffe. 2007. Supervised topic
models. In Advances in Neural Information Processing Systems (NIPS).
David Blei, Andrew Ng, and Michael Jordan. 2003. Latent dirichlet allocation. Journal of Machine Learning
Research, 3.
David Blei, Thomas Grifﬁths, Michael Jordan, and
Joshua Tenenbaum. 2004. Hierarchical topic models
and the nested chinese restaurant process. In Advances
in Neural Information Processing Systems (NIPS), volume 16.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the EM
algorithm. Journal of the Royal Statistical Society. Series B (Methodological), 39(1):1–38.
Jacob Eisenstein, Amr Ahmed, and Eric P. Xing. 2011.
Sparse additive generative models of text. In International Conference on Machine Learning (ICML).
Matthew R. Gormley, Mark Dredze, Benjamin Van
Durme, and Jason Eisner. 2011. Shared components
topic models with application to selectional preference. In Learning Semantics Workshop at NIPS 2011,
December.
Thomas Grifﬁths and Zoubin Ghahramani. 2006. Inﬁnite
latent feature models and the indian buffet process. In
Advances in Neural Information Processing Systems
(NIPS), volume 18.
Gregor Heinrich. 2008. Parameter estimation for text
analysis. Technical report, Fraunhofer IGD.
Katherine A. Heller and Zoubin Ghahramani. 2007. A
nonparametric bayesian approach to modeling overlapping clusters. In Artiﬁcial Intelligence and Statistics (AISTATS), pages 187–194.
Geoffrey Hinton. 1999. Products of experts. In International Conference on Artiﬁcial Neural Networks
(ICANN).
Geoffrey Hinton. 2002. Training products of experts by
minimizing contrastive divergence. Neural Computation, 14(8):1771–1800.
Wei Li and Andrew McCallum. 2006. Pachinko allocation: DAG-structured mixture models of topic correlations. In International Conference on Machine Learning (ICML), pages 577–584.
Wei Li, David Blei, and Andrew McCallum. 2007. Nonparametric bayes pachinko allocation. In Uncertainty
in Artiﬁcial Intelligence (UAI).
David Mimno, Wei Li, and Andrew McCallum. 2007.
Mixtures of hierarchical topics with pachinko allocation. In International Conference on Machine Learning (ICML), pages 633–640.

792

John Paisley, Chong Wang, and David Blei. 2011. The
discrete inﬁnite logistic normal distribution for MixedMembership modeling. In International Conference
on Artiﬁcial Intelligence and Statistics (AISTATS).
Yee Whye Teh, Michael Jordan, Matthew Beal, and
David Blei.
2006.
Hierarchical dirichlet processes. Journal of the American Statistical Association, 101(476):1566–1581.
Hanna Wallach, Ian Murray, Ruslan Salakhutdinov, and
David Mimno. 2009. Evaluation methods for topic
models. In International Conference on Machine
Learning (ICML), pages 1105–1112.
Greg Wei and Martin Tanner. 1990. A monte carlo implementation of the EM algorithm and the poor man’s
data augmentation algorithms. Journal of the American Statistical Association, 85(411):699–704.
Sinead Williamson, Chong Wang, Katherine Heller, and
David Blei. 2010. The IBP compound dirichlet
process and its application to focused topic modeling. In International Conference on Machine Learning (ICML).
Xiaojin Zhu, David Blei, and John Lafferty. 2006.
TagLDA: bringing document structure knowledge into
topic models. Technical Report TR-1553, University
of Wisconsin.

