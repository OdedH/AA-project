Proceedings of the 18th International Conference on Computational Linguistics
(COLING 2000), Saarbr¨cken, Germany, August 2000. [Minor edits for clarity.]
u

Directional Constraint Evaluation in Optimality Theory∗
Jason Eisner
Department of Computer Science / University of Rochester
Rochester, NY 14607-0226 (U.S.A.) / jason@cs.rochester.edu
to-right evaluation, the constraint prefers
forms whose oﬀenses are as late as possible.
To compare two forms, it aligns them (according to their common underlying representation), and scans them in parallel from
left to right, until reaching an underlying
location where one form has more oﬀenses
than the other (“sudden death”). Rightto-left evaluation is similar. The number
of oﬀenses (and amount of surface material)
per underlying location is unbounded.

Weighted ﬁnite-state constraints that can count unboundedly many violations make Optimality Theory
more powerful than ﬁnite-state transduction (Frank
and Satta, 1998). This result is empirically and computationally awkward. We propose replacing these
unbounded constraints, as well as non-ﬁnite-state
Generalized Alignment constraints, with a new class
of ﬁnite-state directional constraints. We give linguistic applications, results on generative power, and
algorithms to compile grammars into transducers.

1

Introduction

Optimality Theory is a grammar framework
that directly expresses constraints on phonological forms. Roughly, the grammar prefers forms
that violate each constraint as little as possible.
Most constraints used in practice describe
disfavored local conﬁgurations in the phonological form (Eisner, 1997a). It is therefore possible for a given form to oﬀend a single constraint
at several locations in the form. (For example,
a constraint against syllable codas will be offended by every syllable that has a coda.)
When comparing forms, then, how do we aggregate a form’s multiple local oﬀenses into an
overall violation level?
A constraint could answer this question in at
least three ways, the third being our proposal:
• Unbounded evaluation (Prince and
Smolensky, 1993). A form’s violation level
is given by the number of oﬀenses. Forms
with fewer oﬀenses are preferred.
• Bounded evaluation (Frank and Satta,
1998; Karttunen, 1998). A form’s violation level is min(k, number of oﬀenses) for
some k. This is like unbounded evaluation
except that the constraint does not distinguish among forms with ≥ k oﬀenses.1
• Directional evaluation. A form’s violation level considers the location of offenses, not their total number. Under left∗

I am grateful to the 3 anonymous referees for feedback.
1
Note that k = 1 gives “binary” constraints that can
be described simply as languages. Any k-bounded constraint can easily be simulated by k binary constraints.

§2 of this paper gives linguistic and computational motivation for the proposal. §3 formalizes
the idea and shows that composing a transducer
with a directional constraint yields a transducer.
Thus directional constraints, like bounded ones,
keep OT within the class of regular relations.
(But we also show them to be more expressive.)

2

Motivation

2.1 Intuitions
Recall that OT’s constraint ranking mechanism
is an answer to the question: How can a grammar evaluate a form by aggregating its violations of several constraints? Above we asked
the same question at a ﬁner scale: How can a
constraint evaluate a form by aggregating its offenses at several locations? Figure 1 illustrates
that our answer is just constraint ranking redux.
Directional evaluation strictly ranks the importance of the locations within a form, e.g.,
from left to right. This exempliﬁes OT’s “do
only when necessary” strategy: the constraint
prefers to postpone oﬀenses until they become
strictly necessary toward the right of the form,
even at the cost of having more of them.
One might think from Figure 1 that each directional constraint could be decomposed into
several binary or other bounded constraints,
yielding a grammar using only bounded constraints. However, no single such grammar is
general enough to handle all inputs: the number of constraints needed for the decomposition
corresponds to the length (i.e., the number of
locations) of the underlying representation.

(a)
ban.to.di.bo
ban.ton.di.bo
ban.to.dim.bon
ban.ton.dim.bon



C1 NoCoda (b)

C1

σ1

*!

*!

*

*
**
***!

σ4

(c)

C1

σ4

σ3

σ2
*

*

*

*!

σ1
*



*!

*

***!*

σ3

*!

*



σ2

*

*!

*

*

*

*!

*

*
*

*

*

Figure 1: Directional evaluation as subconstraint ranking. All candidates have 4 syllables; we simplify here
by regarding these as the locations. C1 is some high-ranked constraint that eliminates ban.to.di.bo; NoCoda
is oﬀended by syllable codas. (a) Traditional unbounded evaluation of NoCoda. (b) Left-to-right evaluation
of NoCoda, shown as if it were split into 4 constraints evaluating the syllables separately. (c) Right-to-left.

as possible (McCarthy and Prince, 1995),
postponing the appearance of an aﬃx edge
or other aﬃx material within the stem.2
• Floating non-lexical material must also appear somewhere. If a high-ranked constraint, Culminativity, requires that a
primary stress mark appear on each word,
then a directional constraint against primary stress will not only prevent additional
marks but also push the single mark to the
ﬁrst or last available syllable—the traditional “End Rule” (Prince, 1983).
• Harmony must decide how far to spread
features, and OCP eﬀects such as Grassman’s Law must decide which copies of
a feature to eliminate. Again, directional
evaluation seems to capture the facts.

2.2 Iterative and ﬂoating phenomena
The main empirical motivation for directionally evaluated constraints is the existence of “iterative” phenomena such as metrical footing.
(Derivational theories described these with procedures that scanned a form from one end to
the other and modiﬁed it; see (Johnson, 1972).)
For most other phenomena, directional constraints are indistinguishable from traditional
unbounded constraints. Usually, the candidates
with the fewest oﬀenses are still the ones that
survive. (Since their competitors oﬀend at exactly the same locations, and more.) This is
precisely because most phonology is local: satisfying a constraint at one location does not usually block satisfying it at another.
Distinguishing cases, like the artiﬁcial Fig.
1—where the constraint can only trade oﬀenses
at one location for oﬀenses at another—arise
only under special conditions involving nonlocal phenomena. Just as directional evaluation
would predict, such a forced tradeoﬀ is always
resolved (to our knowledge) by placing oﬀenses
as late, or as early, as higher constraints allow:

2.3 Why not Generalized Alignment?
In OT, following a remark by Robert Kirchner, it has been traditional to analyze such phenomena using highly non-local Generalized
Alignment (GA) constraints (McCarthy and
Prince, 1993). For example, left-to-right footing is favored by Align-Leftσ (Foot, Stem),
which requires every foot to be left-aligned
with a morphological stem. Not only does
each misaligned foot oﬀend the constraint, but
the seriousness of its oﬀense is given by the

• Prosodic groupings force each segment or
syllable to choose which constituent (if
any) to associate with. So-called left-toright directional syllabiﬁcation (Mester and
Padgett, 1994) will syllabify /CVCCCV/
greedily as CVC.CV.CV rather than
CV.CVC.CV, postponing epenthetic material until as late as possible.
Similarly, left-to-right binary footing (Hayes,
1995) prefers (σσ)(σσ)σ over σ(σσ)(σσ) or
(σσ)σ(σσ), postponing unfooted syllables.
• Floating lexical material must surface
somewhere in the form. Floating features
(e.g., tone) tend to dock at the leftmost
or rightmost available site, postponing the
appearance of these marked features. Inﬁxed morphemes tend to be inﬁxed as little

2

“Available site” and “possible” amount of inﬁxation
are deﬁned here by higher-ranked constraints. These
might restrict the allowed tone-bearing units and the
allowed CV shape after inﬁxation, but do not fully determine where the ﬂoating material will surface.
A referee asks why codas do not ﬂoat (to postpone
NoCoda oﬀenses). For the same reason they do not
delete: “moving” an underlying coda violates local deletion and insertion constraints, so is even worse! Floating tones require special mechanisms for non-local faithfulness: Gen or a constraint must ensure that the anchored tone sequence resembles the underlying ﬂoating
one, which may be represented locationlessly, on an auxiliary input tape (or, if bounded, as an input preﬁx).

2

number of syllables by which it is misaligned.
These numbers are summed over all oﬀending
feet to obtain the violation level. For example, [σ(σσ)(σσ)σ(σσ)]Stem has 1+3+6=10 violations, and [σσσσ(σσ)(σσ)]Stem is equally bad
at 4+6=10 violations. Shifting feet leftward or
eliminating them reduces the violation level.
Stemberger (1996) argued that GA constraints were too powerful. Ellison (1995)
showed that no single ﬁnite-state unbounded
constraint could deﬁne the same violation levels as a GA constraint. Eisner (1997a) showed
more strongly that since GA can be made to
center a ﬂoating tone on a phrase,3 no hierarchy of ﬁnite-state unbounded constraints could
even deﬁne the same optimal candidates as a
GA constraint. Thus GA cannot be simulated
in Ellison’s (1994) ﬁnite-state framework (§3.2).
For this reason, as well as the awkwardness
and non-locality of evaluating GA oﬀenses, we
propose to replace GA with directional constraints. Directional constraints appear to more
directly capture the observed phenomena.
We do note that another, trickier possibility is
to eliminate GA in favor of ordinary unbounded
constraints that are indiﬀerent to the location
of oﬀenses. Ellison (1994) noted that GA constraints that evaluated the placement of only
one element (e.g., primary stress) could be replaced by simpler NoIntervening constraints.
Eisner (1997b) gives a GA-free treatment of the
metrical stress typology of (Hayes, 1995).
2.4

ing many relevant tasks rapidly: generation (obtaining all possible SRs for a UR), comprehension (conversely), characterizing the set of forms
on which two grammars (even one derivational
and one OT grammar) would diﬀer, etc. Moreover, FSTs can be applied in parallel to regular
sets of forms. For example, one can obtain a
weighted set of possible SRs (a phoneme lattice) from a speech recognizer, pass it through
the inverted transducer, intersect the resulting
weighted set of URs with the lexicon, and then
extract the best surviving URs.
Ellison (1994) and Eisner (1997a) frame OT
within this tradition, by modeling Gen and the
constraints as weighted ﬁnite-state machines
(see §3.2). But although those papers showed
how to generate the set of SRs for a single given
UR, they did not compile the OT grammar into
an FST, or obtain the other beneﬁts thereof.
In fact, Frank and Satta (1998) showed
that such compilation is impossible in the general case of unbounded constraints. To see
why, consider the grammar Max, Dep, Harmony[height]
Ident-IO[height]. This grammar insists on height harmony among surface
vowels, but dislikes changes from the UR. The
result is the unattested phenomenon of “majority assimilation” (Bakovi´, 1999; Lombardi,
c
1999): a UR with more high vowels than low
will surface with all vowels high, and vice-versa.
So OT may compare unbounded counts in a way
that an FST cannot and phonology does not.
This suggests that OT with unbounded constraints is too powerful. Hence Frank and Satta
(1998) and Karttunen (1998) propose using only
bounded constraints. They show this reduces
OT’s power to ﬁnite-state transduction.
The worry is that bounded constraints may
not be expressive enough. A 2-bounded version
of NoCoda would not distinguish among the
ﬁnal three forms in Figure 1: it is agnostic when
the input forces multiple codas in all candidates.
To be sure, a k-bounded approximation may
work well for large k.5 But its automaton (§3.2)
will typically have k times as many states as the
unbounded original, since it unrolls loops: the

Generative power

It has recently been proposed that for computational reasons, OT should eliminate not only
GA but all unbounded constraints (Frank and
Satta, 1998; Karttunen, 1998). As with GA,
we oﬀer the less extreme approach of replacing
them with directional constraints instead.
Recall that a phonological grammar, as usually conceived, is a description of permissible
(UR, SR) pairs.4 It has long been believed
that naturally occurring phonological grammars
are regular relations (Johnson, 1972; Kaplan
and Kay, 1994). This means that they can
be implemented as ﬁnite-state transducers
(FSTs) that accept exactly the grammatical
pairs. FSTs are immensely useful in perform3
4

5
In particular, in the case of phonological generation,
an output of this approximate grammar is guaranteed
correct unless it achieves k violations for some k-bounded
constraint. One can then raise k, recompile the grammar, and try again. But k may grow quite large for long
inputs like phonological phrases.

This is indeed too powerful: centering is unattested.
UR = underlying representation, SR = surface repn.

3

state must keep track of the oﬀense count. Intersecting many such large constraints can produce very large FSTs—while still failing to capture simple generalizations, e.g., that all codas
are dispreferred.
In §3, we will show that directional constraints are more powerful than bounded
constraints, as they can express such
generalizations—yet they keep us within
the world of regular relations and FSTs.

Given a UR, σ, as input, the grammar admits
as its SRs all the outputs δ such that C(δ) is lexicographically minimal in {C(δ) : δ ∈ Gen(σ)}.
The values taken by Ci are called its violation levels. Conventionally these have been
natural numbers, but any ordered set will do.
Our directional constraints require the following innovations. Each input σ is a string as
usual, but the outputs are not strings. Rather,
each candidate δ ∈ Gen(σ) is a tuple of |σ| + 1
strings. We write δ for the concatenation of
these strings (the “real” SR). So δ speciﬁes an
alignment of δ ∈ ∆∗ with σ ∈ Σ∗ . Directional
constraint Ci maps δ to a violation level—itself
a length-(|σ| + 1) tuple of natural numbers that
counts the oﬀenses per position in underlying σ.
Violation levels are compared lexicographically.

2.5 Related Work
Walther (1999), working with intersective constraints, deﬁnes a similar notion of Bounded
Local Optimization (BLO). Trommer (1998;
1999) applies a variant of Walther’s idea to OT.
The motivation in both cases is linguistic.
We sketch how our idea diﬀers via 3 examples:
UR uuuuu uu uuu uuuuu
candidate X vvvbb vv vbb vvvbb
candidate Y vvbaa vvvvbaa vz baa
Consider ∗b, a left-to-right constraint that is offended by each instance of b. On our proposal,
candidate X wins in each column, because Y
always oﬀends ∗b ﬁrst, at position 3 in the UR.
But under BLO, this oﬀense is not fatal. Y
can survive ∗b by inserting epenthetic material
(column 2: Y wins by postponing b relative to
the SR), or by changing v to z (column 3: Y ties
X, since vv = vz and BLO merely requires the
cheapest choice given the surface output so far).
In the same way, NoCoda under BLO would
trigger many changes unrelated to codas. Our
deﬁnition avoids these apparent inconveniences.
Walther and Trommer do not consider the
expressive power of BLO (cf. §3.3) or whether
grammars can be compiled into UR-to-SR FSTs
(our main result; see discussion in §3.4).

3

3.2 Finite-state assumptions
We now conﬁne our attention to ﬁnite-state OT
grammars, following (Ellison, 1994; Tesar, 1995;
Eisner, 1997a; Frank and Satta, 1998; Karttunen, 1998). Gen ⊆ Σ∗ × ∆∗ is a regular
relation,6 and may be implemented as an unweighted FST. Each constraint is implemented7
as a possibly nondeterministic, weighted ﬁnitestate automaton (WFSA) that accepts ∆∗ and
whose arcs are weighted with natural numbers.
An FST, T , is a ﬁnite-state automaton in
which each arc is labeled with a string pair α : γ.
Without loss of generality, we require |α| ≤ 1.
This lets us deﬁne an aligned transduction
that maps strings to tuples: If σ = a1 · · · an ,
we deﬁne T (σ) as the set of (n + 1)-tuples
δ = (δ0 , δ1 , . . . δn ) such that T has a path transducing σ : δ along which δ0 · · · δi−1 is the complete output before ai is read from the input.
We now describe how to evaluate C(δ) where
C is a WFSA. Consider the path in C that accepts δ.8 In (un)bounded evaluation, C(δ) is
the total weight of this path. In left-to-right
evaluation, C(δ) is the n + 1 tuple giving the respective total weights of the subpaths that consume δ0 , . . . δn . In right-to-left evaluation, C(δ)
is the reverse of the previous tuple.9

Formal Results

3.1 Deﬁnition of OT
An OT grammar is a pair (Gen, C) where
• the candidate generator Gen is a relation
that maps each input string (in Σ∗ ) to a
nonempty set of candidate outputs;

6

Ellison required only that Gen(σ) be regular (∀σ).
Space prevents giving the equivalent characterization as a locally weighted language (Walther, 1999).
8
If there are multiple accepting paths (nondeterminism), take the one that gives the least value of C(δ).
9
R
R
This is equivalent to C R (δn , . . . , δ0 ) where R denotes reversal of the automaton or string as appropriate.
7

• the hierarchy C = (C1 , C2 , . . .) is a ﬁnite
tuple of constraint functions that evaluate
outputs.
We write C(δ) for the tuple (C1 (δ), C2 (δ), . . .).
4

3.3 Expressive power
Thanks to Gen, ﬁnite-state OT can trivially implement any regular input-output relation with
no constraints at all! And §3.4 below shows that
whether we allow directional or bounded constraints does not aﬀect this generative power.
But in another sense, directional constraints
are strictly more expressive than bounded ones.
If Gen is ﬁxed, then any hierarchy of bounded
constraints can be simulated by some hierarchy
of directional constraints10 —but not vice-versa.
Indeed, we show even more strongly that directional constraints cannot always be simulated even by unbounded constraints.11 Deﬁne
∗b as in §2.5. This ranks the set (a|b)n in lexicographic order, so it makes 2n distinctions. Let
Gen be the regular relation

3.4.1 The outer loop of compilation
Let T0 = Gen. For i > 0, we will construct
an FST Ti that implements the partial grammar (Gen, C1 , C2 , . . . Ci ). We construct Ti from
Ti−1 and Ci only: Ti (x) contains the forms
y ∈ Ti−1 (x) for which Ci (y) is minimal.
If Ci is k-bounded, we use the construction of
(Frank and Satta, 1998; Karttunen, 1998).
If Ci is a left-to-right constraint, we compose
Ti−1 with the WFSA that represents Ci , obtaining a weighted ﬁnite-state transducer (WFST),
ˆ
Ti . This transducer may be regarded as assigning a Ci -violation level (an (|σ| + 1)-tuple) to
each σ : δ it accepts. We must now prune away
the suboptimal candidates: using the DBP algorithm below, we construct a new unweighted
ˆ
FST Ti that transduces σ : δ iﬀ the weighted Ti
can transduce σ : δ as cheaply as any σ : δ .
If Ci is right-to-left, we do just the same, exˆ
cept DBP is used to construct TiR from TiR .
3.4.2 Directional Best Paths: The idea
All that remains is to give the construction of
ˆ
Ti from Ti , which we call Directional Best
Paths (DBP). Recall standard best-paths or
shortest-paths algorithms that pare a WFSA
down to its paths of minimum total weight (Dijkstra, 1959; Ellison, 1994). Our greedier version does not sum along paths; it always immediately takes the lightest “available” arc. But:
Crucially, available arcs are deﬁned relative to
the input string, because we must retain one or
more optimal output candidates for each input.
So availability requires “lookahead”: we must
take a heavier arc (b : x below) just when the
rest of the input (e.g., abd) cannot otherwise be
accepted on any path.
c:c

(a : a|b : b)∗ (c : a(a : a|b : b)∗ | c : b(a : a|b : b|a : b|b : a)∗ )
We claim that the grammar (Gen, ∗b) is not
equivalent to (Gen, C1 , . . . , Cs ) for any bounded
or unbounded constraints C1 , . . . Cs . There exists k such that for all (n, δ ∈ ∆n ), each Ci (δ) <
kn.12 So candidates δ of length n have at most
(kn)s diﬀerent violation proﬁles C(δ). Choose n
such that 2n > (kn)s . Then the set of 2n strings
(a|b)n must contain two distinct strings, δ =
x1 · · · xn and δ = y1 · · · yn , with C(δ) = C(δ ).
Let i be minimal such that xi = yi , and without loss of generality assume xi = a, yi = b. Put
σ = x1 · · · xi−1 cxi+1 · · · xn . Now δ, δ ∈ Gen(σ)
and δ is lexicographically minimum in Gen(σ).
So the grammar (Gen, ∗b) maps σ to δ only,
whereas (Gen, C) cannot distinguish between δ
and δ , so it maps σ to neither or both.
3.4 Grammar compilation: OT = FST
It is trivial to translate an arbitrary FST grammar into OT: let Gen be the FST, and C = ().
The rest of this section shows, conversely, how
to compile a ﬁnite-state OT grammar (Gen, C)
into an FST, provided that the grammar uses
only bounded and/or directional constraints.

b:b
1

a:a

2

4

3

b:x

a: ε
5

c:c

6

ˆ
d:d
Ti (abd) = {axd, xd}
ˆ
Ti (abc) = {abc, bc, axc, xc}
7
(xd abbreviates ( , x, d)) suboptimal
On this example, DBP would simply make state
6 non-ﬁnal (disallowing the b : x arc for input
abc but not abd); but often it must add states!
This input sensitivity is what lets us compile a
hierarchy of directional constraints, once and for
all, into an single FST that can ﬁnd the optimal
output for any of the inﬁnitely many possible inputs. We saw in §2.4 why this is so desirable. By

10
How? By using states to count, a bounded constraint’s WFSA can be transformed so that all the weight
of each path falls on its ﬁnal arc. This deﬁnes the same
optimal candidates, even when interpreted directionally.
11
Nor vice-versa, since only unbounded constraints can
implement non-regular relations (§2.4,§3.4).
12
Eliminate from the constraints’ WDFAs (regard as
outputless WFSTs, use §3.4.4) so δ-reading paths have
length n. Take k to exceed all arc weights in the result.

5

contrast, Ellison’s (1994) best-paths construction for unbounded constraints, and previously
proposed constructions for directional-style constraints (see §2.5) only ﬁnd the optimal output
for a single input, or at best a ﬁnite lexicon.
3.4.3 Dir. Best Paths: A special case
§3.2 restricted our FSTs such that for every arc
label α : γ, |α| ≤ 1. In this section we construct
ˆ
Ti from Ti under the stronger assumption that
ˆ
|α| = 1, i.e., Ti is -free on the input side.
ˆ
If Q is the stateset of Ti , then let the stateset
of Ti be {[q; R; S] : R ⊆ S ⊆ Q, q ∈ S − R}.
This has size |Q| · 3|Q|−1 . However, most of
these states are typically unreachable from the
start state. Lazy “on-the-ﬂy” construction techniques (Mohri, 1997) can be used to avoid allocating states or arcs until they are discovered
during exploration from the start state.
For σ ∈ Σ∗ , q ∈ Q, deﬁne V (σ, q) as the minimum “cost” (a |σ|-tuple of arc weights) of any
ˆ
σ-reading path from Ti ’s start state q0 to q.
The start state of Ti is [q0 ; ∅; {q0 }]. The intent
is that Ti have a path from its start state to
[q; R; S] that alignedly transduces σ : δ 13 iﬀ
ˆ
• Ti has a q0 to q, σ : δ path of cost V (σ, q);
• R = {q ∈ Q : V (σ, q ) < V (σ, q)}; and
• S = {q ∈ Q : V (σ, q ) ≤ V (σ, q)}.
ˆ
So as Ti reads σ, it “follows” Ti ’s cheapest σreading paths to q, while calculating R, to which
yet cheaper (but perhaps dead-end) paths exist.
Let [q; R; S] be a ﬁnal state (in Ti ) iﬀ q is ﬁnal
ˆ
and no q ∈ R is ﬁnal (in Ti ). So an accepting
ˆi survives into Ti iﬀ there is no lowerpath in T
ˆ
cost accepting path in Ti for the same input.
The arcs from [q; R; S] correspond to arcs
from q. For each arc from q to q labeled a : γ
and with weight W, add an unweighted a : γ
arc from [q; R; S] to [q ; R ; S ], provided that
the latter state exists (i.e., unless q ∈ R , indicating that there is a cheaper path to q ). Here
R is the set of states that are either reachable
from R by a (single) a-reading arc, or reachable
from S by an a-reading arc of weight < W . S
is the union of R and all states reachable from
S by an a-reading arc of weight W .
3.4.4 Dir. Best Paths: The general case
To apply the above construction, we must ﬁrst
ˆ
transform Ti so it is -free on the input side. Of
13

course input ’s are crucial if Gen is to be allowed
to insert unbounded amounts of surface material (to be pruned back by the constraints).14
To eliminate ’s while still allowing unbounded
insertion, we are forced to introduce FST arc
labels of the form a : Γ where Γ is actually a
regular set of strings, represented as an FSA or
regular expression. Following -elimination, we
can apply the construction of §3.4.3 to get Ti ,
and ﬁnally convert Ti back to a normal transducer by expanding each a : Γ into a subgraph.
When we eliminate an arc labeled : γ, we
must push γ and the arc’s weight back onto
a previous non- arc (but no further; contrast
(Mohri, 1997)). The resulting machine will imˆ
plement the same aligned transduction as Ti but
more transparently: in the notation of §3.2, the
arc reading ai will transduce it directly to δi .15
ˆ
Concretely, suppose Ti can get from state q
to q via a path of total weight W that begins
with a : γ1 on its ﬁrst arc followed by : γ2 ,
: γ3 , . . . on its remaining arcs. We would like
to substitute an arc from q to q with label
a : γ1 γ2 γ3 . . . and weight W . But there may
be inﬁnitely many such q–q paths, of varying
weight, so we actually write a : Γ, where Γ describes just those q–q paths with minimum W .
The exact procedure is as follows. Let G be
ˆ
the possibly disconnected subgraph of Ti formed
by -reading arcs. Run an all-pairs shortestpaths algorithm16 on G. This ﬁnds, for each
state pair (q , q ) connected by an -reading
path, the subgraph Gq ,q of G formed by the
minimum-weight -reading paths from q to q ,
as well as the common weight Wq ,q of these
ˆ
paths. So for each arc in Ti from q to q , with
weight W and label a : γ, we now add an arc
ˆ
to Ti from q to q with weight W + Wq ,q and
label a : γGq ,q ( ). (G( ) denotes the regular
language to which G transduces .) Having done
this, we can delete all -reading arcs.
ˆ
The modiﬁed -free Ti is equivalent to the
14

As is conventional. Besides epenthetic material, Gen
often introduces copious prosodic structure.
15
That arc is labeled ai : Γ where δi ∈ Γ. But what is
a0 ? A special symbol E ∈ Σ that we introduce so that
δ0 can be pushed back onto it: Before -elimination, we
ˆ
modify Ti by giving it a new start state, connected to
the old start state with an arc E : . After -elimination,
we apply DBP and replace E with in the result Ti .
16
Cormen et al. (1990) cite several, including fast algorithms for when edge weights are small integers.

δ is a tuple of |σ|+1 strings, but δ0 = by -freeness.

6

original except for eliminating some of the suboptimal subpaths. Here is a graph fragment before and after -elimination:
ε :b
ε :b
1

a:a

3

a:a

ε :c
1

2

ε :d

4

2

a:ab+
a:ad

ing primitive constraints in OT. In Benjamin Bruening, editor, Proc. of SCIL VIII, MIT Working
Papers in Linguistics 31, Cambridge, MA.
T. Mark Ellison. 1994. Phonological derivation in
optimality theory. In Proc. of COLING.
T. Mark Ellison. 1995. OT, ﬁnite-state representations and procedurality. In Proc. of the Conference on Formal Grammar, Barcelona.
Robert Frank and Giorgio Satta. 1998. Optimality Theory and the generative complexity of constraint violability. Comp. Ling., 24(2):307–315.
Bruce Hayes. 1995. Metrical Stress Theory: Principles and Case Studies. U. of Chicago Press.
C. Douglas Johnson. 1972. Formal Aspects of
Phonological Description. Mouton.
Ronald M. Kaplan and Martin Kay. 1994. Regular
models of phonological rule systems. Computational Linguistics, 20(3):331–378.
Lauri Karttunen. 1998. The proper treatment of
optimality in computational phonology. In Proc.
of FSMNLP’98, 1–12, Bilkent U., Ankara, Turkey.
Linda Lombardi. 1999. Positional faithfulness and
voicing assimilation in Optimality Theory. Natural Language and Linguistic Theory, 17:267–302.
John McCarthy and Alan Prince. 1993. Generalized
alignment. In Geert Booij and Jaap van Marle,
editors, Yearbook of Morphology, 79–153. Kluwer.
John McCarthy and Alan Prince. 1995. Faithfulness
and reduplicative identity. In Jill Beckman et al.,
editor, Papers in Optimality Theory, 259–384. U.
of Massachusetts, Amherst: GLSA.
Armin Mester and Jaye Padgett. 1994. Directional
syllabiﬁcation in Generalized Alignment. Phonology at Santa Cruz 3, October.
Mehryar Mohri. 1997. Finite-state transducers in
language & speech processing. Comp. Ling. 23(2).
Alan Prince and Paul Smolensky. 1993. Optimality Theory: Constraint interaction in generative
grammar. Ms., Rutgers U. and U. of Colorado.
Alan Prince. 1983. Relating to the grid. Linguistic
Inquiry, 14:19–100.
J. P. Stemberger. 1996. The scope of the theory:
Where does beyond lie? In L. McNair, K. Singer,
L. M. Dobrin, and M. M. Aucoin, eds. Papers from
the Parasession on Theory and Data in Linguistics, CLS 23, 139–164. Chicago Linguistic Society.
Bruce Tesar. 1995. Computational Optimality Theory. Ph.D. thesis, U. of Colorado, Boulder.
Jochen Trommer. 1998. Optimal morphology. In
T. Mark Ellison, editor, Proc. of the 4th ACL
SIGPHON Workshop, Quebec, July.
Jochen Trommer. 1999. Mende tone patterns revisited: Tone mapping as local constraint evaluation.
In Linguistics in Potsdam Working Papers.
Markus Walther. 1999. One-level prosodic morphology. Arbeiten zur Linguistik 1, U. of Marburg.

3
4

Note: Right-to-left evaluation applies DBP to
ˆ
TiR , so consistency with our previous deﬁnitions
means it must push ’s forward, not backward.

4

Conclusions

This paper has proposed a new notion in OT:
“directional evaluation,” where underlying locations are strictly ranked by their importance.
Traditional ﬁnite-state OT constraints have
enough power to compare arbitrarily high
counts; Generalized Alignment is even worse.
Directional constraints seem to capture the pros
of these constraints: they appropriately militate against every instance of a disfavored conﬁguration in a candidate form, no matter how
many, and they naturally capture iterative and
edgemost eﬀects. Yet they do not have the excess power: we have shown that an grammar of
directional and/or bounded constraints can be
compiled into a ﬁnite-state transducer. That is
both empirically and computationally desirable.
The most obvious future work comes from linguistics. Can directional constraints do all the
work of unbounded and GA constraints? How
do they change the style of analysis? (E.g., directional versions of markedness constraints pin
down the locations of marked objects, leaving
lower-ranked constraints no say.) Finally, directional constraints can be variously formulated
(is *Cluster oﬀended at the start, or end, of
each cluster? or of its enclosing syllable?). So
what conventions or restrictions should apply?

References
Eric Bakovi´. 1999. Assimilation to the unmarked.
c
Ms., Rutgers Optimality Archive ROA-340.
T. H. Cormen, C. E. Leiserson, and R. L. Rivest.
1990. Introduction to Algorithms. MIT Press.
Edsger W. Dijkstra. 1959. A note on two problems
in connexion with graphs. Numerische Mathematik, 1:269–271.
Jason Eisner. 1997a. Eﬃcient generation in primitive Optimality Theory. In Proc. of the 35th Annual ACL and 8th EACL, Madrid, July, 313–320.
Jason Eisner. 1997b. FootForm decomposed: Us-

7

