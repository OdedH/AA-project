Machine Learning with Annotator Rationales
to Reduce Annotation Cost
Jason Eisner
Department of Computer Science
Johns Hopkins University
Baltimore, MD 21218
jason@cs.jhu.edu

Omar F. Zaidan
Department of Computer Science
Johns Hopkins University
Baltimore, MD 21218
ozaidan@cs.jhu.edu

Christine D. Piatko
Human Language Technology Center of Excellence
Johns Hopkins University
Baltimore, MD 21211
Christine.Piatko@jhuapl.edu

Abstract
We review two novel methods for text categorization, based on a new framework
that utilizes richer annotations that we call annotator rationales. A human annotator provides hints to a machine learner by highlighting contextual “rationales” in
support of each of his or her annotations. We have collected such rationales, in the
form of substrings, for an existing document sentiment classiﬁcation dataset [1].
We have developed two methods, one discriminative [2] and one generative [3],
that use these rationales during training to obtain signiﬁcant accuracy improvements over two strong baselines. Our generative model in particular could be
adapted to help learn other kinds of probabilistic classiﬁers for quite different
tasks. Based on a small study of annotation speed, we posit that for some tasks,
providing rationales can be a more fruitful use of an annotator’s time than annotating more examples.

1

Introduction

Annotation cost is a bottleneck for many natural language processing applications. While supervised
machine learning systems are effective, it is labor-intensive and expensive to construct the many
training examples needed. Previous research has explored possible ways to lessen this burden, such
as active [4] or semi-supervised [5] learning or adaptation from a different but related domain [6].
A rather different paradigm is to change the actual task that is given to annotators, giving them a
greater hand in shaping the learned classiﬁer. After all, human annotators themselves are more than
just black-box classiﬁers to be run on training data. They possess some introspective knowledge
about their own classiﬁcation procedure. Our hope is to mine this knowledge rapidly via appropriate
questions and use it to help train a machine classiﬁer.
Annotators currently indicate what the correct answers are on training data. We propose that they
should also provide coarse hints about why. Since annotators do not know what features a future
machine learner will use, we propose that they should simply highlight relevant portions of the example, e.g., substrings or spatial regions that help to justify their annotations. We offer two learning
approaches that beneﬁt from these “rationales” at training time.
In some circumstances, rationales should not be too expensive or time-consuming to collect. As long
as the annotator is spending the time to study example xi and classify it, it may not require much
extra effort to indicate some reasons along the way. We show this experimentally in one domain.
1

2

Rationale Annotation for Movie Reviews

In order to demonstrate that annotator rationales help machine learning, we chose a dataset that
would be enjoyable to re-annotate: the movie review dataset of [1].1 The dataset consists of 1000
positive and 1000 negative movie reviews obtained from the Internet Movie Database (IMDb) review
archive, all written before 2002 by a total of 312 authors, with a cap of 20 reviews per author per
category. Pang and Lee have divided the 2000 documents into 10 folds, each consisting of 100
positive reviews and 100 negative reviews. These gold-standard classiﬁcations were derived from
the number of “stars” assigned by each review’s author.
The original dataset is arguably artiﬁcial in that it keeps only reviews where the reviewer provided
a rather high or rather low numerical rating, allowing Pang and Lee to designate the review as
positive or negative. Nonetheless, most reviews contain a difﬁcult mix of praise, criticism, and
factual description. In fact, it is possible for a mostly critical review to give a positive overall
recommendation, or vice versa.
2.1

Annotation procedure

Rationale annotators were given guidelines2 that instructed them to read reviews and justify why
a review is positive or negative by highlighting rationales for the document’s class—for a positive
review, phrases that would tell someone to see the movie; for a negative review, phrases that would
tell someone not to see the movie. The instructions provided only light guidance on how many
rationales to annotate, encouraging annotators to do their best to mark enough rationales to provide
convincing support, but emphasizing that they need not go out of their way to mark everything.
Annotators were shown the following examples3 of positive and negative rationales, among others:
• you will enjoy the hell out of American Pie. (Positive)
• he is one of the most exciting martial artists on the big screen, continuing to perform his own
stunts and dazzling audiences with his ﬂashy kicks and punches. (Positive)
• the romance was enchanting. (Positive)
• A woman in peril. A confrontation. An explosion. The end. Yawn. Yawn. Yawn. (Negative)
• the movie is so badly put together that even the most casual viewer may notice the miserable pacing
and stray plot threads. (Negative)
• don’t go see this movie. (Negative)

The annotation involves boldfacing the rationale phrases using an HTML editor. One of the authors
(A0) annotated folds 0–8 of the dataset (1,800 documents) with rationales that supported the goldstandard classes. That was our main training/development set, with fold 9 serving as the test fold.
The process of annotating rationales does not require the annotator to think about the feature space,
nor even to know anything about it. Arguably this makes annotation easier and more ﬂexible. It also
preserves the reusability of the annotated data. Anyone is free to reuse our collected rationales2 to
aid in learning a classiﬁer with richer features, using our methods or methods yet to be developed.
2.2

Inter-annotator agreement and annotation time

Our work in [2] considered the questions of inter-annotator agreement and annotation time. We
carried out a pilot study (two of the authors) and a later, more controlled study (two paid students),
where each annotator was given 150 un-annotated documents, spread equally over three tasks:
• Task 1: Given the document, annotate only the class (positive/negative).
• Task 2: Given the document and its class, annotate some rationales for that class.
• Task 3: Given the document, annotate both the class and some rationales for it.
1

Polarity dataset version 2.0.
Available at http://cs.jhu.edu/˜ozaidan/rationales.
3
For our controlled study of annotation time (section 2.2), different examples were given with full document
context.
2

2

The annotators’ classiﬁcation accuracies in Tasks 1 and 3 (against Pang & Lee’s labels) ranged from
92%–97%, with 4-way agreement on the class for 89% of the documents, and pairwise agreement
also ranging from 92%–97%. Results in [2] show they provided an average of 5–11 rationales per
document, with most rationales enjoying some degree of consensus, especially those marked by the
least thorough rationale annotator.
As for annotation time, we found that rationales did not take too much extra time for most annotators
to provide (see [2] for exact times). In general, providing rationales only took roughly twice the time
(Task 3 vs. Task 1) Why this low overhead? Because marking the class already required the Task 1
annotator to read the document and mentally ﬁnd some rationales. The only extra work in Task 3 is
in making them explicit. This synergy was demonstrated by the fact that doing both annotations at
once (Task 3) was faster than doing them separately (Tasks 1+2).
2.3

Other scenarios

We remark that the above task—binary classiﬁcation on full documents—seems to be almost a
worst-case scenario for the annotation of rationales. At a purely mechanical level, it was rather
heroic of A0 to attach 8–9 new rationale phrases rij to every bit yi of ordinary annotation.
Imagine by contrast a more local task of identifying entities or relations. Each lower-level annotation
yi will tend to have fewer rationales rij , while yi itself will be more complex and hence more difﬁcult
to mark. We expect that the overhead of collecting the rationales will be even less in such scenarios
than the factor of 2 we measured. For example, [2] brieﬂy discusses how one might conduct rationale
annotation for named entities, linguistic relations, or handwritten digits.
The overhead of rationale annotation could be further reduced by asking annotators to mark rationales for only a fraction of the annotations, or fewer rationales per annotation, since additional rationales may yield diminishing returns (see section 6.2). As a special case, for a multi-class problem
like relation detection, one could ask the annotator to provide rationales only for the rarer classes.
This small amount of extra time where the data is sparsest would provide extra guidance where it
was most needed.
Another possibility is passive collection of rationales via eye tracking. Rationale annotations might
also be collected for free via game play. In the visual domain, when classifying an image as containing a zoo, the annotator might circle some animals or cages and the sign reading “Zoo.” The
Peekaboom game [7] was in fact built to elicit such approximate yet relevant regions of images.

3

A Discriminative Approach to Utilizing Rationales

We ﬁrst suggest a modiﬁcation to the training of a Support Vector Machine (SVM) to incorporate
−
rationales. From the rationale annotations on a positive example →, we construct several “notxi
− . Each contrast document − is obtained by starting with
→
→
quite-as-positive” contrast examples vij
vij
the original and “masking out” one of the rationale substrings (rij ). The intuition: the correct
→
model should be less sure of a positive classiﬁcation on the contrast example − than on the original
vij
→
example xi , because − lacks evidence the annotator found signiﬁcant.
vij
We translate this intuition into additional constraints on the correct model: in addition to the usual
−
→
SVM constraint on positive examples that w·→ ≥ 1, we also want (for each j) that w·xi −w·− ≥ µ,
xi
vij
where µ ≥ 0 controls the size of the desired margin between original and contrast examples. In other
words, an ordinary soft-margin SVM chooses w and ξ to minimize
1
w 2 + C(
ξi )
(1)
2
i
subject to the constraints
−
(∀i) w · → · yi ≥ 1 − ξi
xi
(2)
(∀i) ξi ≥ 0
(3)
→ is a training example, y ∈ {−1, +1} is its desired classiﬁcation, and ξ is a slack variable
−
where xi
i
i
−
that allows training example → to miss satisfying the margin constraint if necessary. We add the
x
i

contrast constraints

− v
→
(∀i, j) w · (→ − − ) · yi ≥ µ(1 − ξij ),
xi
ij
3

(4)

→
−
where − is one of the contrast examples constructed from example →, and ξij ≥ 0 is an associated
vij
xi
slack variable. Just as these extra constraints have their own margin µ, their slack variables have
their own cost, so the objective function (1) becomes
1
w 2 + C(
ξi ) + Ccontrast (
ξij )
(5)
2
i
i,j
The parameter Ccontrast ≥ 0 determines the importance of satisfying the contrast constraints. It
should generally be less than C if the rationales are noisier than the training examples.
In practice, it is possible to solve this optimization using a standard soft-margin SVM learner. Dividing equation (4) through by µ, it becomes
→
(∀i, j) w · − · yi ≥ 1 − ξij ,
xij
(6)
→ −−
− v
→
− = xi ij . Since equation (6) takes the same form as equation (2), we simply add the
→ def
where xij
µ
→
pairs (− , y ) to the training set as pseudoexamples, weighted by C
x
rather than C so that the
ij

i

contrast

learner will use the objective function (5). In our experiments, we optimize µ, C, and Ccontrast on
held-out data (see subsection 5.1).
For our feature set, we exactly follow [1] in merely using binary unigram features, corresponding to
the 17,744 unstemmed word or punctuation types with count ≥ 4 in the full 2000-document corpus.
Thus, each document is reduced to a 0-1 vector with 17,744 dimensions (normalized to unit length).

4

A Generative Approach to Utilizing Rationales

Unlike our discriminative approach, our generative approach relies on an explicit, parametric model
of the relationship between the rationales and the optimal classiﬁer parameters θ (i.e., that we would
learn on an inﬁnite training set). This auxiliary model’s parameters, φ, capture what the annotator is
doing when marking rationales. Most importantly, they capture how he or she is inﬂuenced by the
true θ. Our learning method will prefer values for θ that would adequately explain the rationales r
(as well as explaining class labels y). To that end, we jointly choose parameter vectors θ and φ to
maximize the following regularized conditional likelihood:
n

n
def

pθ (yi | xi ) · pφ (ri | xi , yi , θ) · pprior (θ, φ)

p(yi , ri | xi , θ, φ) · pprior (θ, φ) =

(7)

i=1

i=1

Here we are trying to model all the annotations, both yi and ri . The ﬁrst factor predicts yi using
an ordinary probabilistic classiﬁer pθ , while the novel second factor predicts ri using some lowdimensional model pφ of how a particular annotator generates the rationale annotations.
The crucial point is that the second factor depends on θ (since ri is supposed to reﬂect the relation
between xi and yi that is modeled by θ). As a result, the learner has an incentive to modify θ in a way
that increases the second factor, even if this somewhat decreases the ﬁrst factor on training data.4
After training (subsection 5.2), we simply use the ﬁrst factor pθ (y | x) to classify test documents
x. The second factor is irrelevant for test documents, since they have not been annotated with
rationales r. The second factor may likewise be omitted for any training documents i that have not
been annotated with rationales, as there is no ri to predict in those cases. In the extreme case where
no documents are annotated with rationales, equation (1) reduces to the standard training procedure.
Main model: Modeling class annotations with pθ

4.1

Let us ﬁrst deﬁne the main classiﬁer pθ in equation (1) to be a standard conditional log-linear model:5
def

pθ (y | x) =

exp(θ · f (x, y)) def u(x, y)
=
Zθ (x)
Zθ (x)

4

(8)

Interestingly, even examples where the annotation yi is wrong or unhelpful can provide useful information
about θ via the pair (yi , ri ). Two annotators marking the same movie review might disagree on whether it is
overall a positive or negative review—but the second factor still allows learning positive features from the ﬁrst
annotator’s positive rationales, and negative features from the second annotator’s negative rationales.
5
In our binary classiﬁcation setting (y ∈ {−1, +1}), equation (8) is equivalent to logistic regression. We
nonetheless give the multi-class formulation (8), both for generality and for consistency with equation (9).

4

Figure 1: Rationales as sequence annotation: the annotator highlighted two textual segments as rationales for a
positive class. Highlighted words in x are tagged I in r, and other words are tagged O. The ﬁgure also shows
some φ-features. For instance, gO(,)-I is a count of O-I transitions that occur with a comma as the left word.
Notice also that grel is the sum of the underlined values.

where f (·) extracts a feature vector from a classiﬁed document, θ are the corresponding weights of
def
those features, and Zθ (x) = y u(x, y) is a normalizer. We use the same set of binary unigram
features as in section 3. That is, deﬁne fw (x, y) to be y if word type w appears at least once in x,
and 0 otherwise. Thus θ ∈ R17744 , and positive weights in θ favor class label y = +1 and equally
discourage y = −1, while negative weights do the opposite.
4.2

Rationales as a noisy channel

The interesting part of our model is pφ (r | x, y, θ), which models the rationale annotation process.
The rationales r reﬂect θ, but in noisy ways. pφ (r | x, y, θ) should consider two questions when
assessing whether r is a plausible set of rationales given θ. First, it needs a “language model” of
rationales: does r consist of rationales that are well-formed a priori, i.e., before θ is considered?
Second, it needs a “channel model”: does r faithfully signal the features of θ that strongly support
classifying x as y?
If a feature contributes heavily to the classiﬁcation of document x as class y, then the “channel
model” should tell us which parts of document x tend to be highlighted as a result. The channel
model must know about the particular kinds of features that are extracted by f and scored by θ.
The “language model,” however, is independent of the feature set θ. It models what rationales tend
to look like. In our document task, φ should describe things like: How frequent and how long are
typical rationales? Do their edges tend to align with punctuation or major syntactic boundaries in x?
4.3

Auxiliary model: Modeling rationale annotations with pφ

The rationales collected in this task are textual segments of a document to be classiﬁed. The document itself is a word token sequence x = x1 , ..., xM . We encode its rationales as a corresponding
tag sequence r = r1 , ..., rM , as illustrated in Figure 1. Here rm ∈ {I, O} according to whether the
token xm is in a rationale (i.e., xm was at least partly highlighted) or outside all rationales. x1 and
xM are special boundary symbols, tagged with O. We predict the full tag sequence r at once using a
conditional random ﬁeld [8]. A CRF is just another conditional log-linear model:
def

pφ (r | x, y, θ) =

exp(φ · g(r, x, y, θ))
Zφ (x, y, θ)
5

def

=

u(r, x, y, θ)
Zφ (x, y, θ)

(9)

Figure 2: The function family Bs in equation (12), shown for s ∈ {10, 2, −2, −10}.

where g(·) extracts a feature vector, φ are the corresponding weights of those features, and
def
Zφ (x, y, θ) = r u(r, x, y, θ) is a normalizer.
As usual for linear-chain CRFs, g(·) extracts two kinds of features: ﬁrst-order “emission” features
that relate the tag rm to (xm , y, θ), and second-order “transition” features that relate the tag rm to
rm−1 (although some of these also look at x). These two kinds of features respectively capture
the “channel model” and “language model” of section 4.2. The former says rm is I because xm is
associated with a relevant θ-feature.6 The latter says rm is I simply because it is next to another I.
4.3.1

Emission φ-features (“channel model”)

Recall that our θ-features (at present) correspond to unigrams. Given (x, y, θ), let us say that a
unigram w ∈ x is relevant, irrelevant, or anti-relevant if y · θw is respectively
0, ≈ 0, or
0.
That is, w is relevant if its presence in x strongly supports the annotated class y, and anti-relevant if
its presence strongly supports the opposite class −y. We would like to learn the extent φrel to which
annotators try to include relevant unigrams in their rationales, and the (perhaps lesser) extent φantirel
to which they try to exclude anti-relevant unigrams. This will help us infer θ from the rationales.
The details are as follows. φrel and φantirel are the weights of two emission features extracted by g:
M
def

I(rm = I) · B10 (y · θxm )

(10)

I(rm = I) · B−10 (y · θxm )

grel (x, y, r, θ) =

(11)

m=1
M
def

gantirel (x, y, r, θ) =
m=1

Here I(·) denotes the indicator function, returning 1 or 0 according to whether its argument is
true or false. Relevance and negated anti-relevance are respectively measured by the differentiable
nonlinear functions B10 and B−10 , shown in Figure 2, which are deﬁned by
Bs (a) = (log(1 + exp(a · s)) − log(2))/s

(12)

How does this work? The grel feature is a sum over all unigrams in the document x. It does not
ﬁre strongly on the irrelevant or anti-relevant unigrams, since B10 is close to zero there. But it ﬁres
positively on relevant unigrams w if they are tagged with I, and the strength of such ﬁring increases
approximately linearly with θw . Since the weight φrel > 0 in practice, this means that raising a
relevant unigram’s θw (if y = +1) will proportionately raise its log-odds of being tagged with I.
Symmetrically, since φantirel > 0 in practice, lowering an anti-relevant unigram’s θw (if y = +1)
will proportionately lower its log-odds of being tagged with I. See [3] for further discussion.
4.3.2

Transition φ-features (“language model”)

Annotators highlight more than just relevant unigrams. (After all, they aren’t aware of the feature
set.) They tend to mark full phrases, and φ models these phrases’ shape, via weights for several “language model” features, such as the 4 traditional CRF tag transition features gO-O , gO-I , gI-I , gI-O .
6

[3] sketches how to model a channel that transmits θ-features more complex than our present unigrams.

6

For example, gO-I counts the number of O-to-I transitions in r (see Figure 1). Other things equal,
an annotator with high φO-I is predicted to have many rationales. And if φI-I is high, rationales are
predicted to be long phrases (including more irrelevant unigrams around relevant ones).
We also learn weights for more reﬁned versions of these 4 basic transition features. For instance, a
feature of the form gt1 (v)-t2 counts the number of times an t1 –t2 transition occurs in r conditioned
on v appearing as the ﬁrst of the two word tokens where the transition occurs. We limit v to a few
frequent punctuation marks and function words. Other features condition transitions on syntactic
boundaries in the text. Further motivation and full details of all the φ-features can be found in [3].

5
5.1

Experimental Details
Discriminative Approach

We transformed this problem to an SVM problem (as described in section 3) and used the SVMlight
software [9] for training and testing, using the default linear kernel.
For any given value of T and any given training method, we chose hyperparameters θ∗ =
(C, µ, Ccontrast ) to maximize the following cross-validation performance:
8

θ∗ = argmax
θ

acc(Fi | θ, Fi+1 ∪ . . . ∪ Fi+T )

(13)

i=0

where Fj denotes the fold numbered j mod 9, and acc(Z | θ, Y ) means classiﬁcation accuracy on
the set Z after training on Y with hyperparameters θ. We used a simple alternating optimization
procedure that begins at θ0 = (1.0, 1.0, 1.0) and cycles repeatedly through the three dimensions,
optimizing along each dimension by a local grid search with resolution 0.1. Of course, when training
without rationales, we did not have to optimize µ or Ccontrast .
5.2

Generative Approach

To train our model, we used L-BFGS to locally maximize the log of the objective function (1):7
n

log pθ (yi | xi ) −
i=1

1
2 θ
2σθ

n
2

log pφ (ri | xi , yi , θ)) −

+ C(
i=1

1
2 φ
2σφ

2

(14)

2
2
This deﬁnes pprior from (1) to be a standard diagonal Gaussian prior, with variances σθ and σφ for
2
2
the two sets of parameters. We optimized σθ on held-out data, but always took σφ = 1. (Different
2
values of σφ did not affect the results, since the prior distribution over the relatively few φ weights
was overwhelmed by the large number of observed {I,O} rationale tags available to train them.)

Note the new C factor in equation (14). Initial experiments showed that optimizing equation (14)
without C led to an increase in the likelihood of the rationale data at the expense of classiﬁcation
accuracy, which degraded noticeably. This is because the second sum in (14) has a much larger
magnitude than the ﬁrst: in a set of 100 documents, it predicts around 74,000 binary {I,O} tags,
versus the one hundred binary class labels. While we are willing to reduce the log-likelihood of the
training classiﬁcations (the ﬁrst sum) to a certain extent, focusing too much on modeling rationales
(the second sum) is clearly not our ultimate goal, so we optimized C on development data to achieve
1
1
some balance between the two terms of equation (14).8 Typical values of C ranged from 300 to 50 .
We perform alternating optimization on θ and φ:
1. Initialize θ to maximize equation (14) but with C = 0 (i.e. based only on class data).
7
One might expect this function to be convex because pθ and pφ are both log-linear models with no hidden
variables. However, log pφ (ri | xi , yi , θ) is not necessarily convex in θ.
8
C also balances our conﬁdence in the classiﬁcation data y against our conﬁdence in the rationale data r;
either may be noisy. One may regard the second half of equation (14) as a kind of regularizer that biases θ
toward a solution compatible with the rationale annotations, and C controls the strength of this regularizer.

7

Figure 3: Classiﬁcation accuracy curves for the 4 methods: the two baseline learners that only utilize class data,
and the two learners that also utilize rationale annotations provided by annotator A0.

2. Fix θ, and ﬁnd φ that maximizes equation (14).
3. Fix φ, and ﬁnd θ that maximizes equation (14).
4. Repeat 2 and 3 until convergence.
The L-BFGS method requires calculating the gradient of the objective function (14). The partial
derivatives with respect to components of θ and φ involve calculating expectations of the feature
functions, which can be computed in linear time (with respect to the size of the training set) using
the forward-backward algorithm for CRFs. The partial derivatives also involve the derivative of (12),
to determine how changing θ will affect the ﬁring strength of the emission features grel and gantirel .

6

Experimental Results

Figure 3 shows learning curves for training set sizes up to 1600 documents for four methods: a loglinear baseline, an SVM baseline, the discriminative method of section 3, and the generative method
of section 4. A data point in the ﬁgure reports an averaged accuracy over 9 experiments with different subsets of the training set (see [2, 3]). The top two curves indicate signiﬁcant9 improvements
in accuracy over the respective baselines when rationales are introduced using our two proposed
approaches. Further, our generative method outperforms10 our masking SVM method, which starts
with a slightly better baseline classiﬁer (an SVM) but incorporates the rationales more crudely.
To show that the results generalized beyond annotator A0, we performed the same comparison for
three additional annotators, A3–A5, each of whom provided class and rationale annotations on a
small 100-document training set.11 Experiments reported in [3] again show improvements for both
methods, usually signiﬁcant, over the two baselines. [2] further analyze the masking SVM’s beneﬁt.
6.1

Analysis for the generative method

For the generative approach, examining the learned weights φ gives insight into annotator behavior.
High weights include I-O and O-I transitions conditioned on punctuation, e.g., φI(.)-O = 3.55,12 .
The large emission feature weights, e.g., φrel = 14.68 and φantirel = 15.30, tie rationales closely to
θ values, as hoped. For example, in Figure 1, the word w = succeeds, with θw = 0.13, drives up
9

We call a difference signiﬁcant at p < 0.05, under a paired permutation test that is designed to be appropriate for comparing the averaged accuracies that we report (see [2] for details).
10
Differences are not signiﬁcant at sizes 200, 1000, and 1600.
11
A0 and A3–A5 were blind test data—we developed our method (e.g., φ-features) using annotators A6–A8.
12
When trained on folds F4 –F8 with A0’s rationales.

8

p(I)/p(O) by a factor of 7 (in a positive document) relative to a word with θw = 0. We also found
that annotators’ styles differ enough that it helps to tune φ to the “target” annotator A who gave the
rationales. Results reported in [3] show that a φ model trained on A’s own rationales does best at
predicting new rationales from A, and similarly, classiﬁcation performance is usually best if it was
A’s own φ that was used to help learn θ from A’s rationales.
Feature ablation experiments in [3] showed that almost all the classiﬁcation beneﬁt from rationales can be obtained by using only the 2 emission φ-features and the 4 unconditioned transition
φ-features. Our full φ (115 features) merely improved our ability to predict the rationales.13
6.2

Using fewer rationales

In [2], we investigated improvements over the baseline if one has time to annotate rationales for
only some documents. The key discovery is that much of the beneﬁt can actually be obtained with
relatively few rationales. For example, with 800 training documents, annotating (0%, 50%, 100%) of
them with rationales gives accuracies of (86.9%, 89.2%, 89.3%), and with 1600 training documents,
annotating (0%, 50%, 100%) with rationales gives (88.5%, 91.7%, 92.2%).
We also experimented with keeping a randomly chosen subset of rationales but spread out over all
training documents. We found that, for a ﬁxed number of rationales, it did not matter whether they
were spread out or came from a subset of the documents. Spreading out the rationales simulates
a “lazy annotator” less assiduous than A0. Such annotators may in fact be more desirable, as they
should be able to add rationales at a higher rate (some rationales are simply more noticeable than
others, and a lazy annotator will probably be marking those).14
Results in [2] also suggest that rationales and class labels may be somewhat orthogonal in their
beneﬁt. With many documents and few rationales, the learning curve starts to ﬂatten out, but adding
more rationales provides a fresh beneﬁt: rationales have not yet reached their point of diminishing
returns. In practice, we suggest dynamically adjusting the fraction of documents annotated with
rationales. For a given annotator, should the next half-hour of annotation include rationales or not?
This is a quantiﬁable cost-beneﬁt tradeoff. One knows the time cost of the annotator’s most recent
rationales, and one can determine their marginal beneﬁt on accuracy using cross-validation.

7

Related Work

Our rationales resemble “side information” in machine learning—supplementary information about
the target function that is available at training time. Past work generates such information, by automatically transforming the training examples in ways that are expected to preserve or alter the
classiﬁcation [10], sometimes having to manually annotate the extra examples [11]. Our approach
differs because a human helps to generate the virtual examples. Our two methods for utilizing the
virtual examples also appear new (e.g., enforcing a margin between ordinary and contrast examples).
One could instead ask annotators to examine or propose some features instead of rationales. In
document classiﬁcation, Raghavan et al. [12] show that feature selection by an oracle could be
helpful, and that humans are both rapid and reasonably good at distinguishing highly useful n-gram
features from randomly chosen ones. Druck et al. [13] show annotators some features f from a
ﬁxed set, and ask them to choose a class label y such that p(y | f ) is as high as possible. Haghighi
and Klein [14] do the reverse: for each class label y, they ask the annotators to propose a few
“prototypical” features f such that p(y | f ) is as high as possible.
However, we have several concerns about asking annotators to identify globally relevant features.
First, by committing to a particular feature set at annotation time, it restricts subsequent research
on the costly annotated dataset. Second, it is not clear how an annotator would easily view and
highlight features in context, except for the simplest feature sets. In the phrase Apple shares up
3%, there may be several features that ﬁre on the substring Apple—responding to the string Apple,
its case-invariant form apple, its part of speech noun, etc. How does the annotator indicate which
of these features are relevant? Third, annotating features is only appropriate when the feature set
13
Their likelihood does increase signiﬁcantly with more features. A good predictive model of rationales, pφ ,
could be independently useful for selecting “snippets” that explain a machine’s classiﬁcation to a human.
14
The “most noticeable” rationales might also be the most effective ones for learning.

9

can be easily understood by a human. It would be hard for annotators to read, write, or evaluate a
description of a convolution ﬁlter in machine vision, for instance.

8

Conclusions

We have presented two machine learning algorithms that exploit the cleverness of annotators, by
considering enriched annotations that they provide. Our simple discriminative training method incorporated “annotator rationales”—on some or all of the training set—to learn a signiﬁcantly better
linear classiﬁer for positive vs. negative movie reviews. Our generative approach achieved small but
signiﬁcant further improvements by explicitly modeling each annotator’s rationale-marking process.
Most annotators provided several rationales per classiﬁcation without taking too much extra time—
even in our text classiﬁcation scenario, where the rationales greatly outweigh the classiﬁcations in
number and complexity. To lower annotation cost further while preserving most of the empirical
beneﬁt, one should solicit rationales for only a subset of the examples. Greater speed might also be
possible through an improved user interface or passive feedback (e.g., eye tracking).
In principle, many machine learning methods might be modiﬁed to exploit rationale data, as in our
modiﬁed SVM. And our generative approach, being essentially Bayesian inference, is potentially
extensible to many situations—other tasks, other classiﬁer architectures, and more complex features.
Acknowledgments
This work was supported by the JHU WSE/APL Partnership Fund, by National Science Foundation
grant No. 0347822 to the second author, and by a JHU APL Hafstad Fellowship.
References
[1] B. Pang and L. Lee. A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts. In Proceedings of ACL, pages 271–278, 2004.
[2] O. Zaidan, J. Eisner, and C. Piatko. Using “annotator rationales” to improve machine learning
for text categorization. In Proceedings of NAACL HLT, pages 260–267, April 2007.
[3] O. Zaidan and J. Eisner. Modeling annotators: A generative approach to learning from annotator rationales. In Proceedings of EMNLP, pages 31–40, October 2008.
[4] D. Lewis and W. Gale. A sequential algorithm for training text classiﬁers. In Proceedings of
ACM SIGIR, pages 3–12, 1994.
[5] O. Chapelle, A. Zien, and B. Sch¨ lkopf, editors. Semi-Supervised Learning. MIT Press, 2006.
o
[6] H. Daum´ III and D. Marcu. Domain adaptation for statistical classiﬁers. Journal of Artiﬁcial
e
Intelligence Research (JAIR), 26:101–126, 2006.
[7] L. von Ahn, R. Liu, and M. Blum. Peekaboom: A game for locating objects. In CHI’06: Proc.
of the SIGCHI Conference on Human Factors in Computing Systems, pages 55–64, 2006.
[8] J. Lafferty, A. McCallum, and F. Pereira. Conditional random ﬁelds: Probabilistic models for
segmenting and labeling sequence data. In Proceedings of ICML, pages 282–289, 2001.
[9] T. Joachims. Making large-scale SVM learning practical. In B. Sch¨ lkopf, C. Burges, and
o
A. Smola, editors, Advances in Kernel Methods—Support Vector Learning, pages 169–185.
MIT Press, 1999. Software available at svmlight.joachims.org.
[10] Y. Abu-Mostafa. Hints. Neural Computation, 7:639–671, July 1995.
[11] P. Kuusela and D. Ocone. Learning with side information: PAC learning bounds. J. of Computer and System Sciences, 68(3):521–545, May 2004.
[12] H. Raghavan, O. Madani, and R. Jones. Active learning on both features and instances. Journal
of Machine Learning Research, 7:1655–1686, Aug 2006.
[13] G. Druck, G. Mann, and A. McCallum. Learning from labeled features using generalized
expectation criteria. In Proceedings of ACM SIGIR, 2008.
[14] A. Haghighi and D. Klein. Prototype-driven learning for sequence models. In Proceedings of
NAACL HLT, pages 320–327, New York City, USA, 2006.

10

