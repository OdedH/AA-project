Unsupervised Learning on
an Approximate Corpus
Jason Smith, Jason Eisner

1

Learning from n-grams
Sentences:

Counts

time

n-grams:

your

like

an

arrow

20

fruit
your

ﬂies
ﬂies

like

an

orange

3

plane

ﬂies

like

an

ostrich

2

time
fruit

ﬂies
ﬂies
ﬂies

like
like
like
like

plane

ﬂies
...
2

an
an

arrow

20
3
25
20
2

Contributions
•
•

•

Learning from a ﬁnite-state distribution over sentences

•

e.g. an n-gram language model over sentences,
instead of individual sentences

Why?

•
•
•

Original corpus unavailable
Speed (learning from compressed data)
(Fundamental question about weighted grammars)

Exact and approximate solutions
3

Task
• HMM POS tagging (Merialdo 94)
• Many approaches build off of EM

4

Previous Work

• (slide taken from Lin et al., 2009)
5

Previous Work

• (slide taken from Lin et al., 2009)
6

Previous Work

• (slide taken from Lin et al., 2009)
7

Motivation: learning from n-grams
Full context:
N

V

Adv

Det

N

time

ﬂies

like

an

arrow

Adj

N

V

Det

N

fruit

ﬂies

like

an

orange

8

Motivation: learning from n-grams
Full context:
N

V

Adv

Det

N

time

ﬂies

like

an

arrow

Adj

N

V

Det

N

fruit

ﬂies

like

an

orange

?

V?
N?

Adv?
V?

Det

?

?

ﬂies

like

an

?

Local n-gram context:

9

Motivation: learning from n-grams
Local n-gram context:
?

V?
N?

Adv?
V?

Det

?

?

ﬂies

like

an

?

Overlapping n-grams:

Counts

time

ﬂies

like

20

fruit

ﬂies

like

3

plane

ﬂies

like

2

10

Motivation: learning from n-grams
Local n-gram context:
?

V?
N?

Adv?
V?

Det

?

time 80%
fruit 12%
plane 8%

ﬂies

like

an

?

Overlapping n-grams:

Counts

time

ﬂies

like

20

fruit

ﬂies

like

3

plane

ﬂies

like

2

11

Motivation: learning from n-grams
Local n-gram context:

N

V

Adv

Adj

N

V

time 80%
fruit 12%
plane 8%

ﬂies

like

Overlapping n-grams:

Det

?

an

?

Counts

time

ﬂies

like

20

fruit

ﬂies

like

3

plane

ﬂies

like

2

12

Exploit Overlapping n-grams
Counts
time

like

ﬂies

like

an

like
fruit

ﬂies

an

ﬂies

plane

ﬂies

arrow

20
3

an

orange

ﬂies

plane

25

like
like

your

20

3
2

like
like
13

2
an

ostrich

2

Exploit Overlapping n-grams
n-gram language model!
fruit ﬂies

time ﬂies

an orange

like
like
like

orange

ﬂies

like

an

like

an

bigram history

plane ﬂies

arrow

an arrow

ostrich

next words an ostrich
14

N-gram language models
fruit ﬂies

time ﬂies

p(orange|like an) = 0.12
p(arrow|like an) = 0.80
p(ostrich|like an) = 0.08
like
like

an orange

orange/0.12

ﬂies

like

an

like

an

arrow/0.8

an arrow

ostrich/0.08

like
plane ﬂies

an ostrich

15

N-gram language models
fruit ﬂies

time ﬂies

p(orange|like an) = 0.12
p(arrow|like an) = 0.80
p(ostrich|like an) = 0.07
p(-backoff-|like an) = 0.01
like
like

an orange

orange/0.12

ﬂies

like

an

like

an

arrow/0.8

an arrow

ostrich/0.07

like
Φ/0.01
plane ﬂies

an ostrich
an
16

N-gram language models
fruit ﬂies

time ﬂies

an orange

like
like

orange

ﬂies

like

an

like

an

arrow

an arrow

ostrich

like
plane ﬂies

an ostrich

17

N-gram language models
c(w):

- probability distribution over sentences
- “approximate corpus”

fruit ﬂies

time ﬂies

an orange

like
like

orange

ﬂies

like

an

like

an

arrow

an arrow

ostrich

like
plane ﬂies

an ostrich

18

N-gram language models
c(w):

- probability distribution over sentences
- “approximate corpus”

fruit ﬂies

time ﬂies

like

nc = 3

like

an

an orange

ﬂies

like

like

orange

an

arrow

an arrow

ostrich

like
plane ﬂies

an ostrich

19

Task
• HMM POS tagging (Merialdo 94)
• Many approaches build off of EM
￿

￿

1
max
log
p(t, w)
n
t
w∈corpus
￿
￿
max
c(w) log
p(t, w)
t
w∈corpus
20

HMM Tagging
Sentence: time ﬂies like an arrow

21

HMM Tagging
Sentence: time ﬂies like an arrow

p(Tag|Last Tag)
Det N
Det 0.1

V

0.1

...

0.5

N

0.8

0.3

0.4

V

0.1

0.6

0.1

...

...

...
22

HMM Tagging
Sentence: time ﬂies like an arrow

p(Tag|Last Tag)
Det N
Det 0.1

V

0.1

p(Word|Tag)

...

Det N
time 0.01 0.3

N

0.8

0.3

0.4

V

0.1

0.6

0.1

...

...

0.1

ﬂies 0.01 0.2

0.5

V

0.2

an 0.33 0.01 0.01
...

...
23

...

...

...

HMM Tagging
Sentence: time ﬂies like an arrow

p(Tag|Last Tag)
Det N
Det 0.1
N

0.8

V

p(Word|Tag)

...

Det N

V

time 0.01 0.3

0.1

ﬂies 0.01 0.2

0.2

p(t,w) : np = 2

0.1

0.5

0.3

0.4

...

...

...

nc : 0.1 0.6 0.1
c(w)’s word context 0.01 0.01
window
V
an 0.33
np : p(t,w)’s tag context window
...
...
...
...
24

Supervised learning: HMM

N

V

Adv

Det

N

time

ﬂies

like

an

arrow

25

Supervised learning: HMM
transition counts: estimating p(Tag|Last Tag)

N

V

Adv

Det

N

time

ﬂies

like

an

arrow

26

Supervised learning: HMM
emission counts: estimating p(Word|Tag)

N

V

time

ﬂies

Adv
like

27

Det

N

an

arrow

What if someone tagged our n-grams?
c(w):
fruit ﬂies

time ﬂies

like
like
like

plane ﬂies

an orange
orange

ﬂies

like

an

like

an

arrow

an arrow

ostrich
an ostrich

What if someone tagged our n-grams?
c(w):
fruit ﬂies

time ﬂies

N
time
like Adj
fruit
like
like

plane ﬂies

ﬂies

V
ﬂies
N
ﬂies
like

an

Adv
like
V
like
...
like

Det
N
an
orange
an orange
Det
N
orange
an
arrow
an

arrow

an arrow

ostrich
an ostrich

What if someone tagged our n-grams?
c(w) c(t,w):
Adj N
fruit ﬂies

N
V
time ﬂies

N
V
plane ﬂies

V
like
Adv
like
Adv
like

N
ﬂies

V
like

Det
an

...
V
like

Det
an

Det
V Adv an Adv Det
ﬂies like
like an

30

Det N
an orange

Det N
an arrow

Det N
an ostrich

What if someone tagged our n-grams?

nq = 3

c(w) c(t,w):
Adj N
fruit ﬂies

N
V
time ﬂies

V
like
Adv
like

N
ﬂies

V
like

Det
an

...
V
like

Det
an

Det N
an orange

Det N
an arrow

nc : c(w)’s word context window
Adv
like
nNp : V
p(t,w)’s tag context window
Det N
plane ﬂies
an ostrich
nq : c(t,w)’s tag context window
Det
V Adv an Adv Det
ﬂies like
like an

31

What if someone tagged our n-grams?

nq = 3

c(w) c(t,w):
Adj N
fruit ﬂies

N
V
time ﬂies

V
like
Adv
like

transition
Adv

N
V
plane ﬂies

like

N
ﬂies

V
like

Det
an

...
V
like

Det
an

Det N
an orange

Det N
an arrow

Det
counts:Adv an Advp(Tag|Last Tag)
estimating Det
V
ﬂies

like

like

32

an

Det N
an ostrich

What if someone tagged our n-grams?

nq = 3

c(w) c(t,w):
Adj N
fruit ﬂies

N
V
time ﬂies

N
V
plane ﬂies

V
like
Adv
like

N
ﬂies

V
like

Det
an

...
V
like

Det
an

Det
emission counts:Adv Det
V Adv an p(an|Det)
Adv
like ﬂies like
like an

33

Det N
an orange

Det N
an arrow

Det N
an ostrich

What if someone tagged our n-grams?

nq = 2

c(w) c(t,w):
?
N
fruit ﬂies

?
V
time ﬂies

?
V
plane ﬂies

V
like
Adv
like
Adv
like

?
ﬂies

V
like

N
orange

Det
an
Det
an

?
Adv
ﬂies like

?
like

?
N
an orange

N
Det arrow
?
N
an
N an arrow
ostrich
?
N
an ostrich

34

What if someone tagged our n-grams?

nq = 2

c(w) c(t,w):
?
N
fruit ﬂies

?
V
time ﬂies

V
like
Adv
like

?
ﬂies

V
like

N
orange

Det
an
Det
an

?
like

?
Advcounts: Adv
transition
estimating
like ﬂies like
?
V
plane ﬂies
35

?
N
an orange

N
Det arrow
?
N
an
N an arrow
ostrich

p(Tag|Last Tag)

?
N
an ostrich

What if someone tagged our n-grams?

nq = 1

c(w) c(t,w):
?
?
fruit ﬂies

?
?
time ﬂies

V
like
Adv
like
Adv
like

N
orange
?
ﬂies

?
like

Det
an

?
?
plane ﬂies

?
like

?
an

?
?
an orange

N
?
?
arrow
N an arrow
ostrich
?
?
an ostrich

36

What if someone tagged our n-grams?

nq = 1

c(w) c(t,w):
?
?
fruit ﬂies

?
?
time ﬂies

?
?
plane ﬂies

V
like
Adv
like
Adv
like

N
orange
?
ﬂies

?
like

Det
an

?
like

?
an

N
?
?
arrow
N an arrow
ostrich

transition counts?
must look at short paths
37

?
?
an orange

?
?
an ostrich

Unsupervised learning
Sentence: time ﬂies like an arrow

p(Tag|Last Tag)
Det N
Det 0.1

V

0.1

p(Word|Tag)

...

Det N
time 0.01 0.3

N

0.8

0.3

0.4

V

0.1

0.6

0.1

...

...

0.1

ﬂies 0.01 0.2

0.5

V

0.2

an 0.33 0.01 0.01
...

...
38

...

...

...

HMM Tagging Trellis
p(t,w) o c(w):
N
time
<S>

Adj
time

V

N
ﬂies

Adv
like

V

V
like

Adv

Det
an
Det

V
ﬂies
Adj

time

V
ﬂies

N
ﬂies

ﬂies

Adv
like
N

N
arrow

Det
an

V
like

like
39

V

an

arrow

N

HMM Tagging Trellis
p(t,w) o c(w):
N
time
<S>

Adj
time

V

N
ﬂies

Adv
like

V

V
like

Adv

Det
an
Det

V
ﬂies
Adj

time

V
ﬂies

N
ﬂies

ﬂies

Adv
like
N

N
arrow

Det
an

V
like

like
40

V

an

arrow

N

HMM Tagging Trellis
p(t,w) o c(w):
N
time
<S>

Adj
time

V

N
ﬂies

Adv
like

V

V
like

Adv

Det
an
Det

V
ﬂies
Adj

time

V
ﬂies

N
ﬂies

ﬂies

Adv
like
N

N
arrow

Det
an

V
like

like
41

V

an

arrow

N

Let’s tag our own n-grams (EM)
nq = 2

c(w) c(t,w) c(w)q(t|w):
?
N
fruit ﬂies

?
V
time ﬂies

?
V
plane ﬂies

V
like
Adv
like
Adv
like

Det
like
?
V
?
N
ﬂies like like like
Det
like
?
Adv
?
ﬂies like
like
N
like
42

...
Det
an

?
N
an orange

?
N
an arrow
N
an

?
N
an ostrich

Let’s tag our own n-grams (EM)
nq = 2

c(w) c(t,w) c(w)q(t|w):
?
N
fruit ﬂies

?
V
time ﬂies

V
like
Adv
like

Det
like
?
V
?
N
ﬂies like like like
Det
like
?
Adv
?
ﬂies like
like
N
like

...
Det
an

?
N
an orange

?
N
an arrow

nc : c(w)’s word context window
N
Adv
like
an
n?p : V
p(t,w)’s tag context window
?
N
plane ﬂies
an ostrich
nq : q(t|w)’s tag context window
43

Variational EM

44

Variational EM

Jensen’s inequality

45

Variational EM

46

Variational EM
nq = np : variational bound is “tight”
nq < np : we are approximating

nc : c(w)’s word context window
np : p(t,w)’s tag context window
nq : q(t|w)’s tag context window
47

How to maximize this bound

• Updating p(t,w) (M-step): shown earlier
• Updating q(t|w) (E-step): more complex,

but has a dynamic programming solution
which makes use of ﬁnite-state machines

• Expectation semirings (Eisner 2002),
details in paper

48

Experiments: EM vs. n-gram EM

•
•
•
•

How does EM on a full corpus compare to ngram EM on an approximate corpus?

•

POS tagging accuracy and likelihood

Standard setup for unsupervised POS tagging
with a dictionary
Reduced tag set (17 tags)
Limited tag dictionary from WSJ (words must
appear 5 times, otherwise all tags are possible)
49

Experiments: EM vs. n-gram EM

• n-gram EM parameter choices:
• n =5 - c(w) uses up to 5-grams
• n =2 - p(t,w) is a bigram HMM
• n =1 - q(t|w) conditions tag only on nc

p
q

gram word context
(approximate, but saves space)

50

Results: WSJ
1 million words, count cutoff of 3, 430k n-grams

EM
N-gram EM

EM
N-gram EM

Negative log-likelihood

Error rate

30%
26%
22%
18%
14%

0

17.5

35

52.5

70

Time (seconds)

3.70E+05
3.63E+05
3.55E+05
3.48E+05
3.40E+05

0

17.5

35

52.5

Time (seconds)
51

70

Results: 20m Gigaword
20 million words, count cutoff of 10, 2.8m n-grams

EM
N-gram EM

Negative log-likelihood

Error rate

30%
26%
22%
18%
14%

EM
N-gram EM

0

250

500

750 1000

Time (seconds)

8.10E+06
8.00E+06
7.90E+06
7.80E+06
7.70E+06

0

250 500 750 1000

Time (seconds)
52

Results: 200m Gigaword
200 million words, count cutoff of 20, 14m n-grams

EM
N-gram EM

Negative log-likelihood

Error rate

30%
26%
22%
18%
14%

EM
N-gram EM

0

2500 5000 7500 10000

Time (seconds)

8.30E+06
8.18E+06
8.05E+06
7.93E+06
7.80E+06

0

2500 5000 7500 10000

Time (seconds)
53

Conclusions
• New problem: train on an inﬁnite
corpus (distribution over sentences)

• New algorithms: exact and approximate
likelihood maximization

• New results: faster (sublinear) training

by compressing corpus into n-gram model

54

