In Harry C. Bunt and Anton Nijholt (eds.), Advances in Probabilistic and
Other Parsing Technologies, Chapter 3, pp. 29-62. c 2000 Kluwer Academic
Publishers. [Text of this preprint may differ slightly, as do chapter/page nos.]

Chapter 1
BILEXICAL GRAMMARS AND THEIR
CUBIC-TIME PARSING ALGORITHMS
Jason Eisner
Dept. of Computer Science, University of Rochester
P.O. Box 270226
Rochester, NY 14627-0226 U.S.A.∗
jason@cs.rochester.edu

Abstract

1.
1.1

This chapter introduces weighted bilexical grammars, a formalism in which individual lexical items, such as verbs and their arguments, can have idiosyncratic
selectional inﬂuences on each other. Such ‘bilexicalism’ has been a theme of
much current work in parsing. The new formalism can be used to describe bilexical approaches to both dependency and phrase-structure grammars, and a slight
modiﬁcation yields link grammars. Its scoring approach is compatible with a
wide variety of probability models.
The obvious parsing algorithm for bilexical grammars (used by most previous
authors) takes time O(n5 ). A more efﬁcient O(n3 ) method is exhibited. The
new algorithm has been implemented and used in a large parsing experiment
(Eisner, 1996b). We also give a useful extension to the case where the parser
must undo a stochastic transduction that has altered the input.

INTRODUCTION
THE BILEXICAL IDEA

Lexicalized Grammars. Computational linguistics has a long tradition of lexicalized grammars, in which each grammatical rule is specialized for some individual word. The earliest lexicalized rules were word-speciﬁc subcategorization
frames. It is now common to ﬁnd fully lexicalized versions of many grammatical
formalisms, such as context-free and tree-adjoining grammars (Schabes et al.,
1988). Other formalisms, such as dependency grammar (Mel’ˇ uk, 1988) and
c
∗ This material is based on work supported by an NSF Graduate Research Fellowship and ARPA Grant
N6600194-C-6043 ‘Human Language Technology’ to the University of Pennsylvania.

1

2
head-driven phrase-structure grammar (Pollard and Sag, 1994), are explicitly
lexical from the start.
Lexicalized grammars have two well-known advantages. When syntactic
acceptability is sensitive to the quirks of individual words, lexicalized rules are
necessary for linguistic description. Lexicalized rules are also computationally
cheap for parsing written text: a parser may ignore those rules that do not
mention any input words.

Probabilities and the New Bilexicalism. More recently, a third advantage of
lexicalized grammars has emerged. Even when syntactic acceptability is not
sensitive to the particular words chosen, syntactic distribution may be (Resnik,
1993). Certain words may be able but highly unlikely to modify certain other
words. Of course, only some such collocational facts are genuinely lexical (the
storm gathered/*convened); others are presumably a weak reﬂex of semantics
or world knowledge (solve puzzles/??goats). But both kinds can be captured
by a probabilistic lexicalized grammar, where they may be used to resolve
ambiguity in favor of the most probable analysis, and also to speed parsing
by avoiding (‘pruning’) unlikely search paths. Accuracy and efﬁciency can
therefore both beneﬁt.
Work along these lines includes (Charniak, 1995; Collins, 1996; Eisner,
1996a; Charniak, 1997; Collins, 1997; Goodman, 1997), who reported stateof-the-art parsing accuracy. Related models are proposed without evaluation in
(Lafferty et al., 1992; Alshawi, 1996).
This ﬂurry of probabilistic lexicalized parsers has focused on what one might
call bilexical grammars, in which each grammatical rule is specialized for
not one but two individual words.1 The central insight is that speciﬁc words
subcategorize to some degree for other speciﬁc words: tax is a good object for
the verb raise. These parsers accordingly estimate, for example, the probability
that word w is modiﬁed by (a phrase headed by) word v, for each pair of words
w, v in the vocabulary.

1.2

AVOIDING THE COST OF BILEXICALISM

Past Work. At ﬁrst blush, bilexical grammars (whether probabilistic or not)
appear to carry a substantial computational penalty. We will see that parsers
derived directly from CKY or Earley’s algorithm take time O(n3 min(n, |V |)2 )
for a sentence of length n and a vocabulary of |V | terminal symbols. In practice
n ≪ |V |, so this amounts to O(n5 ). Such algorithms implicitly or explicitly
regard the grammar as a context-free grammar in which a noun phrase headed
by tiger bears the special nonterminal NPtiger . These O(n5 ) algorithms are used
by (Charniak, 1995; Alshawi, 1996; Charniak, 1997; Collins, 1996; Collins,
1997) and subsequent authors.

Bilexical Grammars and O(n3 ) Parsing

3

Speeding Things Up. The present chapter formalizes a particular notion of
bilexical grammars, and shows that a length-n sentence can be parsed in time
only O(n3 g3 t), where g and t are bounded by the grammar and are typically
small. (g is the maximum number of senses per input word, while t measures the
degree of interdependence that the grammar allows among the several lexical
modiﬁers of a word.) The new algorithm also reduces space requirements to
O(n2 g2 t), from the cubic space required by CKY-style approaches to bilexical
grammar. The parsing algorithm ﬁnds the highest-scoring analysis or analyses
generated by the grammar, under a probabilistic or other measure.
The new O(n3 )-time algorithm has been implemented, and was used in the
experimental work of (Eisner, 1996b; Eisner, 1996a), which compared various
bilexical probability models. The algorithm also applies to the Treebank Grammars of (Charniak, 1995). Furthermore, it applies to the head-automaton grammars (HAGs) of (Alshawi, 1996) and the phrase-structure models of (Collins,
1996; Collins, 1997), allowing O(n3 )-time rather than O(n5 )-time parsing,
granted the (linguistically sensible) restrictions that the number of distinct Xbar levels is bounded and that left and right adjuncts are independent of each
other.

1.3

ORGANIZATION OF THE CHAPTER

This chapter is organized as follows:
First we will develop the ideas discussed above. §2. presents a simple formalization of bilexical grammar, and then §3. explains why the naive recognition
algorithm is O(n5 ) and how to reduce it to O(n3 ).
Next, §4. offers some extensions to the basic formalism. §4.1 extends it to
weighted (probabilistic) grammars, and shows how to ﬁnd the best parse of the
input. §4.2 explains how to handle and disambiguate polysemous words. §4.3
shows how to exclude or penalize string-local conﬁgurations. §4.4 handles the
more general case where the input is an arbitrary rational transduction of the
“underlying” string to be parsed.
§5. carefully connects the bilexical grammar formalism of this chapter to
other bilexical formalisms such as dependency, context-free, head-automaton,
and link grammars. In particular, we apply the fast parsing idea to these formalisms.
The conclusions in §6. summarize the result and place it in the context of
other work by the author, including a recent asymptotic improvement.

2.

A SIMPLE BILEXICAL FORMALISM

The bilexical formalism developed in this chapter is modeled on dependency
grammar (Gaifman, 1965; Mel’ˇ uk, 1988). It is equivalent to the class of split
c
bilexical grammars (including split bilexical CFGs and split HAGs) deﬁned

4
in (Eisner and Satta, 1999). More powerful bilexical formalisms also exist, and
improved parsing algorithms for these are cited in §5.6 and §5.8.

Form of the Grammar. We begin with a simple version of the formalism,
to be modiﬁed later in the chapter. A [split] unweighted bilexical grammar
consists of the following elements:
A set V of words, called the (terminal) vocabulary, which contains a
distinguished symbol root.
For each word w ∈ V , a pair of deterministic ﬁnite-state automata ℓw
and rw . Each automaton accepts some regular subset of V ∗ .
t is deﬁned to be an upper bound on the number of states in any single
automaton. (g will be deﬁned in §4.2 as an upper bound on lexical ambiguity.)
The dependents of word w are the headwords of its arguments and adjuncts. Speaking intuitively, automaton ℓw speciﬁes the possible sequences of
left dependents for w. So these allowable sequences, which are word strings in
V ∗ , form a regular set. Similarly rw speciﬁes the possible sequences of right
dependents for w.
By convention, the ﬁrst element in such a sequence is closest to w in the
surface string. Thus, the possible dependent sequences (from left to right) are
speciﬁed by L(ℓw )R and L(rw ) respectively. For example, if the tree shown
in Figure 1.1a is grammatical, then we know that ℓplan accepts the, and rplan
accepts of raise.
To get fast parsing, it is reasonable to ask that the automata individually have
few states (i.e., that t be small). However, we wish to avoid any penalty for
having
many (distinct) automata—two per word in V ;
many arcs leaving an automaton state—one per possible dependent in V .
That is, the vocabulary size |V | should not affect performance at all.
We will use Q(ℓw ) and Q(rw ) to denote the state sets of ℓw and rw respectively; I(ℓw ) and I(rw ) to denote their initial states; and predicate F (q) to mean
that q is a ﬁnal state of its automaton. The transition functions may be notated
as a single pair of functions ℓ and r, where ℓ(w, q, w′ ) returns the state reached
by ℓw when it leaves state q on an arc labeled w′ , and similarly r(w, q, w′ ).
Notice that as an implementation matter, if the automata are deﬁned in any
systematic way, it is not necessary to actually store them in order to represent
the grammar. One only needs to choose an appropriate representation for states
q and deﬁne the I, F , ℓ, and r functions.

Bilexical Grammars and O(n3 ) Parsing

5

Meaning of the Grammar. We now formally deﬁne the language generated
by such a grammar, and the structures that the grammar assigns to sentences of
this language.
Let a dependency tree be a rooted tree whose nodes (both internal and
external) are labeled with words from V , as illustrated in Figure 1.1a; the root
is labeled with the special symbol root ∈ V . The children (‘dependents’) of
a node are ordered with respect to each other and the node itself, so that the
node has both left children that precede it and right children that follow it.
A dependency tree is grammatical iff for every word token w that appears
in the tree, ℓw accepts the (possibly empty) sequence of w’s left children (from
right to left), and rw accepts the sequence of w’s right children (from left to
right).
A string ω ∈ V ∗ is generated by the grammar, with analysis T , if T is a
grammatical dependency tree and listing the node labels of T in inﬁx order
yields the string ω followed by root. ω is called the yield of T .
Bilexicalism. The term bilexical refers to the fact that (i) each w ∈ V may
specify a wholly different choice of automata ℓw and rw , and furthermore (ii)
these automata ℓw and rw may make distinctions among individual words that
are appropriate to serve as children (dependents) of w. Thus the grammar is
sensitive to speciﬁc pairs of lexical items.
For example, it is possible for one lexical verb to select for a completely
idiosyncratic set of nouns as subject, and another lexical verb to select for an
entirely different set of nouns. Since it never requires more than a two-state
automaton (though with many arcs!) to specify the set of possible subjects
for a verb, there is no penalty for such behavior in the parsing algorithm to be
described here.

3.

O(n5 ) AND O(n3 ) RECOGNITION

This section develops a basic O(n3 ) recognition method for simple bilexical
grammars as deﬁned above. We begin with a naive O(n5 ) method drawn from
context-free ‘dotted-rule’ methods such as (Earley, 1970; Graham et al., 1980).
Second, we will see why this method is inefﬁcient. Finally, a more efﬁcient
O(n3 ) algorithm is presented.
Both methods are essentially chart parsers, in that they use dynamic programming to build up an analysis of the whole sentence from analyses of its
substrings. However, the slow method combines traditional constituents, whose
lexical heads may be in the middle, while the fast method combines what we
will call spans, whose heads are guaranteed to be at the edge.

6
(a)

ROOT

plan
The

raise

of
government

to

tax

the

income

(b)
Det

The

N

P

Det

N

To

V

N

N

plan of the government to raise income tax

ROOT

(c)
The plan of

of the government to raise

no

yes

plan of the government to raise

yes

(d)
The

plan of the government to raise income tax

ROOT

The

plan of the government to raise income tax

ROOT

plan of the government to raise income tax

ROOT

plan of the government to raise

Figure 1.1 [Shading in this ﬁgure has no meaning.] (a) A dependency parse tree. (b) The same
tree shown ﬂattened out. (c) A span of the tree is any substring such that no interior word of
the span links to any word outside the span. One non-span and two spans are shown. (d) A
span may be decomposed into smaller spans as repeatedly shown; therefore, a span can be built
from smaller spans by following the arrows upward. The parsing algorithm (Fig. 1.3–1.4) builds
successively larger spans in a dynamic programming table (chart). The minimal spans, used to
seed the chart, are linked or unlinked word bigrams, such as The→plan or tax root, as shown.

Bilexical Grammars and O(n3 ) Parsing

3.1

7

NOTATION AND PRELIMINARIES

The input to the recognizer is a string of words, ω = w1 w2 . . . wn ∈ V ∗ .
We put wn+1 = root, a special symbol that does not appear in ω. For i ≤ j,
we write wi,j to denote the input substring wi wi+2 . . . wj .

Generic Chart Parsing. There may be many ways to analyze wi,j . Each
grammatical analysis has as its signature an item, or tuple, that concisely and
completely describes its ability to combine with analyses of neighboring input
substrings. Many analyses may have the same item as signature. This chapter
will add some syntactic sugar and draw items as schematic pictures of analyses.
C (the chart) is an (n + 1) × (n + 1) array. The chart cell Ci,j accumulates
the set of signatures of all analyses of wi,j . It must be possible to enumerate the
set—or more generally, certain subsets deﬁned by particular ﬁxed properties—
in time O(1) per element.2 In addition, it must be possible to perform an O(1)
duplicate check when adding a new item to a cell. A standard implementation
is to maintain linked lists for enumerating the relevant subsets, together with a
hash table (or array) for the duplicate check.
Analysis. If S bounds the number of items per chart cell, then the space
required by a recognizer is clearly O(n2 S). The time required by the algorithms
we consider is O(n3 S 2 ), because for each of the O(n3 ) values of i, j, k such
that 1 ≤ i ≤ j < k ≤ n + 1, they will test each of the ≤ S items in Ci,j against
each of the ≤ S items in Cj+1,k , to see whether analyses with those items as
signatures could be grammatically combined into an analysis of wi,k .
Efﬁciency therefore requires keeping S small. The key difference between
the O(n5 ) method and the O(n3 ) method will be that S is O(n) versus O(1).

3.2

NAIVE BILEXICAL RECOGNITION

An Algorithm. The obvious approach for bilexical grammars is for each analysis to represent a subtree, just as for an ordinary CFG. More precisely, each
analysis of wi,j is a kind of dotted subtree that may not yet have acquired all
its children.3 The signature of such a dotted subtree is an item (w, q1 , q2 ). This
may be depicted more visually as
q1 q2
i w j
where w ∈ wi,j is the head word at the root of the subtree, q1 ∈ Q(ℓw ), and
q2 ∈ Q(rw ). If both q1 and q2 are ﬁnal states, then the analysis is a complete
constituent.
The resulting algorithm is speciﬁed declaratively using sequents in Figure 1.2a–b, which shows how the items combine.

8
Analysis. It is easy to see from Figure 1.2a that each chart cell Ci,j can contain
S = O(min(n, |V |)t2 ) possible items: there are O(min(n, |V |)) choices for
w, and O(t) choices for each of q1 and q2 once w is known. It follows that the
runtime is O(n3 S 2 ) = O(n3 min(n, |V |)2 t4 ).
More simply and generally, one can ﬁnd the runtime by examining Figure 1.2b and seeing that there are O(n3 min(n, |V |)2 t4 ) ways to instantiate
the four rule templates. Each is instantiated at most once and in O(1) time.
(McAllester, 1999) proves that with appropriate indexing of items, this kind
of runtime analysis is correct for a very general class of algorithms speciﬁed
declaratively by inference rules.
An Improvement. It is possible to reduce the t4 factor to just t, since each
attachment decision really depends only on one state (at the parent), not four
states. This improved method is shown in Figure 1.2c. It groups complete
constituents together under a single item even if they ﬁnished in different ﬁnal
states—a trick we will be using again.
Note that the revised method always attaches right children before left children, implying that a given dependency tree is only derived in one way. This
property is important if one wishes to enhance the algorithm to compute the
total number of distinct trees for a sentence, or their total probability, or related
quantities needed for the Inside-Outside estimation algorithm.
Discussion. Even with the improvement, parsing is still O(n5 ) (for n < |V |).
Why so inefﬁcient? Because there are too many distinct possible signatures.
Whether Link-L can make one tree a new child of another tree depends on the
head words of both trees. Hence signatures must mention head words. Since the
head word of a tree that analyzes wi,j could be any of the words wi , wi+1 , . . . wj ,
and there may be n distinct such words in the worst case (assuming n < |V |),
the number S of possible signatures for a tree is at least n.
In more concrete terms, the problem is that each chart cell may have to
maintain many differently-headed analyses of the same string. Chomsky’s
noun phrase visiting relatives has two analyses: a kind of relatives vs. a kind
of visiting. A bilexical grammar knows that only the ﬁrst is appropriate in the
context hug visiting relatives, and only the second is appropriate in the context
advocate visiting relatives. So the two analyses must be kept separate in the
chart: they will combine with context differently and therefore have different
signatures.

3.3

EFFICIENT BILEXICAL RECOGNITION

Constituents vs. Spans. To eliminate these two additional factors of n, we
must reduce the number of possible signatures for an analysis. The solution is
for analyses to represent some kind of contiguous string other than constituents.

Bilexical Grammars and O(n3 ) Parsing

9

q1 q2
(a)

(1 ≤ i ≤ j ≤ n + 1, w ∈ V, q1 ∈ Q(ℓw ), q2 ∈ Q(ℓr ))

i w j

(b)
Seed:

Accept:
q1 q2

1 root n + 1
F (q1 ), F (q2 )
accept

i wi i
q1 q2

Link-L:

q1 q2

q1 = I(ℓwi ), q2 = I(rwi )

q3 q4

i w j j + 1 w′ k
′
F (q3 ), F (q4 ), q2 = r(w, q2 , w′ )
′
q1 q2
i w k
q1 q2
q3 q4

Link-R:

i w′ j j + 1 w k
′
F (q1 ), F (q2 ), q3 = ℓ(w, q3 , w′ )
′
q3 q4
i w k
(c)
Seed:

Flip:
q
i

Link-L:

i j
q′ F

i
q
i

Link-R:

Finish:

q

q = I(rwi )

q F
i w j
F (q)
F F

F (q), q ′ = I(ℓwi )

i wi j

i w j

F F
j j + 1 w′ k ′
q = r(wi , q, w′ )
q′
i

k

F F

q F

i w′ j j + 1 w k ′
q = ℓ(w, q, w′ )
q′ F

Accept:

F F
1 root n + 1
accept

i w k
Figure 1.2 Declarative speciﬁcation of an O(n5 ) algorithm. (a) Form of items in the parse
chart. (b) Inference rules. The algorithm can derive an analysis with the signature below ——–
by combining analyses with the signatures above ——–, provided that the input and grammar
satisfy any properties listed to the right of ——–. (c) A variant that reduces the grammar factor
from t4 to t. F is a literal that means ‘an unspeciﬁed ﬁnal state.’

10
Each analysis in Ci,j will be a new kind of object called a span, which consists
of one or two ‘half-constituents’ in a sense to be described. The headword(s)
of a span in Ci,j are guaranteed to be at positions i and/or j in the sentence.
This guarantee means that where Ci,j in the previous section had up to n-fold
uncertainty about the location of the headword of wi,j , here it will have only
3-fold uncertainty. The three possibilities are that wi is a headword, that wj is,
or that both are.
Given a dependency tree, we know what its constituents are: a constituent
is any substring consisting of a word and all its descendants. The inefﬁcient
parsing algorithm of §3.2 assembled the correct tree by ﬁnding and gluing
together analyses of the tree’s (dotted) constituents in an approved way. For
something similar to be possible with spans, we must deﬁne what the spans of
a given dependency tree are, and how to glue analyses of spans together into
analyses of larger spans. Not every substring of the sentence is a constituent of
this (or any) sentence’s correct parse, and in the same way, not every substring
is a span of this (or any) sentence’s correct parse.

Deﬁnition of Spans. Figure 1.1a–c illustrates what spans are. A span of the
dependency tree in (a) and (b) is any substring wi,j of the input such that none
of the interior words of the span communicate with any words outside the span.
Formally: if i < k < j, and wk is a child or parent of wk′ , then i ≤ k′ ≤ j.
Thus, just as a constituent links to the rest of the sentence only through its
head word, which may be located anywhere in the constituent, a span wi,j links
to the rest of the sentence only through its endwords wi and wj , which are
located at the edges of the span. We call wi+1,j−1 the span’s interior.
Assembling Spans. Since we will build the parse by assembling possible
spans, and the interiors of adjacent spans are insulated from each other, we
crucially are allowed to forget the internal analysis of a span once we have built
it. When we combine two adjacent such spans, we never add a link from or to
the interior of either. For, by the deﬁnition of span, if such a link were necessary,
then the spans being combined could not be spans of the true parse anyway.
There is always some other way of decomposing the true parse (itself a span)
into smaller spans so that no such links from or to interiors are necessary.
Figure 1.1d shows such a decomposition. Any span analysis of more than two
words, say wi,k , can be decomposed uniquely by the following deterministic
procedure. Choose j such that wj is the rightmost word in the interior of
the span (i < j < k) that links to or from wi ; if there is no such word, put
j = i + 1. Because crossing links are not allowed in a dependency tree—
a property known as projectivity—the substrings wi,j and wj,k must also be
spans. We can therefore assemble the original wi,k analysis by concatenating
the wi,j and wj,k spans, and optionally adding a link between the endwords,

Bilexical Grammars and O(n3 ) Parsing

11

wi and wk . By construction, there is never any need to add a link between any
other pair of words. Notice that when the two narrower spans are concatenated,
wj gets its left children from one span and its right children from the other, and
will never be able to acquire additional children since it is now span-internal.
By our choice of j, the left span in the concatenation, wi,j , is always simple
in the following sense: it has a direct link between wi and wj , or else has only
two words. (wi,k is decomposed at the maximal j such that i < j < k and wi,j
is simple.) Requiring the left span to be simple assures a unique decomposition
(see §3.2 for motivation); the right span need not be simple.

Signatures of Spans. A span’s signature needs to record only a few pertinent
facts about its internal analysis. It has the form shown in Figure 1.3a. i, j
indicate that the span is an analysis of wi,j . q1 is the state of rwi after it has
read the sequence of wi ’s right children that appear in wi+1,j , and q2 is the state
of ℓwj after it has read the sequence of wj ’s left children that appear in wi,j−1 .
b1 and b2 are bits that indicate whether wi and wj , respectively, have parents
within wi,j . Finally, s is a bit indicating whether the span is simple in the sense
described above.
The signature must record q1 and q2 so that the parser knows what additional
dependents wi or wj can acquire. It must record b1 and b2 so that it can detect
whether such a link would jeopardize the tree form of the dependency parse
(by creating multiple parents, cycles, or a disconnected graph). Finally, it must
record s to ensure that each distinct analysis is derived in at most one way.
It is useful to note the following four possible types of span:
b1 = b2 = 0. Example: of the government to raise in Figure 1.1c. In
this case, the endwords wi and wj are not yet connected to each other:
that is, the path between them in the ﬁnal parse tree will involve words
outside the span. The span consists of two ‘half-constituents’—wi with
all its right descendants, followed by wj with all its left descendants.
b1 = 0, b2 = 1. Example: plan of the government to raise in Figure 1.1c.
In this case, wj is a descendant of wi via a chain of one or more leftward
links within the span itself. The span consists of wi and all its right
descendants within wi+1,j . (wi or wj or both may later acquire additional
right children to the right of wj .)
b1 = 1, b2 = 0. Example: the whole sentence in Figure 1.1b. This is the
mirror image of the previous case.
b1 = 1, b2 = 1. This case is impossible, for then some word interior
to the span would need a parent outside it. We will never derive any
analyses with this signature.

12

(a)

b1
b2
q1 s q2
i
j

(1 ≤ i < j ≤ n + 1, q1 ∈ Q(rwi ) ∪ {F}, q2 ∈ Q(ℓwj ) ∪ {F}, b1 , b2 , s ∈
{0, 1}, ¬(q1 = F ∧ q2 = F), ¬(b1 ∧ b2 ))

(b) Seed:

Combine: b1
b3
b2 ¬b2
q1 1 F
F s q3
i
j
j
k
b1
b3
q1 0 q3
i
k
Opt-Link-R: 0
0
q1 s q2
q1 = F, q2 = F ,
i
j q1 = F, q2 = F ,
′
′
q1 = r(wi , q1 , wj )
1
0′ q2 = ℓ(wj , q2 , wi )
q1 1 q2
i
j

q1 = I(rwi ), q2 = I(ℓwi+1 )

0
0
q1 1 q2
i
i+1

Opt-Link-L: 0
0
q1 s q2
i
j
0
1
′
q1 1 q2
i
j
Seal-L: b1
b2
q1 s q2
i
j
b1
b2
F s q2
i
j

Seal-R: b1
q1 s
i
q1 = F, q2 = F, F (q1 )
b1
q1 s
i
Accept: 1
0
F s q2
1
n+1
F (q2 )
accept

b2
q2
j
b2
F
j

q1 = F, q2 = F, F (q2 )

Figure 1.3 Declarative speciﬁcation of an O(n3 ) algorithm. (a) Form of items in the parse
chart. (b) Inference rules. As in Fig. 1.2b, F is a literal that means ‘an unspeciﬁed ﬁnal state.’

Bilexical Grammars and O(n3 ) Parsing
1.
2.
3.
4.
5.
6.
7.
8.
9.
10.
11.
12.
13.
14.
15.
16.
17.
18.
19.
20.

13

for i := 1 to n
s := the item for wi,i+1 produced by Seed
Discover(i, i + 1, s)
Discover(i, i + 1, Opt-Link-L(s))
Discover(i, i + 1, Opt-Link-R(s))
for width := 2 to n
for i := 1 to (n + 1) − width
k := i + width
for j := i + 1 to k − 1
L
foreach simple item s1 in Ci,j
R
foreach item s2 in Cj,k such that Combine(s1 , s2 ) is deﬁned
s := Combine(s1 , s2 )
Discover(i, k, s)
if Opt-Link-L(s) and Opt-Link-R(s) are deﬁned
Discover(i, k, Opt-Link-L(s))
Discover(i, k, Opt-Link-R(s))
R
foreach item s in C1,n+1
if Accept(s) is deﬁned
return accept
return reject

Figure 1.4 Pseudocode for an O(n3 ) recognizer. The functions in small caps refer to the
(deterministic) inference rules of Figure 1.3. Discover(i, j, s) adds Seal-L(s) (if deﬁned) to
R
L
Ci,j and Seal-R(s) (if deﬁned) to Ci,j .

The Span-Based Algorithm. A declarative speciﬁcation of the algorithm is
given in Figure 1.3, which shows how the items combine. The reader may
choose to ignore s for simplicity, since the unique-derivation property may
speed up recognition but does not affect its correctness. For concreteness,
pseudocode is given in Figure 1.4.
The Seed rule seeds the chart with the minimal spans, which are two words
wide. Combine is willing to combine two spans if they overlap in a word wj
that gets all its left children from the left span (hence ‘F’ appears in the rule), all
its right children from the right span (again ‘F’), and its parent in exactly one
of the spans (hence ‘b2 , ¬b2 ’). Whenever a new span is created by seeding or
combining, the Opt-Link rules can add an optional link between its endwords,
provided that neither endword already has a parent.
The Seal rules check that an endword’s automaton has reached a ﬁnal (accepting) state. This is a precondition for Combine to trap the endword in the
interior of a larger span, since the endword will then be unable to link to any
more children. While Combine could check this itself, using Seal is asymptotically more efﬁcient because it conﬂates different ﬁnal states into a single
item—exactly as Finish did in Figure 1.2c.

14
Analysis. The time requirements are O(n3 t2 ), since that is the number of
ways to instantiate the free variables in the rules of Figure 1.3b (McAllester,
1999). As t is typically small, this compares favorably with O(n5 t) for the
naive algorithm of §3.2. Even better, §3.4 will obtain a speedup to O(n3 t).
The space requirements are naively O(n2 t2 ), since that is the number of
ways to instantiate the free variables in Figure 1.3a, i.e., the maximum number
of items in the chart. The pseudocode in Figure 1.4 shows that this can be
reduced to O(n2 t) by storing only items for which q1 = F or q2 = F (in
separate charts C R and C L respectively). The other items need not be added to
any chart, but can be fed to the Opt-Link and Seal rules immediately upon
creation, and then destroyed.

3.4

AN ADDITIONAL O(t) SPEEDUP

The above algorithm can optionally be sped up from O(n3 t2 ) to O(n3 t), at
the cost of making it perhaps slightly harder to understand.
Every item in Figure 1.3 has either 0 or 1 of the states q1 , q2 instantiated as
the special symbol F. We will now modify the algorithm so that either 1 or 2 of
those states are always instantiated as F (except in items produced by Seed).
This is possible because q2 does not really matter in Opt-Link-L, nor does
q1 in Opt-Link-R. The payoff is that these rules, as well as Combine, will
only need to consider one state at a time.
All that is necessary is to modify the applicability conditions of the inference
rules. Combine gets the additional condition q1 = F ∨ q3 = F. Opt-Link-L
and Seal-L drop the condition that q2 = F, while Opt-Link-R and Seal-R
drop the condition that q1 = F.
To preserve the property that derivations are unique, two additional modiﬁcations are now necessary. To eliminate the freedom to apply Seal either
before or after Combine, the Seal rules should be restricted to apply only to
simple spans (i.e., s = 1). And to eliminate the freedom to apply both Seal-L
and Seal-R in either order to the output of Seed, the Seal-L rule should
require that q2 = F ∨ b2 = 1.

4.

VARIATIONS

In this section, we describe useful modiﬁcations that may be made to the
formalism and/or the algorithm above.

4.1

WEIGHTED GRAMMARS

The ability of a verb to subcategorize for an idiosyncratic set of nouns, as
above, can be used to implement black-and-white (‘hard’) selectional restrictions. Where bilexical grammars are really useful, however, is in capturing

Bilexical Grammars and O(n3 ) Parsing

15

gradient (‘soft’) selectional restrictions. A weighted bilexical grammar can
equip each verb with an idiosyncratic probability distribution over possible object nouns, or indeed possible dependents of any sort. We now formalize this
notion.

Weighted Automata. A weighted DFA, A, is a deterministic ﬁnite-state automaton that associates a real-valued weight with each arc and each ﬁnal state
(Mohri et al., 1996). Following heavily-weighted arcs is intuitively ‘good,’
‘probable,’ or ‘common’; so is stopping in a heavily-weighted ﬁnal state. Each
accepting path through A is automatically assigned a weight, namely, the sum
of all arc weights on the path and the ﬁnal-state weight of the last state on the
path. Each string α accepted by A is assigned the weight of its accepting path.
Weighted Grammars. Now, we may deﬁne a weighted bilexical grammar as
a bilexical grammar in which all the automata ℓw and rw are weighted DFAs.
We deﬁne the weight of a dependency tree under the grammar as the sum, over
all word tokens w in the tree, of the weight with which ℓw accepts w’s sequence
of left children plus the weight with which rw accepts w’s sequence of right
children.
Given an input string ω, the weighted parsing problem is to ﬁnd the highestweighted grammatical dependency tree whose yield is ω.
From Recognition to Weighted Parsing. One may turn the recognizer of
§3.3 into a parser in the usual way. Together with each item stored in a chart
cell Ci,j , one must also maintain the highest-weighted known analysis with that
item as signature, or a parse forest of all known analyses with that signature. In
the implementation, items may be mapped to analyses via a hash table or array.
When we apply a rule from Figure 1.3b to derive a new item from old ones,
we must also derive an associated analysis (or forest of analyses), and the weight
of this analysis if the grammar is weighted.
When parsing, how should we represent an analysis of a span? (For comparison, an analysis of a constituent can be represented as a tree.) A general
method is simply to store the span’s derivation: we may represent any analysis
as a copy of the rule that produced it together with pointers to the analyses that
serve as inputs (i.e., antecedents) to that rule. Or similarly, one may follow
the decomposition of §3.3 and Figure 1.1d. Then an analysis of wi,k is a triple
(α, β, linktype), where α points to an analysis of a simple span wi,j , β points
to an analysis of a span wj,k , and linktype ∈ {←, →, none} speciﬁes the direction of the link (if any) between wi and wk . In the base case where k = i+ 1,
then α and β instead store wi and wk respectively.
We must also know how to compute the weight of an analysis. Any convenient deﬁnition will do, so long as the weight of a full parse comes out correctly.

16
In all cases, we will deﬁne the weight of an analysis produced by a rule to be
the total weight of the input(s) to that rule, plus another term derived from the
conditions on the rule. For Seed and Combine, the additional term is 0; for
′
′
Opt-Link-L or Opt-Link-R, it is the weight of the transition to q1 or q2
respectively; for Seal-L, Seal-R, or Accept, it is the ﬁnal-state weight of
q1 , q2 , or q2 respectively.
As usual, the strategy of maintaining only the highest-weighted analysis of
each signature works because context-free parsing has the optimal substructure property. That is, any optimal analysis of a long string can be found by
gluing together just optimal analyses of shorter substrings. For suppose that
a and a′ are analyses of the same substring, and have the same signature, but
a has less weight than a′ . Then suboptimal a cannot be part of any optimal
analysis b in the chart—for if it were, the deﬁnition of signature ensures that
we could substitute a′ for a in b to get an analysis b′ of greater total weight than
b and the same signature as b, which contradicts b’s optimality.

4.2

POLYSEMY

We now extend the formalism to deal with lexical selection. Regrettably, the
input to a parser is typically not a string in V ∗ . Rather, it contains ambiguous
tokens such as bank, whereas the ‘words’ in V are word senses such as bank1 ,
bank2 , and bank3 , or part-of-speech-tagged words such as bank/N and bank/V.
If the input is produced by speech recognition or OCR, even more senses are
possible.
One would like a parser to resolve these ambiguities simultaneously with the
structural ambiguities. This is particularly true of a bilexical parser, where a
word’s dependents and parent provide clues to its sense and vice-versa.

Confusion Sets. We may modify the formalism as follows. Consider the
unweighted case ﬁrst. Let Ω be the real input—a string not in V ∗ but rather in
P(V )∗ , where P denotes powerset. Thus the ith symbol of Ω is a confusion
set of possibilities for the ith word of the input, e.g., {bank1 , bank2 , bank3 }.
Ω is generated by the grammar, with analysis T , if some string ω ∈ V ∗ is so
generated, where ω is formed by replacing each set in Ω with one of its elements.
Note that the yield of T is ω, not Ω.
For the weighted case, each confusion set in the input string Ω assigns a
weight to each of its members. Again, intuitively, the heavily-weighted members are the ones that are commonly correct, so the noun bank/N would be
weighted more highly than the verb bank/V. We score parses as before, except
that now we also add to a dependency tree’s score the weights of all the words
that label its nodes, as selected from their respective confusion sets. Formally,
we say that Ω = W1 W2 . . . Wn ∈ P(V )∗ is generated by the grammar, with
analysis T and weight µT µ1 + · · · + µn , if some string ω = w1 w2 . . . wn ∈ V ∗

Bilexical Grammars and O(n3 ) Parsing

17

is generated with analysis T of weight µT , and for each 1 ≤ i ≤ n, ωi appears
in the set Wi with weight µi .

Modifying the Algorithm. Throughout the algorithm of Figure 1.3, we must
replace each integer i (similarly j, k) with a pair of the form (i, wi ), where
wi ∈ Wi . That ensures that the signature of an analysis of Wi,j will record the
senses wi and wj of its endwords. The Opt-Link rules refer to these senses
when determining whether wj can be a child of wi or vice-versa. Moreover,
Combine now requires its two input spans to agree not only on j but also on
the sense wj of their overlapping word Wj , so that this word’s left children,
right children, and parent are all appropriate to the same sense. The Seed
rule nondeterministically chooses senses wi ∈ Wi and wi+1 ∈ Wi+1 ; to avoid
double-counting, the weight of the resulting analysis is taken to be the weight
with which wi appears in Wi only.
If g is an upper bound on the size of a confusion set, then these modiﬁcations
multiply the algorithm’s space requirements by O(g2 ) and its time requirements
by O(g3 ).

4.3

STRING-LOCAL CONSTRAINTS

When the parser is resolving polysemy as in §4.2, it can be useful to implement string-local constraints. The Seed rule may be modiﬁed to disallow an
arbitrary list of word-sense bigrams wi wi+1 . More usefully, it may be made
to favor some bigrams over others by giving them higher weights. Then the
sense of one word will affect the preferred sense of adjacent words. (This is in
addition to affecting the preferred sense of the words it links to).
For example, suppose each word is polysemous over several part-of-speech
tags, which the parser must disambiguate. A useful hack is to deﬁne the weight
of a parse as the log-probability of the parse, as usual, plus the log-probability
of its tagged yield under the trigram tagging model of (Church, 1988). Then a
highly-weighted parse will tend to be one whose tagged dependency structure
and string-local structure are simultaneously plausible. This has been shown
useful for probabilistic systems that simultaneously optimize tagging and parsing (Eisner, 1996a). (See (Lafferty et al., 1992) for a different approach.)
To add in the trigram log-probability in this way, regard each input word as
a confusion set Wi whose elements have the form wi = (vi , ti , ti+1 ). Here
each vi is an ordinary word (or sense) and ti , ti+1 are hypothesized part-ofspeech tags for vi , vi+1 respectively. Now Seed should be restricted to produce
only word-sense bigrams (vi , ti , ti+1 )(vi+1 , ti+1 , ti+2 ) that agree on ti+1 . The
score of such a bigram is log Pr(vi | ti ) + log Pr(ti | ti+1 , ti+2 ). (If i = 1,
it is also necessary to add log Pr(stop | t1 , t2 ).) Notice that (for notational
convenience) we are treating the word sequence as generated from right to left,
not vice-versa.

18

4.4

RATIONAL TRANSDUCTIONS

Polysemy (§4.2) and string-local constraints (§4.3) are both simple, local
string phenomena that are inconvenient to model within the bilexical grammar.
Many other such phenomena exist in language: they tend to be morphological
in nature and easily modeled by ﬁnite-state techniques that apply to the yield
of the dependency tree. This section conveniently extends the formalism and
algorithm to accommodate such techniques. The previous two sections are
special cases.

Underlying and Surface Strings. We distinguish the “underlying” string ω =
w1 w2 . . . wn ∈ V ∗ from the “surface” string Ω = W1 W2 . . . WN ∈ X ∗ .
Thus V is a collection of morphemes (word senses), whereas X is typically a
collection of graphemes (orthographic words). It is not necessary that n = N .
It is the underlying string ω that is described by the bilexical grammar. In
general, ω is related to our input Ω by a possibly nondeterministic, possibly
weighted ﬁnite-state transduction R (Mohri et al., 1996), as deﬁned below.
We say that the surface string Ω is grammatical, with analysis (T, P ), if T
is a dependency parse tree whose fringe, ω root, is transduced to Ω along an
accepting path P in R. Notice that the analysis describes the tree, the underlying
string, and the alignment between the underlying and surface strings.
The weighted parsing problem is now to reconstruct the best analysis (T, P )
of Ω. The weight of an analysis is the weight of T plus the weight of P . For
example, if weights are deﬁned to be log-probabilities under a generative model,
then the weight of T is the log-probability of stochastically generating the parse
tree T and then stochastically transducing its fringe to the observed input.
Linguistic Uses. The transducer R may be used for many purposes. It can
map different senses onto the same grapheme (polysemy) or vice-versa (spelling
variation, contextual allomorphy). If the output alphabet X consists of letters
rather than words, the transducer can apply morphological rules, such as the
afﬁxation and spelling rule in try -ed → tried (Koskenniemi, 1983; Kaplan and
Kay, 1994). It can also perform more interesting kinds of local morphosyntactic
processes (PAST TRY → try -ed (afﬁx hopping), NOT CAN → {can’t, cannot},
PRO → ǫ, ”. → .”).
In another vein, R may be an interestingly weighted version of the identity
transducer. This can be used to favor or disfavor local patterns in the underlying
string ω. A classic example is the “that-trace” ﬁlter. Similarly, the trigram model
of §4.3 can be implemented easily with a transducer that merely removes the
tags from tagged words, and whose weights are given by log-probabilities under
a trigram model.
Finally, if R is used to describe a stochastic noisy channel that has corrupted
or translated the input in some way, then the parser will automatically correct

Bilexical Grammars and O(n3 ) Parsing

19

for the noise. Most ambitiously, R could be a generative acoustic model, and
X an an alphabet of acoustic observations. In this case, the bilexical grammar
would essentially be serving as the language model for a speech recognizer.
It is often convenient to deﬁne R as a composition of several simpler weighted
transducers (Mohri et al., 1996), each of which handles just one of the above
phenomena. For example, in order to map a sequence of abstract morphemes
and punctuation tokens (∈ V ∗) to a sequence of ASCII characters (∈ X∗), one
could use the following transducer cascade: afﬁx hopping, “that-trace” penalization, followed by deletion of phonological nulls, then conventional processes
such as capitalization marking and comma absorption, then realization of abstract morphemes as lemmas or null strings, then various morphological rules,
and ﬁnally a stochastic model of typographical errors. Given some text Ω that
is supposed to have emerged from this pipeline, the parser’s job is to ﬁnd a
plausible way of renormalizing it that leads to a good parse.

Transducer Notation. The ﬁnite-state transducer R has the same form as
a (nondeterministic) ﬁnite-state automaton. However, the arcs are labeled not
by symbols w ∈ V but rather by pairs γ : Γ, where γ ∈ V ∗ and Γ ∈ X ∗ .
The transducer R is said to transduce γ to Γ along path P if the arcs of P
are consecutively labeled γ1 : Γ1 , γ2 : Γ2 , . . . γk : Γk , and γ1 γ2 · · · γk = γ and
Γ1 Γ2 · · · Γk = Γ. We call this transduction terminal if γk = γ (or k = 0).
One says simply that R transduces ω to Ω if it does so along an accepting
path, i.e., a path from the initial state of R to a ﬁnal state. The path’s weight
can be deﬁned as in §4.1, in terms of weights on the arcs and ﬁnal states of R.
We may assume without loss of generality that the strings γ have length ≤ 1.
That is, all arc labels have the form w : Γ where w ∈ V ∪ {ǫ} and Γ ∈ X ∗ .
We reuse the notation of §2. as follows. Q(R) and I(R) denote the set
of states and the initial state of R, and the predicate F (r) means that state
r ∈ Q(R) is ﬁnal. The transition predicate R(r, r ′ , w : Γ) is true if there is an
arc from r to r ′ with label w : Γ. Its ǫ-left-closure R∗ (r, r ′ , w : Γ) is true iff R
terminally transduces w to Γ along some path from r to r ′ .
Modifying the Inference Rules. Recall that when modifying the algorithm
to handle polysemy, we replaced each integer i in Figure 1.3 with a pair (i, w).
For the more general case of transductions, we similarly replace i with a triple
(i, w, r), where w ∈ V, r ∈ Q(R). An item of the form
···
···
· · · ··· · · ·
(0 ≤ i ≤ j ≤ n; w, w′ ∈ V ; r, r ′ ∈ Q(R); · · ·)
′ , r′
i, w, r
j, w
represents the following hypothesis about the correct sentential analysis (T, P ):
that the tree T has a span wβw′ (for some string β) such that βw′ is terminally
transduced to the surface substring Wi+1,j along a subpath of P from state r to

20
(a) Seed: R∗ (r, r ′ , w′ : Wi+1,j )
0
0
q1 1 q2
i, w, r
j, w′ , r ′

q1 = I(rwi ), q2 = I(ℓwi+1 )

Accept:

1
0
F s q2
R (I(R), r, w : W1,i )
i, w, r
j, Root, r ′
accept
∗

R∗ (r ′ , r ′′ , ǫ : Wj+1,n )

Final-ǫ:

(b) Final-w:
R∗ (r, r ′ , w : Wi+1,j )

R(r, r ′ , w : Wi+1,j )

Ext-Left: R∗ (r ′ , r ′′ , w : Wj+1,k )
R∗ (r, r ′′ , w : Wi+1,k )
(c)
Start-Prefix: R∗ (I(R), r, w : W1,i )
′

i

r −→

i,w
Ext-Prefix: −→ r R∗ (r, r ′ , w′ : W
i+1,j )

1.
2.
3.
4.
5.
6.
7.
8.
9.
10.
11.
12.
13.
14.

j,w′

−→ r ′
Ext-Suffix: R∗ (r, r ′ , w′ : W
′ j
i+1,j ) r −→
F (r ′ )
i
r −→

i,w
(d) Seed: −→ r R∗ (r, r ′ , w′ : W
′ j
i+1,j ) r −→

0
0
q1 1 q2
i, w, r
j, w′ , r ′

R∗ (r, r, ǫ : Wi+1,i )

R(r, r ′ , ǫ : Wi+1,j )

i,w

−→ r
Start-Suffix: R (r, r , ǫ : Wi+1,n )
∗

(e)

F (q2 ), F (r ′′ )

q1 = I(rwi ), q2 = I(ℓwi+1 )

Agenda := {}
(* priority queue of items by weight of their associated derivations *)
Done := {}
(* set of items indexed as discussed in §3.1, §3.2 *)
foreach x that can be produced by a rule with no inputs
AddAgenda(x, Agenda)
(* if duplicate, then also removes copy with the lighter derivation *)
while Agenda = {}
x := Pop(Agenda)
(* highest-weighted item *)
if x = accept then return accept
(* also return associated derivation *)
if x ∈ Done
AddDone(x, Done)
(* updates indices appropriately *)
foreach rule r
if r(x) is deﬁned then AddAgenda(r(x), Agenda)
(* as above *)
(* use indices *)
foreach z ∈ y∈Done {(x, y), (y, x)} with r(z) deﬁned
AddAgenda(r(z), Agenda)
(* as above *)
return reject

Figure 1.5 All non-trivial changes to Figure 1.3 needed for handling transductions of the input.
(a) The minimal modiﬁcation to ensure correctness. The predicate R∗ (r, r ′ , w′ : Wi+1,j ) is
used here as syntactic sugar for an item [r, r ′ , w′ , i + 1, j] (where i ≤ j) that will be derived
iff the predicate is true. (b) Rules for deriving those items during preprocessing of the input.
(c) Deriving “forward-backward” items during preprocessing. (d) Adding “forward-backward”
antecedents to parsing to rule out items that are impossible in context. (e) Generic pseudocode for
agenda-based parsing from inference rules. Line 12 uses indices on y to enumerate z efﬁciently.

Bilexical Grammars and O(n3 ) Parsing

21

state r ′ .4 Notice that if i = j then Wi+1,j = ǫ by deﬁnition. Also notice that
no claim is made about the relation of w to W1,i (but see below).
Combine must be modiﬁed along the same lines as for polysemy: it must
require its two input spans to agree not only on j but on the entire triple (j, w ′ , r ′ ).
As before, Opt-Link should be deﬁned in terms of the underlying words w, w′ .
It is only the Seed and Accept rules that actually need to examine the
transducer R. Modiﬁed versions are shown in Figure 1.5a. These rules make
reference to the ǫ-left-closed transition relation R∗ (· · ·), which Figure 1.5b
shows how to precompute on substrings of the input Ω.

From Recognition to Parsing. This modiﬁed recognition algorithm yields
a parsing algorithm just as in §4.1. An analysis with the signature shown
above has two parts: an analysis of the span wβw′ , and the r-to-r ′ subpath
that terminally transduces βw′ to Wi+1,j . Its weight is the sum of the weights
of these two parts. To compute this weight, each rule in Figure 1.5a–b should
deﬁne the weight of its output to be the total weight of its inputs, plus the arc
or ﬁnal-state weight associated with any R(r, r ′ , . . .) or F (· · ·) that it tests.
Cyclic Derivations. If R can transduce non-empty underlying substrings to
ǫ, we must now use chart cells Ci,i , for spans that correspond to the surface
substring Wi+1,i = ǫ. In the general case where R can do so along cyclic
paths, so that such spans may be unbounded, items can no longer be combined
in a ﬁxed order as in Figure 1.4 (lines 10–16). 5 This is because combining
items from Ci,i and Ci,j (i ≤ j) may result in adding new items back into Ci,j ,
which must be allowed to combine with their progenitors in Ci,i again. The
usual duplicate check ensures that we will terminate with the same time bounds
as before, but managing this incestuous computation requires a more general
agenda-based control mechanism (Kay, 1986), whose weighted case is shown
in Figure 1.5e.6
Analysis. The analysis is essentially the same as for polysemy (§4.2), i.e.,
O(n3 g3 t2 ) time, or O(n3 g3 t) if we use the speedup of §3.4. The priority queue
in Figure 1.5e introduces an extra factor of log |Agenda| = O(log ngt). An
ordinary FIFO or LIFO queue can be substituted in the unweighted case or if
there are no cycles of the form discussed. 7
However, g now bounds the number of possible triples (i, w, r) compatible
with a position i in the input Ω. Notice that as with ℓw and rw , there is no
penalty for the number of arcs in R, i.e. the sizes of the vocabularies V, X.
Is g small? The intuition is that most transductions of interest give a small
bound g, since they are locally “almost” invertible: they are constrained by the
surface string Ω to only consider a few possible underlying words and states
at each position i. For example, a transducer to handle polysemy (map senses

22
onto words) allows only a few underlying senses w per surface word Wi , and
it needs only one state r.
But alas, the algorithm so far does not respect these constraints. Consider
the Seed rule in Figure 1.5a: w (though not w′ ) is allowed to take any value in
V regardless of the input, and r, r ′ are barely more constrained. So the parser
would allow many unnecessary triples and run very slowly. We now ﬁx it to
reclaim the intuition above.

Restoring Efﬁciency. We wish to constrain the (i, w, r) triples actually considered by the parser, by considering Wi and more generally the broader context
provided by the entire input Ω. A triple (i, w, r) should never be considered
unless it is consistent with some transduction that could have produced Ω.
We introduce two new kinds of items that let us check this consistency.
i,w
The rules in Figure 1.5 derive the “forward item” −→ r iff R can terminally
transduce αw (for some α) to W1,i on a subpath from I(R) to r. They derive
i

the “backward item” r −→ iff R can transduce some β to Wi+1,n on a subpath
from r to a ﬁnal state. Figure 1.5d modiﬁes the Seed rule to require such items
as antecedents, which is all we need.

Remark. The new antecedents are used only as a ﬁlter. In parsing, they
contribute no weight or detail to the analyses produced by the revised rule
Seed. However, their weights might be used to improve parsing efﬁciency.
Work by (Caraballo and Charniak, 1998) on best-ﬁrst parsing suggests that the
total weight of the three items
j

i,w

−→ r

j, w′ , r ′

r ′ −→

i, w, r
may be a good heuristic measure of the viability of the middle item (representing
a type of span) in the context of the rest of the sentence. (Notice that the middle
item cannot be derived at all unless the other two also can.)

5.

RELATION TO OTHER FORMALISMS

The bilexical grammar formalism presented here is ﬂexible enough to capture
a variety of grammar formalisms and probability models. On the other hand,
as discussed in §5.6, it does not achieve the (possibly unwarranted) power of
certain other bilexical formalisms.

5.1

MONOLEXICAL DEPENDENCY GRAMMAR

Lexicalized Dependency Grammar. It is straightforward to encode dependency grammars such as those of (Gaifman, 1965). We focus here on the case
that (Milward, 1994) calls Lexicalized Dependency Grammar (LDG). Milward

Bilexical Grammars and O(n3 ) Parsing

23

demonstrates a parser for this case that requires O(n3 g3 t3 ) time and O(n2 g2 t2 )
space, using a left-to-right algorithm that maintains its state as an acyclic directed graph. Here t is taken to be the maximum number of dependents on a
word.
LDG is deﬁned to be only monolexical. Each word sense entry in the lexicon
is for a word tagged with the type of phrase it projects. An entry for helped/S,
which appears as head of the sentence Nurses helped John wash, may specify
that it wants a left dependent sequence of the form w1 /N and a right dependent
sequence of the form w2 /N, w3 /V. However, under LDG it cannot constrain the
lexical content of w1 , w2 , or w3 , either discretely or probabilistically. 8
By encoding a monolexical LDG as a bilexical grammar, and applying the
algorithm of this chapter, we can reduce parsing time and space by factors of t2
and t, respectively. The encoding is straightforward. To capture the preferences
for helped/S as above, we deﬁne ℓhelped/S to be a two-state automaton that
accepts exactly the set of nouns, and rhelped/S to be a three-state automaton
that accepts exactly those word sequences of the form (noun, verb).
Obviously, ℓhelped/S includes a great many arcs—one arc for every noun in
V . This does not however affect parsing performance, which depends only on
the number of states in the automaton.

Optional and Iterated Dependents. The use of automata to specify dependents is similar to the idea of allowing regular expressions in CFG rules, e.g.,
NP → (Det) Adj* N (Woods, 1969). It makes the bilexical grammar above
considerably more ﬂexible than the LDG that it encodes. In the example above,
rhelped/S can be trivially modiﬁed so that the dependent verb is optional (Nurses
helped John). LDG can accomplish this only by adding a new lexical sense of
helped/S, increasing the polysemy term g.
Similarly, under a bilexical grammar, ℓnurses/N can be speciﬁed to accept
dependent sequences of the form (adj, adj, adj, . . . adj, (det)). Then nurses may
be expanded into weary Belgian nurses. Unbounded iteration of this sort is not
possible in LDG, where each word sense has a ﬁxed number of dependents.
In LDG, as in categorial grammars, weary Belgian nurses would have to be
headed by the adjunct weary. Thus, even if LDG were sensitive to bilexicalized
dependencies, it would not recognize nurses→helped as such a dependency in
weary Belgian nurses helped John. (It would see weary→helped instead.)

5.2

BILEXICAL DEPENDENCY GRAMMAR

In the example of §5.1, we may arbitrarily weight the individual noun arcs of
the ℓhelped automaton, according to how appropriate those nouns are as subjects
of helped. (In the unweighted case, we might choose to rule out inanimate
subjects altogether, by removing their arcs or assigning them the weight −∞.)

24
This turns the grammar from monolexical to bilexical, without affecting the
cubic-time cost of the parsing algorithm of §3.3.

5.3

TEMPLATE MATCHING

(Becker, 1975) argues that much naturally-occurring language is generated
by stringing together ﬁxed phrases and templates. To the bilexical construction
of §5.2, one may add handling for special phrases. Consider the idioms (a) run
scared, (b) run circles [around NP], and (c) run NP [into the ground]. (a), like
most idioms, is only bilexical, so it may be captured ‘for free’: simply increase
the weight of the scared arc in rrun/V . But because (b) and (c) are trilexical, they
require augmentation to the grammar, possibly increasing t and g. (b) requires
a special state to be added to rrun/V , so that the dependent sequence (circles,
around) may be recognized and weighted heavily. (c) requires a specialized
lexical entry for into; this sense is a preferred dependent of run and has ground
as a preferred dependent.

5.4

PROBABILISTIC BILEXICAL MODELS

(Eisner, 1996a) compares several distinct probability models for dependency
grammar. Each model simultaneously evaluates the part-of-speech tags and
the dependencies in a given dependency parse tree. Given an untagged input
sentence, the goal is to ﬁnd the tagged dependency parse tree with highest
probability under the model.
Each of these models can be accomodated to the bilexical parsing framework,
allowing a cubic-time solution. In each case, V is a set of part-of-speech-tagged
words. Each weighted automaton ℓw or rw is deﬁned so that it accepts any
dependent sequence in V ∗ —but the automaton has 8 states, arranged so that
the weight of a given dependent w′ (or the probability of halting) depends on
the major part-of-speech category of the previous dependent.9 Thus, any arc
that reads a noun (say) terminates in the Noun state. The w′ -reading arc leaving
the Noun state may be weighted differently from the w′ -reading arcs from other
states; so the word w′ may be more or less likely as a child of w according to
whether its preceding sister was a noun.
As sketched in (Eisner, 1996b), each of Eisner’s probability models is implemented as a particular scheme for weighting these automaton. For example,
model C regards ℓw and rw as Markov processes, where each state speciﬁes a
probability distribution over its exit options, namely, its outgoing arcs and the
option of halting. The weight of an arc or a ﬁnal state is then the log of its
probability. Thus if rhelped/V includes an arc labeled with bathe/V and this
arc is leaving the Noun state, then the arc weight is (an estimate of)
log Pr(next right dependent is bathe/V | parent is helped/V and previous
right dependent was a noun
)

Bilexical Grammars and O(n3 ) Parsing

25

The weight of a dependency parse tree under this probability model is a sum
of such factors, which means that it estimates log Pr(dependency links & input words) according to a generative model. By contrast, model D estimates
log Pr(dependency links | input words), using arc weights that are roughly of
the form
log Pr(bathe/V is a right dep. of helped/V | both words appear in sentence
and prev. right dep. was a noun)
which is similar to the probability model of (Collins, 1996). Thus, different
probability models are simply different weighting schemes within our framework. Some of the models use the trigram weighting approach of §4.3.

5.5

BILEXICAL PHRASE-STRUCTURE GRAMMAR

Nonterminal Categories as Sense Distinctions. In some situations, conventional phrase-structure trees appear preferable to dependency trees. (Collins,
1997) observes that since VP and S are both verb-headed, the dependency
grammars of §5.4 would falsely expect them to appear in the same environments. (The expectation is false because continue subcategorizes for VP only.)
Phrase-structure trees address the problem by subcategorizing for phrases that
are labeled with nonterminals like VP and S.
Within the present formalism, the solution is to distinguish multiple senses
(§4.2) for each word, one for each of its possible maximal projections. Then
help/VPinf and help/S are separate senses: they take different dependents (yielding to help John vs. nurses help John), and only the former is an appropriate
dependent of continue.
Unﬂattening the Dependency Structure. A second potential advantage of
phrase-structure trees is that they are more articulated than dependency trees. In
a (headed) phrase-structure tree, a word’s dependents may attach to it at different
levels (with different nonterminal labels), providing an obliqueness order on the
dependents. Obliqueness is of semantic interest; it is also exploited by (Wu,
1995), whose statistical translation model preserves the topology (ID but not
LP) of binary-branching parses.
For the most part, it is possible to recover this kind of structure under the
present formalism. A scheme can be deﬁned for converting dependency parse
trees to labeled, binary-branching phrase-structure trees. Then one can use
the fast bilexical parsing algorithm of §3.3 to generate the highest-weighted
dependency tree, and then convert that tree to a phrase-structure tree, as shown
in Figure 1.6.
For concreteness, we sketch how such a scheme might be deﬁned. First label
the states of all automata ℓw , rw with appropriate nonterminals. For example,
rhelp/S might start in state V; it transitions to state VP after reading its object,

26

help/S
Nurses/NP

John/NP

=⇒
readily/AdvP

S
NP

VP

Nurses

VP

AdvP

V

NP

help

John

readily

Figure 1.6 Unﬂattening a dependency tree when the word senses and automaton states bear
nonterminal labels.

John/NP; and it loops back to VP when reading an adjunct such as readily/AdvP.
Now, given a dependency tree for Nurses help John readily, we can reconstruct
the sequence V, VP, VP of states encountered by rhelp/S as it reads help’s right
children, and thereby associate a nonterminal attachment level with each child.
To produce the full phrase-structure tree, we must also decide on an obliqueness order for the children. Since this amounts to an order for the nodes at
which the children attach, one approach is to derive it from a preferred total
ordering on node types, according to which, say, right-branching VP nodes
should always be lower than left-branching S nodes. We attach the children
one at a time, referring to the ordering whenever we have a choice between
attaching the next left child and the next right child.
This kind of scheme is adequate for most linguistic purposes. (For example,
together with polysemy (§4.2) it can be used to encode the Treebank grammars of (Charniak, 1995).) It is interesting to compare it to (Collins, 1996),
who maps phrase-structure trees to dependency trees whose edges are labeled
with triples of nonterminals. In that paper Collins deﬁnes the probability of a
phrase-structure tree to be the probability of its corresponding dependency tree.
However, since his map is neither ‘into’ nor ‘onto,’ this does not quite yield a
probability distribution over phrase-structure trees; nor can he simply ﬁnd the
best dependency tree and convert it to a phrase-structure tree as we do here,
since the best dependency tree may correspond to 0 or 2 phrase-structure trees.
Neither the present scheme nor that of (Collins, 1996) can produce arbitrary
phrase-structure trees. In particular, they cannot produce trees in which several
adverbs alternately left-adjoin and right-adjoin to a given VP. We now consider
the more powerful class of head-automaton grammars and bilexical context-free
grammars, which can describe such trees.

Bilexical Grammars and O(n3 ) Parsing

5.6

27

HEAD AUTOMATA

Weighted bilexical grammars are essentially a special case of head-automaton
grammars (Alshawi, 1996). As noted in the introduction, HAGs are bilexical in
spirit. However, the left and right dependents of a word w are accepted not separately, by automata ℓw and rw , but in interleaved fashion by a single weighted
automaton, dw . dw assigns weight to strings over the alphabet V × {←, →};
each such string is an interleaving of lists of left and right dependents from V .
Head automata, as well as (Collins, 1997), can model the case that §5.5
cannot: where left and right dependents are arbitrarily interleaved. (Alshawi,
1996) points out that this makes head automata fairly powerful. A head automaton corresponding to the regular expression ((a, ←)(b, →))∗ requires its word
to have an equal number of left and right dependents, i.e,. an wbn . (Bilexical
or dependency grammars are context-free in power, so they can also generate
{an wbn : n ≥ 0}—but only with a structure where the a’s and b’s depend
bilexically on each other, not on w. Thus, they allow only the usual linguistic
analysis of the doubly-center-embedded sentence Rats cats children frequently
mistreat chase squeak.)
For syntactic description, the added generative power of head automata is
probably unnecessary. (Linguistically plausible interactions among left and
right subcat frames, such as fronting, can be captured in bilexical grammars
simply via multiple word senses.)
Head automaton grammars and an equivalent bilexical CFG-style formalism
are discussed further in (Eisner and Satta, 1999), where it is shown that they
can be parsed in time O(n4 g2 t2 ).

5.7

LINK GRAMMARS

There is a strong connection between the algorithm of this chapter and the
O(n3 ) link grammar parser of (Sleator and Temperley, 1993). As Alon Lavie
(p.c.) has pointed out, both algorithms use essentially the same decomposition
into what are here called spans. Sleator and Temperley’s presentation (as a topdown memoizing algorithm) is rather different, as is the parse scoring model
introduced by (Lafferty et al., 1992). (Link grammars were unknown to this
author when he developed and implemented the present algorithm in 1994.)
This section makes the connection explicit. It gives a brief (and attractive)
deﬁnition of link grammars and shows how a minimal variant of the present
algorithm sufﬁces to parse them. As before, our algorithm allows an arbitrary
weighting model (§4.1) and can be extended to parse the composition of a link
grammar and a ﬁnite-state transducer (§4.4).

Formalism. A link grammar may be speciﬁed exactly as the bilexical grammars of §2. are. A link grammar parse of Ω = W1 W2 . . . Wn , called a linkage,

28
is a connected undirected graph whose vertices {1, 2, . . . n + 1} are respectively labeled with w1 ∈ W1 , w2 ∈ W2 , . . . wn ∈ Wn , wn+1 = root, and
whose edges do not ‘cross,’ i.e., edges i–k and j–ℓ do not both exist for any
i < j < k < ℓ. The linkage is grammatical iff for each vertex i, ℓwi accepts
the sequence of words wj : j < i, i–j is an edge (ordered by decreasing j),
and rwi accepts the sequence of words wj : j > i, i–j is an edge (ordered by
increasing j).
Traditionally, the edges of a linkage are labeled with named grammatical
relations. In this case, ℓwi should accept the sequence of pairs (wj , R) : j <
i, i–j is an edge labeled by R , and similarly for rwi .

Discussion. The above formalism improves slightly on (Sleator and Temperley, 1993) by allowing arbitrary DFAs rather than just straight-line automata
(cf. §5.1). This makes the formalism more expressive, so that it is typically
possible to write grammars with a lower polysemy factor g. In addition, any
weights or probabilities are sensitive to the underlying word senses wi (known
in link grammar as disjuncts), not merely the surface graphemes Wi .
Allowing ﬁnite-state post-processing as in §4.4 also makes the formalism
more expressive. It allows a modular approach to writing grammars: the link
grammar handles dependencies (topology-local phenemona) while the transducer handles string-local phenomena.
Modifying the Algorithm. Linkages have a less restricted form than dependency trees. Both are connected graphs without crossing edges, but only dependency trees disallow cycles or distinguish parents from children. The algorithm
of Figure 1.3 therefore had to take extra pains to ensure that each word has a
unique directed path to root. It can be simpliﬁed for the link grammar case,
where we only need to ensure connectedness. In place of the bits b1 and b2 , the
signature of an analysis of wi,j should include a single bit indicating whether
the analysis is a connected graph; if not, it has two connected components.
The input to Accept and at least one input to Combine must be connected.
(As for output, obviously Seed’s output is not connected, Opt-Link’s is, and
Combine or Seal’s output is connected iff all its inputs are.) To prevent
linkages from becoming multigraphs, each item needs an extra bit indicating
whether it is the output of Opt-Link; if so, it may not be input to Opt-Link
again.
Figure 1.3 (or Figure 1.5) needs one more change to become an algorithm
for link grammars. There should be only one Opt-Link rule, which should
′
advance the state q1 of rwi to some state q1 by reading wj (like Opt-Link-L),
′
and simultaneously advance the state q2 of ℓwj to some state q2 by reading wi
(like Opt-Link-R). (Or if edges are labeled, there must be a named relation
R such that rwi reads (wj , R) and ℓwj reads (wi , R).) This is because link

Bilexical Grammars and O(n3 ) Parsing

29

grammar’s links are not directional: the linked words wi and wj stand in a
symmetric relation wherein they must accept each other.

Analysis. The resulting link grammar parser runs in time O(n3 g3 t2 ); so does
the obvious generalization of (Sleator and Temperley, 1993) to our automatonbased formalism. A minor point is that t is measured differently in the two
algorithms, since the automata ℓw , rw used in the Sleator-Temperley-style topdown algorithm must be the reverse of those used in the above bottom-up
algorithm. (The minimal DFAs accepting a language L and its reversal LR
may have exponentially different sizes t.)
The improvement of §3.4 to O(n3 g3 t) is not available for link grammars.
Nor is the improvement of (Eisner and Satta, 1999) to O(n3 g2 t), which uses a
different decomposition that relies on acyclicity of the dependency graph.

5.8

LEXICALIZED TREE-ADJOINING GRAMMARS

The formalisms discussed in this chapter have been essentially context-free.
The kind of O(n3 ) or O(n4 ) algorithms we have seen here cannot be expected
for the more powerful class of mildly context-sensitive grammars (Joshi et al.,
1991), where the best known parsing algorithms are O(n6 ) even for nonlexicalized cases. However, it is worth remarking that similar problems and
solutions apply when bilexical preferences are added. In particular, Lexicalized Tree-Adjoining Grammar (Schabes et al., 1988) is actually bilexical, since
each tree contains a lexical item and may select for other trees that substitute
or adjoin into it. (Eisner and Satta, 2000) show that standard TAG parsing
essentially takes O(n8 ) in this case, but can be sped up to O(n7 ).

6.

CONCLUSIONS

Following recent trends in probabilistic parsing, this chapter has introduced
a new grammar formalism, weighted bilexical grammars, in which individual
lexical items can have idiosyncratic selectional inﬂuences on each other.
The new formalism is derived from dependency grammar. It can also be
used to model other bilexical approaches, including a variety of phrase-structure
grammars and (with minor modiﬁcations) all link grammars. Its scoring approach is compatible with a wide variety of probability models.
The obvious parsing algorithm for bilexical grammars (used by most authors)
takes time O(n5 g2 t). A new method is exhibited that takes time O(n3 g3 t). An
extension parses sentences that have been “corrupted” by a rational transduction.
The simpliﬁed O(n3 g3 t2 ) variant of §3.3 was originally sketched in (Eisner,
1996b) and presented (though without beneﬁt of Figure 1.3) in (Eisner, 1997).
It has been used successfully in a large parsing experiment (Eisner, 1996a).

30
The reader may wish to know that more recently, (Eisner and Satta, 1999)
found an alternative algorithm that combines half-constituents rather than spans.
It has the same space requirements, and the asymptotically faster runtime of
O(n3 g2 t)—achieving the same cubic time on the input length but with a grammar factor as low as that of the naive n5 algorithm.
While the algorithm presented in this chapter is not as fast asymptotically as
that one, there are nonetheless a few reasons to consider using it:
It is perhaps simpler to implement, as the chart contains not four types
of subparse but only one.10
With minor modiﬁcations (§5.7), the same implementation can be used
for link grammar parsing. This does not seem to be true of the faster
algorithm.
In some circumstances, it may run faster despite the increased grammar
constant. This depends on the grammar (i.e., the values of g and t) and
other constants in the implementation.
Using probabilities or a hard grammar to prune the chart can signiﬁcantly affect average-case behavior. For example, in one unpublished
experiment on Penn Treebank/Wall Street Journal text (reported by the
author at ACL ’99), probabilistic pruning closed the gap between the
O(n3 g3 t2 ) and O(n3 g2 t) algorithms. (Both still substantially outperformed the pruned O(n5 ) algorithm.)
With the improvement presented in §3.4, the asymptotic penalty of the
span-based approach presented here is reduced to only O(g).
Thus, while (Eisner and Satta, 1999) is the safer choice overall, the relative
performance of the two algorithms in practice may depend on various factors.
One might also speculate on algorithms for related problems. For example,
the g3 factor in the present algorithm (compared to Eisner and Satta’s g 2 ) reﬂects
the fact that the parser sometimes considers three words at once. In principle this
could be exploited. The probability of a dependency link could be conditioned
on all three words or their senses, yielding a ‘trilexical’ grammar. (Lafferty et al.,
1992) use precisely such a probability model in their related O(n3 ) algorithm
for parsing link grammars, although it is not clear how relevant their third word
is to the probability of the link (Eisner, 1996b).

Acknowledgments
I am grateful to Michael Collins, Joshua Goodman, and Alon Lavie for useful discussion of
this work.

Bilexical Grammars and O(n3 ) Parsing

31

Notes
1. Actually, (Lafferty et al., 1992) is formulated as a trilexical model, though the inﬂuence of the third
word could be ignored: see §6..
2. Having uniﬁed an item with the left input of an inference rule, such as Combine in Figure 1.3, the
parser must enumerate all items that can then be uniﬁed with the right input.
3. In the sense of the dotted rules of (Earley, 1970).
4. Notice that our assumption about the form of arc labels, above, guarantees that any span of T will
be transduced to some substring of Ω by an an exact subpath of P . Without that assumption, the span might
begin in the middle of some arc of P .
5. Cycles that transduce ǫ to ǫ would create a similar problem for the rules of Figure 1.5b, but R can
always be transformed so as to eliminate such cycles.
6. We assume that the output of a rule is no heavier than any of its inputs, so that additional trips around
a derivational cycle cannot increase weight unboundedly. (E.g., all rule weights are log-probabilities and
hence ≤ 0.) In this case the code can be shown correct: it pops items from the agenda only after their
highest-weighted (Viterbi) derivations are found, and never puts them back on the agenda.
The algorithm is actually a generalization to hypergraphs of the single-source shortest-paths algorithm
of (Dijkstra, 1959). In a hypergraph such as the parse forest, each parent of a vertex (item) is a set of
vertices (antecedents). Our single source is taken to be the empty antecendent set. Note that ﬁnding the
total weight of all derivations would be much harder than ﬁnding the maximum, in the presence of cycles
(Stolcke, 1995; Goodman, 1998).
7. The time required for the agenda-based algorithm is proportional to the number of rule instances
used in the derivation forest. The space is proportional to the number of items derived.
8. What would happen if we tried to represent bilexical dependencies in such a grammar? In order to
restrict w2 to appropriate objects of helped/S, the grammar would need a new nonterminal symbol, Nhelpable .
All nouns in this class would then need additional lexical entries to indicate that they are possible heads of
Nhelpable . The proliferation of such entries would drive g up to |V | in Milward’s algorithm, resulting in
O(n3 |V |3 t3 ) time (or by ignoring rules that do not refer to lexical items in the input sentence, O(n6 t3 )).
9. The eight states are start, Noun, Verb, Noun Modiﬁer, Adverb, Prep, Wh-word, and Punctuation.
10. On the other hand, for indexing purposes it is helpful to partition this type into at least two subtypes:
see the two charts of Figure 1.4.

References
Alshawi, H. (1996). Head automata and bilingual tiling: Translation with minimal representations. In Proceedings of the 34th ACL, pages 167–176, Santa
Cruz, CA.
Becker, J. D. (1975). The phrasal lexicon. Report 3081 (AI Report No. 28),
Bolt, Beranek, and Newman.
Caraballo, S. A. and Charniak, E. (1998). New ﬁgures of merit for best-ﬁrst
probabilistic chart parsing. Computational Linguistics.
Charniak, E. (1995). Parsing with context-free grammars and word statistics.
Technical Report CS-95-28, Department of Computer Science, Brown University, Providence, RI.
Charniak, E. (1997). Statistical parsing with a context-free grammar and word
statistics. In Proceedings of the Fourteenth National Conference on Artiﬁcial
Intelligence, pages 598–603, Menlo Park. AAAI Press/MIT Press.

32
Church, K. W. (1988). A stochastic parts program and noun phrase parser for
unrestricted text. In Proceedings of the 2nd Conf. on Applied NLP, pages
136–148, Austin, TX.
Collins, M. J. (1996). A new statistical parser based on bigram lexical dependencies. In Proceedings of the 34th ACL, pages 184–191, Santa Cruz, July.
Collins, M. J. (1997). Three generative, lexicalised models for statistical parsing.
In Proceedings of the 35th ACL and 8th European ACL, pages 16–23, Madrid,
July.
Dijkstra, E. W. (1959). A note on two problems in connexion with graphs.
Numerische Mathematik, 1:269–271.
Earley, J. (1970). An efﬁcient context-free parsing algorithm. Communications
of the ACM, 13(2):94–102.
Eisner, J. (1996a). An empirical comparison of probability models for dependency grammar. Technical Report IRCS-96-11, Institute for Research in Cognitive Science, Univ. of Pennsylvania.
Eisner, J. (1996b). Three new probabilistic models for dependency parsing: An
exploration. In Proceedings of the 16th International Conference on Computational Linguistics (COLING-96), pages 340–345, Copenhagen.
Eisner, J. (1997). Bilexical grammars and a cubic-time probabilistic parser. In
Proceedings of the 1997 International Workshop on Parsing Technologies,
pages 54–65, MIT, Cambridge, MA.
Eisner, J. and Satta, G. (1999). Efﬁcient parsing for bilexical context-free grammars and head-automaton grammars. In Proceedings of the 37th ACL, pages
457–464, University of Maryland.
Eisner, J. and Satta, G. (2000). A faster parsing algorithm for lexicalized treeadjoining grammars. In Proceedings of the 5th Workshop on Tree-Adjoining
Grammars and Related Formalisms (TAG+5), Paris.
Gaifman, H. (1965). Dependency systems and phrase structure systems. Information and Control, 8:304–337.
Goodman, J. (1997). Probabilistic feature grammars. In Proceedings of the
1997 International Workshop on Parsing Technologies, pages 89–100, MIT,
Cambridge, MA.
Goodman, J. (1998). Parsing Inside-Out. PhD thesis, Harvard University.
Graham, S. L., Harrison, M. A., and Ruzzo, W. L. (1980). An improved contextfree recognizer. ACM Transactions on Programming Languages and Systems, 2(3):415–463.
Joshi, A. K., Vijay-Shanker, K., and Weir, D. (1991). The convergence of mildly
context-sensitive grammar formalisms. In Sells, P., Shieber, S. M., and Wasow, T., editors, Foundational Issues in Naural Language Processing, chapter 2, pages 31–81. MIT Press.
Kaplan, R. M. and Kay, M. (1994). Regular models of phonological rule systems. Computational Linguistics, 20(3):331–378.

Bilexical Grammars and O(n3 ) Parsing

33

Kay, M. (1986). Algorithm schemata and data structures in syntactic processing. In Grosz, B. J., Sparck Jones, K., and Webber, B. L., editors, Natural
Language Processing, pages 35–70. Kaufmann, Los Altos, CA.
Koskenniemi, K. (1983). Two-level morphology: A general computational model
for word-form recognition and production. Publication 11, Department of
General Linguistics, University of Helsinki.
Lafferty, J., Sleator, D., and Temperley, D. (1992). Grammatical trigrams: A
probabilistic model of link grammar. In Proceedings of the AAAI Fall Symposium on Probabilistic Approaches to Natural Language, pages 89–97,
Cambridge, MA.
McAllester, D. (1999). On the complexity analysis of static analyses. In Proceedings of the 6th International Static Analysis Symposium, Venezia, Italy.
Mel’ˇ uk, I. (1988). Dependency Syntax: Theory and Practice. State University
c
of New York Press.
Milward, D. (1994). Dynamic dependency grammar. Linguistics and Philosophy, 17:561–605.
Mohri, M., Pereira, F., and Riley, M. (1996). Weighted automata in text and
speech processing. In Workshop on Extended Finite-State Models of Language (ECAI-96), pages 46–50, Budapest.
Pollard, C. and Sag, I. A. (1994). Head-Driven Phrase Structure Grammar.
University of Chicago Press and Stanford: CSLI Publications, Chicago.
Resnik, P. (1993). Selection and Information: A Class-Based Approach to Lexical Relationships. PhD thesis, University of Pennsylvania. Technical Report
IRCS-93-42, November.
Schabes, Y., Abeill´ , A., and Joshi, A. (1988). Parsing strategies with ‘lexicale
ized’ grammars: Application to Tree Adjoining Grammars. In Proceedings
of COLING-88, pages 578–583, Budapest.
Sleator, D. and Temperley, D. (1993). Parsing English with a link grammar.
In Proceedings of the 3rd International Workshop on Parsing Technologies,
pages 277–291.
Stolcke, A. (1995). An efﬁcient probabilistic context-free parsing algorithm that
computes preﬁx probabilities. Computational Linguistics, 21(2):165–201.
Woods, W. A. (1969). Augmented transition networks for natural language
analysis. Report CS-1, Harvard Computation Laboratory, Harvard University, Cambridge, MA.
Wu, D. (1995). An algorithm for simultaneously bracketing parallel texts by
aligning words. In Proceedings of the 33rd ACL, pages 244–251, MIT.

