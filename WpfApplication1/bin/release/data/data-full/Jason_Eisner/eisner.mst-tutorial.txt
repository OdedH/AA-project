State-of-the-Art Algorithms
for Minimum Spanning Trees∗
A Tutorial Discussion
Jason Eisner
University of Pennsylvania
April 1997

∗

This report was originally submitted in fulﬁllment of the Written Preliminary Exam II, Department
of Computer and Information Science, University of Pennsylvania, and was supported in part by a
National Science Foundation Graduate Research Fellowship. Many thanks to my prelim committee—
Tandy Warnow, Sampath Kannan, and Fan Chung—not only for pointing me to the papers that I discuss
here, but also for their friendship and willingness over the past years to hang out with me and talk about
algorithms, puzzles, or even the Aristotelian world!
A future version ought to illustrate the ideas with pictorial examples.

Abstract
The classic “easy” optimization problem is to ﬁnd the minimum spanning tree (MST) of a
connected, undirected graph. Good polynomial-time algorithms have been known since 1930.
Over the last 10 years, however, the standard O(m log n) results of Kruskal and Prim have been
improved to linear or near-linear time. The new methods use several tricks of general interest
in order to reduce the number of edge weight comparisons and the amount of other work. This
tutorial reviews those methods, building up strategies step by step so as to expose the insights
behind the algorithms. Implementation details are clariﬁed, and some generalizations are given.
Speciﬁcally, the paper attempts to shed light on the classical algorithms of Kruskal, of Prim,
and of Boruvka; the improved approach of Gabow, Galil, and Spencer, which takes time only
˙
O(m log(log* n − log* m )); and the randomized O(m) algorithm of Karger, Klein, and Tarjan,
n
which relies on an O(m) MST veriﬁcation algorithm by King. It also considers Frederickson’s
method for maintaining an MST in time O(m1/2 ) per change to the graph. An appendix
explains Fibonacci heaps.

Contents
§1 Introduction

7

§2 A Whimsical Overview
§2.1 Problem. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
§2.2 Formalization. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
§2.3 Implementation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

8
8
8
8

§3 Classical Methods
10
§3.1 Notation and Conventions. . . . . . . . . . . . . . . . . . . . . . . . . . . 10
§3.2 Properties of spanning trees. . . . . . . . . . . . . . . . . . . . . . . . . . 10
Spanning trees are connected and acyclic

§3.3

Properties of the MST. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10

§3.4
§3.5

Proof. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
Remark. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11

§3.6

Kruskal’s algorithm. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11

§3.7
§3.8

Visualization. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
Implementation and analysis. . . . . . . . . . . . . . . . . . . . . . . . . . 11

§3.9

Prim’s algorithm (Overview). . . . . . . . . . . . . . . . . . . . . . . . . . 12

MST has cut and cycle properties

Using cut and cycle properties to ﬁnd the MST
Try adding edges in increasing weight order

Kruskal’s algorithm is Θ(m log m)
Grow the tree using the lightest edge possible

§3.10 Prim’s algorithm (Development). . . . . . . . . . . . . . . . . . . . . . . . 12
Use “buckets” to reduce the set of candidate edges

§3.11 Analysis. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
Prim’s algorithm is O(n log n + m) with Fibonacci heaps

§3.12 Remark. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
Prim’s is optimal for dense graphs

§3.13 Discussion. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
Prim’s sorts only the MST edges

§3.14 Greedy discipline. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
The key idea shared by Kruskal’s and Prim’s

§4 Bottom-up clustering and Boruvka’s algorithm
˙
15
The generalized bottom-up method . . . . . . . . . . . . . . . . . . . . . . . . . . 15
§4.1 Strategy. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
Allow graph contraction during greedy algorithms

§4.2

Discussion. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15

§4.3

Notation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16

The graph shrinks from pass to pass

1

CONTENTS

2

§4.4

Implementation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16

§4.5

Implementation detail. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17

§4.6

Implementation detail. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17

Data structures and pseudocode
Finding components
Finding contracted endpoints

Boruvka’s algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
˙
§4.7 Preliminary version. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
Grow trees from every vertex at once

§4.8

Boruvka’s algorithm. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
Contract trees after each step

§4.9 Implementation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
§4.10 Analysis. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
Boruvka’s is O(min(m log n, n2 ))
˙

§4.11 Improved analysis. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
Boruvka’s is O(m · (1 + log(n2 /m)))
˙

§4.12 Implementation detail. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
Eliminating redundant edges after contraction

§4.13 Discussion. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
Boruvka’s sorts only the MST edges
˙

§4.14 Discussion. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
Boruvka’s scans the same edges on every pass
˙

§4.15 Discussion. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
Explicit contraction is unnecessary for Boruvka’s
˙

§5 Faster deterministic algorithms
22
§5.1 The β function. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
The new algorithms are nearly linear, and always better than Prim’s

§5.2

Remark. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
MST is not bound by edge-sorting

Fredman & Tarjan’s algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
§5.3 Motivation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
Keep the heap small for speed

§5.4

Development. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24

§5.5

Clariﬁcation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24

§5.6

Development. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25

§5.7

Analysis. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25

§5.8
§5.9

Implementation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
Remark. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25

Quit and start an additional tree if the heap gets too big
Trees may grow into each other
Contract all the trees and repeat
The graph shrinks extremely fast

No need to eliminate redundant edges

The packet algorithm of Gabow et al. . . . . . . . . . . . . . . . . . . . . . . . . 26
§5.10 Motivation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
Fredman & Tarjan scan the same edges on every pass

§5.11 Development. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
Sort a few edges at the start to postpone heavy edges

§5.12 Remark. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
Relation to Kruskal’s and medians-of-5

3

CONTENTS

§5.13 Development. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
Manipulating edge packets

§5.14 Analysis. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
Now a pass need not consider all edges

§5.15 Remark. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
Only O(m) total time wasted on repeats

§5.16 Analysis. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
We must use Union-Find when contracting

§5.17 Remark. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
Relation to Kruskal’s

§5.18 Development. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
We must merge undersized packets

§5.19 Remark. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
Applying packets to Boruvka’s
˙

§6 A linear-time veriﬁcation algorithm
32
Achieving O(m) comparisons . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32
§6.1 Overview. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32
Reduction to ﬁnding heaviest edges on tree paths

§6.2

Approach. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33

§6.3

Development. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33

§6.4

Development. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33

§6.5

Deﬁnitions. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34

§6.6

Theorem. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34

§6.7
§6.8
§6.9
§6.10
§6.11

Remark. . . . . . . . . .
Lemma. . . . . . . . . .
Lemma. . . . . . . . . .
Proof of theorem §6.6 . .
Remark. . . . . . . . . .

Construct a tree as its own MST to ﬁnd heaviest edges
Answering MaxEdge queries via Prim’s
Binary search of sparse vectors isn’t fast enough
Minimax(u, v) and F (w)
All greedy MST algorithms ﬁnd Minimax edges

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

34
34
34
34
35

Minimax, Cut, and Cycle are equivalent MST characterizations

§6.12 Discussion. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
Boruvka’s beats Prim’s for MaxEdge
˙

§6.13 Remark. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
Why Boruvka’s runs fast on trees
˙

§6.14 Remark. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
Most edges on a path are never considered as possible maxima

§6.15 Development. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
Storing the selected edges

§6.16 Notation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
Reducing to MaxEdge queries on a balanced tree

§6.17 Development. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
An array of running maxima at each node of B

§6.18 Analysis. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
§6.19 Remark. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38
Comparisons that witness the veriﬁcation

Reducing overhead to O(m) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39

4

CONTENTS

§6.20 Data Structure. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
Precomputing functions on bitstrings

§6.21 Strategy.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39

Make overhead proportional to comparisons

§6.22 Data Structure. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40
A particular sparse array scheme

§6.23 Strategy.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40

Process a batch of edges at a time

§6.24 Implementation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40
Sizes and quantities of bitstrings

§6.25 Deﬁnitions. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41
Appending and removing bits quickly

§6.26 Development. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41
Fast algorithm for computing each node’s array of heavy ancestors

§6.27 Development. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42
Finding edges from edge tags

§6.28 Implementation details. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43
Implementing more informative edge tags

§6.29 Development. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44
Use pointers whenever we don’t have time to copy

§7 A linear-time randomized algorithm
46
§7.1 Remark. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46
Randomized divide & conquer

§7.2

Motivation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46

§7.3

Motivation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47

§7.4

Scenario. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47

§7.5

Discussion. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47

§7.6

Discussion. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48

§7.7
§7.8

Justiﬁcation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48
Development. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48

§7.9

Implementation detail. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48

Building an MST by successive revision
Batching queries using King’s
Kruskal’s with random errors
False positives, not false negatives
Kruskal’s with random errors is an edge ﬁlter

Recovering the result of the ﬁlter
Handling disconnected graphs

§7.10 Discussion. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48
How to implement random Kruskal’s?

§7.11 Development. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49
MSF of a random subgraph

§7.12 Development. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49
Recursion with Boruvka contraction
˙

§7.13 Analogy. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49
Understanding “blind split” on a simpler problem

§7.14 Remark. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51
Edge competitions for eliminating edges

§7.15 Remark. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52
Alternatives to edge competitions

5

CONTENTS

§7.16 Analysis. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52
The binary tree of recursive calls

§7.17 Analysis (expected time on arbitrary input). . . . . . . . . . . . . . . . . 52
Expected time O(m)

§7.18 Analysis (probability of running in O(m0 ) time). . . . . . . . . . . . . . . 52
Actual time O(m) exponentially likely

§7.19 Analysis (worst-case running time).

. . . . . . . . . . . . . . . . . . . . . 53

Worst-case O(m · (1 + log(n2 /m))), like Boruvka’s
˙

§8 A dynamic algorithm
54
§8.1 Motivation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54
A changing communications network

§8.2

Overview. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54

§8.3

Development. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55

§8.4

Strategy.

§8.5

Rough Analysis. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56

§8.6

Remark. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56

§8.7

Strategy.

§8.8

Development. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58

§8.9

Implementation details. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58

Operations to support dynamically
Dynamic trees plus a new data structure

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55

Dividing F into vertex clusters connected by edge bundles
Choosing an intermediate cluster size
Dynamic algorithms need a balanced data structure

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57

Recursively bundling edges, heap-style
Initial build of the recursive data structure
Storage of edges

§8.10 Development. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58
Trivial to bridge cuts on the fully contracted graph

§8.11 Development. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59
Exploding and partitioning the graph

§8.12 Development. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59
Repairing the structure after changes

§8.13 Remark. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60
§8.14 Development. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61
Repairing unstable clusters

§8.15 Analysis. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61
Recursive structure has O(log n) levels

§8.16 Analysis. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62
O(m) to create, O(m1/2 ) to update

§8.17 Remark. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62
Frederickson’s choice of the zi appears optimal

§8.18 Development. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63
Updating the graph topology

§9 Lessons Learned

64

6

CONTENTS

§A Fibonacci Heaps
65
§A.1 Problem. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65
Operations supported by heaps

§A.2

Simpliﬁcation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66

§A.3

Binomial heaps. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66

§A.4
§A.5

Implementation details. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67
Remarks. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68

§A.6

Precursor to Fibonacci heaps. . . . . . . . . . . . . . . . . . . . . . . . . . 69

§A.7
§A.8
§A.9

Analysis. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69
Implementation details. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69
Discussion. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69

Union, Locate-Min, and Decrease-Key are central
A short list of heap trees

Binomial trees balance depth and breadth
Deferred consolidation of binomial trees

Postponing work creates economies of scale

§A.10 Fibonacci heaps. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70
Decrease-Key by detaching a subtree

§A.11 Deﬁnition. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71
Fibonacci trees balance depth and breadth, too

§A.12 Observation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72
The Fibonacci property has some “give”

§A.13 Development. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72
§A.14 Remark. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72
§A.15 Implementation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72
Deathbed bits

§A.16 Analysis. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72
An amortized analysis

§A.17 Remark. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73
Why only Delete is intrinsically slow

§A.18 Remark. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73
The strategic diﬀerence from binary heaps

Chapter §1

Introduction
The minimum spanning tree or MST problem is one of the simplest and best-studied optimization problems in computer science. Given an undirected, connected graph with m weighted
edges, it takes an O(m)-time depth-ﬁrst search to ﬁnd an arbitrary spanning tree, i.e., a tree
that connects all the vertices of G using only edges of G. The MST problem is to ﬁnd a spanning tree of minimum total weight. Remarkably, this optimization version of the problem can
be solved in little worse than O(m) time.
The problem has obvious applications to network organization and touring problems. It
also arises frequently in other guises. For example, the single-link clustering technique, used
by statisticians to partition data points into coherent groups, or to choose a tree topology for
dendroid distributions, is essentially MST under another name.
We will only consider algorithms for the case where edge weights may be compared but not
otherwise manipulated. The paper begins by reviewing the classical 1950’s MST-construction
algorithms of Kruskal [11] (previously invented by Varn´ in 1930) and Prim [13], as well as the
ık
1926 algorithm of Boruvka [2]. With this foundation, the paper proceeds with its main task:
˙
to explicate four algorithms from the more recent literature.
(a) The asymptotically fastest deterministic algorithm known is [7], an improvement on [5].
These algorithms are all-but-linear in m, i.e., linear times a log*-type factor.
(b) A randomized Las Vegas algorithm [8] achieves expected time that is truly linear in m, and
in fact guarantees linear-time performance with all but exponentially small probability.
(c) Veriﬁcation of a putative MST [4, 9] can be done in time linear in m. (The randomized
algorithm above uses this result.)
(d) The MST (more generally, MSF) of a fully dynamic graph can be maintained in time
√
O( m) per change to the graph [6].
While these algorithms are quite diﬀerent in many respects, it is notable that they all rely
on a recursive partitioning of the MST into connected subtrees. We will be able to use some
common notation and data structures to discuss this recursive partitioning.
Throughout the presentation, I will try to elucidate where the algorithms came from—that
is, how might someone have come up with them?—and how each gains a purchase on the
problem. My principal goal is to convey clear intuitions. Although I will rarely waste time
repeating material from the original papers unless I can ﬁll in signiﬁcant missing details or
oﬀer a fresh perspective, the present paper is essentially a tutorial, and the level of detail does
resemble that of the original texts.

7

Chapter §2

A Whimsical Overview
This paper could also have been called: “How to Organize a Grass-Roots Revolutionary Movement As Quickly As Possible.” The present section explains why.
§2.1 Problem. Russia, 1895. Idealists are scattered throughout the country. Their revolutionary ambitions hinge on their ability to make and maintain contact with each other. But
channels of communication can be risky to establish, use, and maintain.
The revolutionaries need to establish enough channels between pairs of individuals that a
message can be sent—perhaps indirectly—from anyone to anyone else. Naturally they prefer
safe channels, and no more channels than necessary: that is, the total risk is to be minimized. In
addition, they want to speed the revolution by establishing this network as quickly as possible!
§2.2 Formalization. This is, of course, an instance of the minimum spanning tree problem.
Imagine a graph whose vertices correspond to the revolutionaries. If two revolutionaries can
open up a channel of communication, there is a graph edge between them, weighted by the risk
of that channel.1 We wish to rapidly choose a subset of these edges so as to connect all the
vertices together with minimum total edge weight (i.e., risk).
§2.3 Implementation. Various organizing strategies are presently being proposed by theoreticians. Remarkably, all strategies arrive at the same communications network—a historical
inevitability. But the inevitable may arrive quickly or slowly, depending on whether the intelligentsia join the workers to hasten the revolutionary tide.
Kruskal’s (serial distributed) “Safe channels should be opened ﬁrst and risky ones left for a
last resort. The ﬁrst two individuals in Russia to establish contact should be those who
can most safely do so. The next safest pair, even if in far-oﬀ Siberia, should establish
contact with each other next.”
Prim’s (serial centralized) “Starting with Lenin, the central revolutionary organization in St.
Petersburg must grow by recruiting new members who are close friends of some existing
member—as close as can be found.”
1
One could deﬁne the risk of a channel as − log Pr(channel remains secure). The total risk for a set of channels
is then − i log Pr(channel i remains secure) = − log i Pr(channel i remains secure), which equals − log Pr(all
channels remain secure) if the channels are assumed to face independent dangers. Minimizing the latter quantity
means minimizing the chance that the network will somewhere be compromised.
The optimal network (the minimal spanning tree) can be found rapidly only because of the above assumption
that the risks of the various channels are independent. Formally, it is crucial that edge weights can be determined
independently and need only be summed to determine the total risk of the network. In other words, edges do
not interact combinatorially. They can be chosen fairly independently of each other.

8

CHAPTER §2. A WHIMSICAL OVERVIEW

9

Boruvka’s (parallel distributed) “Every one of us should at once reach out to his or her closest
˙
acquaintance. This will group us into cells. Now each cell should reach out to another
nearby cell, in the same way. That will form supercells, and so forth.”
Fredman & Tarjan’s “The St. Petersburg organization should grow just as Comrade Prim
suggests, but only until its Rolodex is so full that it can no longer easily choose its next
recruit.2 At that time we can start a new revolutionary organization in Moscow, and
then another in Siberia. Once we have covered Russia with regional organizations, the
St. Petersburg organization can start recruiting other entire organizations—so that cells
will be joined into supercells, as Comrade Boruvka has already proposed.”
˙
Gabow, Galil & Spencer’s “We approve of the radical ideas of Comrades Fredman and
Tarjan. However, may we suggest that each of us take a moment to glue together some
cards in his personal Rolodex, so that our close friends are a bit easier to ﬁnd than our
distant acquaintances? The cells and supercells and supersupercells will all be ﬂipping
through those cards in search of new recruits, once our Rolodexes are combined. It is our
duty to make it easy for them.”
Karger, Klein & Tarjan’s “Suppose we make this easier on ourselves by ignoring half the
possible channels, at random. We will still [by recursion] be able to establish some fairly
good paths of communication—so good that the channels we ignored are mainly riskier
than these paths. A few of the channels we ignored, but only a few, will be safe enough
that we should try to use them to improve the original paths.”
Frederickson’s “Comrades! Surely you see that we must adapt to events. If any channel
whatsoever should be compromised, splitting us into two movements, then the movements
must be able to ﬁnd another safe channel by which to communicate. I propose that we
build a resilient hierarchy of cells, supercells, supersupercells, and so on. Should we be
split temporarily into two movements, each movement must be able to reorganize quickly
as a hierarchy of its own. Then individuals and cells who know of safe channels to members
of the other movement can rapidly pass their ideas up the hierarchy to the leadership.”

2

As in Prim’s algorithm, the organization’s Rolodex grows rapidly. Every new member’s Rolodex is merged
into the organization’s, so that the organization can keep track of all friends of existing members.

Chapter §3

Classical Methods
This section presents the two classic MST algorithms, Kruskal’s [11] and Prim’s [13]. The
remarks focus on how the high-level strategies of these algorithms diﬀer, and why, intuitively,
this leads to diﬀerences in asymptotic performance.
We begin with some general notation and a brief discussion of why the Cut/Cycle properties
are both true and useful.
§3.1 Notation and Conventions. G = (V, E) denotes the (undirected) input graph, M
denotes the correct minimum spanning tree, and F denotes a forest that will eventually be the
MST. G has n vertices and m edges. We standardly assume that G is connected, so m ≥ n − 1.
An edge e ∈ E has weight w(e); if e’s endpoints in a graph are u and v we may sometimes
denote it by uv.
Edge weights need not be real numbers. We assume they are distinct elements of an ordered
set W, which is closed under an associative, commutative operation + that preserves the ordering, in the weak sense that (w < w′ ) ⇒ (w + x < w′ + x). Our assumption that the edges have
distinct weights is a harmless simpliﬁcation, as we may break ties arbitrarily in any consistent
manner (e.g., with reference to an enumeration of the edges).
For any vertex v, Γ(v) or ΓG (v) denotes the set of edges incident on v in graph G. All
graphs are stored using a standard adjacency-list representation. This allows a vertex of degree
d to enumerate its incident edges Γ(v) in time O(d + 1).
log n denotes logarithm to the base 2. lg n denotes log(n + 1), the number of bits in the
base-2 representation of n; so lg 0 = 0 and lg 1 = 1.
§3.2 Properties of spanning trees.
A spanning tree, T , is deﬁned as a connected Spanning trees are
acyclic spanning subgraph of G. “Connected” means that T includes at least one edge across connected and
each cut of G. “Acyclic” means that T excludes at least one edge from each cycle of G. A acyclic
minimum spanning tree is a spanning tree of G whose edges have minimal total weight.
§3.3 Properties of the MST.
Under our convention that all edges have distinct weights MST has cut and
(§3.1), the minimum spanning tree M has the following well-known complementary properties: cycle properties
• Strong cut property:
e ∈ M ⇔ e is the lightest edge across some cut of G.
• Strong cycle property:
e ∈ M ⇔ e is the heaviest edge on some cycle of G.
Either property implies at once that M is unique.

10

CHAPTER §3. CLASSICAL METHODS
§3.4

Proof.

11

The following proof is especially quick and symmetric.

⇒: Every e ∈ M , when removed from M , determines a cut across which it is lightest; every
e ∈ M , when added to M , determines a cycle on which it is heaviest.
Speciﬁcally, e ∈ M is the lightest edge between the two components of M − e (for any
lighter alternative e′ would yield a lighter spanning tree M − e + e′ ). Similarly, e ∈ M is
the heaviest edge on the cycle it completes in M + e (for any heavier alternative e′ would
yield a lighter spanning tree M + e − e′ .)

⇐: We derive the contrapositives from the cases just proved. e ∈ M cannot be lightest across
any cut, because we have just seen that it is heaviest on some cycle, which will cross the
cut again more lightly. Likewise e ∈ M cannot be heaviest on any cycle, because we have
just seen that it is lightest across some cut, which the cycle will cross again more heavily.
§3.5 Remark.
As we will see, all MST algorithms are rapid methods for ruling edges in Using cut and
or out of M . While either the Strong Cut Property or the Strong Cycle Property would suﬃce cycle properties to
to deﬁne which edges appear in M , the most practical techniques for ruling edges in or out use ﬁnd the MST
only the ⇐ implications of §3.3.1 The reason: the ⇐ implications have existential antecedents
(“if ∃ cut”) while their converses have universal ones (“if ∃ cut”). Why does this matter?
Because the ⇐ statements let us classify an edge as soon as we ﬁnd a single cut or cycle to
witness the classiﬁcation. Moreover, such witnesses can be produced in any order, because their
qualiﬁcation as witnesses depends only on G, not on what has already been determined about
M.
§3.6 Kruskal’s algorithm.
This classical algorithm [11] is derived directly from §3.3. Try adding edges
We consider each edge e, and use the Cut Property and Cycle Property to decide correctly in increasing
whether e ∈ M . If so, we add it to a growing forest F ⊆ M ; if not, we discard it. F is simply weight order
the graph (V, {edges added so far}).
Suppose Kruskal’s algorithm is considering whether to add edge uv to F . If added, uv would
be either F ’s ﬁrst-added edge across some cut or F ’s last-added edge in some cycle, depending
on whether there was previously a uv path in F . Kruskal’s insight is to consider the edges in
order of increasing weight. Then if uv is the ﬁrst edge across a cut, it is the lightest such, and
so must be added to F . If it is the last edge in a cycle, it is the heaviest such, and so must be
excluded from F . One of these two possibilities always holds, so we can classify each edge as
soon we see it.2
§3.7 Visualization. Whenever Kruskal’s algorithm adds an edge, it connects two component trees in F , reducing the number of components by 1. At the start, F has n components,
the isolated vertices. By the end, when n − 1 edges have been added, it has only a single
component, namely the true MST.
§3.8 Implementation and analysis.
Kruskal’s algorithm has two main chores. First, it Kruskal’s
must enumerate the edges in weight order; since only comparisons are allowed, this requires an algorithm is
1

Indeed, the ⇒ statements will play no role in this paper except to enable the pretty proof we just saw in
§3.4!
2
While I am doing my best to frame this as a symmetric proof, it is slightly less symmetric than it sounds.
Negative decisions uv ∈ F are justiﬁed by “local” evidence, namely an exhibitable path of lighter edges from u
to v. But to justify a positive decision uv ∈ F requires the inductive assumption that the algorithm has behaved
correctly so far. To wit, the algorithm adds uv if uv would be the ﬁrst edge added that crosses the cut, whereas
what we really need to know is that uv is the ﬁrst edge considered that crosses the cut. The latter proposition
follows, but only because we know inductively that the algorithm made the right choice at previous steps: the
ﬁrst edge considered that crosses the cut deﬁnitely belongs in M (by §3.3), so if that edge had been considered
earlier it would have been added then.

Θ(m log m)

CHAPTER §3. CLASSICAL METHODS

12

Ω(m log m) sort, which dominates the runtime. The other chore is to determine for arbitrary
edges uv whether there is already a u . . . v path in F . This can be done in a total of mα(m, n)
time using the Union-Find algorithm [16].
§3.9 Prim’s algorithm (Overview).
Prim’s algorithm is faster than Kruskal’s. Like Grow the tree
Kruskal’s, it starts with no edges in F and grows the forest one edge at a time. However, rather using the lightest
than growing many trees simultaneously, it devotes all its energy to growing a single tree T of edge possible
the forest F .
1.
2.
3.
4.
5.
6.

F := (V, ∅)
(* Kruskal’s also starts this way *)
u1 := an arbitrary “start vertex” in V
repeat n − 1 times
(* once for each remaining vertex *)

(* connect an isolated vertex to u1 ’s component of F , which we call T *)

e := the lightest edge of G leaving T (i.e., having just one endpoint in T )
F := F ∪ {e}

It is easy to see that F is now the MST. Every edge added belonged in F , because it was
lightest across the cut from T to V − T . Conversely, every edge belonging in F must have been
added, since we added n − 1 such edges.
§3.10 Prim’s algorithm (Development).
Each pass of Prim’s method needs to ﬁnd Use “buckets” to
the lightest edge leaving T . How to do this rapidly? It is ineﬃcient to search through all m reduce the set of
edges on each step. We could somehow sort the edges leaving T , or maintain them in a heap, candidate edges
but this would be as expensive as Kruskal’s algorithm.3
Prim’s solution is essentially to “bucket” the edges leaving T , according to their terminal
vertices. Whenever a new vertex u is added to T (via line 2 or 6 of §3.9), we consider each edge
uv ∈ Γ(u). If v ∈ T , we throw uv into a bucket ℓ[v] maintained at v, which keeps a running
minimum of all edges thrown into it.4 Thus, ℓ[v] remembers just the lightest edge that runs
from T to v. We may think of this as recording the “distance” from T to each vertex v.
Since every edge e leaving the new version of T (as augmented with u) has now been thrown
into some bucket ℓ[v], it is now easier to ﬁnd the lightest such edge. Each bucket retains only
the lightest edge thrown into it, so all we have to do is to ﬁnd the lightest bucket.
Obviously we do not want to search the buckets one by one on each iteration through §3.9.
Nor can we imitate Kruskal’s and sort the buckets by weight once and for all, since as T grows
and acquires new edges that leave it, some of the buckets may become lighter. The right solution
is to maintain the vertices V − T in a heap (see Appendix A), keyed on their “distance” w(ℓ[v])
from T . This heap has size O(n). On every iteration we Extract-Min the vertex u that is
closest to T , and add the edge ℓ[u] to F . We then bucket the new edges Γ(u) as discussed above;
when a light edge uv displaces the old contents of ℓ[v], so that the bucket becomes lighter, we
perform a Decrease-Key operation so that v’s key continues to reﬂect the decreased bucket
weight w(ℓ[v]).
§3.11 Analysis.
Prim’s algorithm removes n − 1 vertices from the heap and adds them
to the tree. Each removal takes an Extract-Min opration. Each time it removes a vertex,
it examines all edges incident on that vertex, so that it eventually examines each of the m
edges.5 Each of the m edges must be bucketed, requiring a weight comparison and perhaps a
3

The heap of edges leaving T might contain Θ(m) edges. For example, T might be a path containing half the
vertices, with every vertex in T connected to every vertex in V − T and no other connections.
4
One may regard a bucket as a simple type of collection that supports only the operations Insert and
Minimum, both in O(1) time. (See Appendix A for the obvious deﬁnitions of these operations.) However, I will
often speak of a bucket of edges ℓ[v] as containing just its lightest edge, or even identify it with that edge, since
that is how it is trivially implemented.
5
To be precise, each edge is examined exactly twice, when each of its endpoints is added to T , but bucketed
exactly once, when its ﬁrst endpoint is added to T .

Prim’s algorithm
is O(n log n + m)
with Fibonacci
heaps

CHAPTER §3. CLASSICAL METHODS

13

Decrease-Key operation.
If the heap of vertices is implemented as a standard binary heap of size O(n), ExtractMin and Decrease-Key each take time O(log n). Thus, the total time for the algorithm is
O(n log n + m log n) = O(m log n). Notice that Kruskal’s O(m log m) is asymptotically just as
good, since n − 1 ≤ m ≤ n2 implies log m = Θ(log n).
The bottleneck in this analysis is the Decrease-Key operation, which is performed O(m)
times, once per comparison. If we use a Fibonacci heap (explained in Appendix A), the cost
of the bottleneck Decrease-Key operation drops to O(1). Total time is then O(n log n + m).
This is considerably better than Kruskal’s O(m log m).
§3.12 Remark.
Indeed, if we know that the input will consist of dense graphs where Prim’s is optimal
m = Ω(n log n), then Prim’s algorithm using Fibonacci heaps uses time O(m). Since any for dense graphs
correct algorithm must examine the weights of all m edges in the worst case,6 this is in general
the best we can do (asymptotically speaking). Much of the work discussed below is therefore
about how to improve performance on sparse graphs.
§3.13 Discussion.
Why does Prim’s algorithm work better than Kruskal’s? Superﬁcially, Prim’s sorts only
the reason is that we are bucketing edges: rather than sort m edges (time O(m log m)), we are the MST edges
distributing them among n buckets (time O(m)) and then sorting the buckets (time O(n log n)).
But what feature of the problem lets us use these simple, constant-space buckets?
Each edge that is displaced from a bucket ℓ[v] is forgotten forever; it will never be added
to F . To see that such an edge in fact cannot be in the MST M , suppose that uv and u′ v are
both bucketed into ℓ[v], and uv (as the heavier edge) is discarded. The heaviest edge on the
cycle u . . . u′ v cannot be in M (where u . . . u′ denotes the path in T from u to u′ ), by §3.3. This
heaviest edge must be uv: for it cannot be u′ v, which is lighter than uv, nor can it be any edge
on u . . . u′ ⊆ T , since these edges are already known to be in M .
The crucial point is that we discard bad edges without sorting them with respect to each
other: once we know they’re not in M , we don’t care how heavy they are. Thus we spend
only O(1) time to discard a bad edge (via a bucket comparison and a fast implementation of
Decrease-Key), as compared to O(log n) time to identify a good edge (via Extract-Min).
By contrast, Kruskal’s algorithm spends O(log n) time on either a good or a bad edge. It puts
even the bad edges into the right order, only to discard them as soon as it sees them!
§3.14 Greedy discipline.
Let us close this section with an unoriginal observation that
will come in handy later. The algorithms of Kruskal and Prim are instances of what [7] calls
the “generalized greedy algorithm” for constructing MSTs.
The generalized greedy algorithm initializes a forest F to (V, ∅), and adds edges one at a
time until F is connected. Every edge that it adds is the lightest edge leaving some component
T of F . An algorithm that behaves in this way is said to observe “greedy discipline.”
It is easy to see that at the end of such an algorithm, F is the MST. Every edge e that we
add belongs in the MST, since it is the lightest edge across the cut from some T to V − T . So at
every stage, F is a spanning subgraph of the MST (hence a forest). Now when the loop ends,
F is connected, so it cannot be any proper spanning subgraph of the MST; rather it must be
the whole MST, as claimed.
In Kruskal’s algorithm, each edge uv added is the lightest one leaving u’s component, or
indeed any component. (All lighter edges of G have both endpoints within a single component.)
6

Suppose an algorithm fails to examine some edge e of a 2-connected graph. The algorithm cannot correctly
tell whether e belongs in the MST or not without looking at its weight. If e happens to be the lightest edge,
then Kruskal’s algorithm (hence any correct algorithm) would put it in the MST. If e happens to be the heaviest,
than Kruskal’s would exclude it, since the other edges are suﬃcient to connect the graph and will be considered
ﬁrst.

The key idea
shared by
Kruskal’s and
Prim’s

CHAPTER §3. CLASSICAL METHODS

14

In Prim’s algorithm, an edge is added only if it is the lightest one leaving the the start vertex’s
component. So both algorithms observe “greedy discipline.”

Chapter §4

Bottom-up clustering and
Boruvka’s algorithm
˙
The remaining algorithms discussed in this paper depend on a recursive grouping of the graph
vertices V into successively larger “vertex clusters,” each of which induces a subtree of the
minimum spanning tree M . In this section I will describe this strategy, and at the risk of some
initial tedium, introduce a set of notations and implementational data structures that will serve
for the rest of the paper.
As a ﬁrst example of these data structures, I will also discuss a simple but less familiar
MST algorithm due to Boruvka [2]. In Boruvka’s algorithm, each vertex of the graph colors its
˙
˙
lightest incident edge blue; then the blue edges are contracted simultaneously, and the process
is repeated until the graph has shrunk down to a single vertex. The full set of blue edges then
constitutes the MST. Both King [9] and Karger et al. [8] make use of this algorithm.

The generalized bottom-up method
§4.1 Strategy.
To understand the generalized bottom-up strategy for constructing an Allow graph
MST, consider that the generalized greedy algorithm (§3.14) can be modiﬁed to incorporate contraction during
contraction. Recall that the algorithm continually adds edges to a growing forest F ⊆ G. We greedy algorithms
observe that it is free to pause at any time and contract G (and simultaneously F ⊆ G) across
any of the edges already in F . So the modiﬁed algorithm intersperses edge contractions with the
edge additions. Can the contractions aﬀect the algorithm’s behavior? No; they merely shrink
components without changing the edges incident on them, so they do not aﬀect whether an edge
is the lightest edge leaving a compoment (i.e., whether it can be selected by the algorithm).
We will be working with a generalized bottom-up strategy, which incorporates contraction
in a particular way:
1.
2.
3.
4.
5.

G := (V, E)
F := (V, ∅)
until F is connected
add some edges to F , observing greedy discipline (§3.14)
contract F and G across all edges in F

(* a “pass” of the algorithm *)

F and G are eventually reduced to a single vertex. We must keep a separate list of all edges
added to F ; when the algorithm ends, the MST is speciﬁed by this list (not shown in the
pseudocode above).
§4.2 Discussion.
Why is such a strategy useful? Following each pass through the main The graph shrinks
loop, the contraction step shrinks every tree in F down to a single vertex, in both F and G. from pass to pass

15

˙
CHAPTER §4. BOTTOM-UP CLUSTERING AND BORUVKA’S ALGORITHM

16

The next pass therefore starts anew with F as an empty graph on G’s vertices. The advantage
is that now G is smaller, so that the this next pass can run faster.
In what sense is G smaller? It certainly has fewer vertices than before. It may also have
substantially fewer edges, if the contraction operation of line 5 is implemented (§4.12) so as
to immediately or lazily eliminate multiple edges (2-cycles) and/or self-loops (1-cycles), which
would otherwise arise from contracting edges on longer cycles in G. (An implementation that
eliminates multiple edges should keep only the lightest edge between a given pair of vertices,
since the Cycle Property (§3.3) excludes the others from the MST; if not, G becomes a multigraph.)
§4.3 Notation. For some of the methods discussed in this paper, we must keep track of
the F and G graphs constructed on each pass through the algorithm of §4.1. Formally, we will
construct a sequence of graphs G0 , F0 ; G1 , F1 ; . . . GN , FN . G0 is the input graph G. Given Gi ,
we choose Fi (on pass i of the algorithm, counting from 0) to be some acyclic subgraph of Gi ,
containing only edges of the MST. We then construct Gi+1 as the contraction of Gi over all the
edges that appear in Fi .
Let ni and mi be the number of vertices and edges, respectively, in Gi . Because Fi ⊆ Gi ,
these two graphs share a vertex set of size ni , which we call Vi . Suppose i ≥ 1; then these
vertices correspond 1-1 to the trees in the forest Fi−1 , of which they are contracted versions.
We describe this correspondence using functions C and P (for “children” and “parent”). For
v ∈ Vi , let C(ˆ) ⊆ Vi−1 denote the vertex set of the corresponding tree in Fi−1 . Conversely, for
ˆ
v
each v in that tree, let P (v) denote v .
ˆ
A little more notation will be helpful. We deﬁne general ancestors via P 0 (x) = x, P i+1 (x) =
P (P i (x)). We also deﬁne M eet(x, y) to be the smallest i such that P i (x) = P i (y). Finally, if v
ˆ
is a vertex of Fi , we write C ∞ (ˆ) for {v ∈ V0 : P i (v) = v }; this is the full set of vertices of the
v
ˆ
original graph that have been successively merged to become v .
ˆ
§4.4

Implementation.

We make use of the following simple data structures.

edges All the edges of G = (V, E) are stored in a set E; each edge records its weight and
(pointers to) its endpoints in V .
vertices The vertex sets V0 = V, V1 , V2 , . . . are stored in disjoint sets. Each vertex v ∈ Vi stores
pointers to its adjacency lists ΓGi (v) and ΓFi (v). For some algorithms, we will also want
v to store P (v) and C(v); these are respectively a pointer into Vi+1 and a circular list of
pointers into Vi−1 .
adjacency lists An adjacency list Γ(v) is a linked list of pointers to weighted edges in E. Some
algorithms require this list to be doubly linked to support O(1) deletion of edges. Some
also require it to be circular, so that when we merge two vertices, we can (destructively)
concatenate their adjacency lists in O(1) time.
Notice that the contracted graphs Gi all reuse the same edges E (on their adjacency lists).
Thus, when we select an edge from the contracted graph Gi to add to the MST, we know which
original vertices of G it connects and hence where it belongs in the MST. The subgraphs Fi
likewise reuse the edges E.
We now ﬂesh out the generalized bottom-up algorithm of §4.1:
1.
2.
3.
4.
5.
6.

V0 := V
(* with empty adjacency lists *)
for each edge uv ∈ E
(* create G0 *)
add uv to ΓG0 (u) and ΓG0 (v)
i := 0
while |Vi | > 1
(* each iteration is called a “pass” *)
(* create Fi from Gi *)

Data structures
and pseudocode

˙
CHAPTER §4. BOTTOM-UP CLUSTERING AND BORUVKA’S ALGORITHM
7.
8.
9.
10.
11.
12.
13.
14.
15.
16.
17.

17

for various edges uv chosen from Gi , each choice observing greedy discipline w.r.t. Fi ’s components
add uv to the list of MST edges that we will return
(* see §4.6 for discussion *)
add uv to ΓFi (u) and ΓFi (v)
(* create Gi+1 from Gi , Fi *)

Vi+1 := ∅
for each component tree T of Fi
(* see §4.5 for discussion *)
add a new vertex v to Vi+1 , with empty adjacency lists
ˆ
C(ˆ) := ∅
v
for each v ∈ T
v
v
P (v) := v ; C(ˆ) := C(ˆ) ∪ {v}; ΓGi+1 (ˆ) := ΓGi+1 (ˆ) ∪ ΓGi (v)
ˆ
v
v
i := i + 1

Notice that this basic version of the algorithm does not eliminate multiple edges or self-loops (cf.
§4.12), so the Gi are in general multigraphs. The most important diﬀerence among instances
of this generalized algorithm is their realization of line 7.
§4.5 Implementation detail.
The algorithm of §4.4 needs to enumerate the vertices (in Finding
Fi ) of the components T of Fi , one component at a time (lines 12, 15). This can always be done components
in O(1) time per vertex, using the usual depth-ﬁrst search algorithm for ﬁnding components
(simpliﬁed by the acyclicity of Fi ). For certain algorithms such as [5] (see §5), where components
rarely merge, it is simpler and just as fast to keep track of the vertex list of each component
as the components are constructed; that is, the work of line 16 can be moved into the loop at
lines 7–9.1
§4.6 Implementation detail.
Because edges are reused, the step of §4.4 where we add Finding contracted
uv to ΓFi (u) and ΓFi (v) (line 9) is trickier than it might appear. The edge uv is taken from endpoints
an adjacency list, so it is an element of E. Since such an edge records only its endpoints from
the original graph, we cannot tell that it is the edge uv in Gi ; all we know is that it is the edge
u0 v0 (say) in G0 . We must rapidly determine u and v so that we can add the edge to their
adjacency lists and subsequently contract it. Three solutions are available:
O(mi ) overhead per pass, O(1) per edge lookup Assume that we ﬁnd edges only on adjacency lists. Since uv is taken from u’s adjacency list ΓGi (u), we simply arrange to have
stored v on the list as well. Thus, u’s adjacency list has entries of the form u0 v0 , v ,
where u0 v0 ∈ E and v = P i (v0 ). This tells us in O(1) time that the endpoints of the edge
are u and v.
Under this representation, once we have created the adjacency lists of Gi+1 by concatenating adjacency lists of Gi , we must walk them and update each copied entry of the form
u0 v0 , v to u0 v0 , P (v) . This takes time O(mi ).
O(n) overhead per pass, O(1) per edge lookup We will ﬁnd v directly from v0 each time
we consult an edge u0 v0 . Note that v is uniquely determined as P i (v0 ). During pass i,
we have each v0 ∈ V0 store P i (v0 ) ∈ Vi (which represents v0 ’s contracted component), so
that we can look up P i (v0 ) in O(1) time. At the end of pass i, we spend O(n) time to
replace each stored P i (v0 ) with its new parent P (P i (v0 )) = P i+1 (v0 ).
no overhead per pass, O(α(m, n)) per edge lookup (assuming ≥ m lookups) Again,
we will ﬁnd v directly from v0 each time we consult an edge u0 v0 . We maintain a fast
1

In [5], components are grown one vertex at a time, so we can easily maintain the two-way correspondence
(via functions P , C) between components and their vertices. We do have to handle merger of components, since
the algorithm does allow a component to grow until it “bumps into” another component, and in this case, the
vertices of the former must be relabeled as being in the latter. However, on a given pass of [5], each vertex is
labeled at most once and relabeled at most once, so the time is still O(1) per vertex.

˙
CHAPTER §4. BOTTOM-UP CLUSTERING AND BORUVKA’S ALGORITHM

18

disjoint-set structure [16] such that during pass i, we can do so in amortized O(α(m, n))
time.
The disjoint sets during pass i are the equivalence classes of V under P i ; that is, two
original vertices are in the same set just if they have been contracted together before pass
i. The operation Find(v0 ) returns a canonical element v0 of v0 ’s equivalence class. At
this element we store the ancestor P i (v0 ), which equals P i (v0 ) as desired.
To maintain these properties, we must update the equivalence classes and the stored
ancestors at the end of pass i. We add some instructions to §4.4’s loop (lines 12–16) over
components T :
1.
2.
3.
4.
5.
6.
7.
8.

for each component tree T of Fi
.
.
.
for each edge of T
(* we can discover T ’s edges in parallel with its vertices in line 15 of §4.4 *)
Union(u0 , v0 ) where u0 , v0 ∈ V are the endpoints of the edge
.
.
.
v0 :=Find(v0 ) where v0 is an arbitrary vertex of T
(* the new canonical vertex for C ∞ (ˆ) *)
v
i+1
∼ contracted T ) was added to Fi+1 in §4.4 line 13 *)
let v0 store v as the value of P (v0 )
ˆ
(* v (=
ˆ
.
.
.

Notice that with this approach, once we have ﬁnished pass i, we are able to compute
P i+1 , but we can no longer compute P i , because of the set unions. The other approaches
can be implemented non-destructively.
As for the complexity, notice that we will perform a total of n Make-Set operations (that
is, the data structure maintains a partition of the n vertices), n − 1 Union operations
corresponding to contractions across the MST edges as we discover them, and some number M ≥ m of Find operations. Thus we have 2n + M − 1 operations in toto, for which
the total time required under [16] is O((2n + M − 1)α(2n + M − 1, n)). Weakening this
bound to get a more convenient expression, observe that (2n + M − 1)α(2n + M − 1, n) <
3M α(2n + M − 1, n) < 3M α(m, n). Hence we take amortized O(α(m, n)) time per Find
operation.

Boruvka’s algorithm
˙
§4.7 Preliminary version.
Although I have not read the original Czech description Grow trees from
of Boruvka’s algorithm [2], it is fairly clear from some remarks in [8] how it must work. It every vertex at
˙
makes a good, simple example of how bottom-up clustering can help the performance of an once
MST algorithm. To see this point more clearly, let us consider two versions of the algorithm: a
preliminary version without contraction, and the real version, which uses contraction.
Without contraction, the algorithm runs as follows. We initialize our growing forest F to
(V, ∅) as usual. On each pass, every tree T in F locates the lightest edge of G leaving T and
colors it blue. (Some edges may be colored by two trees.) Then all these blue edges are added
to F , following which we uncolor them in G and proceed to the next pass. We repeat this
procedure until F is connected.
§4.8 Boruvka’s algorithm.
Like Kruskal’s and Prim’s algorithms, §4.7 is essentially Contract trees
an example of the generalized greedy algorithm (§3.14), and therefore correctly ﬁnds F . To see after each step
this, imagine that we add each batch of blue edges to F in decreasing order of weight, rather
than simultaneously. (This cannot aﬀect correctness.) Then each addition observes greedy
discipline: if edge e was originally colored blue by component T , then at the time e is added it
is the lightest edge leaving T ’s component. Why? T ’s component results from the merger of T

˙
CHAPTER §4. BOTTOM-UP CLUSTERING AND BORUVKA’S ALGORITHM

19

with other trees whose blue edges were added before e. As e is the lightest blue edge leaving
any of these trees, it is in fact the lightest edge leaving any of them.
We may freely add contraction to any generalized greedy algorithm (see §4.1), so it does
not damage the correctness of the algorithm to contract the blue edges at the end of each pass.
This yields Boruvka’s algorithm.
˙
§4.9 Implementation. Since Boruvka’s algorithm is an instance of the generalized bottom˙
up method of §4.1, we can implement it exactly as in §4.4. We must specify which edges uv to
add on pass i (line 7): the blue edges, found by iterating over all vertices u ∈ Vi of the contracted
graph and, for each, iterating over its adjacent edges uv ∈ ΓGi (u) to ﬁnd the lightest one. This
takes time O(mi ). We are careful to mark edges so that we do not add any edge twice (i.e., as
uv and vu both), and not to add any self-loops (i.e., uu) that might appear in Gi .
We must also specify which bookkeeping method to use in §4.6: the O(mi )-per-pass method.
The cost is that after pass i has iterated over all the edges of Gi , we must iterate over them
once more, which obviously does not aﬀect the O(mi ) asymptotic analysis.
§4.10 Analysis.
It is easy to see that we need O(log n) passes: each pass i at least Boruvka’s is
˙
halves the number of vertices, by merging every vertex of Fi with at least one other vertex. O(min(m log n, n2 ))
(Or equivalently, without contraction, each pass attaches every component of F to at least one
other component, halving the number of components.)
Each pass takes O(m) time to iterate through all the edges of Gi , so the algorithm runs
in O(m log n) steps in toto. However, we can say more strongly that pass i takes O(mi ) time.
This is a stronger bound than O(m) if, when we form Gi from Gi−1 , we also take the trouble
to eliminate multiple edges and self-loops in Gi . (For a fast implementation see §4.12 below.)
Then mi may be less than m; in particular it is bounded by n2 . Since ni ≤ n/(2i ), the total time
i
spent iterating through edges, summed over all i, is only O(n2 + (n/2)2 + (n/4)2 + · · ·) = O(n2 ).
So Boruvka’s algorithm runs in O(min(m log n, n2 )), as [8] mentions.
˙
§4.11 Improved analysis.
The stated O(min(m log n, n2 )) bound (§4.10) beats the Boruvka’s is O(m ·
˙
O(m log n) bound achieved either by Kruskal’s or by a binary-heap implementation of Prim’s. (1 + log(n2 /m)))
However, it is an asymptotic improvement only for very dense graphs, where m = ω(n2 / log n).
We can tighten the analysis to see that Boruvka’s algorithm gives an improvement even for
˙
slightly sparser graphs. Each pass i takes time O(mi ) edges, where mi is bounded both by m
(“intrinsic bound”) and by n2 (“extrinsic bound”). Rather than consider one type of bound
i
throughout, as in §4.10, we consider on each pass i whichever bound is smaller: i.e., the intrinsic
bound until we are sure that n2 ≤ m, and the extrinsic bound thereafter. Since ni ≤ n/2i ,
i
1
˙
the crossover point is pass ˆ = 2 log(n2 /m). Boruvka’s algorithm requires time O(m) for each
ı
of the ﬁrst ˆ passes, and total time O( i≥ˆ n2 ) = O(m + m/4 + m/16 + · · ·) = O(m) for all
ı
ı i
subsequent passes, for a total time cost of O(m(1 + ˆ)) = O(m · (1 + log(n2 /m))).
ı
This bound on Boruvka’s algorithm is an improvement over naive Prim’s (that is, it’s
˙
o(m log n)) just if m = ω(n2−ǫ ) for all ǫ > 0. For example, it is an improvement if m =
cn2 /(log n)k for some c, k.
§4.12 Implementation detail.
For the analyses of §4.10–§4.11 to go through, pass i Eliminating
2 edges in G . So we need to have eliminated redundant edges at the redundant edges
must consider only mi ≤ ni
i
end of pass i − 1 (see §4.2). Speciﬁcally, at the end of pass i − 1 we have in hand a multigraph after contraction
version of Gi , with ni vertices and mi−1 edges including multiple edges and self-loops. After
redundancy elimination, only mi ≤ n2 edges will remain in Gi , and pass i may begin.
i
One solution is to scan through the mi−1 edges, discarding self-loops and placing the other

˙
CHAPTER §4. BOTTOM-UP CLUSTERING AND BORUVKA’S ALGORITHM

20

edges in bins2 according to their endpoints in Gi . That is, multiple edges connecting u, v ∈ Gi
will be placed in the same bin, which is keyed on the pair u, v , and only the lightest such edge
kept. Unfortunately, this may take more than just the O(mi−1 ) steps for binning the edges. We
also need O(n2 ) time to initialize the n2 bins beforehand, not to mention O(n2 ) time afterwards
i
i
i
to scan the bins and create from them new, pruned versions of the adjacency lists for Gi .3
A nice ﬁx (perhaps the one used by [2], but I don’t know, as I can think of others) is to use
a two-pass stable sort. Basically, we represent each edge as a two-digit number in base n, and
radix-sort the edges. The passes of a radix sort are bucket sorts. Unlike the one-pass bucket
sort above, this solution is dominated by only the O(mi−1 ) time to iterate through the mi−1
edges of the multigraph. This is possible because the number of bins is now ni rather than n2 ,
i
and the overhead is proportional to the number of bins.
Each bin stores a linked list of all edges placed in it, with the most recently added edge at
the head of the list. We use two arrays of bins, B and B ′ . Note that we can ﬁnd an edge’s
endpoints in Gi in O(1) time, because of the bookkeeping choice made in §4.9.
1.
2.
3.
4.
5.
6.
7.
8.
9.
10.
11.
12.
13.
14.

for each edge e of Gi
let u, v be the endpoints in Gi of e, where v has the higher vertex number
add e to the bin B[v]
clear the adjacency lists of Gi
for v ∈ Vi , from high-numbered to low-numbered
for e ∈ B[v]
let u be the other endpoint of e
add e to the bin B ′ [u]
for u ∈ Vi , from low-numbered to high-numbered
for e ∈ B ′ [u]
(* this enumerates edges e of Gi in lexicographic order by their endpoints! *)

(* each group of multiple edges is enumerated together—now easy to identify the lightest edge in the group *)

if e is the lightest edge between its endpoints, and not a self-loop
add e to the adjacency lists of its endpoints in Gi

§4.13 Discussion.
Why is Boruvka’s algorithm asymptotically faster than Kruskal’s or Boruvka’s sorts
˙
˙
naive Prim’s? It is faster only if we eliminate multiple edges, so the speedup evidently results only the MST
from the fact that the size of the graph shrinks from pass to pass (see §4.2). More deeply, the edges
point is that elimination of multiple edges is a fast way to rule edges out of the graph. As we
just saw (§4.12), Boruvka’s algorithm takes only O(1) time per edge to eliminate all but the
˙
lightest edge between each pair of vertices of Gi . When deleting the other edges, we do not
waste time sorting them with respect to each other, which would take O(log mi ) per edge. This
argument should sound familiar: we saw in §3.13 that it is for just the same reason that a good
implementation of Prim’s algorithm (with Fibonacci heaps) will outperform Kruskal’s!
§4.14 Discussion.
Given the above remark, why is Boruvka’s algorithm not quite as Boruvka’s scans
˙
˙
good as Prim’s with Fibonacci heaps? For very dense or very sparse graphs it is as good: the same edges on
if m = Ω(n2 ), both algorithms take only O(m) time, and if m = O(n), both algorithms every pass
take O(n log n). A moderately sparse graph does eventually become dense during Boruvka’s
˙
2

Usually called buckets, since this is a bucket sort. I use the term “bins” to avoid confusion with the buckets
of §3.10, which could hold only one element.
3
Actually, it happens that §4.11’s bounds for Boruvka’s algorithm are not irrevocably damaged by the O(n2 )
˙
i
overhead. The only reason we care about eliminating edges is to achieve an O(n2 ) bound on the later passes. We
i
can simply omit the elimination step for the ﬁrst ˆ− 1 passes. Just as claimed in §4.11, passes 0 through ˆ− 1 take
ı
ı
time O(m) apiece, while passes i ≥ ˆ—since multiple edges have been eliminated prior to those passes—take time
ı
O(n2 ) apiece. The very ﬁrst elimination step, at the end of pass i = ˆ − 1, takes time O(n2 + m) = O(m), and
ı
i
i
the elimination step at the end of each subsequent pass i ≥ ˆ takes time O(n2 + mi ) = O(n2 ). These additions
ı
i+1
i
only multiply the work for each pass by a constant, so they do not aﬀect the asymptotic analysis. Nonetheless,
for thinking about MST algorithms, it is helpful to know that redundant edges can if necessary be eliminated
without this O(n2 ) overhead, as shown below.
i

˙
CHAPTER §4. BOTTOM-UP CLUSTERING AND BORUVKA’S ALGORITHM

21

algorithm—by pass ˆ or so, it has lost enough vertices that m = Ω(n2 ), and the rest of the
ı
algorithm is therefore fast. But the early passes of Boruvka’s on such a graph do not do as
˙
much work: at this stage there are not many multiple edges to eliminate, and indeed our analysis
§4.11 assumes pessimistically that each of these passes takes fully Θ(m) steps. The extra cost
of Boruvka’s algorithm is due to reconsidering nearly all the same edges on each of these early
˙
passes; nearly m edges are considered on each pass, in order to include a paltry ni /2 to ni of
them in the tree and perhaps exclude a few multiple edges.
§4.15 Discussion.
Prim’s algorithm does well without using contraction at all. What is
the beneﬁt of contraction in Boruvka’s algorithm? Without contraction (§4.7), we would simply
˙
add the lightest edge incident on each uncontracted tree in the forest F . We would still have
to keep track of the correspondence between trees and the vertices in them: given a tree, we
need its vertices so that we can enumerate the edges incident on the tree, and given an edge,
we need to know which trees its endpoints are in, so that we can detect and delete within-tree
edges (corresponding to self-loops in the contracted graph) and multiple edges between a pair
of trees.
The main consequence of not contracting is that we never merge the adjacency lists of the
vertices in a tree. So to enumerate the edges incident on a tree, we must explicitly loop through
all the tree’s vertices and follow the adjacency list of each. To enumerate all edges incident on all
trees therefore takes time O(mi + n), rather than only time O(mi + ni ) = O(mi ) for the version
with contraction. The extra O(n) work on each pass leads to total extra work of O(n log n). It
so happens that this does not damage the asymptotic complexity of §4.10, nor (less obviously)
that of §4.11.4 So in the case of Boruvka’s algorithm, it turns out that contraction has only
˙
pedagogy to recommend it! This is also the case for the algorithms of Fredman & Tarjan
(§5), King (§6), and Frederickson (§8), although the latter two (like Boruvka’s algorithm) must
˙
eliminate multiple edges as if they had contracted. By contrast, the explicit merger of adjacency
lists under contraction is crucial in the algorithms of Gabow et al. (see §5.18) and Karger et
al. (see §7.16).

4
§4.10 gives O(min(m log n, n2 )), and n log n is less than both terms. §4.11 gives O(m · (1 + log(n2 /m))).
Rewrite m · (1 + log(n2 /m)) as cn · (1 + log n − log c), where we have chosen 1 ≤ c ≤ n such that m = cn.
This exceeds n log n for all these values of c: it is greater for c = 1, and increases with c up till c = n, since its
derivative w.r.t. c is n(log n − log c).

Explicit
contraction is
unnecessary for
Boruvka’s
˙

Chapter §5

Faster deterministic algorithms
In this section, we will consider the MST construction algorithms of Fredman & Tarjan [5] and
of Gabow, Galil, Spencer, & Tarjan [7]. These methods achieve performance that is remarkably
close to linear: O(mβ(m, n)) for [5], and O(m log β(m, n)) using an additional idea of [7]. I will
concentrate on showing how such good performance is achieved.
§5.1 The β function.
Let us take a moment to understand the asymptotic complexity.
β(m, n) is deﬁned to be min{i : log(i) n ≤ m/n}. This may be usefully regarded as log* n −
log*(m/n), which it approximates to within plus or minus 1. (For, once β(m, n) successive
logarithms of n have gotten it below m/n, an additional log*(m/n) logarithms will get it below
1, for a total of log* n logarithms.) Thus, β(m, n) is better than log* n, particularly for dense
graphs. However, it is not as good as the inverse Ackerman function α(m, n); nor is log β(m, n).
One might think that on dense graphs, O(mβ(m, n)) would be inferior to the Prim’s performance of O(n log n + m) = O(m), and indeed [5] and [3] leave one with this impression.
However, notice that the denser the graph, the smaller the value of β(m, n); for instance, for

22

The new
algorithms are
nearly linear, and
always better than
Prim’s

CHAPTER §5. FASTER DETERMINISTIC ALGORITHMS

23

m = Θ(n2 ) we have β(m, n) = O(1). It is possible to prove that mβ(m, n) = O(n log n + m).1
Hence [5] actually matches or improves on the asymptotic performance of Prim’s algorithm, for
all graph densities. And it is substantially faster for sparse graphs—those where n log n would
dominate m in the running time of Prim’s algorithm. [7] is better yet.
§5.2 Remark.
It is signiﬁcant that [5] breaks the O(n log n+m) barrier on some families of MST is not bound
graphs (although it was not the ﬁrst to do so). It means that the MST problem is not inherently by edge-sorting
bound by the time of edge-sorting. We saw in §3.13 that Prim’s algorithm manages to avoid
sorting the bad edges with respect to each other. However, it does sort up to Θ(n) of the good
edges, on the heap, as well as considering all O(m) edges, resulting in its O(n log n + m)-time
performance. Faster algorithms such as [5] and [7] cannot aﬀord to sort so many as Θ(n) of the
good edges.
While a decision-tree argument shows that comparison-sorting n numbers requires at least
log n! = Ω(n log n) comparisons on some input, because n! answers are possible before we have
done any comparisons, there is no such argument showing that MST construction requires that
many comparisons. An input graph of m edges has 2m subgraphs, some k ≤ 2m of which are
spanning trees. So all we can conclude is that some input requires at least ⌈log k⌉ comparisons,
where log k < m.
1

That is, there exist c > 0 and n0 such that for all connected graphs with more than n0 vertices, we have
mβ(m, n) ≤ c(n log n + m). Thus all but ﬁnitely many graphs satisfy this property; i.e., for any inﬁnite sequence
of distinct graphs, [5] eventually runs in time proportional to Fibonacci Prim or better.
I will show speciﬁcally that for all real n ≥ 16, mβ(m, n) ≤ n log n + m. Let d = m/n log n. The claim is now
that regardless of d,
m min{i : log(i) n ≤ d log n} ≤ m/d + m.
Equivalently, dividing by m,

min{i : log(i) n ≤ d log n} ≤ 1/d + 1.

Equivalently,

Equivalently, exponentiating each side,

log(⌊1/d⌋+1) n ≤ d log n.

Putting k = ⌊1/d⌋, it is enough to show that

log(⌊1/d⌋) n ≤ nd .

log(k) n ≤ n1/(k+1) (again, just for n ≥ 16)
since 1/(k + 1) < d. We may see k = 0, 1, 2 directly and k > 2 by induction.
√
The k = 0 case just states that n ≤ n. The k = 1 case states that log n ≤ n, which holds for n ≥ 16 by
√
checking derivatives. The k = 2 case states that log log n ≤ 3 n: claim that this holds not only for n ≥ 16 but
(crucially) wherever these functions are deﬁned. We take derivatives to see that the RHS grows faster just when
√
√
3
n log n > 3/(ln 2)2 . Now 3 n log n is an increasing function, which passes the constant 3/(ln 2)2 at some n1 < 9,
since it is already greater when n = 9. Now observe that
√
(log log n1 )(log n1 ) < (log log 9)(log 9) < 3/(ln 2)2 (by numerical check) = 3 n1 log n1
√
So log log n < 3 n for n = n1 . By the deﬁnition of n1 , the RHS grows more quickly than the LHS as n increases
above n1 , and falls more slowly than the LHS as n decreases below n1 , so the RHS is greater everywhere as
claimed.
Finally, to see that log(k) n ≤ n1/(k+1) for k > 2, we write
log(k) n

=
<
=

log log log(k−2) n
3

√
3

log(k−2) n (by the k = 2 case, which holds everywhere)
n1/(k−1) provided that n ≥ 16 (by induction, and monotonicity of

1/(3k−3)

=

n

<

n1/(k+1) (since k > 2).

√
3

)

CHAPTER §5. FASTER DETERMINISTIC ALGORITHMS

24

Fredman & Tarjan’s algorithm
§5.3 Motivation.
Recall from §3.11 that Prim’s algorithm takes n + 2m steps simply to Keep the heap
enumerate neighbors, and there are also n Extract-Min and m Decrease-Key operations. small for speed
Thus, a binary-heap implementation requires time O(n log n + m log n), and a Fibonacci-heap
implementation requires time O(n log n + m). Having achieved the latter performance by introducing Fibonacci heaps in [5], Fredman & Tarjan proceed by attacking the other log n factor,
in order to achieve close to O(m) performance.
The log n factor arises as the penalty for a large heap: as the Prim tree T grows, it may
accumulate up to O(n) neighboring vertices that the heap must keep track of. If we were
able to keep the heap small—no more than k of the n vertices on the heap at any time—then
each Extract-Min would be faster, and we would have reduced the time requirements to
O(n log k + m).
We might try to arrange for a constant size heap, say a heap of size k = 100. If we could
do this we would certainly have O(m) performance. In point of fact, for dense graphs—which
have fewer vertices for the number of edges—we could let k be quite a bit larger: we only have
to extract n vertices from the heap, and if n is small we can aﬀord to take more time per
extraction. In particular, we may put k = 22m/n and still have O(n log k + m) be O(m). Note
that k is deﬁned here as 2 to the average vertex degree of G; this is large for dense graphs.
§5.4 Development.
How do we keep the Prim’s heap small? The heap maintains just
those vertices v that are adjacent to the growing tree T , keyed by their distance from T (i.e,.
the weight of the lightest T –v edge, ℓ[v]).2 Thus, the trick is to ensure that T does not acquire
too many neighbors. We will simply stop growing T as soon as it has more than k neighbors,
so that we never Extract-Min a vertex from a heap that has grown to size > k.
Our ﬁrst tree T might actually grow to span the whole graph; then we are done. But it
might happen that owing to heap overﬂow, we have to stop growing T prematurely. In that
case, we settle for building a forest. We choose a new start vertex arbitrarily from V − T , and
grow a companion tree from it by the same procedure, starting afresh with an empty heap and
again stopping when the heap gets too large. We repeat this procedure until every vertex of the
graph has been sucked, Prim-like, into some large but not unmanageable tree—that is, until we
have covered the graph with a spanning forest F0 in which (speaking roughly) every tree has
just over k neighbors.

Quit and start an
additional tree if
the heap gets too
big

§5.5 Clarification.
In the above section, I say “speaking roughly” because the picture Trees may grow
is complicated slightly by the possibility of tree merging. As we grow a given tree T ′ , it might into each other
acquire as neighbors not only isolated vertices of F0 , but also vertices in previously-built trees
of F0 . (Such vertices may be identiﬁed by their non-empty ΓF0 lists.) Eventually one of these
neighbors—let us say a vertex in T ′′ —may be pulled oﬀ the heap and connected to T ′ , thereby
linking all of T ′′ to T ′ . If so, we stop growing T ′ immediately, just as if the heap had overﬂowed
(indeed, T ′ now has > k neighbors), and start growing a new tree.
¯
How big is the resulting forest? Every vertex of G ends up in a tree of the form T =
′ ∪ T ′′ ∪ T ′′′ ∪ · · ·, where T ′ has more than k neighbors in G. (Some of these neighbors may be
T
¯
in T ′′ and hence in the combined tree!) It follows that every tree T in F “touches” more than
k edges in the sense that it contains at least one endpoint of each. But there are only m graph
2

So v is ﬁrst added to the heap only when it becomes adjacent to T . That is, if while growing T we discover
an edge from T to a vertex v that is not yet on the heap, we set ℓ[v] to that edge and insert v on the heap with
key w(ℓ[v]). The heap starts out empty.
This discipline diﬀers slightly from the traditional implementation of Prim’s algorithm in that it omits vertices
of inﬁnite key from the heap. In the traditional implementation, all vertices in V − T are maintained on the
heap, but the key of v is ∞ until the discovery of an edge from T to v causes it to decrease. The discussion of
Prim’s algorithm in §3.10 was deliberately vague on this distinction.

CHAPTER §5. FASTER DETERMINISTIC ALGORITHMS

25

edges to go around. Each graph edge can be touched by at most two trees. It follows that F
contains 2m/k or fewer trees.
The length of this pass is still O(m) as intended. Although we have modiﬁed Prim’s approach
to grow multiple trees instead of just one, each vertex of G is still extracted from a heap exactly
once,3 at O(log k) cost, and each edge of G is still considered exactly twice and bucketed exactly
once, at O(1) cost. So the analysis of §5.3 is unaﬀected: the pass takes time O(n log k + m) =
O(m).
§5.6 Development.
This completes one pass of the generalized bottom-up algorithm of Contract all the
§4.1. As §4.1 prescribes, we now contract G over all the trees of F0 , giving G1 , and perform a trees and repeat
new pass to link together these contracted trees as far as possible given the limitation on heap
size. We repeat until the graph is ﬁnally connected.
Since every added edge was chosen to be the lightest edge incident on the currently growing
tree, §3.14 and §4.1 show that the algorithm correctly ﬁnds the MST.
On each pass there are fewer vertices in the graph. Since we have fewer vertices to extract,
we can take longer to extract them. Indeed, we can dramatically increase the heap bound k
from pass to pass, and take only O(m) steps per pass. Since the goal is to keep the ExtractMin time of O(n log k) within O(m), even a small reduction in n allows a correspondingly
exponential increase in k, which will lead to an equally large reduction in n on the next pass.
The eﬀect snowballs so that the algorithm ﬁnishes rapidly.
Speciﬁcally, on pass i, we take k = ki = 22m/ni , so that the pass takes time O(ni log ki +m) =
O(m). (Note that we do not bother to eliminate multiple edges; §5.9 explains why.)
§5.7 Analysis.
On pass i, we have ki = 22m/ni . We saw in §5.5 that this heap bound The graph shrinks
suﬃces to cover the graph with, if not a single tree, then at most 2m/ki trees. So ni+1 ≤ 2m/ki , extremely fast
and then ki+1 = 22m/ni+1 ≥ 2ki . Thus k increases tetrationally while n decreases tetrationally:4
it does not take long before ki ≥ ni , at which point Prim’s algorithm can bring all ni vertices
into one tree without overﬂowing the ki -sized heap. How long does it take? Observe that
ki ≥ n (≥ ni ) iﬀ log(i) ki ≥ log(i) n iﬀ 2m/n ≥ log(i) n. Now i = β(m, n) is minimal such that
(2m/n >) m/n ≥ log(i) n, so we require at most β(m, n) passes of time O(m) each.
As noted in §5.3, k is deﬁned as 2 to the average vertex degree, which keeps shrinking as
the multigraph contracts. Thus even if the graph starts out as the kind of sparse graph that
Prim’s algorithm ﬁnds diﬃcult, it rapidly becomes dense.
§5.8 Implementation. We can implement the method as an instance of the general bottomup algorithm of §4.4. For contraction, we use the O(n)-per-pass or O(mi )-per-pass method of
§4.6, since neither endangers our time bound of O(m) per pass. Notice that the Union-Find
method, by contrast, would add an extra α(m, n) factor to each pass and to the overall analysis.
§5.9 Remark.
Why don’t we bother to eliminate redundant edges that result from con- No need to
traction? Fredman & Tarjan actually specify that we should eliminate edges at the end of each eliminate
pass, using an O(m) radix sort with two passes as in §4.12. But nothing at all in their paper redundant edges
hinges on reducing the number of edges. In particular, their analysis allows each pass to take
fully O(m) steps, even if mi is much less than m.
3
This is not quite precise: in truth some vertices are never extracted and some are extracted twice. A new
tree’s start vertex is chosen without an extraction, while the last vertex we add to it might be have been previously
extracted as well (if it happens to fall in another tree). These two eﬀects cancel each other out in each tree.
4
Tetration is the fourth in the sequence of operations addition, multiplication, exponentiation, tetration, where
aa
a opi+1 b = a opi (a opi (a opi · · ·)). Thus 4 a (“a tetrated to the fourth power”) is aa . The inverse operation
b times
of b a is log* to the base a (usually 2).

CHAPTER §5. FASTER DETERMINISTIC ALGORITHMS

26

Would a policy of edge elimination let us improve on Fredman & Tarjan’s O(mβ(m, n))
analysis? Edge elimination means that we only require O(mi ) time per pass (provided that we
reduce ki from 22m/ni to 22mi /ni ). Unfortunately mi does not decrease quickly enough for this
to help. One might expect that the rapidly decreasing n2 would be a useful upper bound on
i
mi , as it is for Boruvka’s algorithm (§4.10); but once n2 becomes a better bound than m is,
˙
i
i.e., m < n2 , a single O(m) Fredman-Tarjan-style pass is enough to ﬁnish the job anyway.
i
Not only does n2 fail to be a useful upper bound on mi , but mi genuinely need not decrease
i
very fast. We can see this by constructing an inﬁnite family of sparse graphs Gs that require
Θ(log* n) passes of fully Θ(m) steps each. Let s be a positive integer. Gs consists of s light
paths, of s − 1 vertices each, connected to each other by single heavy edges in a K s topology.5
Now we have s(s − 1) vertices, s(s − 2) light edges within paths and s(s − 1)/2 heavy edges
between paths. The light within-path edges are contracted ﬁrst, taking several passes;6 this
leaves us with K s , which we contract on the ﬁnal pass. Since about 1/3 of the edges—the
between-path edges—survive until the ﬁnal pass, every pass takes Θ(m) time. Notice that no
pass creates any multiple edges or self-loops, so edge elimination happens to make no diﬀerence
here.

The packet algorithm of Gabow et al.
§5.10 Motivation.
In all versions of Prim’s algorithm, we incur O(m) Decrease-Key
operations as we bucket all the neighbors of each newly added vertex. For Fredman & Tarjan,
who have carefully controlled the diﬃculty of the harder but less numerous Extract-Min
operations, this Decrease-Key cost, repeated on every pass, has become the performance
bottleneck.
To attack this cost, which is already down to O(1) per call thanks to Fibonacci heaps, we
must reduce the number of calls. Observe that Fredman & Tarjan repeatedly sift through the
same graph edges from pass to pass. Every surviving edge is reconsidered on every pass, so
that all edges that survive till the last pass are seen up to β(m, n) times. There may be Θ(m)
of these in the worst case (§5.9), so the cost is not trivial. Yet we are always doing the same
thing with those edges: looking at their weights so that we can ﬁnd a light one. Perhaps we
can retain some order information from pass to pass, so that we do not have to keep looking at
the same edges.
Let us consider more carefully why an edge would be reconsidered—that is, why O(1) time
per edge (for a total of O(m)) is not enough for this algorithm to decide which edges are in the
tree. Suppose we are growing a tree T during pass i. Throwing s edges into the bucket of a
particular vertex v requires O(1) time per edge. This lets us permanently eliminate s − 1 edges
(all but the lightest), having spent O(1) time on each. But we have also invested O(1) time on
the surviving edge in the bucket. If v happens not to be added to T on this pass, because T
stops growing ﬁrst, the bucket is callously forgotten and that O(1) eﬀort is wasted: we still do
not know whether the surviving edge will be in the tree or not. We will have to look at it again
next pass.
From this point of view, it is better to wait until later to look at edges. Wasted eﬀort is at
most O(1) per bucket. On later passes, there are fewer buckets, thanks to vertex mergers, so
there is less wasted eﬀort. Put another way, it is better to throw 2s edges into a large, merged
bucket and forget about the one survivor than to throw s edges into each of two smaller buckets
and forget about one survivor for each.
5

Call the paths P0 , P1 , . . . Ps−1 , and let Pi,1 through Pi,s−1 be the vertices of Pi . The s − 1 vertices of a given
path are linked respectively to the s − 1 other paths: simply link Pi,j to Pi+j mods,s−j and vice-versa.
6
An adversary can set the edge weights so that the within-path contractions do not involve any tree merging
(§5.5) and therefore take about log* s steps.

Fredman & Tarjan
scan the same
edges on every
pass

CHAPTER §5. FASTER DETERMINISTIC ALGORITHMS

27

The edges that keep on going unused are unused because they are heavy, of course. We
will simply refuse to pay any attention to them until after we’ve dealt with lighter edges! The
solution, due to Gabow et al. [7], is to “hide” heavier edges behind lighter ones; once a lighter
edge has been processed, the heavier ones become visible. This trick will improve complexity
to O(m log β(m, n)).
§5.11 Development.
Let p be a small positive integer. Before the ﬁrst pass of the
Fredman & Tarjan algorithm, we will partition the 2m edges mentioned on G’s adjacency lists
into 2m/p packets of p edges each. (Note that these 2m edges are not all distinct: each of the
m edges of G is mentioned twice and appears in two packets.) For now let us assume that this
division is arbitrary and also exact, so that each packet holds exactly p directed edges. The
initial assignment of directed edges to packets is permanent, so the number of packets stays
ﬁxed from pass to pass, although edges may be deleted from packets.
A key point is that each packet is sorted when it is created. This sorting costs a little time
up front (a total of O(m log p), which is an incentive to keep p small), but it will save us from
having to consider the heavy edges in a packet until competing light edges have already been
dispensed with.
Let us call the minimum-weight edge of a packet its “top edge.” We treat every packet just
as if it were its light top edge. The other edges are essentially invisible, which is what saves us
time. However, when a top edge gets “used up,” in the sense that we conclude it is or is not in
M, we remove it from the packet and a new top edge pops up like a Kleenex.

Sort a few edges at
the start to
postpone heavy
edges

§5.12 Remark.
This idea, of using up a packet’s edges from lightest to heaviest, is a kind Relation to
of localized version of Kruskal’s insight that light edges should be considered before heavy ones Kruskal’s and
(§3.6). The localization technique resembles that of the classic linear medians-of-5 algorithm medians-of-5
for order statistics [1]. That algorithm crucially ﬁnds a near-median pivot element about which
to partition m numbers. It divides the numbers into arbitrary packets of p = 5 numbers each;
identiﬁes each packet with its median element; and ﬁnds the median packet. (It is easy to see
that this median-of-medians must fall between the 30th and the 70th percentile.) In much the
same way, the Gabow et al. algorithm divides the edges into arbitrary packets of p edges each;
identiﬁes each packet with its minimum element; and (repeatedly) ﬁnds the minimum packet.
§5.13 Development.
Every packet contains an arbitrary group of p edges originating Manipulating edge
from the same vertex or cluster. Thus, when we initially create the packets, what we are packets
doing is partitioning each vertex’s adjacency list into groups of p edges. Throughout the new
algorithm, we represent the adjacency list of each vertex not as a doubly-linked circular list
of edges, but as a doubly-linked circular list of edge packets. When two vertices are merged
(§4.4 line 16), these circular lists are combined in the usual way to give the adjacency list of
the merged vertex—again a list of packets.
Where we used to iterate through a vertex’s neighbor edges, throwing each into a bucket
of the form ℓ[v], we now iterate through a vertex’s neighbor packets in the same way. Where
the bucket ℓ[v] used to maintain the minimum-weight edge from T to v, now it maintains the
minimum-weight packet from T to v, where the weight and endpoint of a packet are determined
by its top edge. So each top edge is like a handle: if it goes into a bucket or comes out of a
bucket, it carries around the rest of the packet as so much baggage.
While our heap is now structured internally as a Fibonacci heap of sorted packets, it supports
all the same operations as a Fibonacci heap of edges (as in Prim’s). The lightest edge incident
on T is simply the top edge of the lightest packet (a minimum of minima). Wherever the
basic Fredman & Tarjan approach needs to extract the lightest incident edge, the Gabow et al.
modiﬁcation does so as follows:
1.

Extract the lightest packet from the heap.

CHAPTER §5. FASTER DETERMINISTIC ALGORITHMS
2.

3.

28

Remove its top edge uv (which we add to F ).
(This permanently alters the packet where it sits on u’s adjacency list.)
Rebucket the shrunken packet so that its other edges get a chance.

Once we have added uv to F , we then as usual bucket all the packets incident on T ’s new vertex
v.
Just as the extraction procedure must remove known good edges from their packets, salvaging the rest of the packet, the bucketing procedure must do the same with the known bad
edges it discovers. We bucket a packet P as follows:
1.
2.
3.
4.
5.
6.
7.
8.
9.
10.
11.
12.
13.
14.
15.
16.

§5.14

if P contains no edges
return
else
let e = uv be the top edge of P
if e is a self-loop
permanently delete e from P
(* it can’t be in the MST *)
Bucket(P )
(* tail-recurse with new top edge *)
elseif v is not yet marked as on the heap
ℓ[v] := P
(* create a new bucket *)
place v on the heap with key w(ℓ[v]) = w(e)
else
if e is lighter than the current ℓ[v]
swap P with ℓ[v]
Decrease-Min v’s heap key to the new w(ℓ[v]) = w(e)
permanently delete P ’s top edge
(* whichever edge lost by being too heavy can’t be in the MST *)
Bucket(P )
(* tail-recurse with new top edge, which is even heavier but may fall in a diﬀerent bucket *)

Analysis.

We must do the following work:

• Initially, we need O(m) time to partition the edges into packets and a total of O(m log p)
time to sort each packet internally.
• On each of the ≤ β(m, n) passes, we take O(ni log ki ) time to extract vertices, and
O(2m/p) time to bucket all the packets, exclusive of deletions and recursive calls within
Bucket. On each pass, we still have exactly the 2m/p packets we had at the start (still
ignoring roundoﬀ), although some may have lost some or all of their edges.
• Since there are 2m edges mentioned in the packets, and each can die but once, the total
number of deletions over all passes is at most 2m. Each takes O(1) time.
• Since every recursive call of Bucket immediately follows a deletion, we also have ≤ 2m
recursive calls, each of which takes O(1) additional time.
Summing up these times, we get
max pass

(ni log ki + 2m/p) + 2m .

O m + m log p +
i=0

Our goal is to arrange for β(m, n) passes of time O(m/p) rather than O(m) each. This requires
choosing ki on pass i such that ni log ki = O(m/p). Then we can choose p = β(m, n), so that
the expression reduces to


O m + m log p +

p−1
i=0



(m/p) + 2m = O(m log p) = O(m log β(m, n)).

We must put ki = 22m/pni (rather than just 22m/ni ) so that ni log ki will be m/p. Can we in
fact still ﬁnish in β(m, n) steps? In the old analysis (§5.5), we essentially argued that for each

Now a pass need
not consider all
edges

CHAPTER §5. FASTER DETERMINISTIC ALGORITHMS

29

tree of Fi , more than ki of the 2m directed edges originated in that tree, so there could be at
most 2m/ki of these trees. In the new algorithm, more than ki of the 2m/p directed packets
originate in each tree, so there are at most 2m/pki trees—an even smaller number. It follows
that ni+1 ≤ 2m/pki , so ki+1 ≥ 2ki as desired.
Since k0 = 22m/pn , what we have actually shown is that we need ≤ β(2m/p, n) passes. To
get the slightly stronger proposition that we need ≤ β(2m, n) passes (and a fortiori ≤ β(m, n)),
we exceptionally start with k0 = 22m/n (rather than 22m/pn ), so that the ﬁrst pass alone takes
time O(m); this does not hurt the analysis.7 Notice that the ﬁrst pass could not possibly take
time O(m/p) under any choice of k0 , since it certainly takes time Ω(n), which is worse than
m/p for very sparse graphs.
§5.15 Remark.
Notice that the time is dominated by the O(m log β(m, n)) cost of sorting Only O(m) total
the packets before we start. Once we begin to build the forest, the remaining time cost is only time wasted on
repeats
O(m)!
Recall that this O(m) cost is reduced from Fredman & Tarjan’s O(mβ(m, n)), which arose
from the fact that they had to rebucket every edge up to β(m, n) times (see §5.10). Should
we conclude that the packet algorithm considers every edge only once? Not quite, since like
the Fredman & Tarjan algorithm, it does discard full buckets if the heap overﬂows, only to
reconsider the top edges of the packets in those buckets later. However, the total time wasted
on repetition is limited to O(m): on each of the p passes, we allow ourselves to bucket 2m/p
edges that will be forgotten due to heap overﬂow. These are the top edges left in the 2m/p
packets at the end of the pass. Other edges may have been bucketed as well, but those we were
able to rule in or out of the tree rather than risk forgetting them. Over all passes, we waste
time bucketing up to p · 2m/p = 2m edges that we will forget until a later pass, but we correctly
determine the MST status of 2m edges, for a total of O(m) time spent bucketing.
To see the eﬀect, consider the example of §5.9. Fredman & Tarjan would bucket Θ(m) edges
on each pass, including the heavy edges from each vertex. Gabow et al. initially bury most of
those heavy edges deep in a packet, and do not pay any attention to them until later passes
when the lighter edges from the vertex have already been considered.
§5.16 Analysis.
We are forced to make one change to the Fredman & Tarjan imple- We must use
mentation (§5.8), namely in how we deal with contraction (§4.6). We can no longer aﬀord Union-Find when
to spend O(n) or O(mi ) time per pass keeping track of contractions. (Each pass takes only contracting
time O(m/p) = o(m), while both n and mi may be Θ(m).) Instead, we use the solution from
§4.6 that takes O(α(m, n)) time per edge lookup, using Union-Find (§4.6). This means that
bucketing an edge now takes time O(α(m, n)), not O(1), in order to determine which bucket to
use. So the time to build the forest is really O(mα(m, n)), not O(m) as suggested. However,
this is still dominated by the packet-sorting time of O(m log β(m, n)).
§5.17 Remark.
Notice again the strong similarity to Kruskal’s algorithm. Like Kruskal’s, Relation to
Gabow et al. begin with a kind of sort that dominates the running time. Again like Kruskal’s, Kruskal’s
the rest of the algorithm is linear except for an α(m, n) factor needed to look up what trees
each edge connects. The diﬀerence is that in Gabow et al., the linear phase reﬂects a delicate
tradeoﬀ: we maintain a sorted heap of vertices while building the forest, but we compensate
for the sort complexity by not allowing too many vertices on the heap.
§5.18
7

Development.

We now turn to the question of “roundoﬀ error.” Up till now, we We must merge

The discipline for choosing k given in [7] is slightly diﬀerent: it deviates more from [5] in that it simply puts
k0 = 22m/n , ki+1 = 2ki —the smallest rate of increase that will let us ﬁnish in β(m, n) steps. From this choice it
follows that (∀i ≥ 0)ni+1 log ki+1 ≤ (2m/pki ) · ki = 2m/p as desired. Again, the ﬁrst pass is exceptional, taking
O(n0 log k0 ) = O(2m) time.

undersized packets

CHAPTER §5. FASTER DETERMINISTIC ALGORITHMS

30

have explicitly assumed that each adjacency list could be evenly divided into packets of exactly
p edges, so that we have 2m/p packets. But if a vertex’s degree d is not divisible by p, we must
divide its adjacency list into ⌊d/p⌋ packets of size p plus an undersized packet of size d mod p.
The trouble is that if many edges are in undersized packets, then we may end up with many
more than 2m/p or even O(m/p) packets, ruining our analysis. Consider a sparse graph many
of whose vertices have degree much less than p. Each of the n vertices might contribute a
packet of a few edges, and n = ω(m/p), so we have rather more packets than the above analysis
bargained for.
In such an example, the solution is to consolidate those small packets on later passes, as
their vertices merge and the graph gets denser. Thus, there will be fewer and fewer small extra
packets on each successive pass.
Deﬁne the size of a packet to be the number of edges it had when it was created (i.e.,
removing edges from a packet does not reduce its size). Packets of size ≤ p/2 are called
“residual.” When we initially divide adjacency lists into packets as above, any residual packet
is marked as such and placed at the head of its adjacency list, where it is easily accessible.
Clearly each vertex of G0 contributes at most one residual packet—and this is a condition we
can continue to enforce in G1 , G2 , . . . as vertices merge under contraction, as follows. When
we merge the adjacency lists of two vertices v1 and v2 , the new vertex v inherits the residual
packets of both lists, if any. If there are two such, r1 and r2 , they are merged into a new packet
r, of size ≤ p (the sum of r1 ’s size and r2 ’s size). This new packet is marked residual, and put
at the head of the new adjacency list, just if its size is also ≤ p/2.
Now, how many packets are there in Gi ? Each non-residual packet uses up at least p/2
of the 2m original edges, so there can be at most 2m/(p/2) = O(m/p) non-residual packets.
In addition, each of the ni vertices can contribute at most one residual packet. So on pass
i, we must bucket not O(m/p) but rather O(m/p + ni ) packets. The total excess over our
previous analysis is i ni = O(n) edges, taking an extra O(nα(m, n)) time to bucket (see
§5.16). This term becomes insigniﬁcant asymptotically. It is not diﬃcult to check that the
alternative analysis of footnote 7 is also preserved.
We are now stuck with the problem of merging residual packets rapidly. Two circular
adjacency lists of packets can be merged in O(1) time. If we represent each packet not as a
sorted list of edges, but as a miniature Fibonacci heap of ≤ p edges, then we can merge the two
residual packets from the adjacency lists in O(1) time as well (see Appendix A). This requires
only superﬁcial changes to the method.8
§5.19 Remark.
The packet approach seems to be more generally applicable. The discus- Applying packets
˙
sion in §5.10, of how Fredman & Tarjan waste time reconsidering the same edges, is reminiscent to Boruvka’s
of §4.14’s similar criticism of Boruvka’s algorithm. One might therefore expect that the same
˙
packet technique could speed up Boruvka’s algorithm. Indeed this is the case: by using packets
˙
of size p = log n, we can immediately reduce the time needed on sparse graphs from O(m log n)
to O(m log log n).9 As in the Gabow et al. algorithm, we have chosen the packet size to match
the number of passes.
To ﬁnd the lightest edge incident on each vertex of Gi , as in §4.9, we now examine only
8
In keeping with the nature of a heap, we no longer sort each packet initially, at cost O(log p) per edge. At
the start of the algorithm, we Insert all the edges into their packets, at O(1) time apiece. We incur the O(log p)
cost per edge only later, while building the forest, as we Extract-Min successive top edges of the packet, one
by one. Notice that examining the top edge without extracting it still requires only O(1) time via the Minimum
operation.
9
For dense graphs, our previous Boruvka’s algorithm bound of O(n2 ) cannot be easily improved upon using
˙
packets. (Recall from §4.14 that it is sparse graphs where Boruvka’s tends to reconsider edges, hence sparse
˙
graphs where packets should help.) That bound required reducing the per-pass time to O(mi ) by spending
O(mi ) time eliminating multiple edges after every contraction—an elimination that is diﬃcult to accomplish any
faster, particularly when eliminating edges means deleting them from packets stored as Fibonacci heaps.

CHAPTER §5. FASTER DETERMINISTIC ALGORITHMS

31

the top edge in each packet—with the exception that if the top edge is a self-loop, we delete
it and examine the new top edge instead. As with Gabow et al., there are at most 2m/p + ni
packets to consider on pass i, for a total of O(m + n) = O(m) over all p = log n passes of
Boruvka’s algorithm. Like Gabow et al. we must use a Union-Find method to keep track
˙
of contracted vertices (see §5.16). So, excluding the time to throw out self-loops at the top
of a packet, each packet we consider or reconsider requires O(α(m, n)) time to conﬁrm that
its current top edge has not become a self-loop, and O(1) time to consider the weight of the
edge. The total time to identify lightest edges is therefore O(mα(m, n)). Removing MST
edges (those so identiﬁed) and non-edges (self-loops) from their packets, using Extract-Min,
takes time O(log p) per edge, plus time O(α(m, n)) to detect each self-loop, for a total of
O(m(log p + α(m, n))) = O(m log log n).
For sparse graphs (m = o(n log n/ log log n)) this improved Boruvka method actually beats
˙
Prim’s algorithm implemented with a Fibonacci heap. Of course the new method also uses
Fibonacci heaps, but diﬀerently—many small ones (the packets) instead of one large one. Moreover, it exploits a diﬀerent property of these wonderful heaps—their O(1) Union rather than
their O(1) Decrease-Key.
Gabow et al. get their admirable performance by combining the two uses of Fibonacci
heaps. They employ a large Fibonacci heap of buckets of small Fibonacci heaps (packets)
of edges, where the large heap takes advantage of O(1) Decrease-Key and the small ones
take advantage of O(1) Union. Notice that using such a heap of packets would have given no
advantage for the original Prim’s algorithm, which requires only one pass and considers each
edge only once. It is useful only for a multi-pass approach like that of Fredman & Tarjan or
Boruvka.
˙

Chapter §6

A linear-time veriﬁcation algorithm
This section describes the O(m) MST veriﬁcation algorithm of King [9]. (A previous such
algorithm was due to Dixon [4].) The description falls into two parts. The ﬁrst is a clever
application of general principles that is due to Koml´s [10]: it shows how to perform veriﬁcation
o
using only O(m) edge-weight comparisons. The second part is much more ﬁnicky. It shows how
the additional bookkeeping required can also be kept to O(m) time, thanks to the fact that it
merely performs constant-time operations on a very small set of objects—a set of size ≤ log n.
The idea here is to avoid iterating through these objects individually, by processing them in
√
batches (i.e., tuples of objects). If the batch size is chosen such that there are ≤ n distinct
batches, each batch can be processed in constant time by looking it up in a table (array) with
√
√
O( n) entries. Each entry takes time O(batch size) = O(log n) to precompute, for a total
precomputation time that is o(n).
King’s innovation is to use Boruvka’s algorithm in service of Koml´s’s basic idea for veriﬁ˙
o
cation with O(m) comparisons. The idea of using table-lookup to achieve linear-time overhead
was ﬁrst used in a more complicated O(m) veriﬁcation algorithm [4]. King’s application of this
idea to Koml´s’s algorithm makes use of a labeling scheme of Schieber & Vishkin [15].
o

Achieving O(m) comparisons
§6.1 Overview.
Given a spanning tree M ? of G, there are two basic simple strategies Reduction to
to check whether it is minimal, based respectively on the Strong Cut Property and the Strong ﬁnding heaviest
Cycle Property of §3.3. We could check that each edge e in M ? is lightest across the cut deﬁned edges on tree paths
by M ? − e: if not, M ? cannot be the MST (see §3.3, ⇒), and if so, M ? must be the MST, since
it consists of just those n − 1 edges that do belong in the MST (see §3.3, ⇐). On the other
hand, similarly, we could check that each edge e outside M ? is heaviest across the cycle deﬁned
by M ? + e: again, if not, M ? cannot be the MST, and if so, M ? must be the MST, since it
excludes just those m − (n − 1) edges that do belong outside the MST.
The second strategy appears more promising, at least naively, since there can be O(m)
edges to compare across a cut but only O(n) on a cycle. This is indeed the strategy King uses.
Hence, for every edge uv not in M ? , she must check that its weight exceeds that of heaviest
edge on M ? ’s u-v path. This requires < m comparisons of tree edges with non-tree edges. (See
§7.14 below for further discussion.) The diﬃcult part is to ﬁnd the heaviest edges on all these
m − n − 1 paths, using only O(m) comparisons of tree edges with each other. Looking at each
path separately would take too long.
Thus, the problem to be solved is to spend a total of O(m) time answering a set of O(m)
queries of the form: “What is the heaviest edge on the path between u and v in M ? ?” We refer
to this query as MaxEdge(u, v). The path it asks about is called a query path.

32

CHAPTER §6. A LINEAR-TIME VERIFICATION ALGORITHM

33

§6.2 Approach.
The basic clever idea in King’s solution is to exploit the fact that if we Construct a tree
run any standard MST-ﬁnding algorithm on the tree graph M ? itself, it will reconstruct M ? edge as its own MST to
by edge, adding the light edges of paths ﬁrst and the heavy edges only later. All the standard ﬁnd heaviest edges
algorithms have the property that they defer heavy edges. King uses Boruvka’s algorithm,
˙
because it runs quickly when the input graph is a tree and (as will prove important) because
it completes in O(log n) passes. However, the crucial properties that let her use Boruvka’s
˙
algorithm to detect heavy edges of M ? turn out not to rely on any special magic about that
algorithm, as we will see in §6.6.
§6.3 Development.
Indeed, for purposes of exposition, let us start by considering Prim’s Answering
algorithm rather than Boruvka’s. Suppose we wish to answer the query MaxEdge(u, v). We MaxEdge queries
˙
run Prim’s algorithm on M ? with u as the start vertex, and keep a running maximum over via Prim’s
all the edges we add to the growing tree. Just after v gets connected to the tree, the running
maximum holds the answer to the query. The obvious proof: The heaviest edge added is at
least as heavy as e = MaxEdge(u, v) because all the edges on the u-v path have been added. It
¯
is also at most as heavy, for if we added any edges of M ? not on the u-v path before completing
the path just now, it was because they were lighter than the next edge of the u-v path that we
could have added at the time, and a fortiori lighter than e.
¯
Ignoring running time for the moment, how well does this technique generalize to multiple
queries? If u is ﬁxed, a single run of Prim’s algorithm, with u as the start vertex, will answer
all queries of the form MaxEdge(u, v), by checking the running maximum when we add each
v. Indeed, the situation is even better. It is a consequence of §6.6 below that this single run
starting at u can answer arbitrary queries: MaxEdge(u′ , v) is the heaviest edge added between
the addition of u′ and the addition of v.
§6.4 Development.
Let us continue with this Prim-based approach to see that it takes Binary search of
too long. We need to keep track of the heaviest edges added since various times. Suppose we sparse vectors isn’t
number the vertices v1 , v2 , . . . in the order that Prim’s discovers them. Just after we add vertex fast enough
vj , we will construct an array Aj such that Aj [1], Aj [2], . . . Aj [j − 1] are (so far) the heaviest
edges that have been added since the respective vertices v1 , v2 , . . . vj−1 joined the tree. Thus
Aj [i] holds MaxEdge(vi , vj ). When vj+1 is added to the tree via some edge e, it is easy to
create Aj+1 from Aj : Aj+1 [i] = max(Aj [i], e) for i ≤ j, and Aj+1 [j] = e.
Maintaining the edges in this way would require O(n2 ) array entries Aj [i] (i < j), and O(n2 )
comparisons to create them (the step max(Aj [i], e)). For very dense graphs this is O(m) as
desired, but in general we need to do better. The two tricks we use foreshadow those employed
in King’s actual algorithm, which is based on Boruvka’s method rather than Prim’s.
˙
(a) First, we can drop to O(n lg n) comparisons by using binary search. Observe that each
array Aj lists O(n) edges in decreasing weight order. Creating Aj+1 from Aj is a matter
of updating some tail of the array so that all its cells hold e: this tail consists of those
edges of Aj that are lighter than e, and its starting position can be found in O(lg n)
comparisons by binary-searching Aj for the weight w(e).
(b) Second, we do not really need to keep track of all of Aj . We only need to store Aj [i] if
one of our queries is MaxEdge(vi , vk ) for some k ≥ j > i. We can store the relevant
entries of Aj compactly as a sparse vector, i.e., as an array of pairs of the form i, Aj [i] ;
to the extent that it has fewer than n entries, it can be binary-searched even faster than
Θ(lg n) time.
Unfortunately, the second trick does not give an asymptotic improvement. It is easy to come
up with sparse graphs in which many of the vertices must still maintain large arrays. For example, imagine that we have the queries (corresponding to edges of G − M ? ) MaxEdge(v1 , vn ),

CHAPTER §6. A LINEAR-TIME VERIFICATION ALGORITHM

34

MaxEdge(v2 , vn−1 ), . . .. Then O(n) of the vertices (the middle third) must create arrays that
keep track of O(n) query paths each, so we still need O(n lg n) comparisons.
Moreover, simply running Prim’s algorithm on the n-vertex tree M ? takes time O(n log n).
Fortunately this algorithm—which takes too long and which adds edges in a long sequence to
a single growing tree—is not our only option, as we now see.
§6.5 Definitions.
Given an arbitrary weighted connected graph G, of which u and v are Minimax(u, v) and
vertices. G has one or more u-v paths, each of which has a heaviest edge. Deﬁne Minimax(u, v) F (w)
to be the lightest of these heaviest edges.
Recall that the generalized greedy algorithm (§3.14) constructs the MST of G by repeatedly
adding edges to a forest F . Let F (w) always denote the component tree of F containing the
vertex w. At each step, some tree in F grows by selecting the lightest edge leaving it, causing
this edge to be added to F .
§6.6 Theorem.
Let G be an arbitrary weighted connected graph, and construct the MST All greedy MST
using any algorithm that observes greedy discipline (§3.14). Then for any vertices u, v of G, algorithms ﬁnd
Minimax(u, v) is the heaviest edge having the property that either F (u) or F (v) selected it Minimax edges
while F (u) = F (v).
§6.7 Remark. This interesting theorem crucially gives us a way to ﬁnd Minimax(u, v).
Notice that it cannot be aﬀected by tree contraction (§4.1), which is just a way of regarding
trees like F (u) as single vertices without changing the edges they add.
The theorem is actually stronger than we need. For present purposes we only care about
the case where G is a tree, in which case Minimax(u, v) = MaxEdge(u, v). However, it is
nearly as easy to prove the stronger form. We begin with two lemmas.
§6.8 Lemma. If F (w) ever selects edge e during the generalized greedy algorithm, it already
contains all paths from w consisting solely of edges lighter than e.
This follows trivially from greedy discipline: if it were false, then F (w) would be able to
select some edge lighter than e instead, extending such a path.
§6.9

Lemma.

Let e = xy be the heaviest edge on a particular u-v path, u . . . xy . . . v. Then:

(a) If e is ever selected, it is selected by F (u) or F (v).
Proof: It is necessarily selected by either F (x) or F (y). (These are the only trees that it
leaves.) If by F (x), then lemma §6.8 implies that F (x) already contains the whole u . . . x
path (whose edges are lighter than e). So F (x) = F (u). Similarly, if it is selected by
F (y), then F (y) = F (v).
(b) Nothing heavier than e is ever selected by F (u) or F (v) while F (u) = F (v).
Proof: If F (u) ever selected an edge heavier than e, then by lemma §6.8 it would already
contain the entire u . . . xy . . . v path (whose edges are lighter), so F (u) = F (v).
§6.10 Proof of theorem §6.6. Since the algorithm starts with u and v isolated in F and
ends with them in the same component, it must select and add all edges of some u-v path
while F (u) = F (v). Let e1 be the heaviest edge on such a path. Then e1 is selected while
F (u) = F (v), and by §6.9.a, it is selected by either F (u) or F (v). Let e2 = Minimax(u, v): by
§6.9.b, nothing heavier than e2 is ever selected while F (u) = F (v). But e1 is at least as heavy
as e2 and is so selected. We conclude that e1 = e2 . The theorem now follows from the various
stated properties of e1 and e2 .

CHAPTER §6. A LINEAR-TIME VERIFICATION ALGORITHM

35

§6.11 Remark.
Theorem §6.6 implies, inter alia, that the those edges of G expressible
as Minimax(u, v) are exactly those edges selected during a greedy construction of the MST. So
the MST consists of these “minimax edges” of G—a moderately interesting characterization.
It is amusing to show this minimax characterization equivalent to the Strong Cut Property,
which is expressed in terms of minima, and the Strong Cycle Property, which is expressed in
terms of maxima (§3.3). The equivalence makes no mention of spanning trees.
For the Cut Property, we verify that the minimax edges are exactly those that are lightest across some cut. If xy is lightest across some cut, then xy must be the minimax edge
Minimax(x, y), since any other x-y path has a heavier edge where it crosses the cut. Conversely, if xy is the minimax edge Minimax(u, v), then it is lightest across the cut formed by
removing the heaviest edge from every u-v path.
For the Cycle Property, we verify that the non-minimax edges are exactly those that are
heaviest on some cycle. If xy is a non-minimax edge, then in particular it is not Minimax(x, y),
so it is heaviest on the cycle consisting of xy itself plus an x-y path made of lighter edges.
Conversely, if xy is heaviest on some cycle C, then we can replace it on any u-v path with
lighter edges from C − xy, so it cannot be a minimax edge Minimax(u, v).

Minimax, Cut,
and Cycle are
equivalent MST
characterizations

§6.12 Discussion.
We now return to the insuﬃciently fast method of §6.3–§6.4. We may Boruvka’s beats
˙
Prim’s for
replace Prim’s algorithm by any algorithm that observes greedy discipline. For every query
MaxEdge(u, v), we will endeavor to keep running maxima of the edges selected by F (u) as MaxEdge
it grows and likewise those selected by F (v), at least until these trees merge. At that point,
theorem §6.6 tells us that the greater of these two running maxima is MaxEdge(u, v).
Suppose that at some stage in the growth of the forest, F (v1 ) = F (v2 ) = F (v3 ) = · · ·.
That is, a certain tree T in F contains many vertices, including v1 , v2 , v3 , and so on. Suppose
further that there are query paths from all these vertices. When T next selects an edge, it must
update the records of the heaviest edge selected so far by each of F (v1 ), F (v2 ), F (v3 ), . . . (all of
which are now the same tree T ). To minimize such updates, it is desirable for most edges to be
selected by trees containing few rather than many vertices.
Thus Prim’s algorithm is therefore exactly the wrong one to choose: on average, the tree
doing the selection contains fully Θ(n) vertices. (As we saw in §6.4, this means that each edge
selection can require Θ(n) updates to our heaviest-added-edge records, at a cost of Ω(lg n)
comparisons.) The correct algorithm is Boruvka’s algorithm (§4.7), in which the task of edge
˙
selection on each pass is distributed equitably among the current (contracted or uncontracted)
trees, each tree selecting one edge. Some edges may be selected by two trees at once, but this
does not alter the point or aﬀect theorem §6.6.
King therefore uses Boruvka’s algorithm on M ? to ﬁnd the maximum edges on paths of
˙
? . An additional advantage is that actually running Boruvka’s algorithm does not violate
M
˙
the desired O(m) time bound for the veriﬁcation task. Boruvka’s algorithm takes only O(m)
˙
time, indeed O(n) time, when run on a tree such as M ? . In this case the contracted graphs
Gi considered on successive passes are all trees: so mi = ni − 1. (This is far better than the
bound mi = O(n2 ) that §4.10–§4.11 used for the general case.) Boruvka’s algorithm takes time
˙
i
i ≤ 2n.
proportional to i mi ≤ i ni ≤ i n/2
§6.13 Remark.
Boruvka’s algorithm takes linear time on trees, but not on arbitrary Why Boruvka’s
˙
˙
runs fast on trees
sparse graphs with m = O(n). Why the diﬀerence? Boruvka’s algorithm takes time
˙
mi .
Our earlier analyses (§4.10–§4.11) bounded mi with m and n2 . We are now considering a third
i
bound, m − (n − ni ), where n − ni is the number of edges actually contracted so far. This bound
is not helpful unless m is very close to n, so that m − (n − ni ) is a signiﬁcant reduction in m. To
put this another way, if m is close to n, then the n/2 or more edges actually contracted on the
ﬁrst pass constitute a signiﬁcant fraction of the total edges. The same is true for subsequent
passes; so noticing the loss of those edges makes a diﬀerence to the analysis.

CHAPTER §6. A LINEAR-TIME VERIFICATION ALGORITHM

36

§6.14 Remark.
Under Boruvka’s algorithm, even if there is a long path Φ from u to v,
˙
neither F (u) nor F (v) will select more than log n edges as it grows: each selects just one edge
per pass, which may or may not be on Φ. The rest of Φ is assembled by its internal vertices’
ﬁnding each other. Theorem §6.6 nonetheless guarantees that the heaviest edge of Φ will be
among the 2 log n edges that F (u) and F (v) do select! This provides another way of thinking
about the advantage of Boruvka’s algorithm: no running maximum (e.g., “heaviest edge from
˙
u”) need be updated more than log n times. By comparison, the Prim-based approach could
update each running maximum Θ(n) times.

Most edges on a
path are never
considered as
possible maxima

§6.15 Development.
King’s algorithm begins by running Boruvka’s algorithm on M ? . Storing the
˙
We may implement it as described in §4.8. Recall that at the start of pass i, we have a contracted selected edges
graph Gi whose vertices represent trees in the growing forest F . Each vertex (tree) v of Gi will
select a single edge on the pass; for purposes of King’s algorithm, we record this edge in a ﬁeld
Sel (v). Two vertices may select the same edge.
Now let v0 be a vertex of the original graph. When Boruvka’s algorithm ends, the edges
˙
selected by F (v0 ) will be stored as Sel (v0 ), Sel (P (v0 )), Sel (P (P (v0 ))), . . .. Recall that the vertices v0 ∈ G0 , P (v0 ) ∈ G1 , P (P (v0 )) ∈ G2 , . . . represent (contracted versions of) the tree F (v0 )
as it grows from pass to pass.
§6.16 Notation.
For convenience, King regards our data structure’s P and C ﬁelds Reducing to
(parent and child-list) as implicitly deﬁning a weighted, rooted tree B. The leaves of B are the MaxEdge queries
vertices V0 ; their parents are V1 , and so on. The edge from v to its parent P (v) is weighted by on a balanced tree
w(Sel (v)). The weights of the edges selected by F (v0 ) as it grows can now be read oﬀ the path
from v0 to the root of B.1
Now that we have run the Boruvka algorithm, the remainder of the problem can be reframed
˙
as follows. We are given a rooted tree B with n leaves. The tree is balanced in that all leaves
are the same distance from the root (equal to the number of passes, O(log n)). Every internal
node has at least two children (possibly many more), so there are at most n/2i nodes at level
i, and the height of the tree is at most log n edges.
We are permitted O(m) comparisons to answer m batched queries of the form MaxEdge(u0 , v0 ),
where u0 and v0 are leaves of B. Each such query determines two half-paths in B, from u0 to
lca(u0 , v0 ) and from v0 to lca(u0 , v0 ). The edges on these half-paths correspond to the edges
of M ? selected by F (u0 ) and F (v0 ), respectively, while F (u0 ) = F (v0 ). So our main goal is
to ﬁnd the heaviest edge on each half-path, using O(m) comparisons. Then theorem §6.6 says
that for each MaxEdge query, we can simply compare the heaviest edges of its two half-paths,
and return the heavier. This requires one additional comparison for each of the m edges.
§6.17 Development.
The solution is essentially that of §6.4, with the twist that we will An array of
descend the tree B from the root rather than ascending it. This corresponds to maintaining running maxima
at each node of B
our running maxima backwards in time.
At each node v of B we will create an array Av . We postpone discussion of how Av is
implemented, but conceptually it is a sparse array indexed by certain ancestors of v as follows.2
Suppose u is a proper ancestor of v, and there are half-paths starting at u and passing down
through v to one or more leaves. Then Av [u] exists and records the heaviest edge on the u-v
path in B. This u-v path is an initial segment of the aforementioned half-path(s). If v is a leaf,
1

The weights do not necessarily increase upward on such a path: when F (u) selects its lightest incident edge,
thereby merging with some F (v), a lighter edge may become available. All we can say is that a node’s parent
edge in B is heavier than at least one of its child edges. (This fact is not used.)
2
To achieve the goal of the present section, O(m) comparisons but non-linear overhead, the data structure
suggested in §6.4.b will suﬃce. Thus if Av is deﬁned at a descending sequence of nodes u1 , u2 , . . . uk , we can
store a length-k array whose ith element is ui , Av [ui ] .

CHAPTER §6. A LINEAR-TIME VERIFICATION ALGORITHM

37

the u-v path is the entire half-path, and Av [u] records the heaviest edge on it as desired.
Like Aj in §6.4, each Av is necessarily a decreasing array, in the sense that Av [u] decreases
(or stays the same) for successively lower ancestors u.
We create the arrays Av from the top of B down. Aroot is empty since the root has no
ancestors. For v not the root, we compute Av from the parent array Ap , where p is the v’s
parent in B:
1.
2.
3.
4.
5.
6.
7.
8.
9.

(* Compute a version of Av that does not take the additional pv edge into account. *)

for each element Ap [u] of Ap in descending order of u
if at least one of the half-paths from u to a leaf passes through v
append a new element Av [u] to Av , and set it to Ap [u]
(* Update the running argmaxima in Av with the pv edge, using ≤ lg |Av | comparisons. *)

using binary search, ﬁnd the tail of Av whose elements are lighter than pv
replace every element in this tail with pv
if there is a path from p to a leaf that passes through v
append a new element Av [p] to Av , and set it to pv

(* as in §6.4.a *)

§6.18 Analysis. For each v in B, we use up to lg |Av | comparisons to create Av .3 How
many comparisons are needed in toto, over all v? King says only O(n log m+n ) (= O(m)),
n
citing [10].
Let us check this. The intuition is that each node of B uses its array to keep track of several
half-paths that will terminate at descendants of the node. Each level of the tree is keeping track
of at most 2m half-paths in total, so the total size of arrays at level i of the tree is at most
2m.4 At high levels of the tree, these 2m half-paths are distributed among just a few large
arrays. Binary search on each of these large arrays lets us complete the update in substantially
less than Θ(m) time. At lower levels of the tree, where the half-paths might be split over many
O(1)-size arrays, binary search is less eﬀective and Θ(m) time may be required.
For the formal analysis, consider how many comparisons are needed at the ni nodes at level
i of the tree:
lg |Av | = ni Avg lg |Av |
v

v

= ni Avg log(1 + |Av |)
v

≤ ni log Avg (1 + |Av |)
v

(* using concavity of log, i.e., consider worst case of many small |Av | *)

1 + |Av |
ni
n + v |Av |
≤ ni log
ni
n + 2m
(* see above for discussion *)
≤ ni log
ni
n + 2m
n
= ni log
+ log
n
ni
= ni log

v

We now sum the comparisons needed at all levels of the tree. The sum is at most
∞
i=0
3

ni log

∞
n
n + 2m
ni log
+
n
ni
i=0

lg x = ⌈log(1 + x)⌉. It is important to use lg here rather than log, since |Av | may be 0, and also since if |Av |
is 1 we still need 1 comparison to determine whether the desired tail of Av has length 0 or 1.
4
Speciﬁcally, each entry Av [u] corresponds to the set of half-paths starting at u and passing through the
vertex v at level i. For a ﬁxed i, each half-path appears in at most one such set, according to its uppermost
vertex u and its level-i vertex v. Since there are only 2m half-paths, there can be at most 2m such sets—or
equivalently, array entries—at level i.

CHAPTER §6. A LINEAR-TIME VERIFICATION ALGORITHM

38

Recall that ni < n/2i . The ﬁrst summation is therefore O(n log m+n ). For the second sumn
mation, we observe that x log n is an increasing function for x < n/(21/ ln 2 ) and in particular
x
for x ≤ n/22 . (Its derivative with respect to x is log(n/x) − 1/ ln 2.) Therefore, the second
summation is only
∞
i=0

ni log

n
ni

= n0 log

∞
n
n
n
ni log
+ n1 log
+
n0
n1 i=2
ni

∞
n
n
2−i n log −i
+
≤ 0 + n1 ·
n1 i=2
2 n

= 0+n+

∞

2−i ni

i=2

= 0 + n + 3n/2 = O(n).
The total number of comparisons is therefore O(n log m+n ) as claimed. This is really a tighter
n
bound than necessary; the important fact is that n log m+n < n m+n = m + n = O(m).
n
n
§6.19 Remark.
It is clear that if M ? is the MST of G, then there exists a proof of that Comparisons that
fact—a certiﬁcate for M ? —that can be checked using m − 1 weight comparisons and in O(m) witness the
time. One such proof has the following form: “Here is a sorted list e1 < e2 < · · · < em of the veriﬁcation
edges in G [notice the m − 1 weight comparisons to check], and here is a trace of Kruskal’s
algorithm when run on that list.” Another such proof follows King’s algorithm more closely:
“Here is a sorted list e1 < e2 < · · · < en−1 of the edges in M ? [n − 2 comparisons to check].
Now for each edge uv ∈ M ? , here is a lower bound ei < uv on uv’s position relative to that list
[a total of m − n + 1 comparisons to check], and here is a u-v path Φuv made up of edges drawn
from {e1 , e2 , . . . ei } and therefore lighter than uv; so uv is not in the MST by the Strong Cycle
Property.”
The interest of King’s algorithm is therefore not that an O(m) proof exists, but that one
can be found with only O(m) comparisons (and, as we shall see, in O(m) time). Neither of the
above proofs can be found that quickly. No general O(m) algorithm can sort even the n − 1
edges that fall in M ? , as this would take Ω(n log n) comparisons.
Suppose we modify King’s O(m) algorithm to output its proof that M ? is the MST (when
this is true). This proof resembles the second proof above, but is slightly diﬀerent in that it only
partially sorts e1 < · · · < en−1 . Thus, while the path Φuv does consist of edges of M ? that are
known to be no heavier than ei , the relative order of these edges may not be otherwise known.
(ei was the heaviest edge added by F (u) or F (v) during the call to Boruvka’s algorithm, and
˙
appears in Φuv . The other edges of Φuv , some of which were also added by F (u) or F (v), were
determined to be lighter than ei at various stages of the algorithm.)
How many of the algorithm’s O(m) comparisons must actually be mentioned in the proof?
The proof must ﬁrst show that the Boruvka tree B observes greedy discipline. It can do so
˙
by citing the ≤ 2n comparisons used by the Boruvka algorithm on M ? . Next, the algorithm
˙
spends several comparisons at each node of B, doing a binary-search to see where an edge e
whould fall in an edge sequence. The proof, however, can demonstrate each search’s correctness
merely by showing that e falls between two adjacent edges in the sequence. This adds ≤ 4n
comparisons to the proof (two per node), some of which are redundant. Finally, the m − (n − 1)
edges not in M ? must each be compared to an edge in M ? , which is itself the heavier edge from
a comparison over two half-paths. The total is ≤ 2m + 4n − 2 comparisons, which bound is
rather worse than the m − 1 comparisons possible with proofs found via sorting.
Notice that King’s algorithm runs faster than Kruskal’s yet produces a longer proof. In
general, partial orders can be found more quickly than total ones but may require longer to
verify. (Example: The “diamond” partial order (a < b < c) ∧ (a < b′ < c) states four

CHAPTER §6. A LINEAR-TIME VERIFICATION ALGORITHM

39

comparisons that must be veriﬁed independently. The total order a < b < b′ < c contains more
information, and is harder to discover, but can be veriﬁed with only three comparisons.) Partial
orders can be found with fewer comparisons because the decision tree for distinguishing among
partial orders is shorter: its leaves correspond to internal nodes of the total order decision
tree. But their proofs must make more use of comparisons because they can make less use of
transitivity.

Reducing overhead to O(m)
§6.20 Data Structure.
King’s strategy can be implemented within the desired time Precomputing
bounds, without assuming the availability of O(1) operations that do arithmetic or parallel functions on
bit manipulation on arbitrarily large numbers. To make this fact clear and concrete, I will bitstrings
introduce a simple data structure called the bitstring. This data structure will ﬁgure heavily in
my presentation of the algorithm.
Let s be a ﬁxed positive integer to be deﬁned later. A bitstring b stands for a string of s or
fewer bits: 11 is a diﬀerent bitstring from 0011 or 1100, and 0 is distinct from the null bitstring
ε. There are S = 2s+1 − 1 diﬀerent bitstrings. Each is represented as an arbitrary integer in
[1, S], which means that it can be used as an array index into a lookup table.
Given a linked list of bits, we can ﬁnd the corresponding bitstring, and vice-versa. Both
these operations take time proportional to the length of the bitstring in question; in particular
they are O(s). For the correspondence we rely on an auxiliary data structure, a trie that can be
prebuilt in O(S) time. The trie is a balanced, perfectly binary tree of depth s and has S nodes.
We number the nodes arbitrarily, e.g., consecutively as we create them depth-ﬁrst. Given
the list of bits 01101, we ﬁnd the bitstring by following a left-right-right-left-right sequence
of children from the root of the trie and returning the number of the resulting node, say 75.
Conversely, given this bitstring 75, we can look up a pointer to the node, using an array that
we built when we originally numbered the nodes. Then we can follow parent links up to the
root of the trie, constructing the list 01101 starting with its rightmost bit.5
It is simple to compute functions such as length(b) (the number of bits in a given bitstring),
concat(b1 , b2 ) (the concatenation of two bitstrings), substring(b, i, ℓ) (the ℓ-bit substring of a
bitstring b, starting at position i), and so forth. All the functions we are interested in can
be computed in time O(s) per call. The simplest implementation is to convert the bitstrings
to lists of (s or fewer) bits, do the computation on the lists, and convert back to a bitstring
as necessary. The results can be cached in lookup tables. For example, the lookup table for
substring would be a three-dimensional array of bitstrings, indexed by b, i, and ℓ, which has
O(S · s2 ) entries and takes time O(S · s3 ) to precompute in its entirety. Some entries of the table
are undeﬁned or hold an error value. Once the table is precomputed, entry lookup is O(1).
§6.21 Strategy.
The simple algorithm of §6.17 constructs Av at a node from Ap at its Make overhead
parent node. The work of this construction is dominated by two simple loops. First, we copy proportional to
selected elements of Ap to Av ; then we overwrite some tail of Av (selected by binary search). comparisons
These loops take time O(|Ap |) and O(|Av |) respectively. Our goal is to make both of them take
time O(lg |Av |), the same as the time for the binary search. Then the work at each node will
be proportional to the number of comparisons at that node. The total work will be therefore
be proportional to the total number of comparisons, which we have seen to be O(m).
5

A standard way of storing such a binary tree is in an array a of length S, where a[1] is the root and a[i] has
children a[2i] and a[2i + 1]. If we number the node a[i] with i, there is no need to actually store the array. We
can carry out the above procedure merely by doubling, halving, and incrementing. However, that would require
us to do arithmetic on potentially large numbers rather than just using them as array indices. Note that the
numbering scheme here represents the bitstring 01101 with the integer 45 = 101101two (i.e., a 1 is preﬁxed).
The trie could be numbered according to this scheme in time O(S · s).

CHAPTER §6. A LINEAR-TIME VERIFICATION ALGORITHM

40

§6.22 Data Structure.
How do we represent the array Av ? Number the vertices on A particular sparse
the root-to-v path by their distance from the root. Edges can be numbered similarly. Every array scheme
number is ≤ lg n. If eﬃciency were not a concern, we would store Av directly: Av [0] for the
heaviest edge from 0 to v, Av [1] for the heaviest edge from 1 to v, and so forth. However, recall
that we are going to leave some of these entries undeﬁned: Av [u] is maintained only if there is
a half-path starting at u that passes down through v.
We therefore represent Av as a pair of data structures, Cv (“contents”) and Iv (“indices”).
Cv is an “compacted” array containing just the deﬁned entries of Av , so Cv [0] is the ﬁrst deﬁned
entry, Cv [1] is the second deﬁned entry, and so forth. Notice that Cv can be binary-searched
in time lg |Av |, as desired. One might expect that Iv would be a parallel array giving the
indices for Cv [0], Cv [1], . . .. However, we represent the same information more concisely as a bit
vector of length ≤ log n specifying the deﬁned indices: Iv [u] = 1 just if Av [u] is deﬁned. This
representation better supports the operations we need.
§6.23 Strategy.
Let us ﬁrst see how to do the operations of §6.21 in time O(lg |Ap |). Process a batch of
We will subsequently improve this to our goal of O(lg |Av |), which is more ambitious since edges at a time
|Av | ≤ |Ap | + 1 and is possibly much smaller.
The basic strategy was sketched at the start of §6. Recall that Cv is an array of some
of the edges on the root-to-v path, each of which can be stored as a small number ≤ log n
(called an edge tag) according to its distance from the root.6 We will process these tags in
batches (tuples) of about log(log n) (n1/3 ) tags each. This choice of batch size means there are at
most n1/3 possible batches, and only n2/3 possible pairs of batches. We can therefore compute
(for example) any unary or binary operation on batches in constant time, by looking up the
operand(s) in a table of n2/3 or fewer entries.
How long does it take to precompute such a table, assuming that each entry can be computed
in time O(log n)?7 (This assumption is often reasonable: there are fewer than log n tags in all of
|Cv |, so there are certainly fewer than log n tags per batch of Cv ’s tags, so we just need to spend
O(1) time per tag when computing a function on batches.) We need only O(n2/3 log n) = O(n)
to precompute the whole table. The advantage to precomputation is that the same entries will
be accessed repeatedly.
§6.24 Implementation.
It is convenient to switch to the log domain and use bitstrings. Sizes and
To recast part of the above discussion, Cv consists of an array of some number p ≤ log n edge quantities of
tags of lg log n bits each, for a total of p lg log n bits. We divide this list into batches of tags, bitstrings
1
where each batch holds enough tags to consume s ≈ 3 log n bits. (s is rounded up to a multiple
of lg log n.) Each batch is represented as a single bitstring having the maximum length s, except
that the last batch may be a shorter bitstring. These bitstrings are stored in a random-access
array.
We will never use any bitstrings longer than s. So in the terminology of §6.20, S = 2s+1 −1 =
O(n1/3 ). We can therefore precompute an O(s)-time function of up to two bitstrings and k
integer indices in [0, s] in time O(S 2 · sk+1 ) = O(n2/3 (log n)k+1 ), which is O(n) for any ﬁxed k.
For example, the substring example from §6.20 takes one bitstring and two indices. Looking up
values of such functions is O(1) thereafter. The idea is similar to doing base arithmetic using
a large base that is determined at runtime and increases with the size n of the problem.
The total time required to iterate through all of Cv , performing such operations by O(1)
lookup one batch at a time, is proportional to the number of batches. This is the total number
1
of bits in Cv divided by s, or at most p lg log n/ 3 log n. It is easy to see that this is less than
6

In §6.27 we will revise the edge-numbering scheme.
King does not call attention to the fact that each table entry requires ω(1) time (here O(log n)) to precompute,
although if this were not the case there would be no advantage to using a table. My choice of n1/3 possible batches
rather than n1/2 ensures that lookup tables can be computed in O(n) time.
7

CHAPTER §6. A LINEAR-TIME VERIFICATION ALGORITHM

41

3 lg p = 3 lg |Cv | = O(lg |Cv |) as desired by §6.21.8
We will also have to do some work on Iv . Iv is a vector of only log n bits, which we can
1
store as a list of just three bitstrings of length s = 3 log n. Iterating down the three bistrings
on this list takes only O(1) time. I will refer to these s-bit bitstrings as batches, too, although
they are conceptually batches of bits rather than of edge tags.
§6.25 Definitions.
Before giving the algorithm, we need to clarify the deﬁnitions of Appending and
some bitstring functions. The concatenation of two bitstrings might result in more than our removing bits
maximum of s bits. So concat(b1 , b2 ) is deﬁned to return a list of either one or two bitstrings. If quickly
this list holds two bitstrings, the ﬁrst has length s and the second holds the overﬂow. (Example:
if s = 6, concat(100, 11) = 10011 , but concat(100, 11101) = 100111, 01 .) A lookup table for
concat can be computed in time O(S 2 · s) = O(n).
This deﬁnition of concat, with its table-lookup implementation, helps us maintain sequences
of more than s bits, such as Cv . Suppose L is an array or list of one or more bitstrings, all of
length s except possibly the last, representing a long sequence of bits. b is another bitstring
whose bits are to be added to the end of L. Deﬁne an O(1) procedure Append (L, b) that destructively removes the last bitstring b′ from L and appends concat(b′ , b) instead. (Example: if s = 6,
L = 101010, 000000, 100 , then Append (L, 11101) lengthens L to 101010, 000000, 100111, 01 .)
Similarly, if L is an array or list of bitstrings, all of length s except possibly the ﬁrst and last,
then we can deﬁne an O(1) procedure RemoveHead (L, ℓ) that destructively removes that ﬁrst
ℓ ≤ s bits of L. RemoveHead replaces the ﬁrst two bitstrings of ℓ with one or two bitstrings,
which are found with a looked-up function remove of the ﬁrst two bitstrings. (Example: if
s = 6, L = 010, 000000, 100111, 01 , then RemoveHead (L, 5) shortens L to 0000, 100111, 01 .)
Some other precomputed functions on bitstrings will be needed below; we will give brief
deﬁnitions as necessary.
§6.26 Development.
We begin by spending O(m + n) time to create the vectors Iv at
the leaves v of B, as follows. We iterate through the m queries MaxEdge(u, v). For each
query, we set Iu [lca(u, v)] and Iv [lca(u, v)] to 1, indicating which half-paths we must track.
(This requires a prior computation of the least common ancestor lca(u, v) for each of the m
queries. Such a (batch) computation can be done in total time O(m + n) [15].)
We now spend an additional O(n) time creating Iv for all internal nodes v, from the bottom
of the tree upward. For each v, we bitwise-or together the already-created I vectors of v’s
children, since v will have to maintain running maxima on behalf of all its children. We then
clear bit i of the result to say that v need not maintain the heaviest edge from itself. Each
bitwise-or operation would require iterating over log n bits of Iv if we were not using lookup
tables. With a lookup table for bitwise-or, we iterate over just three pairs of bitstrings in O(1)
time each, as described above. Clearing a bit in Iv using a lookup table similarly takes time
O(1). Thus, we spend O(1) time on each of the O(n) internal nodes.
Having created the I vectors bottom-up, we now create the C vectors top-down, at each
node v determining Cv in O(lg |Cv |) time from its parent Cp . The pseudocode below recasts the
method of §6.17 to use the new data structures. The presentation is substantially briefer and
more precise than the one in [9], thanks to the use of variable-length bitstrings. By convention,
italicized functions are precomputed functions on bitstrings, except for the capitalized ones such
as Append (§6.25), which are O(1) functions or procedures that handle sequences of bitstrings.
1.
2.

3.
8

Fast algorithm for
computing each
node’s array of
heavy ancestors

(* Compute a version of Cv that does not take the additional pv edge into account; cf. §6.17. *)
(* Collect those bits of Iv that have corresponding 1 bits in Ip ; call this compacted bit vector I =select(Ip , Iv ). *)
(* Thus I is a parallel array to the compacted array Cp , indicating which bits of Cp should be copied to Cv . *)

I := ε

(* initialize to list containing just the empty bitstring; we maintain pointers to head and tail *)

We know that p ≤ log n. Since

x
lg x

is an increasing function, we have

p
lg p

≤

log n
,
lg log n

which gives the result.

CHAPTER §6. A LINEAR-TIME VERIFICATION ALGORITHM
4.
5.
6.
7.
8.
9.
10.
11.
12.
13.
14.
15.

42

for each batch bIp of Ip and the corresponding batch bIv of Iv
(* by lookup, of course! compacts one batch of Iv
bI :=select(bIp , bIv )
Append (I, bI )
(* as deﬁned in §6.25 *)

as deﬁned in line 2 *)

(* Now copy certain edges from Cp to Cv as directed by I, one batch of Cp at a time. *)
Cv := ε
(* extensible array containing just the empty bitstring *)

for each batch bCp of edge tags in Cp
(* ℓ ≤ length ≤ s; last batch might be shorter *)
ℓ := number of tags in bCp , i.e., length(bCp )/ lg log n
bI :=Head(I, ℓ)
(* ﬁrst ℓ bits of the list I—computed from the ﬁrst two bitstrings in the list *)
RemoveHead (I, ℓ)
(* destructively remove those ﬁrst ℓ bits, as deﬁned in §6.25 *)
(* ∀j, collect the jth tag from bCp if the jth bit of bI is 1 *)
bCv = select-tags(bI , bCp )
Append (Cv , bCv )
(* Update the running argmaxima in Cv with the pv edge, using ≤ lg |Cv | comparisons; cf. §6.17. *)

17.

bpv := bitstring representing the tag of pv
binary-search Cv to ﬁnd jmin = min{j : w(Cv [j]) < w(pv)}

18.

(* Replace all tags from jmin on with the tag of pv *)

16.

19.
20.
21.
22.
23.
24.
25.
26.

(* Cv [j] denotes jth edge tag, not jth batch *)

j := 0
for each batch bCv of edge tags in Cv
(* as before *)
ℓ := number of tags in bCp
j := j + ℓ
(* number of tags seen so far *)
r := min(max(j − jmin , 0), ℓ)
(* number of tags to overwrite at end of bCp *)
(* glue r copies together *)
replace bCv in the array Cv with replace-tail (bCv ,concatcopies(bpv , r))
if testbit(Iv ,distance of p from root)
(* check status of bit telling us whether Av [p] should be deﬁned *)
Append(Cv , bpv )

Ignoring the binary search of line 17 for the moment, every step here is handled with an
O(1) lookup operation. The only cost is looping through batches. We loop through the O(1)
batches of Ip and Iv , the O(log |Cp |) batches of Cp , and the O(log |Cv |) batches of Cv . Recall
that |Cv | ≤ |Cp | + 1. So in short, by the argument of §6.24, the above algorithm creates Cv
from Cp in total time O(log |Cp |)—as promised in §6.23—with the possible exception of line 17.
We turn now to this binary search.
§6.27 Development.
The binary search of the edge tags of Cv must make each compar- Finding edges from
ison with O(1) overhead. One issue is looking up the array elements Cv [j], given that they are edge tags
stored in batches. But this is not diﬃcult. Cv is “really” represented as an array of batches.
We know which batch holds the tag Cv [j]; we simply look up that batch and extract the tag
Cv [j] from it, in O(1) time.
A more serious issue is the need to look up an edge weight on each comparison. Conceptually,
Cv [j] (once we have extracted it) stands for an edge e. However, it is represented as a short
bitstring that has some value d ≤ log n as a binary number. This is not enough to determine
the edge or the edge weight; it merely indicates that e is the dth edge on the root-to-v path.
We could trace up through the ancestors of v to ﬁnd the edge itself—but not in constant time.
Let us develop a constant-time method by successive reﬁnement. One idea is to look up the
edge in a precomputed array. For example, every node of B could store an array of its ancestors,
indexed by their distance d from the root. Unfortunately, these arrays would have O(n log n)
entries total, so would take too long to precompute. We could avoid some duplication by storing
such arrays only at the leaves of B and having each node point to a canonical leaf descendant.
However, this does not work either. Each leaf needs to store its own log n ancestors, for a total
of O(n log n) entries.
The solution is to maintain arrays of diﬀerent lengths at the leaves. No edge will be stored
in more than one array. We will partition the edges of B into n disjoint paths of various lengths:
each path ascends from a leaf part-way to the root, and its remaining ancestors are stored on
other paths. The leaf stores an array of the edges on its path, indexed by the edge heights. To
ﬁnd an edge quickly, we need to know both its height and which path it is on. Then we can
look it up in O(1) time at the appropriate leaf.

CHAPTER §6. A LINEAR-TIME VERIFICATION ALGORITHM

43

King doubles the size of edge tags so that they store not only the height of the desired edge,
but also a hint as to which path the edge is on. (We can now ﬁt only half as many edge tags
into each each s-bit batch, so we need twice as many batches to store Cv , and the loops in §6.26
take twice as long.)
King’s scheme is modiﬁed from [15]. We will label all nodes of B with integers in [1, n]. Label
the leaves 1 through n in the order encountered by depth-ﬁrst search. Thus the descendant
leaves of any internal node v are labeled with consecutive integers. One (and only one) of these
integers k has maximum rank, and the internal node v inherits the label k. The rank of a
positive integer is deﬁned as the highest power of 2 that divides it (e.g., the rank of 28 is 2,
since it is divisible by 22 but not 23 ).
Example:
(8)

4

8

2
1

2

4
3

4

5

6

8

10

12

7 8
9 10 11 12 13 14
The disjoint paths of B are now deﬁned by edges whose lower endpoints share a label. For
example, the “10” path has length 2. Leaf 10 stores its ancestor edges of heights 0 and 1, but
its ancestor edge at height 2 rises from a node labeled 8, so is stored in the array at leaf 8
instead, together with other ancestors of that leaf.
Simply knowing what path a node is on does not let us determine its ancestor nodes, unless
we search the tree. For example, leaf 11 has ancestors numbered 11, 10, 8, and 8, but with a
slightly diﬀerent tree topology, its ancestors would have been 11, 12, 8, and 8. However, we can
determine the ancestors if we also know their ranks. (The only possible ancestors of 11 having
ranks 0, 1, 2, and 3 are 11, 10, 12, and 8 respectively. In the example shown, there is no rank-2
ancestor.) Thus, we identify an edge by its height (which determines the edge) and the rank of
its lower endpoint (which enables us to ﬁnd it quickly).
This method is based on some nice properties of ranks. The ranks of the numbers 1, 2, 3,
4, 5, . . . are 0, 1, 0, 2, 0, 1, 0, 3, 0, 1, 0, 2, 0, 1, 0, 4 . . . Between any two distinct numbers
of the same rank, there is one of higher rank; this is an inductive generalization of the fact
that between any two distinct odd numbers is an even number. We would like to know that a
sequence of consecutive numbers, in particular, the leaves descending from a given node, will
have a unique element of maximum rank. This is true, for if there were two such, an element
of even higher rank would intervene. We would also like to know that if i is in such a sequence,
of which the maximum-rank element is known to have rank r, then that element (call it j) is
uniquely determined. Let j1 ≤ i be maximal of rank r, and j2 ≥ i be minimal of rank r, and k
be the number of higher rank between them. k is known not to be in the sequence, so if k > i
then necessarily j = j1 , and if k < i then necessarily j = j2 .
§6.28 Implementation details.
We represent each node label as a vector of log n bits, Implementing
stored at the node in three batches (bitstrings of length s). Our ﬁrst job is to number the more informative
leaves of B. We can use a lookup table to implement increment(b), which returns the bitstring edge tags
denoting b + 1 together with a 0 or 1 carry bit; then we can increment a leaf label via three calls
to increment in O(1) time. We must also label the internal nodes of B. The rank of a label is
the number of terminal zeroes in its bit vector; we design lookup tables that will compare the
ranks of bit vectors, so that we can label an internal node with the maximum-rank label of its
children.

CHAPTER §6. A LINEAR-TIME VERIFICATION ALGORITHM

44

An edge tag now consists of not one but two bitstrings of length lg log n each, each representing a number in [0, log n). One gives the edge height (determining the edge), and the other
gives the rank of the lower endpoint (enabling us to ﬁnd it quickly). The rank-r ancestor of a
node with label i is found by removing the ﬁnal r + 1 bits of i and replacing them with a 1
and r zeroes. If Cv contains an edge tag height, rank , we use the label of v together with rank
to discover the label of the lower endpoint of the desired edge. We then look up the edge by
height, in the array at the leaf of that label.
§6.29 Development.
Finally, as promised in §6.23, we must revise the method of §6.26 so Use pointers
that deriving Cv from Cp takes time only O(lg |Cv |), not O(lg |Cp |). The O(lg |Cp |) component whenever we don’t
of §6.26 is the extraction of tags from Cp to copy to Cv , for which we iterate in batches through have time to copy
Cp , guided by the parallel bit array I.
Recall that there are always fewer batches per array (O(lg log n)) than tags per full-sized
batch (Θ(log n)). Suppose that the child Cv is more than one batch long. Then |Cp | (the
number of tags in Cp ) is less than |Cv | times as long as |Cv | itself, whence lg |Cp | < 2 lg |Cv |—so
we have nothing to ﬁx. Moreover, if the parent Cp is one batch or less, then again there is
nothing to ﬁx because the procedure is only O(1). Thus, the problem only arises in the context
of deriving a small child Cv from a big parent Cp . Here “small” has the technical meaning “no
more than one batch’s worth of tags.”
King’s technique is quite clever. Before creating Cv , we determine how many edge tags it
will contain, using a lookup function that counts the number of bits in Iv . If Cv will be longer
than one batch of tags, it is called small, otherwise big. The crucial case is where a small Cv
must be created from a big Cp . Specially for this case, rather than iterate through all of Cp to
extract a small list of edge tags as speciﬁed by Iv (§6.26, line 13), we use I simply to create a
list of “pointers” to the relevant tags in Cp . For example, if Cp is conceptually a list of the edge
tags 3, 5, 8, 9, and 10 (denoting edges at these distances from the root), and I is the indexing
bit vector 10100, then instead of storing copies of the edge tags 3 and 8 in Cv we will store 0
and 2, which indicate at which positions of Cp the real edge tags can be found.
Pointers are the same size as edge tags, namely ≤ log n as integers or lg log n bits as
bitstrings. It is a simple exercise with lookup tables to create our list of pointers directly from
I, in time at most proportional to the number of batches in the resulting list, i.e., O(lg |Cv |) as
desired.9 Cp need not be consulted at all.
Like edge tags, pointers to edge tags can be used to look up edge weights in constant
time. This lookup just takes two operations rather than one, the additional operation being
a “dereference”—an array access into Cp in which one bitstring is used to index another.
However, it is important to keep track of when a ﬁeld is to be interpreted as a pointer requiring
dereference, rather than as an edge tag! Our list of pointers Cv will have its tail overwritten
with genuine edge tags (§6.26.18), and additional edge tags may be appended to it (§6.26.26).
Therefore, at every node v of B, we maintain an integer numpointersv that indicates how many
(perhaps 0) of the initial O(lg log n)-bit ﬁelds in Cv are to be interpreted as pointers rather
than edge tags. (An alternative solution does not use numpointers, but rather extends each
ﬁeld with a type bit that indicates whether it is a pointer.) We also have v store p in a variable
pointeev . This tells us where to go to dereference pointers in Cv .
Now that small Cv has been created from big Cp , what happens to the pointers at Cv ’s
descendants?
• To propagate a small parent list down to a small child list, we follow the usual procedure
of §6.26. That is, we copy the selected ﬁelds of the parent, in batches, without regard
to whether they are edge tags or pointers. This is the same procedure that is used for
9

One solution (not the only one) involves a lookup table whose entries are lists of Θ(lg log n) bitstrings. Each
entry takes longer than O(s) time to compute, but the table can still be easily precomputed in O(n) time.

CHAPTER §6. A LINEAR-TIME VERIFICATION ALGORITHM

45

propagating big lists to big lists. The only twist is that we must set the pointee of the
child to that of the parent, and compute numpointers at the child according to how many
of the parent’s pointers were selected.
• Small lists can also have big descendants, thanks to the accumulation of new edges as we
descend through the tree (§6.26.26). These big descendants may have small children. If
we are not careful, we will end up with multiple levels of pointers, damaging our O(1)
edge lookup. Therefore, when we create a big child Cv from a small parent Cp , we ﬁrst
ˆ
create a version Cp of the parent in which all the pointers have been dereferenced. Then
ˆ
we create Cv from Cp in the usual way. In this way, big lists never contain any pointers.
ˆ
How do we create Cp ? We certainly do not have enough time to dereference all the pointers in
Cp individually, nor is there any apparent way to dereference in batches; but we must somehow
use batches to get the dereferenced versions. Suppose it is an ancestor Ca to which the pointers
ˆ
in Cp point. (Ca is a big list and contains no pointers itself.) We create Cp from Ca just as
if p were the child of a rather than an arbitrary descendant, using Ip and Ia to decide which
ˆ
ˆ
entries of Ca to propagate to Cp .10 However, when creating Cp , we only run the algorithm of
ˆp , it would not be enough to take just
§6.26 up to line 15. To complete the construction of C
one new edge into account: we must take account of all edges on the path from a down to p.
Fortunately this is work we have already done. The tail of Cp that consists of edge tags, rather
than pointers, records precisely the set of updates involving those edges. We therefore delete
ˆ
all but the initial numpointersp ﬁelds of Cp , and replace them with this tail, which starts at
position numpointersp in Cp .

10

Notice that this involves copying a big list to a small list—which takes more time than we have to spend on
a small list (the reason for using pointers in the ﬁrst place). We are allowed to do it here because we are creating
ˆ
Cp in the process of creating a big list Cv .

Chapter §7

A linear-time randomized algorithm
Karger, Klein, & Tarjan [8] have exhibited a Las Vegas algorithm that solves the MST problem
in expected linear time O(m). Indeed, the algorithm violates a certain O(m) bound with only
exponentially small probability p = e−Ω(m) .
This result is remarkable, as no linear deterministic algorithm is known. It suggests that
the MST problem is not intrinsically bound by the cost of Union-Find (assuming there is no
randomized linear algorithm for that problem). In the same way, the Fredman & Tarjan result
[5] showed that MST was not bound by sorting (§5.2).
The original presentation in [8] (given in terms of coin-ﬂips) is an admirably clear introduction to the algorithm. It is largely for the sake of variety, and some extra perspective, that I
organize my exposition diﬀerently. I also oﬀer some remarks about how the algorithm might
have been inspired and how it exploits randomness.
§7.1 Remark.
Karger et al.’s algorithm uses randomness in the service of divide-and- Randomized divide
conquer. A strength of randomized algorithms is that they can give us a reasonably even & conquer
division of a large problem into subproblems, with high probability. This is necessary for
divide-and-conquer strategies to work well.
Classic examples are Quicksort and Order Selection, in which we partition a set of m numbers according to whether they are greater or lesser than some pivot value. The problem must
then be solved recursively for one or both sets of the partition. For this strategy to achieve
good time bounds, we must stay away from extreme pivot values, so that both the partition
sets will be appreciably smaller than the original (i.e., size < cm for some ﬁxed c < 1) and we
therefore do not have to recurse too many times. However, simple deterministic strategies for
choosing a good pivot can be foiled by an adversary. Randomized versions of these algorithms
[12, p. 15] choose the pivot randomly from the set, thereby assuring a fairly even division on
average.1 As we will see in §7.13, Karger et al. use randomness to partition the set of graph
edges in a strikingly diﬀerent way.
§7.2 Motivation.
One might attempt to construct an MST deterministically by suc- Building an MST
cessively revising an initial guess. Use DFS to ﬁnd an arbitrary spanning tree T of G. Now by successive
iterate through the edges of G − T . If an edge uv is heavier than MaxEdge(u, v) in T (as revision
deﬁned in §6.1), uv cannot be in the MST and we discard it; otherwise we use uv to replace
MaxEdge(u, v), which cannot be in the MST. To see correctness, observe that each of the
m − (n − 1) iterations discards one edge that cannot be in the MST. At the end of the loop, T
consists of the only n − 1 edges that we have not discarded, so it is the MST.
1

Randomization is not essential for pivot selection, since there is a complex but asymptotically O(m) deterministic algorithm that will select the median [1]. In the case of MST, randomization is essential for all we know
at present.

46

CHAPTER §7. A LINEAR-TIME RANDOMIZED ALGORITHM

47

The diﬃculty with this algorithm is that T keeps changing, so it is hard to answer the
MaxEdge queries rapidly. One response is to modify the algorithm so that we do not change
T at all until after the loop through edges of G − T , and then make a batch of changes. The
next section explores this approach.
§7.3 Motivation.
King’s veriﬁcation method (described above in §6) makes MaxEdge Batching queries
queries eﬃcient if T is held constant: the queries can share processing. If King’s method is using King’s
asked whether a spanning tree T is the MST of G, it does not merely answer yes or no. The
outer loop of the algorithm (see §6.1) also computes some rather useful information that can
help us revise T into the correct MST, M . In particular, it identiﬁes all edges uv ∈ T that are
heavier than MaxEdge(u, v) in T . Such edges cannot possibly be in M .2
If T = M , then King’s algorithm will rule out all edges of G−T in this way, thereby verifying
that T = M . If T is close to M , then King’s algorithm will still rule out most edges of G − T .
We can then conﬁne our search for M to the edges that have not been ruled out (including
those in T ). Indeed, we can ﬁnd M simply by computing the MST of the few surviving edges.
To exploit this idea, we must be able to ﬁnd a spanning tree T that is “close to correct,” a
problem to which we now turn.
§7.4 Scenario.
Imagine that for the sake of authenticity, we have implemented Kruskal’s Kruskal’s with
1956 algorithm on equally venerable hardware. The machine always manages to creak through random errors
the edges of the input graph G from lightest to heaviest, correctly determining whether each
edge e completes a cycle in the growing forest F . If so, e is called heavy (with respect to this
run of the algorithm), otherwise light. Whenever the algorithm ﬁnds a light edge e, it attempts
to add it to F using a procedure Add. Unfortunately, due to an intermittent hardware failure,
each call to Add has an independent 1/2 chance of failing and not adding anything to F . Note
that whether or not a given light edge is added may aﬀect the classiﬁcation of subsequent edges
as light or heavy.
§7.5 Discussion.
How fault-tolerant is this system? Some fraction of the light edges False positives, not
were eﬀectively rendered invisible by hardware failures. Thus, the device has actually executed false negatives
Kruskal’s algorithm perfectly, but on the wrong graph: it has found the MST of G′ = G −
{invisible light edges}. More precisely, since G′ might not be connected, what the device has
found is its minimum spanning forest (MSF).3 We write M (G′ ) for the MSF of G′ , and likewise
for other graphs.
When a light edge of G is rendered invisible through a hardware error, the classiﬁcation of
subsequent edges is aﬀected. However, this eﬀect is entirely in one direction: some edges outside
M (G) are “incorrectly” classiﬁed as light. That is, if a light edge is not added, a subsequent
edge not in M (G) but across the same cut may also be deemed light. But no edge inside M (G)
is ever classiﬁed as heavy. Indeed, each time the faulty machine leaves a heavy edge e out of
F , it is justiﬁed in doing so by a reliable witness that e ∈ M (G). This witness takes the form
of a cycle in G (in fact in F + e ⊆ G′ ⊆ G) on which e is heaviest.
2

The successive revision algorithm suggested in §7.2 would also make good use of the remaining cases, where
uv ∈ T is lighter than MaxEdge(u, v) in T , indicating that MaxEdge(u, v) ∈ M . King’s algorithm obviously
tells us about those, too: that is, in addition to identifying edges of G − T that cannot be in M , it also identiﬁes
some edges in T that cannot be in M either. However, Karger et al. ignore this property because it is not in
general very helpful at identifying bad edges of T . Even if all T ’s edges are wrong, and G − T has many edges
and hence many queries, it is possible for every query to pick out the same bad edge of T . This bad edge never
gets deleted while we are considering T , because we are holding T constant, and since there may therefore be
only one such edge there is little point in deleting it at the end of the pass.
3
Deﬁned as the spanning forest of G′ having minimum weight, among just those forests with as many components as G′ .

CHAPTER §7. A LINEAR-TIME RANDOMIZED ALGORITHM

48

§7.6 Discussion.
Thus Kruskal’s algorithm is quite useful even on faulty hardware. It Kruskal’s with
lets us eliminate all but about 2n edges as candidate edges of M (G), the rest being exposed random errors is
as heavy and unsuitable during the construction of F . The only edges not eliminated are the an edge ﬁlter
visible and invisible light edges, i.e., the edges sent to the faulty Add routine.
§7.7 Justification. Why won’t there be many more than 2n light edges? Because for F
to ﬁnd 3n, or 4n, or 5n light edges, Add must randomly fail on an inordinate number of these
edges. After all, once Add has randomly succeeded n − 1 times (if it ever does), F is a tree
and all subsequent edges are heavy. So the number of light edges we actually send to Add is
certainly fewer than the number we need to get n successes, which is a random variable with
a negative binomial distribution NegBinn,1/2 . This variable has expected value 2n and low
variance.
§7.8 Development.
Thus, if we run an input graph through our antiquated Kruskal’s Recovering the
device, we may not get quite the correct result, but we do manage to reduce the problem result of the ﬁlter
drastically. All that remains is to ﬁnd the MST of the ≈ 2n visible and invisible light edges
that the device did not explicitly reject.
Of course, to solve the reduced problem of ﬁnding the MST on light edges, we must ﬁrst
identify which ≈ 2n edges were light. The device—a notional black box—produces only F , a
forest of the ≤ n − 1 visible light edges. It does not output any indication as to which of the
other edges of G were heavy, and which were invisible light edges.
Fortunately we can reconstruct this distinction after the fact, given F , in O(m) time. An
edge uv ∈ F will have been rejected as heavy just if the part of F that the device had constructed
before considering uv already had a u-v path. Equivalently, uv ∈ F is heavy iﬀ F has a u-v
path consisting of lighter edges. That is, uv ∈ F is heavy iﬀ it is heavier than MaxEdge(u, v)
in F . King’s veriﬁcation method (§6) is precisely suited to sift through all the edges of G − F
and determine which ones have this property.
§7.9 Implementation detail.
The above is exactly the application of King’s algorithm Handling
foreshadowed in §7.3. We produce a tree F ⊆ G that is close to correct (using a black box disconnected
equivalent to the randomized Kruskal’s method), use it to exclude many edges of G in O(m) graphs
time via King’s algorithm, and then ﬁnd the MST of the surviving edges.
There is just one hitch: because of the use of randomness, the F we produce is in general
not a tree but a forest. Indeed, not even G need be connected: the recursive call in §7.12.2
below may request the MSF of a disconnected graph. But King’s algorithm expects F to be
connected.
A straightforward remedy has O(n) overhead, for a total time of O(m + n). (This does not
damage the analysis in §7.16, even though for disconnected input G it may not be O(m).) We
simply connect the components of F before passing it to King’s algorithm. Pick one vertex in
each component of F , using depth-ﬁrst search (O(n)), and connect these vertices into a path
¯
by adding O(n) new, inﬁnitely heavy edges. We use King’s algorithm on the resulting tree F
¯ for each of the O(m) edges uv of G − F . Note that when we use
to ﬁnd MaxEdge(u, v) in F
this procedure in §7.8, any (ﬁnite) edge of G that connects components of F will be correctly
classiﬁed as an invisible light edge, and hence a candidate for the true tree.4
§7.10 Discussion.
Alas, running our device (or simulating one on modern hardware) is How to implement
random Kruskal’s?
expensive. It means running Kruskal’s algorithm, with all the sorting overhead that implies.
One might try to mimic the output of the device (i.e., ﬁnd F ) using a diﬀerent algorithm.
4

This method seems simpler than using “an adaptation” of the veriﬁcation method of [4] or [9], as Karger et
al. suggest. They note that [4, p. 1188] describes such an adaptation, but do not give one for King’s method [9].

CHAPTER §7. A LINEAR-TIME RANDOMIZED ALGORITHM

49

The device ﬁnds the MSF of G′ = G − {invisible light edges}. We could try to do the same by
ﬁnding G′ and recursively ﬁnding its MSF. But it is hard to see how to ﬁnd G′ without running
Kruskal’s to identify the light edges; and even if we can ﬁnd it, computing its MST is nearly as
hard as computing the MST of G (since G′ is nearly as big as G), so we have not reduced the
problem much.
§7.11 Development.
Fortunately, there is a clever indirect way to ﬁnd M (G′ ) without MSF of a random
′ itself! We will strike our device a precise blow with a crowbar, damaging it further, subgraph
ﬁnding G
so that each heavy edge is now also ignored with independent probability 1/2, just like light
edges. But this blow really has no discernible eﬀect. Heavy edges were never added to F even
when seen; they have no inﬂuence on the behavior of the algorithm, so it makes no diﬀerence
to ignore some of them. So the stricken device still ﬁnds M (G′ ).
In short, Karger et al.’s insight is that by making every edge of G invisible with independent
probability 1/2, we obtain a graph
H = G − {invisible light and heavy edges} = G′ − {invisible heavy edges of G′ }
that has the same MSF as G′ does. The MSF is the same because we can eliminate any number
of non-MSF edges from G′ (or any graph) without changing its MSF. Yet H is much easier to
work with than G′ . We can easily ﬁnd H in time O(m) with the help of a random bit generator,
since there is no need to diﬀerentiate between light and heavy edges. Moreover, H is expected
to have only half as many edges as G, so it is cheap to recursively ﬁnd the MSF of H.
§7.12 Development.
of an arbitrary graph G.
1.
2.
3.
4.
5.
6.

We have now sketched the following algorithm for ﬁnding the MSF Recursion with
Boruvka
˙
contraction

Construct H by randomly selecting edges of G with probability 1/2.
Recursively ﬁnd F , the MSF of H, where H has expected m/2 edges and need not be connected.
¯
Connect F ’s components into a tree F by adding inﬁnitely heavy edges.
¯ , querying edges of G to ﬁnd those too heavy to be in M (G).
Run King’s algorithm on F
Remove these unwanted edges from G, leaving expected 2n or fewer edges.
Recursively ﬁnd the MSF of this pruned G, and return it.

We must specify termination conditions for the recursion: say we return G if m = 0. But
the resulting algorithm cannot be quite right, for two reasons. First, m does not decrease
rapidly to 0: the second recursive call is always on a graph where m ≈ 2n, so the algorithm
gets stuck on that value of m and recurses more-or-less forever. Second, the algorithm never
actually discovers any edges: the termination condition means that only the empty graph can
ever be returned!
To solve these problems, we begin every recursive call by contracting the argument graph
G along some edges of its MSF: we make two passes of Boruvka’s algorithm (§4.7). We then
˙
proceed with steps 1–6. The contractions force n to drop by a factor of at least 4 with every
level of recursion. In particular, the algorithm no longer gets stuck at m ≈ 2n, since n keeps
falling. Our termination condition is now that we return the argument G if n = 1. In line 6,
rather than return the MSF of the contracted graph G, we “undo” the two contractions so that,
as desired, we are returning the MSF of the graph that was passed to us. To do this we replace
each vertex of the contracted MSF with the tree of order ≥ 4 that was contracted to form that
vertex.
§7.13 Analogy.
Before we turn to the complexity analysis, let us ask: What is the Understanding
advantage that randomness confers on us here? One way of formulating the MST problem “blind split” on a
is that we are given m distinctly weighted edges and wish to ﬁnd the smallest n, subject to simpler problem
the constraint that they not cause cycles. It is instructive to compare several strategies for a
simpliﬁed, graph-free version: given m distinct weights, ﬁnd the smallest n.

CHAPTER §7. A LINEAR-TIME RANDOMIZED ALGORITHM

50

O(m log m) “Kruskal’s”: Sort all the weights, then take the smallest n.
O(m + n log m) “Prim’s”: Insert all m weights into a Fibonacci heap, in amortized O(1)
time each; or simply construct a binary heap of the m weights in a single O(m) pass.
Then perform n Extract-Min operations, in amortized O(log m) time each. (Note that
the real Prim’s algorithm “improves” this to O(m+n log n), though that is asymptotically
the same, by exploiting the graph structure and Decrease-Key to limit the heap to n
rather than m elements.)
O(m): We ﬁnd the nth smallest weight using a recursive procedure. (Then we ﬁnish up by
scanning the dataset once to ﬁnd all smaller weights.) The procedure is to pick a pivot
element that is expected to be close to the median, partition the dataset about this
element, and then recurse on the appropriate half of the partition. We can choose the
pivot either randomly or deterministically in O(m) time (see §7.1).
O(m) “Karger et al.”: As above, we will ﬁnd the nth smallest weight. Randomly select
roughly half the dataset, selecting each of the m weights with independent probability
1/2 (cf. §7.12.1). Recursively ﬁnd the nth smallest weight in this half-set (cf. §7.12.2).
Now remove from the original dataset all weights larger than this (cf. §7.12.4–5). What
remains is the smallest 2n (on average) weights. The ﬁnal step is to ﬁnd the nth smallest
of these (using the previous method, so that the algorithm terminates).5
Notice how the last two solutions diﬀer. Both recurse on about a set of size m/2. The
ﬁrst solution carefully splits the dataset into two very diﬀerent halves, so that the desired
element is known to be in one half and the other half can be deﬁnitively eliminated. This would
presumably be diﬃcult for the MST problem. The second solution blindly splits the dataset
into two very similar halves, so that the answer on one half gives a good guess as to the answer
on the other half.
The ﬁrst solution does additional work up front in order to support its careful split. In
particular it carries out O(m) comparisons against a pivot element (which in the deterministic
algorithm is itself chosen by a small recursion). Only then does it recurse. By contrast, the
second solution does its additional work at the end. Once it has recursed on the ﬁrst half, it
deploys O(m) comparisons to “ﬁlter” the second half using the answer to the ﬁrst half.
What does “ﬁlter” mean here? Light sets of weights chase out heavy weights, and light
paths chase out heavy edges. Put another way, if we identify the weight of a set or path with
its heaviest element or MaxEdge:
ˆ
• Any set S of n weights establishes an upper bound on the lightest such set, S. The lighter
ˆ
the set S, the stronger the bound it provides. In particular, every element of S is bounded
ˆ
by max(S). Heavier weights cannot be in S, which we wish to ﬁnd.
ˆ
• Any path Φ from u to v establishes an upper bound on the lightest such path, Φ. The
ˆ
lighter the path Φ, the stronger the bound it provides. In particular, every edge of Φ is
ˆ
bounded by the maximum edge of Φ. Heavier edges cannot be on Φ, which (by the cycle
property of §3.3) is the unique u-v path in the MST, which we wish to ﬁnd. It follows
that if uv is such a heavier edge, it cannot be in the MST.
5

To make the algorithm stand on its own (merely as a point of interest), we would like to search these ﬁnal
≈ 2n weights recursively. I see no analogy to the Boruvka step introduced for this purpose in §7.12. A diﬀerent
˙
strategy to make n decrease is as follows. When recursing on the half-set, we ﬁnd not the nth but the (2n/3)th
smallest weight. As before, we search the full set to ﬁnd all the weights smaller than this. We now recurse on
these k smallest weights to ﬁnd the (n − k + 1)th largest weight, where on average k ≈ 4n/3 and n − k + 1 ≈ n/3.
Or if we are unlucky enough to have k < n, we try again with a new division into half-sets. For n above some
small constant c, the chance of k < n is small enough that the cost of such retries is tolerable. Our base cases
for the recursion include not just n = 1 but rather any case n ≤ c; for these cases, we bottom-out with the
O(m log n) strategy given above, which takes O(m) time since n ≤ c.

CHAPTER §7. A LINEAR-TIME RANDOMIZED ALGORITHM

51

The Karger-style solution is to locate a set of n weights that has a relatively small maximum,
or a tree of ≈ n edges whose paths have relatively light maxima. We locate this set or tree in
one half of a blind partition, recursing on a size-m/2 problem, so that we know it provides a
strong enough bound to chase away the rest of that half. We then spend linear time (courtesy
of King’s algorithm) using it to chase away most of the other half. Since the two halves are
similar, the set or tree should be an equally strong bound on both halves, and chase away all
but about n elements of each. This leaves us with only a shrunken, size-2n problem.
§7.14 Remark.
This “blind split” strategy may generalize to other problems. The essence Edge competitions
of the strategy: some comparisons are more useful than others, so recurse on part of the input for eliminating
in order to ﬁgure out what comparisons are likely to be most beneﬁcial on the rest of the input. edges
This leads to the question: For the MST problem, which comparisons are the useful ones?
We know from §6.19 that an MST can be determined with few comparisons, if we are lucky
enough to choose the right ones.
Let us say that two edges compete if they are known to be the two heaviest edges on some
cycle. The Strong Cycle Property (§3.3) can be restated as follows: “Non-edges of the MST
are exactly those that are heavier than some competitor.” (Equivalently, “Edges of the MST
are exactly those that are lighter than all of their competitors.”)
It is always useful to compare two competing edges directly: for in such a direct competition,
the loser (heavier edge) can immediately be ruled out of the tree. Assuming that a loser
never competes again, m − (n − 1) direct competitions necessarily determine the MST. Other
comparisons are necessary to ﬁnd the competitors.
King’s algorithm is more gladiatorial than most. It is set up speciﬁcally to stage direct
competitions between edges in a given tree and edges outside it. For every edge e outside
the tree, King uses average O(1) comparisons to ﬁnd a worthy competing edge e′ in the tree,
and then compares e with e′ . (Determining this competing edge is analogous to determining
max(S), the maximum of a known set of weights, in §7.13.)
Karger et al.’s algorithm stages exactly the same competitions, but in the service of constructing an MST rather than verifying one. They ﬁnd F from one half H of the dataset, and
then use the edges in F to knock out most edges (or so they hope) from G − H, the other half.
Typically F contains some bad edges as well as good ones. Useful comparisons are just those
direct competitions in which the edge from G − H loses to the edge from F .6
Intuitively, why should the winning edges F from the ﬁrst half of the dataset tend to yield
useful comparisons in the second half? Put diﬀerently, why should we expect cycles from
F ⊆ H to suﬃce to witness the bad edges in G − H? After all, many of the cycles in G are not
completely contained in H, so the witnesses we need may not be available.
Suppose e ∈ G − H is not in the MST, but F does not witness that fact. Then if we
had run our original Kruskal’s device (§7.5), e would have been one of those non-MST edges
“mistakenly” classiﬁed as light (but not added). We concluded in §7.7 that there could not
be more than about n such edges, because if the device considered many light edges it would
quickly ﬁnd a tree. (We might have to consider many edges, but not light ones.) Let us translate
this into a story about G − H. Suppose a particular light path in the MST is needed to witness
the badness of e ∈ G − H, but part of this path is missing from H and hence from F . If there
are many edges like e that rely on the path, at least one of the lightest such will surely appear
in H. It will be added to F , where it will help complete a similar path that can rule out other
such edges, like e ∈ G − H. On the other hand, if there are few edges like e, then not much
harm is done if we fail to rule them out, since there are not many.
6
If the edge from F happens to lose, then we have gained nothing, since we do not eliminate it from consideration. More precisely, the ﬁrst time that an edge from F loses a direct competition, we have gained information
that we do not use. The next time it loses a direct competition, we have not gained even that—and the same
edge in F may lose many times during a call to King’s algorithm. See footnote 2.

CHAPTER §7. A LINEAR-TIME RANDOMIZED ALGORITHM

52

§7.15 Remark.
Direct competitions are eﬃcient, but are by no means the only way to Alternatives to
discover that an edge e is heavier than one of its competitors e′ . There are also indirect ways edge competitions
of discovering this, and thereby eliminating e from the MST:
• If Kruskal’s algorithm uses Quicksort, it may discover that e > p and that p > e′ . But p
is just a pivot element used for partitioning, and in general competes with neither e nor
e′ .
• When Boruvka’s algorithm scans for the lightest edge e′ incident on a vertex, its compar˙
isons are not in general direct competitions. They merely prove by contradiction that if
e′ has any competitors it can beat them.
• When Prim’s algorithm eliminates all but the lightest edge in a bucket, its comparisons
are not in general direct competitions. (One of the edges being compared is indeed
the heaviest edge in the cycle, but the other need not be the second-heaviest.) The
same applies when Boruvka’s algorithm eliminates all but the lightest edge between two
˙
vertices.
§7.16 Analysis.
I use m, n to denote a call of the MSF function on a graph with size The binary tree of
m and order n. The initial call is m0 , n0 It is clear from §7.12 that a call m, n takes time recursive calls
O(m + n), exclusive of recursive calls. It is not quite right to reduce this to O(m), as [8] does,
since G may be horribly disconnected, with n > m. (After all, it may be a randomly chosen
subgraph on a recursive call.) For this general case each Boruvka pass takes time O(m + n),
˙
not O(m), as do each of lines 1, 3, and 4 in §7.12.
So the total running time is proportional to the total number of edges plus the total number
of vertices over all calls. Following [8], we consider the binary tree of recursive calls: each call
m, n (with n > 1) spawns both a left-child call in §7.12.2 and a right-child call in §7.12.6.
These child calls have at most n/4 vertices apiece, thanks to the two Boruvka passes. So the
˙
call tree has O(n0 ) vertices total. We now consider the total number of edges in the call tree,
which will dominate the analyses.
§7.17 Analysis (expected time on arbitrary input).
This is straightforward. The Expected time
call m, n has a left child of expected size m/2, n/4 or smaller, and a right child of expected O(m)
size n/2, n/4 or smaller. We draw two conclusions:
• At depth d in the tree, there are 2d−1 right children of expected size at most 2n0 /4d , n0 /4d .
• The call m, n together with all its left-spine descendants has at most m + m/2 + m/4 +
· · · = 2m expected edges.
Every call is a left-spine descendant of the root or of a right child. The root and its leftspine descendants expect at most 2m0 total edges. A right child at depth d and its left-spine
descendants expect at most 2 · 2n0 /4d = n0 /4d−1 total edges. Summing these bounds over the
root and all right children, we obtain a bound on expected edges of 2m0 + ∞ 2d−1 n0 /4d−1 =
d=1
2m0 + 2n0 . (Not 2m0 + n0 as stated in [8], which forgets momentarily that right children also
have left descendants.) So the expected time is O(m0 ).
§7.18 Analysis (probability of running in O(m0 ) time).
The nub of the argument Actual time O(m)
is that a call m, n is very unlikely to have a left child with many more than m/2 edges (since exponentially likely
each edge is selected with probability 1/2), or a right child with many more than 2n edges (see
§7.7). “Very unlikely” means e−Ω(m) and e−Ω(n) respectively, by Chernoﬀ bounds.
However, there are many recursive calls in the tree, so some of them will probably violate
such bounds. Moreover, calls m, n with small m or n—of which there are many—are not so

CHAPTER §7. A LINEAR-TIME RANDOMIZED ALGORITHM

53

unlikely to violate the bounds anyway. For this reason we do not attempt to prove edge bounds
for individual calls to the MSF function. Rather we consider the total number of edges in right
children, and show that it is O(m0 ) with probability 1 − e−Ω(m0 ) . We then extend this analysis
to cover the total number of edges in all calls.
First consider right children. Each right child consists of the visible and invisible light edges
encountered when running the faulty Kruskal’s device on (a contracted version of) its parent.
That is, it is a forest consisting of the visible light edges, plus about the same number of invisible
light edges. The number of visible light edges is limited by the space available for forests on
right children: the right children have only d 2d−1 n0 /4d = n0 /2 ≤ m0 vertices total, so there
are at most this many visible light edges. If we encountered more than 3m0 light edges while
running the device, then Add must have succeeded less than 1/3 of the time in its ﬁrst 3m0
edges. The probability of this is e−Ω(m0 ) .
Let m′ ≥ m0 be the total number of edges in the root and all right children. We have just
shown that m′ ≤ m0 + 3m0 = 4m0 except for e−Ω(m0 ) of the time. By considering the left-spine
descendants of the root and right children, we will now see that the total number of edges
is ≤ 3m′ except for e−Ω(m0 ) of the time. These results together imply that with probability
1 − e−Ω(m0 ) , the total number of edges is at most ≤ 12m′ .
Each left child consists of just the visible light and heavy edges encountered when running
the device on (a contracted version of) its parent. Put another way, it is a random subgraph of
the contracted parent (see §7.12.1). The root, or a right child, gives rise to a sequence of left
descendants through successive subgraph operations. Each of the m′ “source edges” in the root
and right children is selected on some number of these subgraph operations before it is unlucky
enough to be contracted or explicitly not selected (i.e., invisiblized).
Suppose the left children contain more than 2m′ edges, so that we have more than 3m′
edges total. (This is the event we are claiming is unlikely.) Then the average source edge was
selected by left children more than twice before it was contracted or not selected. Since we
actually selected 2m′ edges, we must have made at least 2m′ selection decisions (though not
necessarily 3m′ , as [8] claims, because of the possibility of contraction). Moreover, at least 2/3
′
of such decisions came out in the aﬃrmative. The probability of this happening is only e−Ω(m ) ,
and a fortiori e−Ω(m0 ) .
§7.19 Analysis (worst-case running time).
Karger et al. show that the algorithm Worst-case O(m ·
has the same asymptotic worst-case bounds as Boruvka’s algorithm. For Boruvka’s algorithm, (1 + log(n2 /m))),
˙
˙
˙
those bounds derive from the fact that the number of edges on pass d is bounded both by like Boruvka’s
d )2 and by m (§4.11). [8] shows straightforwardly that the total number of edges at
(n0 /2
0
depth d in the Karger et al. call tree also meets these both bounds.

Chapter §8

A dynamic algorithm
§8.1 Motivation.
Imagine that we must frequently broadcast messages on a communi- A changing
cations network (a preindustrial example was sketched in §2). The network is represented by communications
a graph G. Each edge of G represents a bidirectional communications link, which is cheap or network
expensive to use according to the weight of the edge. To broadcast a message from v ∈ G to
all vertices of G, we send it along the edges of a spanning tree in the obvious way. Using the
minimum spanning tree minimizes the cost of the broadcast.
Edges may change weight often, due to hardware problems or ﬂuctuating system load.
Frederickson [6] describes a data structure that can eﬃciently maintain the MST under such
weight changes. Given the initial MST, the data structure may be initialized in time O(m);
√
each weight update then takes O( m) time. Frederickson’s scheme extends to handle edge
√
insertions and deletions, also in O( m) time. (Note that true edge deletions reduce m, speeding
subsequent operations. For this reason they are preferable to the “pseudo-deletion” of increasing
edge weights to ∞.) We will add this feature in §8.18.
Frederickson makes the intuitions fairly clear at each step of his exposition, but the series
of steps is complicated (consisting of successive revisions) and the data structures are unfamiliar. I have attempted to compress and clarify the ideas, in part by accomodating the ideas
to the notation and data structures of §4.3. These data structures diﬀer only slightly from
Frederickson’s ﬁnal structure, a “two-dimensional topology tree.” I have also attempted to ﬁll
in some details that appear to be missing in [6]—in particular, the implementation of the data
structures and some details of the bookkeeping.
§8.2 Overview.
Our main problem is to maintain the MST of a graph G when edge Operations to
weights change dynamically but the topology does not change. More precisely, the data struc- support
ture we maintain will always be some subforest of the current MST, since in the course of dynamically
manipulating the MST while weights are changing, we may temporarily remove an edge.
So long as we are dealing with forests, I generalize the problem so that G need not be
connected. Thus the structure we are maintaining is a subforest of the MSF. We will construct
a representation for such a forest, F ⊆ G, that supports the following operations (as well as
the ability to look up the weight of a given edge):
Test(e): Determine whether an edge e is in F .
IncWeight(e, ∆w): Increase by ∆w the weight of edge e ∈ G − F , but without changing F .
Remove(e): Remove e from F (without changing G). This increases the number of components
of F .
Add(e): Add e to F (without changing G). This decreases the number of components of F .
MaxEdgeIn(u, v): Return the heaviest edge on the u-v path in F .
54

CHAPTER §8. A DYNAMIC ALGORITHM

55

MinEdgeOut(u, v): Return the lightest edge of G − F that connects u’s component in F to
v’s component in F .
We are now able to increase the weight of edge uv by ∆w, while maintaining the invariant
that we always store a subforest of the correct MSF:1
1.
2.
3.
4.
5.
6.
7.
8.
9.
10.

if Test(uv) and ∆w > 0
(* edge in F getting heavier *)
Remove(uv)
(* disconnects a component of F *)
IncWeight(uv, ∆w)
Add(MinEdgeOut(u, v))
(* reconnects F with some edge, possibly uv again *)
elseif ¬Test(uv) and (∆w < 0) and (w(uv) + ∆w < w(MaxEdgeIn(u, v)))
Remove(MaxEdgeIn(u, v))
IncWeight(uv, ∆w)
Add(uv)
else
IncWeight(uv, ∆w)

(* edge out of F getting lighter *)

§8.3 Development.
Our representation of F ⊆ G really consists of two data struc- Dynamic trees plus
tures, each of which represents F in its own way. One structure is responsible for answering a new data
MaxEdgeIn queries; it supports all the operations except for MinEdgeOut. The complemen- structure
tary structure is responsible for answering MinEdgeOut queries; it supports all the operations
except for MaxEdgeIn. Modiﬁcations to F or G (i.e., Add, Remove, IncWeight) are carried
out on both structures simultaneously.
The ﬁrst structure represents the forest F via the dynamic trees of Sleator & Tarjan [14],
a paper that I will not dwell on here. The dynamic tree structure treats F as a set of vertexdisjoint, edge-weighted, rooted trees that support several O(log n) operations. So far as we are
concerned, the choice of root in each tree is arbitrary. Indeed, Sleator & Tarjan provide an
operation evert(u) that lets us make u the new root of its tree.
It is trivial to implement the operations of §8.2 using dynamic trees. Our Test(uv) is true
just if v = parent(u) or vice-versa. Remove(uv) is implemented in the dynamic tree structure
as cut(u) if v = parent(u) and cut(v) otherwise. Add(uv) is implemented as evert(u) followed
by link (u, v, w(uv)). MaxEdgeIn(u, v) is evert(u) followed by maxcost(v), which ﬁnds the
maximum edge on the path from v to its new root u. In the same vein, IncWeight(uv, ∆w)
is evert(u) followed by update(v, ∆w), which increments all weights on that path.
The second structure must be able to ﬁnd the lightest edge across a cut, in order to answer
MinEdgeOut. In the worst case there are O(m) such edges. We want to make sure we are
never in the position of considering all those edges individually. Frederickson’s approach is to
group most of the edges into bundles (related to the buckets of §3.10), each of which maintains
its minimum edge. Then to ﬁnd the lightest edge across a cut (MinEdgeOut), we only have
to consider the minima of the bundles crossing that cut. We now develop this idea in detail,
completing Frederickson’s method.
§8.4 Strategy.
Let us begin with a sketch, deferring details and implementation. Suppose we partition the vertices of G into clusters of intermediate size. Given distinct clusters Ci
and Cj , we keep track of G’s lightest edge (if any) from Ci to Cj . Conceptually we are dealing
with bundles of the form {edges from Ci to Cj }.2 But we need not maintain a list of all the
edges in each bundle, only the minimum edge of each bundle.
To ﬁnd MinEdgeOut(u, v), we look at all bundles that cross the cut. We only have to
consider the minimum edge of each bundle; so we can ﬁnd the minimum edge across the cut
1

In the data structure of §8.8 below, this invariant ensures that Fi is a subgraph of Gi , where Gi has eliminated
all but the lightest edges between various subtrees of F .
2
Edges within a cluster Ci are not in any bundle, unless for aesthetic reasons we choose to maintain the
bundle of edges from Ci to itself. Maintaining such a bundle is unnecessary, since we never need to know the
minimum edge within Ci .

Dividing F into
vertex clusters
connected by edge
bundles

CHAPTER §8. A DYNAMIC ALGORITHM

56

more quickly than if we looked at every edge. Speciﬁcally, we consider bundles that run from
any cluster in u’s component to any cluster in v’s component. For this to make sense, we must
choose our vertex clusters in G so that each does fall within a single component of F . More
strongly, we will explicitly choose them by dividing F into subtrees. In general, each component
of F will be divided into several subtrees, which remain linked to each other by edges of F .
How long do our operations now take? The expensive operations are MinEdgeOut and
Remove. MinEdgeOut clearly requires time O(number of clusters2 ). If Remove(e) removes
an edge of F that was within some cluster Ci , we must split Ci into Ci1 and Ci2 so that it does
not straddle two components of F .3 All edge bundles from Ci to other clusters are destroyed;
we must create new bundles from Ci1 and Ci2 to these other clusters. We do so by iterating
through all edges of G incident on vertices of Ci , bucketing them into new bundles of which we
maintain the minima. This requires time O(degree of Ci ), where the degree of a cluster is the
total number of edges in the adjacency lists of its vertices.
The other operations are less critical and do not dominate update time. Add(e) comes for
free; it merely marks some edge between clusters as being in F . (Notice that given our use of
Add in §8.2, e is always minimal in its bundle.) IncWeight(e, ∆w), an operation Frederickson
does not mention, requires us to recompute the minimum edge in e’s bundle, which may have
changed. Suppose e runs from Ci to Cj . A brute-force solution is to examine all edges incident
on vertices of Ci and keep a running minimum of those that lead to Cj . This has complexity
no worse than that of Remove.
§8.5 Rough Analysis.
These implementations of MinEdgeOut and Remove exert Choosing an
opposing forces on cluster size. MinEdgeOut prefers that we have few clusters (so that intermediate
searching the set of bundles is fast). Remove prefers that we have many clusters, each with cluster size
only a small number of vertices (so that splitting is fast) and a low degree in G (so that
rebundling is fast). Of these two criteria for Remove, low degree is the stronger one: a low
degree guarantees an equally low number of vertices, since a large cluster with few incident
edges could not be connected in F .
This tension between few clusters and low cluster degree is best resolved by having O(m1/3 )
clusters, each with degree O(m2/3 ) in G, for a total of 2m directed edges. Then MinEdgeOut
and Remove now take O(m2/3 ) time apiece. These operations determine the time cost of our
data structure’s fundamental update operation, the edge-weight update in §8.2. We will soon
improve this basic result to O(m1/2 ). We postpone till §8.11 the question of how to ﬁnd and
maintain the balanced partition into clusters.
§8.6 Remark.
The edge bundles here resemble the buckets used in Prim’s algorithm
(§3.10). Those buckets maintain the edges that would result from contracting the single growing
Prim’s tree, if we deﬁne contraction to eliminate all but the lightest edge from every set (or
“bundle”) of multiple edges. In the same way, Frederickson maintains the edges that would
result from contracting various subtrees of F (namely, those induced by the vertex clusters). So
Frederickson is applying Prim-style bucketing to a Kruskal-style forest of simultaneous trees!
The goal is the same: to cheapen subsequent searches for a light cut-crossing edge.
To elaborate, both Frederickson’s and Prim’s algorithms maintain k trees and the minimum
edge between each pair of these trees. (In the case of Prim’s, most of these O(k 2 ) edges are the
3
This split operation is the reason for requiring each vertex cluster to induce a subtree of F . If the subtree
loses an edge, we can run DFS on it to determine the split rapidly (in time proportional to the number of vertices
in the cluster).
What is the alternative? In principle, we could have chosen each vertex cluster to be a set of vertices scattered
throughout a component of F . In that case, removing an edge of F might have split not one but several clusters
across the new component boundary. It would have been prohibitively expensive to determine which clusters
were split and how the vertices were to be allocated to the new clusters.

Dynamic
algorithms need a
balanced data
structure

CHAPTER §8. A DYNAMIC ALGORITHM

57

unique edges between isolated vertices.) For the static algorithm, Prim’s, we do not yet know
which of the O(k 2 ) edges between trees will be in the MST; while for the dynamic algorithm,
Frederickson’s, which already has a tree but is maintaining it, such edges are marked as to
whether they are in the current MSF, F . The key operation that the two algorithms share is
to pick a light edge from across the O(k 2 ) minimum edges that they have been maintaining
between trees, in order to bridge a cut in the current forest.
Given this similarity, why do the two algorithms choose their trees so diﬀerently? Frederickson insists on a “balanced” forest in which all the trees are about the same size and have
about the same degree. Prim is at the other extreme, one large tree and many isolated vertices.
Indeed, none of the static algorithms insist on a balanced forest. Although Fredman & Tarjan
ﬁnd it advantageous to limit the degree of Prim’s trees grown using the heap, and Boruvka tries
˙
to grow all trees in the forest in parallel, both those algorithms allow trees to run into each
other and merge as they grow. The result is that some trees may end up much bigger than
others.
There is a deep diﬀerence between static and dynamic algorithms that add edges according
to the Strong Cut property (§3.3): a static algorithm can choose which cuts it will search across,
whereas a dynamic algorithm is at the mercy of its caller. In a dynamic graph, any edge could
vanish or (equivalently) become very heavy and get removed, creating an arbitrary cut in F
that the algorithm must repair. In the worst case there are k/2 trees linked together by edges
of F on each side of the cut, so the algorithm must consider k 2 /4 bundles in order to ﬁnd
the lightest crossing edge. A static algorithm such as Prim’s beats this performance by always
bridging the cut between a single tree and the other k − 1 trees, meaning that there are only
k − 1 bundles for it to consider. That is why the static algorithm is faster.
Absolute performance aside, why does the dynamic case favor equal trees and the static case
favor unequal ones? In Frederickson’s algorithm, any of the k trees (subtrees of F induced by
the vertex clusters) may be split if one of its edges is removed. It is more expensive to split large
trees than small ones, because a large tree has more incident edges that have to be rebundled
after splitting. (Remember, we are maintaining the minimum edge between each pair of trees;
see the discussion of Remove in §8.4.) So for the sake of worst-case performance we avoid
having any large trees. Thus dynamic graphs strongly favor equal trees. Static graphs have no
strong preference for tree size, so long any edge they add is across an cut that separates one
tree from all the others. The reason that Prim’s uneven forest turns out to work well is that
since Prim’s algorithm uses essentially the same cut every time it adds an edge, it does not have
to search anew through the O(n) cut-crossing bundles for every edge it adds.4 Rather it can
take advantage of the fact that the cut does not change very much—using a heap, which is just
a data structure that rapidly takes account of small changes, percolating minima to the top in
log m time. We now turn to a similar idea of Frederickson’s—a kind of heap that percolates
minima to the top in the more diﬃcult case of arbitrary dynamic changes to the graph.
§8.7 Strategy.
The solution of §8.4 reduces the search of all cut-crossing edges to a Recursively
search of only the bundle-minimal ones—still a considerable problem. Fortunately it is a smaller bundling edges,
problem of the same form. Hence we can repeat the solution, creating clusters of clusters in heap-style
order to group the bundle-minimal edges into their own bundles. When an edge weight in G
changes, say, the change propagates up through successive levels of bundles, much as in a heap.
4

An alternative strategy for avoiding such an O(n) search is Boruvka’s algorithm (§4.8), which moves from
˙
tree to tree, making a diﬀerent uneven cut every time. This has the quite diﬀerent advantage that it can do a
single sweep through all m edges and choose n edges across n diﬀerent cuts, at least n/2 of the chosen edges
being distinct. It thereby takes advantage of the fact that only O(m/n) edges will cross an average uneven cut:
it takes time O(m/n) to pick each edge on the ﬁrst pass (and then twice as long on each successive pass, since
n drops). By comparison, Prim’s algorithm would take O(n) rather than O(m/n) per edge if it were not for the
heap, which brings the per-edge cost down to O(log n).

CHAPTER §8. A DYNAMIC ALGORITHM

58

We may think of this idea as contracting the clusters used in §8.4 and then recursing. We
adapt the notation and data structures of §4.3–§4.6.
§8.8 Development.
Let G0 be the current state of the dynamic graph, and F0 ⊆ G0 its Initial build of the
minimum spanning forest, represented in our usual way (§4.4). Initial versions of these objects recursive data
are given to us, and we use them to build our initial data structure from the bottom up. When structure
updates cause us to change F0 and G0 , the changes will propagate from the bottom up. The
resulting structure is one that could have been built in the ﬁrst place from the revised versions
of F0 and G0 .
Given Gi and Fi , we can initially construct Gi+1 and Fi+1 from scratch as follows. We
partition Fi evenly into subtrees of size ≈ zi+1 , for some number zi+1 ≥ 2, in a manner to be
described in §8.11. We then repeat our contraction maneuver from §4.4:
1.
2.
3.
4.
5.
6.
7.
8.

(* create Gi+1 , Fi+1 from Gi , Fi *)

Vi+1 := ∅
for each subtree T in the partition of Fi
(* diﬀers from §4.4, which contracted entire components *)
add a new vertex v to Vi+1 , with empty adjacency lists
ˆ
C(ˆ) := ∅
v
for each v ∈ T
v
v
P (v) := v ; C(ˆ) := C(ˆ) ∪ {v}; ΓGi+1 (ˆ) := ΓGi+1 (ˆ) ∪ ΓGi (v)
ˆ
v
v
(* was not necessary in §4.4, which contracted all edges of Fi so Fi+1
v
v
ΓFi+1 (ˆ) := ΓFi+1 (ˆ) ∪ ΓFi (v)

had none *)

We follow this with an O(mi ) step that eliminates self-loops from Gi+1 and Fi+1 , and eliminates
all but the lightest edge from every bundle of multiple edges. (See §4.12 for one implementation,
√
a radix sort with two passes. If ni ≤ m, as Frederickson arranges throughout by putting
√
z1 ≥ m, a one-pass O(n2 ) bucket sort will suﬃce to preserve the analysis.)
i
We repeat these contractions until we arrive at a graph FN that is trivial (that is, mN =
0, nN = number of components of F0 , so every vertex is isolated). Since ni falls by about zi ≥ 2
on each pass, N = O(log n).
Pass i takes time O(mi ). The total time for the initial construction is then O( N −1 mi ),
i=0
which as we will see is O(m) given appropriate choices of the zi .
§8.9 Implementation details.
There is an important diﬀerence between this data struc- Storage of edges
ture and the one that §4.1 used for discovering spanning trees: F0 , F1 , F2 , F3 are all genuinely
contractions of the completed spanning forest F . In particular, F0 = F itself, and since Fi+1 is
merely the result of contracting some edges of Fi and eliminating some of the multiple edges
that remain, all the edges of Fi+1 are contained in Fi . The subtrees of Fi that we will contract
(“vertex clusters”) are not in general separate components of Fi .5
We maintain all the graphs G0 , F0 ; G1 , F1 ; . . . GN , FN simultaneously. Within each graph,
we want our adjacency list to specify neighbors in that graph. Therefore we represent adjacency
lists as in the O(mi ) merge of §4.6. There is one extra twist: the entry for uv in Γ(u) should
specify not only v and the edge, but also point to the entry for vu in Γ(v). This means
that if we delete the edge uv from u’s adjacency list, we can also delete it from v’s, in constant
time. An alternative storage option, which Frederickson essentially adopts though with diﬀerent
terminology, is to use an adjacency matrix in the representations of G1 , G1 , . . . GN (but not G0
or F0 , F1 , . . . FN ). The matrix element M [u][v] holds the edge uv, if any, and null otherwise.
§8.10 Development.
The broad use of the data structure should now become clear: we Trivial to bridge
cuts on the fully
can always answer MinEdgeOut(x, y) as the edge from P N (x) to P N (y) in GN .
5

This is important because for the clustering to be maintained correctly under changes (§8.14), each cluster in
Fi must know its outdegree, i.e., how many other clusters of Fi it is connected to in F . Under our representation,
this can be determined by considering just the adjacency lists in Fi of vertices in the cluster.

contracted graph

CHAPTER §8. A DYNAMIC ALGORITHM

59

FN and GN are the result of completely contracting F0 and G0 over the components of F0 .
For the usual case, suppose that F0 currently holds the MSF of G (and not a subforest thereof).
Then FN = GN = the trivial graph on VN ; each vertex represents a contracted component of
F0 = G0 . If we Remove an edge from F0 and propagate the changes up, a vertex v ∈ VN will
split into two vertices v1 , v2 . This reﬂects the splitting of a tree in F0 into two components with
vertex sets C ∞ (v1 ) and C ∞ (v2 ), as deﬁned in §4.3. These components remain connected in G0 ,
so v1 and v2 are connected in GN by a single edge—namely, the lightest edge of G between the
new trees in F0 .
§8.11 Development.
All that is left to do is to explain how to partition a forest into Exploding and
clusters in a balanced way, and how to ﬁx up the partition rapidly for every change to the partitioning the
graph
forest.
As discussed in §8.5, we would like to limit the degree of each cluster. This is diﬃcult if the
degree of the graph is unbounded: any vertex of degree k must be in a cluster of degree ≥ k.
Frederickson simply assumes that the degree is bounded: speciﬁcally, ∆(G0 ) ≤ 3, where ∆(G0 )
denotes the maximum degree of any vertex (length of any adjacency list) in G0 . If this is not
true, Frederickson notes, the input graph can be transformed in a standard way: each vertex
of degree k > 3 may be “exploded” into a path of k vertices, each of which is assigned one of
the original vertices’ neighbors. The edges in the path have weight −∞ so that they will be
part of the MSF. This graph has at most three times as many edges as the original, and has
m = O(n). Frederickson’s method maintains the MSF of this exploded graph, which supports
the usual queries on the MSF of the original graph, at only a constant-time penalty. Notice that
the graph gets denser with contraction, but even the denser graphs are guaranteed to respect
certain degree bounds: supposing for simplicity that each cluster in Fi contains exactly zi+1
vertices, then ∆(G0 ) ≤ 3, ∆(G1 ) ≤ 3z1 , ∆(G2 ) ≤ 3z1 z2 , . . . ∆(Gi ) ≤ zi · ∆(Gi−1 ).
In particular, ∆(G0 ) ≤ 3 implies that ∆(F0 ) ≤ 3. The method for partitioning Fi−1 into
subtrees expects that ∆(Fi−1 ) ≤ 3, and ﬁnds subtrees (clusters) in such a way that ∆(Fi ) ≤ 3
as well (after contraction). Moreover, the method guarantees that every cluster is connected
and has from zi to 3zi − 2 vertices, with the exception that a cluster with outdegree 3 (meaning
its degree in Fi after contraction) is allowed to have fewer vertices.
The partitioning method, for which [6] gives brief and simple pseudocode, takes time
O(ni−1 ). We partition each component of Fi−1 separately. We root the component of Fi−1
at an arbitrary node, so each node has at most 2 children except possibly the root, which may
have 3. The idea is to repeatedly break minimal legal clusters oﬀ the bottom of this tree. Legal
clusters are deﬁned as rooted subtrees with outdegree = 3 or ≥ z vertices. Every minimal legal
cluster has at most 2zi − 1 vertices, since its two children are too small to have been broken oﬀ
as clusters of their own, i.e., they have ≤ zi − 1 vertices each. (There is one exception: there
may be a minimal cluster of size 3zi − 2 at the root, if the root has three children.)
Once all the minimal clusters have been broken oﬀ, we may be left with an subminimal
cluster of ≤ zi − 1 vertices and outdegree < 3 at the root. If so, we merge this tree with any
cluster adjacent to it, to get a cluster of ≤ 3zi − 2 vertices. It is not hard to see that this cannot
increase the outdegree of the adjacent cluster.
This last step fails if we run the partitioning method on any component of Fi−1 that is so
small it has < z vertices, for in that case, there is no adjacent cluster to merge with. In that
case, we simply put the whole component into one cluster. It is possible that some components
of F shrink down to a single vertex in this way before others, so that Fi contains some isolated
vertices for i < N . (Frederickson’s scheme would treat the components as “topology trees” of
diﬀerent heights.)
§8.12 Development.
Let us now describe how changes propagate up the data structure Repairing the
structure after
(not fully spelled out in [6]).
changes

CHAPTER §8. A DYNAMIC ALGORITHM

60

We process an Add(uv) or Remove(uv) request in three sequential stages:
(a) Go through F0 , F1 , . . . FN in order, and in each, add or remove the edge in question.
For each i, we must modify ΓFi (P i (u)) and ΓFi (P i (v)), unless P i (u) = P i (v). Each
modiﬁcation takes constant time because these adjacency lists have length ≤ 3.
These modiﬁcations may lead to “instabilities” in the clusters containing u and v. Adding
an edge from P i (u) to P i (v) in Fi may leave either or both with degree 4, which is bad.
If P i (u) and P i (v) are in the same cluster of Fi , then removing the edge between them
will leave the cluster disconnected, which is bad.

(b) Go through F0 , F1 , . . . FN in order again, and reorganize the clusters of each Fi to ﬁx up
any instabilities created during stage (a). This involves merging and splitting a constant
number of clusters of Fi , taking O(zi ) time each, as described in §8.14. These merges and
splits in Fi may create additional instabilities in the parent clusters containing u and v
in Fi+1 , but we will ﬁx these up too later in this stage, when we get to Fi+1 .
N may increase or decrease on this pass. If we ever ﬁnish ﬁxing up a graph Fi and discover
that every vertex is isolated, we set N = i and delete Fi+1 , Fi+2 , . . .. Conversely, if we
have ﬁxed up FN only to discover that some vertices are not isolated, we create FN +1 and
GN +1 from FN using the method of §8.8, increment N , and repeat as necessary.
(c) Go through G0 , G1 , . . . GN in order to recompute lightest edges under the new topology.
In each Gi , a constant number of vertices u have been marked as having out-of-date
adjacency lists, because they were newly created or changed their child sets C(u). We
delete the edges in those lists from Gi , and recreate updated ΓGi (u) by scanning and
bucketing all the edges incident on C(u) in Gi−1 . This takes time proportional to the
number of such edges, so is O(zi ∆(Gi−1 )).6
§8.13 Remark. One can optionally interleave the operations of the stages of §8.12. Instead
of running each stage on all the levels 0, 1, . . . N before proceeding to the next, one can run all
three stages in sequence on level 0, then on level 1, etc.
Note also that we can handle IncWeight(uv, ∆w) by running stage §8.12.c alone, applying
it to update ΓGi P i (u) and ΓGi P i (v). Recall that IncWeight is only called on uv ∈ F .
6

Actually, the naive implementation takes time O(zi ∆(Gi−1 ) + ni ), where the O(ni ) term is for maintaining
√
the ni buckets. Frederickson can tolerate this, since his choice of zi values ensures that each ni = O( m)) for
each i ≥ 1. If we wished to deal with some larger, sparser instances of Gi , it would be desirable to eliminate the
ni term. This is possible if mildly tricky:
1.
2.
3.
4.
5.
6.
7.
8.
9.
10.
11.
12.
13.
14.
15.

for each edge uv in ΓGi (u)
(* see §8.9 *)
remove uv from ΓGi (u) and ΓGi (v)
A := a preallocated, reuseable, timestamped array of length n
N := an empty list
(* list of buckets that are actually in use *)
for x ∈ C(v)
for e = xy ∈ ΓGi−1 (x)
v := P (y)
(* so after contraction, the edge is uv in Gi *)
if A[v] is empty
(* i.e., timestamp is out of date *)
add v to the list N
A[v] := e
elseif e is lighter than A[v]
A[v] := e
for v in N
if v = u
add the edge A[v] to ΓGi (u) and ΓGi (v)

(* buckets *)

CHAPTER §8. A DYNAMIC ALGORITHM

61

§8.14 Development.
We now give the details of §8.12.b, which are not fully speciﬁed in Repairing unstable
[6]. Deﬁne a vertex u ∈ Fi , i ≥ 1, to be unstable (or to represent an unstable cluster C(u) of clusters
Fi−1 ) if any of the following conditions hold.
• C(u) is not connected in Fi−1 . (In this case it will always have just 2 components.)
• C(u) contains fewer than zi vertices, and u has degree 1 or 2 (not 0 or 3) in Fi .
• C(u) contains more than 3zi − 2 vertices. (In this case it will still always contain at most
4zi − 3.)
• u has degree > 3. (In this case it will always have degree 4.)
These are exactly the conditions that are avoided by the method for ﬁnding an initial partition
(§8.11); that is, the clusters we form initially are stable.
Suppose u = P i (u0 ) ∈ Fi ; the following procedure will ensure u’s stability, but possibly
leave P i+1 (u0 ) ∈ Fi+1 unstable. So to carry out step §8.12.b, we simply call the following
procedure with u bound to P i (u0 ) and P i (v0 ) in Fi , for each i in turn. Such a call takes time
O(zi ).
1.
2.
3.
4.
5.
6.
7.
8.
9.
10.
11.
12.
13.
14.
15.
16.
17.
18.
19.

if C(u) has two components
(* we’ll split it into two clusters *)
replace u in Vi with two initially isolated vertices u1 and u2
set C(u1 ) and C(u2 ) to be the vertex sets of the components
(* and set backpointers P
set P (u1 ) and P (u2 ) to the former value of P (u)
(* and modify backpointer C(P (u)) *)
determine ΓFi (u1 ) from ΓFi−1 (C(u1 )), and likewise for u2

*)

(* now P i+1 (u0 ) is unstable by virtue of having two components *)
(* also, u1 and u2 may have too few vertices, so fall through to next case *)

if C(u) contains fewer than z vertices and u has degree 1 or 2 in Fi
(* we’ll merge it with an adjacent cluster *)
let w be an arbitrary neighbor of u in Fi
(* exists since u has degree ≥ 1 in Fi *)
merge u with w: C(w) := C(w) ∪ C(u), remove u from P (u) and Vi , etc.
(* merging u into w cannot increase degree of w, since u has degree ≤ 2 in Fi *)
(* now w may have too many vertices, so fall through to next case *)

if C(u) contains 3zi − 1 to 4zi − 3 vertices or u has degree 4 in Fi
(* we’ll repartition it *)
run §8.11 on just C(u) to split it into two clusters C1 and C2
replace u in Vi with two initially isolated vertices u1 and u2
set C(u1 ) := C1 ; C(u2 ) := C2
(* and set backpointers P *)
set P (u1 ) and P (u2 ) to the former value of P (u)
(* and modify backpointer C(P (u)) *)
determine ΓFi (u1 ) from ΓFi−1 (C(u1 )), and likewise for u2
(* u1 and u2 are unconnected, so now P i+1 (u0 ) may be unstable by virtue of having two components *)

The code at lines 9–10 is not quite right, because when P (u) loses u as a child, it may
become unstable by virtue of having too few vertices. Then we have made something unstable
that is (no longer) an ancestor of u0 , and it will not get properly stabilized on the next pass.
We can avoid this problem by choosing w speciﬁcally to be a neighbor of u that is also a sibling.
Then P (u) loses a child u and may become unstable, but it remains an ancestor of u0 through
w so will get ﬁxed when we stabilize Fi+1 . What if there is no sibling, i.e., u is the only vertex
in its cluster? Then we choose w to be a ﬁrst cousin. Then P (u) loses its only child u and we
delete it; so P 2 (u) has lost a child P (u), and may become unstable, but remains an ancestor of
u0 through P (w) and w. What if P (u) has no sibling either, so there is no ﬁrst cousin? Then
we look for a sibling of P 2 (u) so that we can get a second cousin, and so forth. The overhead
of this scanning will vanish in the runtime analysis.
§8.15 Analysis.
Given the partitioning method of §8.11, how much do Fi and Gi shrink Recursive
as i grows? If every cluster in Fi−1 really did have from zi to 3zi − 2 vertices, then |Fi | would structure has
be at most |Fi−1 |/zi . This is not quite the case, since we allow clusters as small as one vertex; O(log n) levels
however, such small clusters are required to use up more than their share of edges (outdegree
= 3), so there cannot be too many of them.

CHAPTER §8. A DYNAMIC ALGORITHM

62

We will quantify this argument. Suppose Fi has only one component. Then each vertex of
Fi has degree ≥ 1, and acyclicity of Fi means the average vertex has degree < 2. It follows that
at most half the vertices can have degree 3. These correspond to clusters in Fi−1 of at least
one vertex each, and the other half have degree < 3 and correspond to clusters of at least zi
vertices each. So the average cluster in Fi−1 has at least (zi + 1)/2 vertices. |Fi | shrinks from
|Fi−1 | by at least that factor. Even if zi = 2 for all i, each graph is at most 2/3 as large as the
one before, which is enough to make N = O(log n).
In the more general case, where Fi has multiple components, each with fewer than n vertices,
the algorithm will be even faster: the components shrink (in parallel) more quickly because they
are smaller, and updates are faster because the degree of a cluster is more tightly bounded. For
the sake of simplicity, we will assume below the worst case of a connected graph.
§8.16 Analysis.
As sketched in §8.12, the runtime for an update to Fi and Gi (i ≥ 1) is O(m) to create,
dominated by the O(zi ∆(Gi−1 )) term in step §8.12.c. Again, this time is proportional to the O(m1/2 ) to update
number of edges emanating from a single cluster of Gi−1 , which edges must be rebundled. We
are also interested in the time to initially create the structure (§8.8): the runtime to create Gi
in the ﬁrst place is ni times as long as to update it (thus O(mi−1 )), since it must bundle the
edges from all clusters of Gi−1 .
To bound these expressions, we may write:
n0 ≤ m

(* since connected; indeed, n0 = O(m), because we forced G0 to have limited degree (§8.11) *)

ni ≤ ni−1 /((zi + 1)/2)

∆(G0 ) ≤ 3

(* again because G0 was forced to have limited degree *)

∆(Gi ) ≤ min(3zi ∆(Gi−1 ), ni )

(* recall 3zi − 2 is maximum size of a cluster in Gi−1 *)

In the ﬁnal inequality, the two bounds are respectively “intrinsic” and “extrinsic” (see §4.11
for such bounds on a pass of Boruvka’s algorithm). The ﬁrst bound comes from keeping track
˙
of the number of G0 ’s vertices that could have been shrunk together into a vertex of Gi , and
counting the original edges of G0 incident on these. The second bound comes from noting that
there are not too many other vertices in Gi for a given vertex to connect to.
Frederickson puts z1 = Z = a largish number to be chosen below, and z2 = z3 = · · · =
zN = 2. Thus the number of vertices in the graph shrinks on the ﬁrst contraction by a factor
of more than Z/2, and on subsequent contractions by a factor of at least 1.5. It follows that
ni < 2m/(1.5)i−1 Z for i ≥ 1; this is a closed-form bound for ni .
Using the extrinsic bound for ∆(Gi ), Gi can be updated in time O(zi ni−1 ), which is
O(4m/(1.5)i−2 Z) for i ≥ 2. As for the case i = 1, the intrinsic bound says that updates to G1
take O(zi ∆(G0 )) = O(Z). The total update time summed over all i is therefore O(Z + m/Z).
√
√
Frederickson chooses Z = m so that this update time is O( m). As for the time to create
Gi , it is O(zi ni ni−1 ) = O(8m2 /(1.5)2i−3 Z 2 ) for i ≥ 2, and O(6m) for i = 1. The total creation
time is therefore O(m + (m/Z)2 ) = O(m).
§8.17 Remark.
The above scheme beats that of §8.5, namely z1 = m2/3 , z2 = m1/3 . Why Frederickson’s
is there not an even better scheme for choosing the zi ? We might hope for faster performance choice of the zi
if we chose z1 to be less and the later zi to be greater, so that the approach looked more like appears optimal
a true recursive solution. Alternatively, the algorithm could be simpliﬁed considerably—but at
what cost?—if we took z1 = z2 = · · · = zN = 2. Then the methods for partitioning (§8.11) and
stabilization (§8.14) could be specialized for clusters of size ≈ 2.
√
It is clear that we cannot improve the O( m) update time without decreasing z1 below
√
m, since the total update time is at least the O(z1 ∆(G0 )) = O(3z1 ) time to update G1 . But
√
decreasing z1 does not give a speedup either. To so much as match Frederickson’s O( m) per
√
update, we need to require that no cluster in any Gi can have more than c m incident edges

CHAPTER §8. A DYNAMIC ALGORITHM

63

(for some c), since we might have to iterate over all those edges. Let Gˆ be the ﬁrst graph
ı
√
ı
in the contracted series to have ≤ c m vertices. (For Frederickson, ˆ ≈ 1; by decreasing z1
we can contract more slowly and raise ˆ.) How many edges are incident on clusters of Gˆ−1 ?
ı
ı
√
The extrinsic bound zˆnˆ−1 > c m by the deﬁnition of ˆ. So our only hope is to rely on the
ı
ı ı
intrinsic bound, which asks how many endpoints of edges of G0 could have ended up within a
cluster of Gˆ−1 . Notice that the average cluster of Gˆ−1 must have 2m/nˆ such edges, which
ı
ı
ı
√
is at least 2 m/c. The worst-case cluster will be worse than that by a factor of up to about
ı
6ˆ: at each contraction, the biggest clusters (size 3zi − 2) can be about 6 times as large as the
ı
average cluster (size (zi + 1)/2). So unless ˆ and hence 6ˆ is a constant independent of m, as in
ı
√
Frederickson’s method, the intrinsic bound will also allow more than c m incident edges for
some cluster of Gˆ. Provided that we cannot ﬁnd any better bounds, we therefore run more
ı
√
slowly than Frederickson—ω( m) time per update—under any discipline where ˆ increases with
ı
m, e.g., z1 = z2 = · · · = 2.
§8.18 Development.
To adapt the method for insertion or deletion of edges, remember Updating the graph
that G0 is an “exploded” version of the input graph (§8.11). To add an edge uv of the input topology
graph, we must add it to the exploded version G0 : we may have to split a vertex in V0 . (We
also add uv to F0 , provided that u and v are in diﬀerent components (i.e., P N (u) = P N (v));
otherwise we simply add it to G0 with inﬁnite weight that we will decrease later.) These
operations may render clusters in F0 unstable; we stabilize them and propagate the changes up
through the Fi and Gi as usual. Edge deletion is handled similarly. Each insertion or deletion
√
takes time O( m).
√
Frederickson notes that as m grows, Z = ⌈ m⌉ might change. The solution is to gradually
carry out such adjustments as are due to changes in Z. We keep a work list of vertices of F1
that have not been stabilized since the last time Z changed. For each edge insertion or deletion,
we take some vertices of F1 oﬀ the work list and stabilize any that are unstable, recursively
stabilizing their ancestors in F2 , F3 , etc. How many vertices do we need to take oﬀ? The work
list starts with length |V1 | = O(m/Z) = O(Z). For Z to increase or decrease by 1, m must
increase or decrease by O(Z), since m ≈ Z 2 . So each time an edge is inserted or deleted, we
only need to take O(1) vertices of F1 oﬀ the work list, and we will have exhausted the work list
by the time Z changes again. (In the meantime, some clusters may be slightly too big or too
small because they reﬂect a value of Z that is oﬀ by one, but that is not enough to aﬀect the
performance analysis.)
Frederickson does not discuss the addition of new vertices, but it follows naturally from
the generalization to maintaining MSFs that I have presented here. Isolated vertices can be
straightforwardly added or deleted by changing all the Fi and Gi , in time O(N ) = O(log m).

Chapter §9

Lessons Learned
Let us close by reviewing some motifs of the work reviewed here. Throughout the paper, I
have tried to motivate each algorithmic technique as a solution to some speciﬁc problem. Some
useful general algorithm speedup techniques emerge:
• Amdahl’s Law: attack the bad term in the complexity. If the algorithm is spending time
on Decrease-Key, for example, then either make Decrease-Key faster (§3.11) or call
it less often (§5.10).
• Try to introduce new degrees of freedom in the algorithm and choose these free parameters
so as to minimize the complexity. (heap size bound—§5.3; packet size—§5.14; cluster
size—§8.5, §8.16)
• Don’t waste time computing information you don’t need. In particular, don’t ﬁnd a
minimum or other partial order by sorting. (§3.13, §4.13, §6.19)
• Postponing work can lead to economies of scale as work piles up. (packets—§5.10; Fibonacci heaps—§A.9; also note the economy of scale from batched queries—§6.18)
• When you may need an expensive piece of information multiple times, store it for later
lookup. (precomputed lookup tables—§6.23; presorted packets—§5.11, §5.19; not to mention memoization and dynamic programming!)
• A fast algorithm for a related task may discover useful information along the way. (greedy
MST locates path-maximal edges—§6.6, §6.14; veriﬁcation locates many such in batch
mode—§7.3)
• Loose invariants for data structures reduce maintenance time when the structure is modiﬁed. (edge tags or pointers—§6.29; variable size vertex clusters—§8.11, §8.14, §8.18;
variable number of children in Fibonacci trees—§A.10, §A.11, §A.12)
• Divide and conquer: the solution to a subproblem may be used in solving the whole
problem.1 (minimum of minima (not limited to heaps)—§3.10, §5.12, §8.7, §A.17; ﬁnd
subtrees of the MST and contract them to reduce the original problem—§4.2; solution to
a random subproblem gives information about the original problem—§7.13, §7.14)

1

The subproblem might be chosen on the ﬂy rather than identiﬁed in advance: e.g., Fredman & Tarjan’s
algorithm (§3.11) grows subtrees of the true MST. Each of these is the minimum subgraph connecting its own
vertices, hence solves a subproblem not known in advance.

64

Appendix §A

Fibonacci Heaps
§A.1 Problem.
A heap or priority queue is a data structure used to maintain a collection Operations
of nodes. Each node appears in at most one heap, and stores some arbitrary data. In addition, supported by heaps
each node exports a key that determines its order relative to other nodes. Keys may be drawn
from any ordered set, such as real numbers.
In the presentation of [3], a heap must support the operations shown below. It is not a priori
obvious that Decrease-Key should be regarded as a separate operation, since it appears to
be equivalent to deleting the node and inserting a version with lower key. One innovation of
Fibonacci heaps is that they show that Decrease-Key can be made faster than Delete; this
is crucial for good performance on Prim’s algorithm (§3.11).
Make-Heap() : Return a new heap with no nodes.
Insert(x, h) : Add node x to heap h, modifying h.
Minimum(h) : Return a minimal node of heap h.
Extract-Min(h) : Delete a minimal node from heap h and return it.
Union(h1 , h2 ) : Create and return the union of two heaps, destroying the originals.
Decrease-Key(x, k) : Notify node x’s heap that node x’s key has changed to k.
Delete(x) : Delete node x from its heap.
[3] gives pseudocode for three kinds of heap—the standard binary kind, the binomial heaps
of [17], and the Fibonacci heaps that Fredman & Tarjan [5] introduced for the MST problem.
As the table below (adapted from [3]) shows, the asymptotic performance on a heap of n nodes
gets steadily better with these innovations, although the constant factors involved get steadily
worse.
Binary heap Binomial heap Fibonacci heap
(worst-case)
(worst-case)
(amortized)
Make-Heap
O(1)
O(1)
O(1)
Insert
O(lg n)
O(lg n)
O(1)
Minimum
O(1)
O(1)a
O(1)
Extract-Min
O(lg n)
O(lg n)
O(lg n)
Union
O(n)
O(lg n)
O(1)
Decrease-Key
O(lg n)
O(lg n)
O(1)
Delete
O(lg n)
O(lg n)
O(lg n)
a

Trivially improved from [3]’s O(lg n). See §A.2.

65

APPENDIX §A. FIBONACCI HEAPS

66

This appendix oﬀers the reader a more intuitive description of how and why Fibonacci heaps
work, and which details of their implementation are critical. We will begin with their predecessors, binomial heaps. (It is assumed that the reader already possesses a good understanding
of binary heaps.)
§A.2 Simplification.
Some heap operations can be implemented in terms of others. For
all three kinds of heaps, it is easy to implement Delete(x) in O(lg n) time in terms of the
other operations: we Decrease-Key node x sharply, so that it becomes the minimum node
and bubbles up to the top of the heap, and then call Extract-Min to remove it. So the time
for Delete is dominated by Extract-Min.
For binomial and Fibonacci heaps, Insert(x, h) is implemented by creating a singleton
heap holding x—an O(1) procedure we may call Make-Singleton—and Unioning it into h.
Thus Insert takes the same time as Union.
Finally, it is easy to implement Minimum in O(1) time, given a procedure Locate-Min
that does the same in O(lg n) time. (The O(1) Minimum operation is useful if we repeatedly query the minimum of the same heap without doing any other O(lg n) operations in
between; §5.18 depends on this O(1) behavior for Fibonacci heaps.) All that is necessary is
for the heap h to maintain a pointer m(h) to its minimum node, which Minimum can then
look up. Speciﬁcally, Make-Heap initializes m(h) to reference a dummy node of inﬁnite
weight; Insert and Decrease-Key replace it whenever they create a smaller node on the
heap. h = Union(h1 , h2 ) essentially sets m(h) to min(m(h1 ), m(h2 )). Finally, Extract-Min
would ordinarily call Locate-Min and delete the minimal node thus found; we switch the order of these operations so that it ﬁrst deletes the known minimal node m(h), and then uses
Locate-Min to set m(h) to a new minimal node that subsequent operations can use.
Since Make-Heap and Make-Singleton are always trivial, our main concerns will be Union,
Extract-Min (which calls Locate-Min), and Decrease-Key.

Union,
Locate-Min, and
Decrease-Key
are central

§A.3 Binomial heaps.
Binomial heaps were developed from binary heaps to improve the A short list of
time of Union. The problem is easily solved—by ﬁat!—if the heap consists of a list of binary heap trees
heaps (i.e., heap-ordered trees) rather than just a single such tree. Then Union is merely a
matter of list concatenation. Unfortunately, if the list gets too long we have problems: not with
Union, which can be kept fast with (say) circular lists, but with Extract-Min, which must
search through all the trees to discover which has the smallest minimum. In the extreme case,
every tree has just one node and this search (called Locate-Min above) examines all n nodes.
We therefore wish to ensure that the list contains only O(lg n) heap trees. The secret is to
control the size of the trees on the list. If n = 10110two = 16 + 4 + 2, then the list will have
trees of sizes 2, 4, and 16 (in that order). Essentially a binomial heap of n elements is stored
as a linked list of ≤ lg n heap trees. To merge two such lists under Union (or its special case
Insert), we essentially do binary addition to get a new list in the correct format.
For example, suppose we have two heaps of sizes 22 and 23, necessarily represented as lists
of trees of sizes 2, 4, 16 and 1, 2, 4, 16. We merge them (` la mergesort) into a list 1, 4, 8,
a
32, starting at the units place, in a manner that is parallel to the binary addition problem
22 + 23 = 45:
1
4
1/
1
8
11
//
10110 2, 4, 16
101// 4, 16
10
10/// 16
110
1//// 16
0110
1011/ 2, 4, 16
0
−→
−→
−→
−→
10111 1, 2, 4, 16
101// 4, 16
11
10/// 16
111
1//// 16
0111
1011/ 2, 4, 16
1
1 1
01 1
101 1, 4
1101 1, 4, 8
1 //
11
32
1 11
/ //
10110
/////
10110
/////
−→
−→
10111
/////
10111
/////
01101 1, 4, 8
101101 1, 4, 8, 32

APPENDIX §A. FIBONACCI HEAPS

67

To handle the case of a carry, we must be able to link two trees of size 8 (say) into one of
size 16, in constant time. Moreover, if we later Extract-Min the root from the tree of size
16, we need to organize the remaining 15 elements into trees of sizes 1, 2, 4, and 8 that can be
Unioned in with the rest of the heap.1
To make these operations fast, we use binomial trees instead of binary trees. A binomial
tree of 16 = 24 nodes, denoted B4 , has the topology of a B3 with an extra B3 attached to the
root. Given two B3 trees, it takes only constant time to link them together into a B4 while
preserving the heap property, by attaching the root of larger key to the root of smaller key.
Furthermore, the children of the root of B4 are B0 , B1 , B2 , and B3 , so we can simply Union
these back into the heap if we happen to delete the root.
B4 = B3 with another B3 attached as its rightmost child
=

B4

B0

B1
B0

B2
B0

B1
B0

B3
B0

B1
B0

B2
B0

B1
B0

Now Extract-Min can be implemented with two O(lg n) calls: to ﬁnd the minimum
(Locate-Min) we scan the roots of all the ≤ lg n trees, and to extract a known minimum at a
root, we delete the root and Union its ≤ lg n children back into the heap. Decrease-Key(x, h)
may be handled within x’s tree in the usual manner of (not necessarily binary) heap trees, by
bubbling x up the tree as necessary. This too takes time O(lg n), since Bk contains nodes at
depth k − 1.
§A.4 Implementation details. We implement the binomial heap h by connecting all the
tree roots into a singly-linked list, called the root list, ordered so that the size of the trees
increases (strictly) as one moves down the list. Such ordered lists enable Union to use the
“binary-addition” procedure.
It is convenient to represent each node’s children in the same way, by linking them into a
child list of identical format, using the same pointer ﬁelds. In this way, when a root node of h
is deleted (via Extract-Min), its child list may be regarded without any ado as the root list
of a new heap h′ , which we fold back into h simply by calling Union(h′ , h).
It is also convenient for each child list to maintain a tail pointer as well as a head pointer.
Thus, when two instances of Bk are linked together, one can be appended in O(1) time to the
child list of the other’s root.2
Each node in the heap records its degree. In particular, the binary addition procedure can
immediately identify a tree as Bi (corresponding to a 1 in bit i of the binary representation of
1

One might instead try to replace the deleted root with an element of a smaller tree, and then restore the
heap property on the tree, but this is not possible if there is no smaller tree on the list.
2
Neither of the above convenient choices is actually essential. Imagine that child lists were stored in a diﬀerent
format from root lists, and that they allowed only prepending (not appending) of new children. Then we would
have to maintain a node’s children in decreasing rank order. When Extract-Min needed to call Union(h′ , h),
the heap h′ would have to be constructed by reversing and reformatting a child list. Since no sorting is required,
this would need only O(lg n) time, which Extract-Min could aﬀord.

APPENDIX §A. FIBONACCI HEAPS

68

n), by noting that its root has degree i. We will refer to the degree of a tree’s root as the rank
of the tree.
§A.5 Remarks.
The largest tree on the binomial heap is necessarily Blg n−1 , which has Binomial trees
2lg n−1 = 2⌈log(1+n)⌉−1 = 2⌊log n⌋ nodes. We may regard the root list as the child list of an balance depth and
“overall” root node that holds the minimum of the whole heap. This root node is essentially breadth
m(h) of §A.2, and is a copy of its smallest child. In this way we regard the heap h as a single
heap-ordered tree T (h). If n = 2k − 1, so that the trees on the heap are B0 , B1 , . . . Bk−1 , then
T (h) is simply Bk .3
The above picture makes it clear how binomial heaps balance the number of trees (small
for Locate-Min’s sake) against the depth of the trees (small for Decrease-Key’s sake). The
heap is in some sense just a single balanced tree T (h), maintaining its minimum in just the
same way as an ordinary binary heap. The only diﬀerence is that a node may have more than
two children. In particular, T (h) is roughly a binomial tree, which is deﬁned to balance depth
and breadth: a property of binomial trees is that the root of a height-d subtree has d children.
(T (h) relaxes this property very slightly in that the root may have fewer children.)
To better understand how binomial heaps diﬀer from binary heaps (on operations other
than Union), let us imagine a generalized version of the binary heap, where nodes of the heap
tree may have more than two children. (T (h) is such a tree with added restrictions, which we
ignore here, on its form and root node.) Decrease-Key(x, k) is just as for a binary heap:
it bubbles node x up the heap in the usual way, placing it into its ordered list of ancestors `
a
la insertion sort. This takes time proportional to the depth of the tree. A binary-heap-like
implementation of Extract-Min would replace the root with an arbitrary node x and then
restore the heap property by forcing x down the tree, repeatedly exchanging it with its smallest
child. But this requires a search for the smallest child at each level. Such a search may take
more than constant time if nodes have high degree. For example, if the tree is shaped like Bk ,
this version of Extract-Min can take up to Θ(k 2 ) = Θ((lg n)2 ) steps.
Fortunately Extract-Min allows a cheaper O(lg n) method that turns the possibility of
high degree into a beneﬁt. The smallest child x of the deleted root becomes the new root. x
keeps all its existing children, and it also acquires the other children of the deleted root as new
children. In the event that this has burdened x with too many children, we can now distribute
the burden by reattaching some of the children to each other rather than to x.
For binomial heaps, the binary-addition discipline arranges for exactly such reattachments.
It eﬀectively connects the children of x into a set of trees that respect the heap property. The
tallest possible such trees are chains of the form “Bi reattaches to Bi which reattaches to Bi+1
which reattaches to Bi+2 which reattaches to Bi+3 which stays attached to x.” Notice that the
trees that hang lowest in such a chain (viz. Bi ) are also the shortest trees, so that the heap
remains balanced. It is instructive to contrast what would happen if we applied this ExtractMin method in the case of binary trees, which are more balanced than binomial trees. Here the
new root x would have two old children plus a new one, making three, an intolerable number.
3

Indeed, if we were lucky enough to have n in the form 2k , we could simply store the binomial heap as a single
Bk and implement Minimum in O(1) time by treating the root node as m(h). For other values of n, we could
still use a single binomial tree, padding it with up to n − 1 extra nodes of inﬁnite weight. However, the details
of tracking the padding nodes make this construction much clumsier than that of §A.3.
How, for example, can we rapidly handle the transition from n = 2k (stored as Bk with no padding) to
n = 2k + 1 (stored as Bk+1 with 2k − 1 padding nodes, a much larger tree)? We could borrow Frederickson’s
“work list” technique (§8.18): maintain a separate tree consisting entirely of padding nodes, such that if the
main heap Bk has 2k − i occupied nodes, this separate tree has 2k − 2i nodes overall and is a “partially grown”
Bk . If Bk becomes fully occupied, the separate tree then has the size and shape of a second Bk ; we combine it
with the main heap to create Bk+1 , and start a new separate tree with 0 nodes. To go backwards if this Bk+1
becomes only half-occupied, it is necessary to split oﬀ an entire Bk -shaped tree of padding nodes. To make this
work, we must arrange for padding nodes to be segregated in their own child trees of the root. The child trees
consisting of real nodes must therefore be maintained just like a standard binomial heap, only more clumsily.

APPENDIX §A. FIBONACCI HEAPS

69

One child would therefore be reattached to another, but this would just shift the problem one
level down the tree, so that eventually one of the three child subtrees of x would reattach to a
leaf. The resulting tree would be quite unbalanced!
§A.6 Precursor to Fibonacci heaps.
It is possible to improve the time of Union Deferred
(hence also Insert) even further, to O(1). We do so by abandoning the strict discipline of consolidation of
binomial heaps, where the trees on the list have distinct sizes prescribed by n, and fall in a binomial trees
particular order. Instead, we represent the heap as an arbitrary, unordered list of binomial
trees that may contain duplicates. Now Union is just list concatenation. This is O(1) if we
use circular lists or lists with tail pointers, just as suggested at the start of §A.3.
To deal with the problem that long lists handicap Extract-Min, we perform a consolidation step at the start of each Extract-Min, which turns the long list back into a proper
binomial heap. This will take more than Θ(lg n) time, of course. The time is proportional
to the length of the list—but in an amortized analysis, the extra time can be charged to the
operations that lengthened the list beyond lg n trees in the ﬁrst place, such as Insert. So
Extract-Min can still ﬁnd the minimum in only O(lg n) amortized time.
The consolidation step is like adding a long list of binary numbers to a running total, where
each number on the list is a power of 2. A diﬀerent perspective is that it resembles bucket
sort, just as the “binary addition” step of §A.3 resembles one pass of mergesort. We set up lg n
buckets to hold trees of ranks 0, 1, 2, 3, . . . lg n−1, i.e., trees of the form B0 , B1 , B2 , B3 , . . . Blg n−1 .
Each bucket can hold just one tree.
We now consider the k trees on the heap in their arbitrary order, attempting to throw each
tree into the appropriate bucket. A Bi tree wants to go into bucket i. If bucket i already
holds another Bi , however, it cannot hold the new tree as well. In this case we need a “carry”
operation: we withdraw the second Bi from the i bucket and link it together with the ﬁrst Bi
to form a Bi+1 . We then attempt to put this new tree into bucket i + 1 in exactly the same way
(iterating as necessary if that bucket is also full). Once we have disposed of the tree we were
holding, we proceed to the next tree on the heap until we have disposed of all k. To complete
the consolidation, we make a list of the ﬁnal b ≤ lg n bucketed trees to obtain a proper binomial
heap.
§A.7 Analysis. How long did this consolidation take? Initialization and termination were
O(lg n). There were k − b linkages, each of which reduced the number of trees by one; so
bucketing the k trees took time O(k) plus O(k − b) for the linkages. We charge O(b) of this
time to the consolidation step, for a total amortized time of O(lg n) including initialization and
termination. The other O(k − b) is charged against the potential of the heap, which is equal to
the number of trees on it. The heap acquires potential from operations that lengthen it: thus
each Insert contributes an additional O(1), and each Extract-Min contributes an additional
O(lg n) for the several trees that it adds to the heap. These contributions do not damage the
analysis of Insert or Extract-Min. Notice that Union adds no potential; the output heap
inherits its potential from the work previously done on the two input heaps.
§A.8 Implementation details. To implement the modiﬁed binomial heap of §A.6, exactly
the same data structures will do as §A.4 used for standard binomial heaps. However, it has now
become crucial for the root list to maintain a tail pointer: this is what lets Union concatenate
two root lists in O(1) time. (Recall that in the case of standard binomial heaps, the only reason
for the root list to maintain a tail pointer was that it was convenient, though inessential, for
root lists to look like child lists and for child lists to maintain tail pointers.)
§A.9 Discussion.
The heaps of §A.6 appear to improve on the standard binomial heaps Postponing work
of §A.3: they reduce Union and Insert to constant-time. But how much of this apparent creates economies
of scale

APPENDIX §A. FIBONACCI HEAPS

70

improvement is merely the result of amortized analysis?
In fact we have not really improved Insert: standard binomial heaps, too, require only
O(1) amortized time for Insert. (Compute potential as above, and observe that the time spent
on a single-node insertion is proportional to the number of carries, i.e., the number of prepaid
tree linkages.4 )
On the other hand, we have genuinely improved Union, not just found a more favorable
analysis. Union of standard binomial heaps can take time Θ(lg n) simply to merge the tree lists.
This time cannot in general be charged to previous operations. To see the problem, suppose
we start with a binomial heap of the form 111111two , and union it with arbitrarily many heaps
of the form 100000two . Every Union wastes time skipping anew over B0 , B1 , B2 , B3 , B4 , which
are never eliminated but always survive till the next Union.5 The time spent on these trees is
genuinely wasted in that it consists only of iterating over them and looking at their ranks; no
keys are compared and no information is gained.
The deferred-consolidation approach of §A.6 actually does less work on this example: the
111111 heap and many 100000 heaps are concatenated in O(1) time per Union, and if a consolidation step is necessary at the end of all these concatenations (e.g., in order to ﬁnd a minimum),
it considers each tree only once. In particular, each of B0 , B1 , B2 , B3 , B4 is considered only once
overall (during the consolidation step), rather than once for each Union.
Thus, the deferred approach of §A.6 takes advantage of an economy of scale: it is more
eﬃcient to postpone work until there is a lot of it. Precisely the same observation was used in
§5.10 to motivate the use of packets in Gabow et al. In that algorithm, we waited to bucket
heavy edges until the number of buckets decreased; in §A.6, we wait to bucket binomial trees
until the number of trees increases. Both postponements serve to increase the number of objects
per bucket or tree.
The analogy is worth making in more detail. The common idea is that all but one object
per bucket can be eliminated permanently in O(1) time; only the single survivor of a bucket
stays around to create more work later. Many objects per bucket is a good strategy, because it
means a high ratio of cheap eliminated objects to expensive surviving objects.
Thus in §5.10, only the lightest edge in each bucket survived to be considered on the next
pass; the others were eliminated from the MST immediately. We may think of such eliminations
as having been charged to an O(m) budget that paid for each non-MST edge to be eliminated
once. Thus, the surplus cost of Gabow et al., in excess of this budget, was determined by the
number of surviving edges over all passes. In precisely the same way, in §A.6, only the minimum
binomial-tree root in each bucket survives as a root, to be examined again by Locate-Min or
future consolidation operations. The other roots are “eliminated” in that they become internal
nodes of the surviving trees, thanks to linkages. These eliminations are charged against the
potential of the tree—a kind of escrow fund whereby each inserted tree pays for itself to be
linked later. Thus, again, the amortized cost of each consolidation is determined by the number
of survivors, which is at most lg n.
To take a diﬀerent perspective, a bucket with s objects results in s − 1 comparisons. If s is
small, a substantial fraction of the bucketing work does not result in any comparisons, so it takes
more work to gain a given amount of information about the MST or the heap minimum. In
particular we want s to average above 1 + ǫ for some ǫ > 0, so that the overhead per comparison
is constant.
§A.10 Fibonacci heaps.
Recall that we obtained the heaps of §A.6 by modifying binomial Decrease-Key
heaps to have constant-time Union. We now obtain Fibonacci heaps by making a further by detaching a
4

More generally, the worst-case actual cost of Union(h1 , h2 ) is O(lg max(|h1 |, |h2 |)), whereas the worst-case
amortized cost is O(lg min(|h1 |, |h2 |)).
5
Obviously, if we tried to pay for this work by drawing on heap potential accumulated from previous operations,
the potential would run out after some ﬁnite number of such Unions.

subtree

APPENDIX §A. FIBONACCI HEAPS

71

modiﬁcation, designed to give us constant-time Decrease-Key(x, k) as well.
Again we can begin with a naive idea. Rather than bubbling the node x of decreased key
up to the top of its tree T , suppose we simply cut it oﬀ from its parent in T . In this way x
becomes the root of a new tree on the heap, where it can be examined by Locate-Min in the
usual way. This detachment takes O(1) actual time (see §A.15). Since it increases the heap by
one tree, it is also charged an additional O(1) to increase the heap potential; this will be used
later to reattach the detached node.
However, T is left stunted by this amputation. Removing arbitrary subtrees means that the
heap no longer consists of binomial trees. We had relied on the fact that trees were binomial
to tell us that they all had rank < lg n—trees of higher rank would have more than n nodes—
and therefore consolidation required only lg n buckets and O(lg n) amortized time. Now that
the binomial property is gone, a succession of key-decreases and consolidations might leave us
with small trees of many more diﬀerent ranks. Consolidation could therefore require far more
buckets and be far slower.
All we really need to ensure is one important property of binomial trees: that a tree with
high rank has many descendants. This ensures that rank stays small relative to n, and there
cannot be many distinct ranks. In the manner of 2-3-4 trees, we will enforce the desired property
while being somewhat more ﬂexible about how many children each node actually has. Instead
of using binomial trees we will use Fibonacci trees: these have the desired property that their
ranks will be O(lg n), but they have somewhat weaker invariants that we can maintain with
less computation.
§A.11 Definition.
Fredman & Tarjan write that they “impose no explicit constraints Fibonacci trees
on the number or structure of the trees; the only constraints are implicit in the way the trees balance depth and
are manipulated” [5, p. 598]. For the sake of understanding, however, it is valuable to state breadth, too
explicit constraints from the start.
Recall that a binomial tree of rank k, denoted Bk , is deﬁned as a root with k ordered child
trees, which are binomial trees of respective ranks 0, 1, 2, 3, . . . k − 1. Let us deﬁne a Fibonacci
tree of rank k, denoted Fk , to likewise consist of a root with k ordered child trees, which are
Fibonacci trees of respective ranks at least 0, 0, 1, 2, . . . k − 2.6 Thus, the ith child of a root has
degree = i − 1 in Bk and degree ≥ i − 2 in Fk . (In Fk the rank may be as large as i, so deep
narrow trees are possible, including (sorted) vertical chains. In fact the rank may be as large
as k, or even larger.)
How many nodes must a tree of form Fk have? We can show a tight lower bound of fk
nodes, where f0 = 1, f1 = 2, and fk+1 = fk + fk−1 for k ≥ 1. By a simple induction,
fk = 1 + f0 + f0 + f1 + f2 + · · · + fk−2 for k ≥ 2
(For the inductive step, merely add fk−1 to both sides of the equation.) Now observe that
|F0 | = 1 = f0 , |F1 | ≥ 2 = f1 , and by induction for k ≥ 2
|Fk | ≥ 1 + |F0 | + |F0 | + |F1 | + |F2 | + · · · + |Fk−2 | ≥ 1 + f0 + f0 + f1 + f2 + · · · + fk−2 = fk
as desired. Since f denotes the Fibonacci series, we see that |Fk | = Ω

√ k
1+ 5
2

. While this

grows a little more slowly than |Bk | = 2k , it is still exponential, and thus quite adequate to
ensure that all Fibonacci trees of n or fewer nodes have bounded rank O(lg n).
6

There actually is no need in the implementation for the children to be stored in this order, as long as some
child has rank at least k − 2, another has rank at least k − 1, etc. In fact the child list of a Fibonacci tree is
conventionally presented as unordered [5, 3], in contrast to the (standard) binomial trees of §A.3, which rely
on keeping lists sorted because they merge lists rather than implictly bucket-sorting their elements. However,
ordering the children just as for binomial trees costs nothing and is quite convenient for exposition.

APPENDIX §A. FIBONACCI HEAPS

72

It is convenient to say that a node x satisﬁes the Fibonacci property if its ith child has degree
≥ i − 2. Thus, a Fibonacci tree is precisely a tree all of whose nodes satisfy the Fibonacci
property.
§A.12 Observation.
Fibonacci trees can sustain a certain amount of “damage” and still The Fibonacci
qualify as Fibonacci trees. Removing a child of node x will reduce the degree of x, but does not property has some
aﬀect whether x satisﬁes the Fibonacci property. (This is not hard to see from the deﬁnitions “give”
of §A.11.) Thus if x formerly rooted an F4 it now roots an F3 :
remove third child

F4 ← x
F0

F1

F2

−→

F3

F3 ← x
F0

F1

F3

However, x may lose the Fibonacci property if any of its children diminish too severely in
degree.
§A.13 Development. How do we maintain the invariant that all trees of the heap are
Fibonacci trees? We must consider how nodes acquire and lose children. Nodes acquire children
only during consolidation steps: thanks to bucket i − 1, the root of an Fi−1 may acquire another
Fi−1 as its new, ith child tree. Thus when a node x is attached to a parent p as the ith child,
it invariably has degree i − 1. The Fibonacci property at p requires x to have degree ≥ i − 2.
So x is allowed to lose one child “for free,” since its degree may drop to i − 2 without
damaging p’s Fibonacci property. If x loses a second child, however, we detach it from p and
put it onto the heap’s main list of trees, thereby protecting p’s Fibonacci property. Now p
has lost a child, so may possibly have to be detached from its parent (if any), and so on. A
single Decrease-Key operation that detaches a child of x may therefore cause a cascade of
detachments.
§A.14 Remark. If x started out as the ith child of p and has lost two children, it certainly
has degree i − 3 now. Notice that this has not necessarily destroyed the Fibonacci property
at p, since x may no longer be the ith child. (Lower-numbered children of p may have been
deleted.) For example, if x is only the 1st or 2nd child now, the Fibonacci property at p now
only requires x to have degree ≥ 0. But to be safe, the discipline above goes ahead and detaches
x anyway as soon as it has lost two children.
§A.15 Implementation.
The data structures used diﬀer only slightly from those in §A.8. Deathbed bits
Each child list must now be doubly linked, so that an arbitrary node on the list can be detached
from its parent in O(1) time. Moreover, each node now records not only its degree, but also a
“deathbed bit” indicating whether its degree has decreased since it was attached to its current
parent. Deathbed nodes need to be detached if their degrees decrease again.
§A.16 Analysis.
It is crucial that a node is left alone the ﬁrst time it loses a child. An amortized
Otherwise, each Decrease-Key would cause a cascade of detachments all the way up to the analysis
root of its tree: by detaching a child from its parent, it would cause the parent to be detached
from its grandparent and so on up. This would actually force trees to stay “at least binomial”—
the ith child of a node would have degree ≥ i − 1—and the cost of Decrease-Key would be
up to Θ(lg n), just as for binomial trees.
Under the more forgiving Fibonacci discipline, detachments only cascade through nodes
that are already on their deathbeds. They do not cascade past a node that is only now losing
its ﬁrst child. In the worst case, Decrease-Key still cascades up to the top of the tree and

APPENDIX §A. FIBONACCI HEAPS

73

takes Θ(lg n) time. However, this worst case cannot occur often. For Decrease-Key to take
a long time deleting nodes, there must have been many previous Decrease-Key operations
that put nodes on their deathbeds.
To turn this observation into an amortized analysis, we simply require prepaid funerals.
Any operation that marks a node p as being on its deathbed must prepay for that node to
be detached later. Decrease-Key is the only operation that marks nodes in this way. Each
call Decrease-Key(x, k) puts at most one node z on its deathbed, where z is some proper
ancestor of x. Hence the total amortized cost of Decrease-Key(x, k) consists of an O(1)
prepayment for z’s subsequent detachment, together with O(1) to detach x itself, which was
not necessarily on its deathbed.7 All other work performed by Decrease-Key is a prepaid
cascade of detachments of deathbed nodes on the path between x and z.
§A.17 Remark.
The introductory remarks in §A.1 observed that it was surprising that Why only Delete
Decrease-Key could be made faster than Delete. Why is this possible and how did we is intrinsically
slow
accomplish it?
A heap is essentially a device for maintaining some function f on a changing set S of n
elements. It works by maintaining the value of f on various subsets of S, called local sets,
which include the singletons of S. Some local sets are subsets of others; the inclusion lattice
on local sets, also called the local topology, is typically a tree. The function f must have the
property that f (S1 ∪ S2 ) can be determined quickly from f (S1 ) and f (S2 ); this is called an
accumulation. (For binary, binomial, and Fibonacci heaps, the function is “minimum,” and the
elements of S are arranged in a forest whose subtrees are used as the local sets of the heap.
Frederickson’s algorithm (§8.7) provides another example.)
The asymmetry between insertion and deletion hinges on the fact that it is f (S1 ∪ S2 ) that
can be determined quickly, not f (S1 ∩ S2 ).
If we delete an element x from S, then we must recompute f (S − {x}), either from scratch
or from our saved values of f on the local sets of S that are subsets not just of S but of S − {x}.
In general the worst case must take at least log n accumulations, no matter how we deﬁne
the local subsets. Otherwise we would be able to do a comparison sort in better than n log n
comparisons. Delete can be no faster than this if it determines the new value f (S − {x})
(e.g., the new minimum).
By contrast, if we add an element x to S, then we can determine f (S ∪ {x}) with a single
accumulation. Similarly, if we combine two heaps, which represent S and S ′ , a single accumulation gives us f (S ∪ S ′ ). We therefore expect that Insert and Union can in principle take
constant time. In the case where f is the “minimum” function, Decrease-Key(x, k) can also
in principle take constant time, since its eﬀect on the minimum is just as if we had inserted a
new element of key k.
§A.18 Remark.
The diﬃcult part about achieving Insert, Union, and Decrease-Key The strategic
in constant time is that they must ensure that the data structure still maintains f on the local diﬀerence from
sets of §A.17. Otherwise subsequent calls to Delete or Locate-Min—the diﬃcult operations— binary heaps
might take more than logarithmic time.
Binary heaps preserve the size and local topology of the local sets, to the extent that this
is possible as S grows and shrinks; but they spend up to Θ(lg n) time per operation to swap
elements among local sets, changing the minima of the local sets aﬀected by this.
Binomial and Fibonacci heaps rely instead on changing the local topology. Thus they move
entire local sets around rather than individual elements, which allows them to change fewer
local sets per operation. They rely speciﬁcally on three cheap operations: (a) expanding one
7

Each of these two O(1) detachments, as mentioned in §A.10, consists of the actual work to detach the node
plus a contribution to heap potential that will pay for the node to be reattached later.

APPENDIX §A. FIBONACCI HEAPS

74

local set with all the elements of another, (b) removing a local set from the topology, and
(c) shrinking a local set so that it no longer includes any of the elements of one of its local
subsets. This last operation is cheap only if it is guaranteed not to aﬀect the value of f on the
shrunken set. The operations correspond to (a) attaching one tree root to another as a child,
(b) detaching all the children from a root, and (c) detaching a subtree from a tree.

Bibliography
[1] Manuel Blum, Robert W. Floyd, Vaughan Pratt, Ronald L. Rivest, and Robert E. Tarjan.
1973. Time bounds for selection. Journal of Computer and System Sciences, 7(4):448–461.
[2] O. Boruvka. 1926. O jist´m probl´mu minim´ln´
˙
e
e
a ım. Pr´ca Moravsk´ Pˇirodovˇdeck´
a
e r
e
e
Spoleˇnosi 3, 37–58, (In Czech.)
c
[3] Thomas H. Cormen, Charles E. Leiserson, and Ronald L. Rivest. 1990. Introduction to
Algorithms. MIT Press, Cambridge, MA.
[4] B. Dixon, M. Rauch, and R. Tarjan. 1992. Veriﬁcation and sensitivity analysis of minimum
spanning trees in linear time. SIAM Journal of Computing, 21(6):1184–1192.
[5] Michael L. Fredman and Robert E. Tarjan. 1987. Fibonacci heaps and their uses in improved network optimization algorithms. Journal of the ACM, 34(3):596–615
[6] Greg Frederickson. 1985. Data structures for on-line updating of minimum spanning trees,
with applications. SIAM Journal of Computing, 14(4):781–798.
[7] H. N. Gabow, Z. Galil, T. Spencer, and R. E. Tarjan. 1986. Eﬃcient algorithms for ﬁnding
minimum spanning trees in undirected and directed graphs. Combinatorica 6:109–122.
[8] D. R. Karger, P. N. Klein, and R. E. Tarjan. 1995. A randomized linear-time algorithm
for ﬁnding minimum spanning trees. Journal of the ACM, 42(2):321–328.
[9] Valerie King. 1993. A simpler minimum spanning tree veriﬁcation algorithm. Unpublished
manuscript,
ftp://godot.uvic.ca/pub/Publications/King/Algorithmica-MSTverif.ps. (To appear in Proceedings of the Workshop on Algorithms and Data Structures.)
[10] J. Koml´s. 1985. Linear veriﬁcation for spanning trees. Combinatorica, 5:57–65.
o
[11] J. B. Kruskal. 1956. On the shortest spanning subtree of a graph and the traveling salesman
problem. Proceedings of the American Mathematical Society, 7:48–50.
[12] Rajeev Motwani and Prabhakar Raghavan. 1995. Randomized Algorithms. Cambridge University Press.
[13] R. C. Prim. 1957. Shortest connection networks and some generalizations. Bell System
Technical Journal, 36:1389–1401.
[14] D. D. Sleator and R. E. Tarjan. 1983. A data structure for dynamic trees. Journal of
Computing System Science, 26:362–391.
[15] B. Schieber and U. Vishkin. 1988. On ﬁnding lowest common ancestors: simpliﬁcation and
parallelization. SIAM Journal of Computing, 17:1253–1262.

75

BIBLIOGRAPHY

76

[16] Robert E. Tarjan. 1975. Eﬃciency of a good but not linear set union algorithm. Journal
of the ACM, 22(2):215–225.
[17] J. Vuillemin. 1978. A data structure for manipulating priority queues. Communications of
the ACM, 21(4):309–315.

