Novel Inference, Training and Decoding
Methods over Translation Forests
Zhifei Li
Center for Language and Speech Processing
Computer Science Department
Johns Hopkins University

Advisor: Sanjeev Khudanpur
Co-advisor: Jason Eisner

1

Statistical Machine Translation Pipeline

2

Statistical Machine Translation Pipeline
Bilingual
Data

2

Statistical Machine Translation Pipeline
Bilingual
Data

Generative
Training

Translation
Models

2

Statistical Machine Translation Pipeline
Bilingual
Data

Generative
Training

Translation
Models

Monolingual
English

2

Statistical Machine Translation Pipeline
Bilingual
Data
Monolingual
English

Generative
Training
Generative
Training

Translation
Models
Language
Models

2

Statistical Machine Translation Pipeline
Held-out Bilingual Data
Bilingual
Data
Monolingual
English

Generative
Training
Generative
Training

Translation
Models
Language
Models

Discriminative
Training

Optimal
Weights

2

Statistical Machine Translation Pipeline
Held-out Bilingual Data
Bilingual
Data
Monolingual
English

Generative
Training
Generative
Training

Translation
Models
Language
Models

Discriminative
Training

Optimal
Weights

2

Statistical Machine Translation Pipeline
Held-out Bilingual Data
Bilingual
Data

Generative
Training

Monolingual
English

Generative
Training

Translation
Models
Language
Models

Discriminative
Training

Optimal
Weights

Unseen
Sentences

2

Statistical Machine Translation Pipeline
Held-out Bilingual Data
Bilingual
Data

Generative
Training

Monolingual
English

Generative
Training

Unseen
Sentences

Translation
Models
Language
Models

Decoding

Discriminative
Training

Optimal
Weights

Translation
Outputs

2

Training a Translation Model

3

Training a Translation Model
垫子 上 的 猫
dianzi shang de mao

3

Training a Translation Model
垫子 上 的 猫
dianzi shang de mao
a

cat on the mat

3

Training a Translation Model
垫子 上 的 猫
dianzi shang de mao
dianzi shang
a

cat on the mat
the mat

3

d of word may is called word sense ambiguity. C
same ambiguity have different senses/meanings, dep
Training a Translationto a ﬁnancial bank
Modelbe used d
uous word, different be referring should
the word “bank” can translations
translation model (for a translation task from En
mbiguity is called word sense ambiguity. Clearly, w
垫子 上 的 猫
word, different translations should be used depending
s follows,
dianzi shang de mao
ation model (for a translation task from English to C
X → ￿ bank ,on an ￿ mat
he the
ws,
a cat
X → ￿ dianzibankan ￿ the mat ￿
bank he ,
X → ￿ , shang ,yin hang

e → ￿ side , yin hang
XEnglish bank (i.e., bank), ￿but have different Chin

n Chinese. Speciﬁcally, the different Chinese sides
ﬁrst rule has the Chin
lish side (i.e., bank), but have
er, while the second one has the Chinese yinhe an
hang
nese. Speciﬁcally, the ﬁrst rule has the Chinese

le the second one has the Chinese yin hang, which m
4

edword “bank” cancalled word sense ﬁnancial ban
of ambiguity have different senses/meanings, dep
is be referring to a ambiguity. C
same word may
Training a Translationto a ﬁnancial bank
ModelClearly, w
biguity is“bank” can different senses/meanings, dep
uous word, may have besense ambiguity. be used d
the word called word referring
same word different translations should
d, word is model (for referring to used frombank
translation translations translation ﬁnancial En
mbiguity “bank” can be a should be a task dependin
the differentcalled word sense ambiguity. Clearly, w
垫子 上 的 猫
word, different translations should be used Clearly, w
on model (for a translation ambiguity. depending
from
mbiguity
sense
s follows,is called word shangtaskmao English to C
dianzi
de
ation model (for a translation task from English to C
word, different translations should be used depending
,
X → translation an from
he the
ws, model (for a ￿ bank ,on task ￿ mat English to C
ation
a cat
X
ws, → ￿ bank , he an ￿
X → ￿ dianzibankan ￿ the mat ￿
bank he ,
X → ￿ , shang ,yin hang
X ￿ ￿ bank , hang
mao
cat
→ →bank , yinheaan ￿ ￿
e → ￿ side , yin hang
XEnglish bank (i.e., bank), ￿but have different Chin
h side (i.e., Speciﬁcally, thedifferent ChineseChin
n Chinese.bank , yin hang ￿ different Chinese sides
X side (i.e., bank),but have ﬁrst rule has the side
→ ￿ bank), but have
lish
se. Speciﬁcally, the ﬁrst rule has Chinese yin sides
er, while (i.e., secondbut have different Chinese he an
the bank), one has the the Chinese he an
hang
nese.side
lish Speciﬁcally, the ﬁrst rule has the Chinese
the second one has the Chinese yin Chinese he an
hang, which m
which
le the second one has the rule has yin
nese. Speciﬁcally, the ﬁrst Chinese the hang,
4

edword “bank” cancalled word sense ﬁnancial ban
of ambiguity have different senses/meanings, dep
is be referring to a ambiguity. C
same word may
Training a Translationto a ﬁnancial bank
ModelClearly, w
biguity is“bank” can different senses/meanings, dep
uous word, may have besense ambiguity. be used d
the word called word referring
same word different translations should
d, word is model (for referring to used frombank
translation translations translation ﬁnancial En
mbiguity “bank” can be a should be a task dependin
the differentcalled word sense ambiguity. Clearly, w
垫子 上 的 猫
word, different translations should be used Clearly, w
on model (for a translation ambiguity. depending
mbiguity
sense
s follows,is called word shangtask from English to C
dianzi
de
ation model (for a translation task from English to C
word, different translations should be used depending
,
X → translation an from
he the
ws, model (for a ￿ bank ,on task ￿ mat English to C
ation
X
ws, → ￿ bank , he an ￿
X → ￿ dianzibankan ￿ the mat ￿
bank he ,
X → ￿ , shang ,yin hang
X ￿ ￿ bank , hang
mao
cat
→ →bank , yinheaan ￿ ￿
e → ￿ side , yin hang
XEnglish bank (i.e., bank), ￿but have different Chin
h side (i.e., Speciﬁcally, thedifferent ChineseChin
n Chinese.bank , yin hang ￿ different Chinese sides
X side (i.e., bank),but have ﬁrst rule has the side
→ ￿ bank), but have
lish
se. Speciﬁcally, the ﬁrst rule has Chinese yin sides
er, while (i.e., secondbut have different Chinese he an
the bank), one has the the Chinese he an
hang
nese.side
lish Speciﬁcally, the ﬁrst rule has the Chinese
the second one has the Chinese yin Chinese he an
hang, which m
which
le the second one has the rule has yin
nese. Speciﬁcally, the ﬁrst Chinese the hang,
4

edword “bank” cancalled word sense ﬁnancial ban
of ambiguity have different senses/meanings, dep
is be referring to a ambiguity. C
same word may
Training a Translationto a ﬁnancial bank
ModelClearly, w
biguity is“bank” translation are
uous word, may ￿can differentambiguity.
different translations
the word called mao , a senses/meanings, dep
same word → word referring
nese-to-English have besensecat ￿ should be used d
X
d, word is model (for referring to used frombank
translation translations translation ﬁnancial En
mbiguity “bank” can be a should be a task dependin
the differentcalled word sense ambiguity. Clearly, w
垫子 上 的 猫
word, different aacat ￿ sense ambiguity. Clearly, w
on model called word
translation
from
mbiguity
s follows,is ,￿ translations should be ￿ English to C
→ X mao(forX dianzi shangtask X used depending
￿ →
of 0
0 de X1 , X1 de
ation model (for a translation task from English to C
word, different translations should be used depending
,
X X translation thefrom
ws, de X (for a of bank ,on task ismat English enco
ation model , → ￿ X ￿ he an ￿ implicitly to C
nement between nonterminals
￿ X0
1
1
0
X → ￿ bank , he one
ws, In Hiero, there isan ￿ single nonterminal X,
ls. X → ￿ dianzibankan ￿yin hang ￿
bank he implicitly encoded by the
X → ￿ , shang , the mat
,
ween nonterminals is more nonterminals like n
al., 2006) ￿ bank , heaan ￿ ￿
may contain
X ￿is one yin hang
mao
cat
→ →bank ,single nonterminal X, while a syn
, there rule shows that we ￿can have differentChin
e ﬁrst ￿ side , yin hang
XEnglish bank (i.e., bank), but translate the Chin
he →
may containSpeciﬁcally, thedifferent nounthe Chin
h second￿rule shows that theﬁrstlike Chinese side
n side (i.e., bank),yin hang ￿ different Chinese sides
XChinese.bank , nonterminals rule has phrase
→ (i.e.,more but have two phrases (repres
e side
lish
bank), but have
showsgetthe
that we the ﬁrst rule has Chinese yin he Su
word an
se. Speciﬁcally,can but have of in the English. m
er, while (i.e., secondtranslatedifferent Chinese he an
one has the the Chinese sides
hang
will
nese.side reordered around has the Chinese
lish Speciﬁcally, the ﬁrst rule
bank),
e shows English.the the Chinese yin Chineseby Xm
that one has the rule has yin hang, which
the two phrases (represented he an
the second one has ﬁrst Chinese the hang, which 0
le the second
nese. Speciﬁcally,
nese and
ordered around of in the English. Such reorderi
5

edword “bank” the called word mayahave differenC
of ambiguity have different senses/meanings, ban
is be referring to ﬁnancial dep
sense ambiguity.
ural language, can same
same word may
Training athe senseare should Clearly, w
Modelbe used d
e Ambiguity haveTranslationto a ﬁnancial bank
biguity isexample, different“bank” can
uous For “bank” translation cat ￿
different translations
ext. word →
the word called mao , a senses/meanings, dep
same word, may word referring
nese-to-English ￿can be word ambiguity. be referr
X
d, This kind can different senses/meanings,bank
translations translation ﬁnancial dep
translation model (for referring to used from En
mbiguity “bank”have be a shouldcalled word sense
the differentcalled word sense ambiguity. Clearly, w
ver.word is may of ambiguity is be a task dependin
same word
垫子 上 的 猫
word, different aacat ￿be sense ambiguity. Clearly, w
on modelis , translationsdifferent used depending
mbiguity
s follows,“bank” translation de be a ﬁnancial sho
ch word called word , should from
word, X of
→ X mao(forX dianzi shangtask Xtranslations bank
￿ → ￿ can
the an ambiguousde X referring to ￿ English to C
0
1
1
0
ation model called translation task be a English to C
word, different translation model (forused Clearly, wt
,
r example, (for a word sense ambiguity. depending
mbiguity is a translations should from translation
X , translations he thefrom
ws, different→ a of bank ,on task ismat English enco
ation de X asX ￿ X ￿ should be implicitly to C
model (for translation an ￿ used depending
nement
word, rules follows,0
n X0 forbetween nonterminals are are
two Chinese-to-English translation
￿ Hiero rules 1 Chinese-to-English translation
ro rules
e
for 1 , he an ￿
X → ￿ (for
ws, Inmodelbanka translation task from English to C
ation Hiero, there is one single nonterminal X,
ls. X → ￿ dianzibankan ￿yin hang ￿
bank he X the mat
X → ￿ , shang , → ￿ bank , he an ￿the
,
ween nonterminals ￿is implicitly encoded by n
ws, 2006) may ￿contain more nonterminals like
X →
X mao , a , ￿
al., →bank , → heamaocat￿a cat ￿
X
￿ bank , hang
mao
cat
→ ￿is one single nonterminal X, while a syn
yin an ￿
, there rule shows that we ￿can have differentChin
e ﬁrst ￿ side , yin hang
XEnglish bank (i.e., bank), but translate the Chin
he X→→ →￿ dianzi,Xshang→, X￿ of X ￿onyin hang ￿
X X more nonterminals like, noun phrase
￿ →0 de X1deX11 of X0 bank
X
the mat
bank￿ but,an ￿ different Chinese side
he have 1 ￿, 0
de
X Xthe ﬁrst rule has the Chin
0
may containSpeciﬁcally, the two phrases (repres
h second￿rule shows that ￿
n side (i.e., bank),yin hang different Chinese sides
XChinese.bank , but have
→
e side (i.e., bank),
lish alinement between nonterminals is implicitly encoded b
e alinementsame can ﬁrst rule the bank), but have
es, the the between nonterminals implicitly encoded hethe
showsget￿the English side (i.e., the Chinese by m
thatbank yinone has
we
word an
se. while (i.e., secondtranslateishas Chinese yinhe and
er, Speciﬁcally, ,the but have different Chinese sides
have Speciﬁcally, the around of in the English. Su
X side Hiero,bank), oneissinglesingle the Chinese hang
hang ￿
will
nese.→ reordered ﬁrst one nonterminal X, whilewhil
lish In In Hiero, there rule has nonterminal X, a sy
minals.
onterminals.
there is
e showsal., 2006)containtheChinese yin hang,ﬁrst rule0
thatin the the Chinese the like noun he an
the has ﬁrst rule has yin the like noun
two more nonterminals
phrases nonterminals phrase
themeaningmayChinese. Speciﬁcally,hang, which p
second one may contain more (representedwhich
enttheSpeciﬁcally,has but have different Chineseby Xm
le
nese.side (i.e., one
ey et and English.
(Galley 2006)
neseal.,second bank),
lish 2 et
Chinese sides
2
ordered around of in the English. Such reorderi
5

edword “bank” the called word mayahave differenC
of ambiguity have different senses/meanings, ban
is be referring to ﬁnancial dep
sense ambiguity.
ural language, can same
same word may
Training athe sense translation are used w
ModelClearly, d
e Ambiguity haveTranslationto a ﬁnancial bank
biguity Chinese-to-English senses/meanings,
uousfor isexample, be word “bank” can be dep
ext. word different
the For called word referring
same￿word, may can translations should
ulesword “bank” cat ￿differentambiguity. be referr
→
mao , a
d, This kind can different senses/meanings,bank
translations translation ﬁnancial dep
translation model (for referring to used from En
mbiguity “bank”have be a shouldcalled word sense
the differentcalled word sense ambiguity. Clearly, w
ver.word is may of ambiguity is be a task dependin
same word
垫子 上 的 猫
word, different translationsdifferent translations bank
on modelis called word,referring ￿ a English tow
mbiguity
ambiguity.
s follows,“bank” ￿ word sensecat mao ﬁnancial sho
chXword (for a can X , should to
ambiguous of be ￿ a de from depending
X ,
mao
the an de X →X translation task be used Clearly, C
￿ 0
1
1
0
ation model called translation task be a English to C
word, different translation model (forused Clearly, wt
,
r example, (for a word sense ambiguity. depending
mbiguity is a translations should from translation
X (for translation of ￿ ￿ English the
he an from
ws, Xmodel as translations, ,on task beencoded byto C
ation different→ a ￿ bank should X used depending
word, rules ￿ X0 de X1implicitly 0
a cat
ween rules for Chinese-to-English translation
n Hiero for Chinese-to-English translation are are
two nonterminals is X1
→ follows,
ro rules
e
Xthere ￿ banka, translation
he nonterminal X, while a C
ws, → is one singlean ￿ task from English to sy
ation
o, X model￿ (for￿ bankan ￿,yin hang ￿
→ between ,nonterminals bank , he an ￿enc
bank he X → ￿ is
dianzi shang the mat implicitly
X →
,
inement X → ￿→ nonterminals￿ like noun phrase
ws, contain more ￿ ,maocat a cat
X mao a
may →bank , yinheaan ￿ , ￿ ￿
X Hiero,mao hang
→ ￿ ￿ bank , is cat single nonterminal X
ls. In that we (i.e., translate the Chinese word m
one
e English bank , yinbank), ￿but have different Chin
sidethere hang
X →
shows→￿→￿ dianzi,Xshang→, X￿0of X0 ￿onyin hang ￿
canX1deX1￿of X bank , the mat
X X ￿ →0 de he ,an 1
X
X 2006) may contain more ￿,
bank￿ but have different Chineselike n
de 1 nonterminals side
X Xthe ﬁrst rule has the Chin
0
hal., →(i.e., Speciﬁcally, ￿ (represented by X
bank),
n side (i.e., bank), butphrases
XChinese.bank , yin hang different Chinese sides
￿ that the two have
le shows
lish side between nonterminals is implicitly encoded by the
e alinementsamebetweenonewe has Chinese yin he d
es, the the the
alinement the nonterminals the Chinese hang
implicitly encoded
he ﬁrst rule secondﬁrstside (i.e.,isbank), the Chin
se. while (i.e.,shows thathavecan translatebut havean
Speciﬁcally,Englishthe English. Such reorderb
ruledifferent Chinese he an
er, side ￿ bank , of but has the the Chinese sides
have Speciﬁcally, thein rule has
X
hang ￿
ordered In Hiero, there
nese.→ Hiero,bank), oneissinglesingle nonterminal X, whil
lish In around yinﬁrst one nonterminal X, while a sy
minals.
onterminals.
there is
e second ruleone thecontain thehas yin hang,ﬁrst rule
that more nonterminals which
themeaningmayChinese. Chinese the hang, which m
second2006)containthe Speciﬁcally,like noun noun p
one may the Chinese yin the like phrase
enttheSpeciﬁcally,has but have different Chinese he an
inshowsﬁrst rule two phrases (repres
le
has
nese.side al., bank), more nonterminals Chinese sides
ey et al.,second
(Galley 2006)
glish.
lish 2 et (i.e.,
e2 will get reordered around of in the English. S
6

edword “bank” the called word mayahave differenC
of ambiguity have different senses/meanings, ban
is be referring to ﬁnancial dep
sense ambiguity.
ural language, can same
same word may
Training athe sense translation senses/me
Translationto a ﬁnancial dep
ModelClearly, w
e Ambiguity havemay havesenses/meanings,bank
biguity example, translations should
uousthe Chinese-to-English different be
ext. word different
the For called word referring
same￿word, may can
age, wordsame word ￿differentambiguity. are used d
ules for is“bank” cat be word “bank” can be referr
→Ambiguity
mao , a
e Thisis modelambiguity is be a task frombank
d, word word (for referring to
translation translations can senses/meanings, w
mbiguitythemay of “bank”shouldcalled wordto aEn
the differentcalled word sense ambiguity. Clearly, ﬁn
ver.word “bank”have be a translationused dependin
xample, kind can different的 猫 ﬁnancial dep
be referring sense
same
垫子 上
word,ofambiguous word sensecat ￿senseEnglishbank
on andifferent translationsdifferent translations tow
mbiguity is called is be , should from depending
ambiguity.
sindmodel“bank”have calledawordmao ﬁnancial sho
follows, → a can differentde
chX word(forX translation task to used Clearly, C
X ,
mao
the word X may ￿ of X referring be a ambiguity.
ambiguity word, ￿
same de 1
senses/meanings, dep
￿ 0
1
0
ation model called translation task be used Clearly, wt
word, different translationreferring(foraaﬁnancial bank
translations should from translation
, word is (for word model to English used
r example, a different sense ambiguity. depending
mbiguity “bank”acan be translations should be to C
guous word,
the
X (for translation of X used depending
he an ￿ ￿ English to C
ws, Xmodel as modelbank, aX task beencoded from w
ation different→ a ￿ (for ,on ambiguity. Clearly,the
word, rules called de X1implicitly 0 task by E
a cat translation
ween for Chinese-to-English translationfrom are
naHiero rules for Chinese-to-English translation
two nonterminals is sense1
→ follows,
translation X0 word
mbiguity is ￿ translations should are
ro rules
e
Xthere ￿ banka, translation task from English a C
he nonterminal X,
ws, → is one singlean ￿ should be usedwhile to sy
ation
o, X model￿ (for￿ bankan ,yin hang
word, different translations￿ the mat ￿
depending
as Hiero rules for Chinese-to-English translation are
follows, dianzi, shang
→ translation nonterminals is implicitly
bank he ,
X →
ple
to-English between are X → ￿ bank , he an ￿enc
inement X (for ￿a mao ,maocat a task￿ fromnoun phrase
ws, contain→ → nonterminals like English to C
X
a
ation →bank , yinheaan ￿ , ￿
may model￿ bank translation￿ cat
more, ￿hang
X Hiero,mao is cat single nonterminal X
→ ￿
ls. In, a ￿ bankXcan ￿bank), ￿butcat ￿ ￿ different Chin
one
emao→ thatX there translatehehave
English side →yin hang ,, athe Chinese word m
(i.e., ￿bank
ws,
X
￿shows cat ￿
→
mao
we 0, deXshang→, X￿0of an0 ￿onyin hang ￿
X → dianzi, X1deX more ￿, X
￿ → ￿ he X de 1 nonterminals
X
X 2006)￿may contain 1￿of X bank , Chineselike n
→ Xbank), but,an 1 different the mat
bank 0 have
X the ﬁrst rule has the Chin
hal., →(i.e., Speciﬁcally, ￿ (represented by X
side
n side (i.e., bank), butphrases
XChinese.bank , yin hang different Chinese sides
￿ that the two have
le shows ofX ￿ de￿ he an Xa ,cat on X ￿
lishX1 , →X ￿ X0between 0nonterminalsof implicitly encoded b
side 1 between nonterminalsyinimplicitly encoded by the
→ the mao ￿ 1 X1 isbank),
,
edewhile same → ￿ that side canhang Chinese Chin
es, ﬁrst rule bank , X de ruleishastranslatebut he d
alinement
the the
alinement English ,has(i.e.,Chinese yin hang
he X→ X￿the shows bankweEnglish.0 Suchthehavean
se. Speciﬁcally, , of butthe ￿ different Chinese he an
ﬁrsthave the the ￿
er, side (i.e., second in rule has the Chinese sides
have Speciﬁcally, the one
X
bank there
hang
ordered around yinﬁrst one nonterminal X, while a sy
reorder
nese. In Hiero,bank), oneissinglesingle nonterminal X, whil
lish
minals.
onterminals. In Hiero, is
there
n nonterminals one between encoded butthe phrases (repres
ules, English is implicitlythat the by havethe ﬁrst rule
the alinement
e second￿ ruleone (i.e.,theChinese yinimplicitlywhichm
themeaningmayChinese. Speciﬁcally,hang, which p
second2006)containhang moretwoissub-different an
thenonterminalsyin like noun noun
enttheSpeciﬁcally,has butbank),different Chineseencoded
inshowsﬁrst Chinese the hang, he Ch
me
X
yin
le is →2006) Innonterminal X, one single nonterminal X, wh
has
nese.side singlesidemay therehave has syntax- like phrase
rule a
ey et al.,second bank), contain nonterminals Chinese sides
(Galley et al.,bank , the more ￿ nonterminals
glish. (i.e., Hiero,
lish one
ere 2
nonterminals.
is while
2 will get reordered around of in the English. S
e
6

edword “bank” the called word mayahave differenC
of ambiguity have different senses/meanings, ban
is be referring to ﬁnancial dep
sense ambiguity.
ural language, can same
same word may
Training athe sense translation senses/me
Translationto a ﬁnancial dep
ModelClearly, w
e Ambiguity havemay havesenses/meanings,bank
biguity Chinese-to-English different be
uousthe Chinese-to-English “bank” can be
ext. For called word referring
the for example,
same￿word, may can translations should
word different
age, wordsame word ￿differentambiguity. are used d
ules for is“bank” cat be wordtranslation are referr
ules mao , a
→Ambiguity
X → ￿ mao used dependin
, a cat ￿
e Thisis modelambiguity is be a ﬁnancial dep
d, word word (for referring to
translation translations can senses/meanings, w
mbiguitythemay of “bank”shouldcalled wordto aEn
the differentcalled word sense ambiguity. Clearly, ﬁn
ver.word “bank”have be a translation task frombank
xample, kind can different的 猫
be referring sense
same
垫子 上
word,ofambiguous word sensecat ￿￿senseEnglishbank
on andifferent translationsdifferent translations tow
mbiguity is called word, a catsenses/meanings, dep
ambiguity. depending
sindmodel“bank”have calledawordbe a ﬁnancial sho
follows, → a translation task from Clearly, C
chX word(forX ￿ is X referring to
X , can be
mao
the word X may Xof → ,,￿should X usedambiguity.
ambiguity mao ￿ X de
same de 1
different de 1 , X1 of X0 ￿
￿ 0
1
0
0
ation model called translation task be used Clearly, wt
word, different translationreferring(foraaﬁnancial bank
translations should from translation
, word is (for word model to English used
r example, a different sense ambiguity. depending
mbiguity “bank”acan be translations should be to C
guous word,
the
X as X a ￿ X implicitly used depending
,should X
he of ￿
an from
ws, XmodelalinementX sensetask beencoded from w
ation different→ 0 debank, aX of X0 ￿ ￿ English to C
(for translation1 ambiguity. Clearly,the
word,Xrules called de (for Xtranslation task byimp
rules, nonterminals between
ween rules ￿ model is11, on1 nonterminals
naHiero for Chinese-to-English translation are are is E
two the
follows,
→
translation translations
mbiguity isfor Chinese-to-English translation
0 word
0
ro rules
e
Xthere ￿ banka, translation task from single to sy
he nonterminal X, while a C
ws, → is one singlean ￿ should is one Englishnonte
ation model (for
easX → ￿ dianzi, Hiero,
o, nonterminals. ￿In shang ,yin hang used depending
word, different translations￿ theremat ￿
be
follows, for Chinese-to-English translation are
he an
X translation nonterminals is implicitly
ple Hiero rules bank bankX →
to-English between nonterminals bank , he an ￿enc
linement X →X ￿→are￿ ,maocatthe ￿ is implicitly encod
between2006),mayacontain more nonterm
inement etmoretranslation￿task￿ fromnoun phrase
ws, (Galley →
a
mar→contain(for a maohang , ￿ cat like English to C
ation →bankal.,yinheaan ￿
may model￿ bank , nonterminals
XIn ￿ 2 mao is one single nonterminal X, w
cat single nonterminal X
,
als. (VP). bank→yinisbank ￿but that￿we can translat
Hiero, there ￿bank), , hehave different Chin
ls. In, Hiero, there rule showscat ￿
emao→ a ￿ sideXcan translateathe Chinese word m
English The (i.e., hang ,
X ﬁrst ￿ one
an
ws,
X
ase
￿shows that ￿
cat we , →
mao
X X dianzi, X ,an more ￿,
￿X
X 2006) may deXshangmore1 of
bank but X 1 different the hang ￿
he de nonterminals side
X the ﬁrst rule has the
0
tal., →→￿→￿may,containX1￿of￿X0nonterminals mat no
al.,cat.3 The→0 contain→,showsX0 ￿onyintwolike n
2006)bank ￿yin1dehaveX￿ bank , ChineseChin
like
h Chinese. bank),tworule
n a side (i.e., bank), butphrases (represented by X
X shows that the
dsside (i.e., Speciﬁcally,
second hang different Chinese sides
phra
le X , → ofX ￿ de￿thatde Xa can onthat thethe Chines
lishX1 alinement0between 0nonterminalsof X0 ￿￿ encoded by the
have,catimplicitly
X
bank , that side X the
mao ￿ the implicitly
hein Chinese→English we1 cantranslate ofyin hang
ﬁrst rule shows get reordered isbank), in the E
,
ede ﬁrst XX ￿ shows he anruleishastranslatebut Chin
es, the the1same → nonterminalsyin1hang Chinese he d
alinement between ￿ bank ,has(i.e.,Chinese encoded b
he whilerule X willyinoneweEnglish. Suchthehavean
se. Speciﬁcally, ,the butthe ￿ different Chinese he an
ﬁrsthave hasaround reorder
er, side (i.e., second in rule
the bank),
have Speciﬁcally, the hang
de → ￿ bank of ﬁrst
X
ordered around is that the two nonterminal X, whil
nese.
lish
minals. In Hiero, there there issinglesingle the Chinese sides
onterminals. rule showsone one nonterminal X, (represen
In Hiero,
henonterminals oneChinese. Chinesethe phraseswhilerule
secondChinesehas the Speciﬁcally,hang, (repres
phrases ﬁrst a sy
n second rule implicitlyEnglish.but havethe
ules, English is shows that
the alinement betweenbank),
encoded
nonterminals
e etmeaningmay containhang thehas the Chineseencoded
the al.,secondsidemay contain moretwoissub-different an
second2006), the ﬁrst rule by yin hang, which p
enttheSpeciﬁcally, (i.e.,the Chinese yinimplicitlywhichm
in and more nonterminals Chinese he Ch
between al.,bank has
me
X is
￿ different like
lewill get reordered but X, one nonterminals like phrase
nese.side single bank), therehave ofsingle nonterminal noun
ey →et (i.e., one yin
(Galley 2006)
glish. ￿ Innonterminal is while insyntax- noun X, wh
lish one
ere 2
nonterminals.
Hiero,
e will get reordered around of ainthe English.sides
Suc
2
e
around
the English. S
7

on.of language, the called wordtwo kinds of ambigu
Broadlymay is be referring to ﬁnancial dep
speaking, there
edword “bank” can same aresenses/meanings, ban
sense ambiguity.
ural ambiguity have different mayahave differenC
same word
Modelbe referr
e Ambiguity can Translation shouldsenses/me
n-SenseTraining atheambiguity. to a ﬁnancial bank
Ambiguityword ambiguity. Clearly, d
ambiguity “bank”havemay havesenses/meanings, dep
and spurious sense translation are used w
biguity called word
uousthe Chinese-to-English different
ext. word
the word, different translations
same For may
age, wordsame word be referring
ules for isexample, different“bank” can be

ules ￿ mao , a
for Chinese-to-English ￿ mao , a cat
→Ambiguity cat X → translation are ￿
￿
e Thisis modelambiguity senses/meanings,bank
d, wordkind can referring to ﬁnancial dep
translation translations translation
(for can different from w
mbiguitythemay of “bank”shouldcalled wordto aEn
the differentcalled word sense ambiguity.senses/mea
ver.word “bank”have be ahave is be a taskClearly, ﬁn
xample,sameword垫子different的 猫 used dependin
be referring sense
same
ge, the
word may 上
word,ofambiguous word sensecat ￿￿senseEnglishbank
onAmbiguity ￿ is be ,,different translations tow
mbiguity (fortranslations should to
ambiguity. depending
sefollows,“bank”have calledawordbe a ﬁnanciala sho
chXword X may translation catsenses/meanings, ﬁna
X called word,
→ a mao
mao
the modelis wordXof → referring X usedambiguity.
indandifferent X can X ￿can de referringof X dep
ambiguity“bank” ￿ a task fromX Clearly, C
same word
different be 1 , 1 to 0 ￿
ample, the 1 , 1
￿ 0 de
0 X0 de
ation model called translation task be used Clearly, wt
from translation
word, different translationreferring(foraaﬁnancial bank
translationsmodel to
, word is (for word sense ambiguity. depending
r example, a different translations should be used
mbiguity “bank”acancalledshould sense English to C
guousambiguity is different senses/meanings, dep
word,
the of word may have be word
nd
ambiguity.
same model (for a ￿ bank , he an ￿
X as X translation of X used depending
ws, X rulesalinementX sensetask beencoded from w
ation different→ 0 de (for, aX of X0 ￿ ￿ English to C
word,X nonterminals X1implicitly 0 task byimp
rules, rules fordifferentbetweenambiguity.
ween word,called de is1 , translationfrom areClearly,the
naHiero for Chinese-to-English Xtranslation
two the ￿Chinese-to-English translation
→ translations should a
translation model translationsare ﬁnancial bank
mbiguity “bank” 0can be referring toshould be used
is follows, on1 nonterminals is E
guous ￿ bank , word ￿ 1
ro rules
e
the word
Xthere
he nonterminal X,single to sy
ws, → is one singlean should is one Englishnonte
ation model translations there from depending
C
easX → ￿ dianzitranslation
nonterminals. In Hiero,
o,translation(for￿a bankan ￿ translation while a w
word, differentmodelshang a,yintask be ￿ task from En
follows, called , he ,sense hang usedare
ato-English between word → ambiguity. he an ￿enc
(for
mbiguity translation are X the ￿ is implicitly encod
is bank nonterminals bank ,
X between nonterminalsmat implicitly
ple Hiero rules for
linement X →X Chinese-to-English translation Clearly,
inement formoretranslationacontain more nonterm
ws, (Galley et Chinese-to-English fromnoun phrase
→ ￿a 2006) a cat translation are
→ nonterminals is
mar→contain(for yinheaan ￿should￿be used depending
may
iero differentmao
les model translations
Chinese-to-English cat translation are
ationrules ￿ bank , ￿ ,mao
mayfor→bankal., maohang , ￿ ￿task like English to C
X Hiero, there is cat single nonterminal Xw
word,In ￿ 2
, are one
as Hiero Hiero, Chinese-to-English translation are
follows,sidethere is one single nonterminal Chin
als. (VP).translation yinbank), ￿but that￿we can translat
ls. In a ￿ TheXcan rule showscat ￿
lemao , rulesbank → →￿ hang , he have different X,
to-English
e
bank
ws, → thatfor (i.e., translateathe from English to m
XEnglish (for
ase model ￿ ,a translation
￿shows catX ﬁrst ￿ mao , taskan
we 0 deXshang→, X￿0of XChinese hang ￿
ation 2006)￿may containX1￿of X nonterminalswordno
X X dianzi, X1deX more ￿ 0 ￿
￿X
X 2006) may containmore1 bank onyin mat C
bank but, have different the like
de nonterminals side
X , mao ﬁrst
0
1
tal., →→￿→→X ￿ ￿maohang shows cat ,￿Chineselike n
al.,cat.3 The→ , →he￿an a cat, ￿,a rule the two Chin
h side X that → yinbank ,￿ (represented phra
bank),two
n Chinese.bank →￿ butphrases ￿ ￿ has the sides
X a side (i.e., bank), ￿ rule , adifferent
dsmao , (i.e., Speciﬁcally, thehe anthat Chinese by X
second have cat
le showscat￿X X￿ de￿thatde Xa can on X ￿ the Chines
X￿X the , he mao ,cat translate
ws,the , →X shows bank ￿ yinimplicitly encoded by the
￿heX X of bank X an
arule
lishX1 alinement0between 0nonterminalsof implicitly encoded b
ﬁrst 1 betweenthe that we X1 isbank), of in the E
￿ ﬁrst rule the the ￿
,
ede ﬁrst rule secondmaowe canhang Chinese Chin
es, Speciﬁcally,English side ishastranslatebut he d
alinementsame → nonterminals (i.e.,Chinese yin hang
,has
he while ￿the showsgethang 1English.0 Suchthehavean
se. in→ (i.e., → , of butreordered around reorder
er, side around thein the ￿ different Chinese he an
have Chinesebank), ﬁrst
de Speciﬁcally, yinone rule has the Chinese sides
will
X theHiero, there there issinglesingle nonterminal X, whil
bank is one have nonterminal X, while a sy
ordered
nese.
lish
minals. In rule shows thatone two phrases (represen
onterminals. In Hiero,
henonterminals￿X→betweenan,Xyin,on the￿phrases (repres
second1X is 0→ hasX dedeSpeciﬁcally,hang, which
the of of
ndeXsecond →implicitly1bank), 1by X0 sub-different Ch
ules, Englishone de bank,X1XX but 1havethe ﬁrst rule
the→Chinese￿(i.e.,English. hang ￿￿X0 ￿ encoded
alinement￿ deX0 0 encoded twois implicitly
nonterminalsyin
X X bank ￿, X
,
e second￿ of may0containthe ￿ thehas the like noun noun p
￿ X , hasthe nonterminals
theX1 ,secondsidemayhehang more X
→ bankChinese. Chinese
￿ contain 1￿ of
enttheSpeciﬁcally, thebut ,have1different Chinese he an
betweenXruleone andﬁrstChinese 0yin hang, which m
memeaning inshows thatrule nonterminals like phrase
X is one
lewill (i.e., nonterminal is while a the Chinese sides
nese.side single bank), thereX, one single nonterminal X, wh
ey et al.,et al., 2006) yin more
(Galley 2006)
glish.
lish →get reordered around of insyntaxere 2
nonterminals. In Hiero,
e will get reordered around of in the English. S
English. Suc
2
e
7

Decoding a Test Sentence

8

Decoding a Test Sentence

8

Decoding a Test Sentence
垫子

上 的 狗

8

Decoding a Test Sentence
垫子 上 的 狗
dianzi shang de gou

8

Decoding a Test Sentence
垫子 上 的 狗
dianzi shang de gou
the dog on the mat

8

Decoding a Test Sentence
垫子 上 的 狗
dianzi shang de gou
the dog on the mat
X
X
X
S

→
→
→
→

￿ dianzi shang , the mat ￿
￿ gou , the dog ￿
￿ X0 de X1 , X1 on X0 ￿
￿ X0 , X0 ￿

(a) Hiero rules in the test grammar

8

Decoding a Test Sentence
垫子 上 的 狗
dianzi shang de gou
the dog on the mat
X
X
X
S

→
→
→
→

￿ dianzi shang , the mat ￿
￿ gou , the dog ￿
￿ X0 de X1 , X1 on X0 ￿
￿ X0 , X0 ￿

(a) Hiero rules in the test grammar

8

Decoding a Test Sentence
垫子 上 的 狗
dianzi shang de gou
the dog on the mat
X
X
X
S

→
→
→
→

￿ dianzi shang , the mat ￿
￿ gou , the dog ￿
￿ X0 de X1 , X1 on X0 ￿
￿ X0 , X0 ￿

(a) Hiero rules in the test grammar

8

Decoding a Test Sentence
垫子 上 的 狗
dianzi shang de gou
the dog on the mat
X
X
X
S

→
→
→
→

￿ dianzi shang , the mat ￿
￿ gou , the dog ￿
￿ X0 de X1 , X1 on X0 ￿
￿ X0 , X0 ￿

(a) Hiero rules in the test grammar

dianzi shang

de

gou

8

Decoding a Test Sentence
垫子 上 的 狗
dianzi shang de gou
the dog on the mat
X
X
X
S

→
→
→
→

￿ dianzi shang , the mat ￿
￿ gou , the dog ￿
￿ X0 de X1 , X1 on X0 ￿
￿ X0 , X0 ￿

(a) Hiero rules in the test grammar

X→￿dianzi shang, the mat￿

dianzi shang

de

gou

8

is incorporated into the SMT system through using a language model, whi
the monolingual English data.

Decoding a Test Sentence

1.1.3 Discriminatively Training
垫子 上 的 狗 of Relative Weights Amon
With the translation and language models, how much should we trust
dianzi weshang ade gou model, and trust the model prop
Intuitively,
can assign weight to each
weight. These weights are usually found through a discriminative training a
the2003).dog on the mat
Och,

X
X
X
S

→
→
→
→

￿ dianzi shang , the mat ￿
1.1.4 Decoding for Test Data
￿ gou , the dog ￿
￿ X0 de X1 , X1 on XWith the bilingual and monolingual training data, we have trained a SM
0￿
following the pipeline in Figure 1.1), which has translation and language m
￿ X0 , X0 ￿
relative weights among the models. Now, we can generate translation out

test data by using the trained SMT system. For example, we may generate a
dog on
(a) Hiero rules in the test grammar the mat” for “￿￿ ￿ ￿ ￿”, assuming that the translation gramma
a rule “X → ￿ ￿, the dog ￿” that is extracted from other training examples
X
X
X
X

X→￿dianzi shang, the mat￿

dianzi shang

de

→ ￿ dianzi shang , the mat ￿
→ ￿ gou , the dog ￿
→ ￿gou, the dog ￿
→ ￿ X1 de X2 , X2 on X1 ￿

gou rules in the grammar
(a) Hiero

8

is incorporated into the SMT system through using a language model, whi
the monolingual English data.

Decoding a Test Sentence

1.1.3 Discriminatively Training
垫子 上 的 狗 of Relative Weights Amon
With the translation and language models, how much should we trust
dianzi weshang ade gou model, and trust the model prop
Intuitively,
can assign weight to each
weight. These weights are usually found through a discriminative training a
the2003).dog on the mat
Och,

X
X
X
S

→
→
→
→

￿ dianzi shang , the mat ￿
1.1.4 Decoding for Test Data
￿ gou , the dog ￿
￿ X0 de X1 , X1 on XWith the bilingual and monolingual training data, we have trained a SM
0￿
following the pipeline in Figure 1.1), which has translation and language m
￿ X0 , X0 ￿
relative weights among the models. Now, we can generate translation out

test data by using the trained SMT system. For example, we may generate a
dog on
(a) Hiero rules in the test grammar the mat” for “￿￿ ￿ ￿ ￿”, assuming that the translation gramma
a rule “X → ￿ ￿, the dog ￿” that is extracted from other training examples

X→￿X0 de X1 , X1 on X0 ￿
X
X
X
X

X→￿dianzi shang, the mat￿

dianzi shang

de

→ ￿ dianzi shang , the mat ￿
→ ￿ gou , the dog ￿
→ ￿gou, the dog ￿
→ ￿ X1 de X2 , X2 on X1 ￿

gou rules in the grammar
(a) Hiero

8

is incorporated into the SMT system through using a language model, whi
the monolingual English data.

Decoding a Test Sentence

1.1.3 Discriminatively Training
垫子 上 的 狗 of Relative Weights Amon
With the translation and language models, how much should we trust
dianzi weshang ade gou model, and trust the model prop
Intuitively,
can assign weight to each
weight. These weights are usually found through a discriminative training a
the2003).dog on the mat
Och,

X
X
X
S

→
→
→
→

￿ dianzi shang , the mat ￿
1.1.4 Decoding for Test Data
￿ gou , the dog ￿
￿ X0 de X1 , X1 on XWith the bilingual and monolingual training data, we have trained a SM
0￿
following the pipeline in Figure 1.1), which has translation and language m
￿ X0 , X0 ￿
relative weights among the models. Now, we can generate translation out

test data by using the trained SMT system. For example, we may generate a
dog on the mat” for ￿
(a) Hiero rules in the test grammarS→￿X0 , X0“￿￿ ￿ ￿ ￿”, assuming that the translation gramma
a rule “X → ￿ ￿, the dog ￿” that is extracted from other training examples

X→￿X0 de X1 , X1 on X0 ￿
X
X
X
X

X→￿dianzi shang, the mat￿

dianzi shang

de

→ ￿ dianzi shang , the mat ￿
→ ￿ gou , the dog ￿
→ ￿gou, the dog ￿
→ ￿ X1 de X2 , X2 on X1 ￿

gou rules in the grammar
(a) Hiero

8

is incorporated into the SMT system through using a language model, whi
the monolingual English data.

Decoding a Test Sentence

1.1.3 Discriminatively Training
垫子 上 的 狗 of Relative Weights Amon
With the translation and language models, how much should we trust
dianzi weshang ade gou model, and trust the model prop
Intuitively,
can assign weight to each
weight. These weights are usually found through a discriminative training a
the2003).dog on the mat
Och,

X
X
X
S

→
→
→
→

￿ dianzi shang , the mat ￿
1.1.4 Decoding for Test Data
￿ gou , the dog ￿
￿ X0 de X1 , X1 on XWith the bilingual and monolingual training data, we have trained a SM
0￿
following the pipeline in Figure 1.1), which has translation and language m
￿ X0 , X0 ￿
relative weights among the models. Now, we can generate translation out

test data by using the trained SMT system. For example, we may generate a
dog on the mat” for ￿
(a) Hiero rules in the test grammarS→￿X0 , X0“￿￿ ￿ ￿ ￿”, assuming that the translation gramma
a rule “X → ￿ ￿, the dog ￿” that is extracted from other training examples

Derivation Tree

X→￿X0 de X1 , X1 on X0 ￿
X
X
X
X

X→￿dianzi shang, the mat￿

dianzi shang

de

→ ￿ dianzi shang , the mat ￿
→ ￿ gou , the dog ￿
→ ￿gou, the dog ￿
→ ￿ X1 de X2 , X2 on X1 ￿

gou rules in the grammar
(a) Hiero

8

is incorporated into the SMT system through using a language model, whi
the monolingual English data.

Decoding a Test Sentence

1.1.3 Discriminatively Training
垫子 上 的 狗 of Relative Weights Amon
With the translation and language models, how much should we trust
dianzi weshang ade gou model, and trust the model prop
Intuitively,
can assign weight to each
weight. These weights are usually found through a discriminative training a
the2003).dog on the mat
Och,

X
X
X
S

→
→
→
→

￿ dianzi shang , the mat ￿
1.1.4 Decoding for Test Data
￿ gou , the dog ￿
￿ X0 de X1 , X1 on XWith the bilingual and monolingual training data, we have trained a SM
0￿
following the pipeline in Figure 1.1), which has translation and language m
￿ X0 , X0 ￿
relative weights among the models. Now, we can generate translation out

test data by using the trained SMT system. For example, we may generate a
dog on the mat” for ￿
(a) Hiero rules in the test grammarS→￿X0 , X0“￿￿ ￿ ￿ ￿”, assuming that the translation gramma
a rule “X → ￿ ￿, the dog ￿” that is extracted from other training examples

Derivation Tree

Translation is easy?

X→￿X0 de X1 , X1 on X0 ￿
X
X
X
X

X→￿dianzi shang, the mat￿

dianzi shang

de

→ ￿ dianzi shang , the mat ￿
→ ￿ gou , the dog ￿
→ ￿gou, the dog ￿
→ ￿ X1 de X2 , X2 on X1 ￿

gou rules in the grammar
(a) Hiero

8

Translation Ambiguity
垫子 上 的 猫
dianzi shang de mao
a

cat on the mat

9

Translation Ambiguity
垫子 上 的 猫
dianzi shang de mao
a

cat on the mat
X→￿X0 de X1 , X1 on X0 ￿

9

Translation Ambiguity
垫子 上 的 猫
dianzi shang de mao
a

cat on the mat
X→￿X0 de X1 , X1 on X0 ￿

zhongguo de shoudu
capital of China

9

Translation Ambiguity
垫子 上 的 猫
dianzi shang de mao
a

cat on the mat
X→￿X0 de X1 , X1 on X0 ￿

zhongguo de shoudu
capital of China

9

Translation Ambiguity
垫子 上 的 猫
dianzi shang de mao
a

cat on the mat
X→￿X0 de X1 , X1 on X0 ￿

zhongguo de shoudu
capital of China

X→￿X0 de X1 , X1 of X0 ￿

9

Translation Ambiguity
垫子 上 的 猫
dianzi shang de mao
a

cat on the mat
X→￿X0 de X1 , X1 on X0 ￿

zhongguo de shoudu
capital of China
wo de mao
my cat

X→￿X0 de X1 , X1 of X0 ￿

9

Translation Ambiguity
垫子 上 的 猫
dianzi shang de mao
a

cat on the mat
X→￿X0 de X1 , X1 on X0 ￿

zhongguo de shoudu
capital of China
wo de mao
my cat

X→￿X0 de X1 , X1 of X0 ￿

X→￿X0 de X1 , X0 X1 ￿

9

Translation Ambiguity
垫子 上 的 猫
dianzi shang de mao
a

cat on the mat
X→￿X0 de X1 , X1 on X0 ￿

zhongguo de shoudu
capital of China
wo de mao
my cat
zhifei de mao
zhifei ’s cat

X→￿X0 de X1 , X1 of X0 ￿

X→￿X0 de X1 , X0 X1 ￿

9

Translation Ambiguity
垫子 上 的 猫
dianzi shang de mao
a

cat on the mat
X→￿X0 de X1 , X1 on X0 ￿

zhongguo de shoudu
capital of China
wo de mao
my cat
zhifei de mao
zhifei ’s cat

X→￿X0 de X1 , X1 of X0 ￿

X→￿X0 de X1 , X0 X1 ￿

X→￿X0 de X1 , X0 ’s X1 ￿
9

S→￿X0 0XX0 ￿￿ ￿
S→￿X,0,,00X0
S→￿X X￿
S→￿X , 0
X→￿X00de0 X11 X11of 0 X0 ￿
X→￿X0 de X1,, ,X1 , ’s X0 ￿1
X→￿X X X on
X→￿X de de X 0 X 1
X→￿dianzi shang, the mat￿
X→￿dianzi shang, the mat￿
X→￿dianzi shang, the mat￿
X→￿dianzi shang, the mat￿
X→￿mao, aacat￿
X→￿mao, a a cat￿
X→￿mao, cat￿
X→￿mao, cat￿

dianzi00 0shang11 1 de22 2 mao33 3
dianzi 0 shang 1 de 2 mao 3
dianzi shang de
de mao
mao

dianzi shang de mao

10

S→￿X0 0XX0 ￿￿ ￿
S→￿X,0,,00X0
S→￿X X￿
S→￿X , 0
X→￿X00de0 X11 X11of 0 X0 ￿
X→￿X0 de X1,, ,X1 , ’s X0 ￿1
X→￿X X X on
X→￿X de de X 0 X 1
X→￿dianzi shang, the mat￿
X→￿dianzi shang, the mat￿
X→￿dianzi shang, the mat￿
X→￿dianzi shang, the mat￿
X→￿mao, aacat￿
X→￿mao, a a cat￿
X→￿mao, cat￿
X→￿mao, cat￿

dianzi00 0shang11 1 de22 2 mao33 3
dianzi 0 shang 1 de 2 mao 3
dianzi shang de
de mao
mao

Joshua

(chart parser)

dianzi shang de mao

10

S→￿X0 , X0 ￿

S→￿X0 , X0 ￿

X→￿X0 de X1 , X0 X1 ￿

X→￿X0 de X1 , X1 on X0 ￿
X→￿dianzi shang, the mat￿

dianzi0 shang1

X→￿mao, a cat￿

de2

mao3

X→￿dianzi shang, the mat￿

dianzi0 shang1

S→￿X0 , X0 ￿

dianzi0 shang1

de2

mao3

S→￿X0 , X0 ￿

X→￿X0 de X1 , X1 of X0 ￿
X→￿dianzi shang, the mat￿

X→￿mao, a cat￿

X→￿mao, a cat￿

de2

mao3

X→￿X0 de X1 , X0 ’s X1 ￿
X→￿dianzi shang, the mat￿

dianzi0 shang1

X→￿mao, a cat￿

de2

mao3

Joshua

(chart parser)

dianzi shang de mao

11

S→￿X0 , X0 ￿

S→￿X0 , X0 ￿

X→￿X0 de X1 , X0 X1 ￿

X→￿X0 de X1 , X1 on X0 ￿
X→￿dianzi shang, the mat￿

dianzi0 shang1

X→￿mao, a cat￿

de2

mao3

X→￿dianzi shang, the mat￿

dianzi0 shang1

S→￿X0 , X0 ￿

mao3

S→￿X0 , X0 ￿

X→￿X0 de X1 , X1 of X0 ￿

dianzi0 shang1

de2

the mat a cat

a cat on the mat

X→￿dianzi shang, the mat￿

X→￿mao, a cat￿

X→￿mao, a cat￿

de2

mao3

X→￿X0 de X1 , X0 ’s X1 ￿
X→￿dianzi shang, the mat￿

dianzi0 shang1

a cat of the mat

X→￿mao, a cat￿

de2

mao3

the mat ’s a cat

Joshua

(chart parser)

dianzi shang de mao

11

S→￿X0 , X0 ￿

S→￿X0 , X0 ￿

X→￿X0 de X1 , X0 X1 ￿

X→￿X0 de X1 , X1 on X0 ￿
X→￿dianzi shang, the mat￿

dianzi0 shang1

X→￿mao, a cat￿

de2

mao3

X→￿dianzi shang, the mat￿

dianzi0 shang1

S→￿X0 , X0 ￿

mao3

S→￿X0 , X0 ￿

X→￿X0 de X1 , X1 of X0 ￿

dianzi0 shang1

de2

the mat a cat

a cat on the mat

X→￿dianzi shang, the mat￿

X→￿mao, a cat￿

X→￿mao, a cat￿

de2

mao3

X→￿X0 de X1 , X0 ’s X1 ￿
X→￿dianzi shang, the mat￿

dianzi0 shang1

a cat of the mat

X→￿mao, a cat￿

de2

mao3

the mat ’s a cat

Joshua

(chart parser)

dianzi shang de mao

11

S 0,4
S→￿X0 , X0 ￿

ypergraph
h

X 0,4
X→￿X0 de X1 , X1 on X0 ￿

S→￿X0 ’s0 , X ￿
0
X→￿X0 de X1S→￿X,0XX0 ￿0 ￿
, S→￿X, X￿1
X0
X→￿X0 de X1 , X0 X1 ￿

X→￿X0 de X1 , X1 of X0 ￿

X→￿X0 dede 1 , X0 ’s on1X0 ￿
X
￿
X→￿X0 de X1 ,,X1 X X0 ￿
X→￿X0
X1 X1 of

X 0,2

X→￿dianzi shang, the mat￿
X→￿dianzi shang, the mat￿
X→￿dianzi shang, the mat￿ X→￿mao, a cat￿
X→￿mao, a cat￿
X→￿mao, a cat￿

X 3,4

dianzi0 0 0shang11 1 de22 2 mao33 3
dianzi shang de mao
mao
dianzi shang de

X→￿dianzi shang, the mat￿

dianzi0 shang1

X→￿mao, a cat￿

de2

mao3

(a) A hypergraph encodes four different derivation trees as shown
in the four ﬁgures below. Rectangles represent items (or nodes),
where each item is identiﬁed by the non-terminal symbol and
source span. An item has one or more incoming hyperedges,
(chart deriving the
which represent different ways of parser) item. A hyperedge consists of a rule, and a pointer to an antecedent item for
each non-terminal symbol in the rule.

Joshua

dianzi shang de mao

12

S 0,4
S→￿X0 , X0 ￿

X 0,4
X→￿X0 de X1 , X1 on X0 ￿

X→￿X0 de X1 , X0 ’s X1 ￿
X→￿X0 de X1 , X0 X1 ￿

X→￿X0 de X1 , X1 of X0 ￿

X 0,2
X→￿dianzi shang, the mat￿

dianzi0 shang1

X 3,4
X→￿mao, a cat￿

de2

mao3

(a) A hypergraph encodes four different derivation trees as shown

13

A hypergraph is a compact data structure to
encode exponentially many trees.
S 0,4
S→￿X0 , X0 ￿

X 0,4
X→￿X0 de X1 , X1 on X0 ￿

X→￿X0 de X1 , X0 ’s X1 ￿
X→￿X0 de X1 , X0 X1 ￿

X→￿X0 de X1 , X1 of X0 ￿

X 0,2
X→￿dianzi shang, the mat￿

dianzi0 shang1

X 3,4
X→￿mao, a cat￿

de2

mao3

(a) A hypergraph encodes four different derivation trees as shown

13

A hypergraph is a compact data structure to
encode exponentially many trees.
S 0,4
S→￿X0 , X0 ￿

X 0,4
X→￿X0 de X1 , X1 on X0 ￿

X→￿X0 de X1 , X0 ’s X1 ￿
X→￿X0 de X1 , X0 X1 ￿

node

X→￿X0 de X1 , X1 of X0 ￿

X 0,2
X→￿dianzi shang, the mat￿

dianzi0 shang1

X 3,4
X→￿mao, a cat￿

de2

mao3

(a) A hypergraph encodes four different derivation trees as shown

13

A hypergraph is a compact data structure to
encode exponentially many trees.
S 0,4
S→￿X0 , X0 ￿

X 0,4

yperedge
h

X→￿X0 de X1 , X1 on X0 ￿

X→￿X0 de X1 , X0 ’s X1 ￿
X→￿X0 de X1 , X0 X1 ￿

node

X→￿X0 de X1 , X1 of X0 ￿

X 0,2
X→￿dianzi shang, the mat￿

dianzi0 shang1

X 3,4
X→￿mao, a cat￿

de2

mao3

(a) A hypergraph encodes four different derivation trees as shown

13

A hypergraph is a compact data structure to
encode exponentially many trees.
S 0,4
S→￿X0 , X0 ￿

X 0,4

yperedge
h

X→￿X0 de X1 , X1 on X0 ￿

X→￿X0 de X1 , X0 ’s X1 ￿
X→￿X0 de X1 , X0 X1 ￿

node

X→￿X0 de X1 , X1 of X0 ￿

X 0,2
X→￿dianzi shang, the mat￿

dianzi0 shang1

X 3,4
X→￿mao, a cat￿

de2

mao3

(a) A hypergraph encodes four different derivation trees as shown

13

A hypergraph is a compact data structure to
encode exponentially many trees.
S 0,4

hyperedge

S→￿X0 , X0 ￿

X 0,4

yperedge
h

X→￿X0 de X1 , X1 on X0 ￿

X→￿X0 de X1 , X0 ’s X1 ￿
X→￿X0 de X1 , X0 X1 ￿

node

edge

X→￿X0 de X1 , X1 of X0 ￿

X 0,2
X→￿dianzi shang, the mat￿

dianzi0 shang1

X 3,4
X→￿mao, a cat￿

de2

mao3

(a) A hypergraph encodes four different derivation trees as shown

13

A hypergraph is a compact data structure to
encode exponentially many trees.
S 0,4

hyperedge

S→￿X0 , X0 ￿

X 0,4

yperedge
h

X→￿X0 de X1 , X1 on X0 ￿

X→￿X0 de X1 , X0 ’s X1 ￿
X→￿X0 de X1 , X0 X1 ￿

node

edge

X→￿X0 de X1 , X1 of X0 ￿

FSA

X 0,2
X→￿dianzi shang, the mat￿

dianzi0 shang1

X 3,4
X→￿mao, a cat￿

de2

mao3

(a) A hypergraph encodes four different derivation trees as shown

13

A hypergraph is a compact data structure to
encode exponentially many trees.
S 0,4

hyperedge

S→￿X0 , X0 ￿

X 0,4

yperedge
h

X→￿X0 de X1 , X1 on X0 ￿

X→￿X0 de X1 , X0 ’s X1 ￿
X→￿X0 de X1 , X0 X1 ￿

node

edge

X→￿X0 de X1 , X1 of X0 ￿

FSA

X 0,2
X→￿dianzi shang, the mat￿

dianzi0 shang1

X 3,4
X→￿mao, a cat￿

de2

Packed
Forest

mao3

(a) A hypergraph encodes four different derivation trees as shown

13

A hypergraph is a compact data structure to
encode exponentially many trees.
S 0,4
S→￿X0 , X0 ￿

X 0,4
X→￿X0 de X1 , X1 on X0 ￿

X→￿X0 de X1 , X0 ’s X1 ￿
X→￿X0 de X1 , X0 X1 ￿

X→￿X0 de X1 , X1 of X0 ￿

X 0,2
X→￿dianzi shang, the mat￿

dianzi0 shang1

X 3,4
X→￿mao, a cat￿

de2

mao3

(a) A hypergraph encodes four different derivation trees as shown

14

A hypergraph is a compact data structure to
encode exponentially many trees.
S 0,4
S→￿X0 , X0 ￿

X 0,4
X→￿X0 de X1 , X1 on X0 ￿

X→￿X0 de X1 , X0 ’s X1 ￿
X→￿X0 de X1 , X0 X1 ￿

X→￿X0 de X1 , X1 of X0 ￿

X 0,2
X→￿dianzi shang, the mat￿

dianzi0 shang1

X 3,4
X→￿mao, a cat￿

de2

mao3

(a) A hypergraph encodes four different derivation trees as shown

14

A hypergraph is a compact data structure to
encode exponentially many trees.
S 0,4
S→￿X0 , X0 ￿

X 0,4
X→￿X0 de X1 , X1 on X0 ￿

X→￿X0 de X1 , X0 ’s X1 ￿
X→￿X0 de X1 , X0 X1 ￿

X→￿X0 de X1 , X1 of X0 ￿

X 0,2
X→￿dianzi shang, the mat￿

dianzi0 shang1

X 3,4
X→￿mao, a cat￿

de2

mao3

(a) A hypergraph encodes four different derivation trees as shown

15

A hypergraph is a compact data structure to
encode exponentially many trees.
S 0,4
S→￿X0 , X0 ￿

X 0,4
X→￿X0 de X1 , X1 on X0 ￿

X→￿X0 de X1 , X0 ’s X1 ￿
X→￿X0 de X1 , X0 X1 ￿

X→￿X0 de X1 , X1 of X0 ￿

X 0,2
X→￿dianzi shang, the mat￿

dianzi0 shang1

X 3,4
X→￿mao, a cat￿

de2

mao3

(a) A hypergraph encodes four different derivation trees as shown

15

A hypergraph is a compact data structure to
encode exponentially many trees.
S 0,4
S→￿X0 , X0 ￿

X 0,4
X→￿X0 de X1 , X1 on X0 ￿

X→￿X0 de X1 , X0 ’s X1 ￿
X→￿X0 de X1 , X0 X1 ￿

X→￿X0 de X1 , X1 of X0 ￿

X 0,2
X→￿dianzi shang, the mat￿

dianzi0 shang1

X 3,4
X→￿mao, a cat￿

de2

mao3

(a) A hypergraph encodes four different derivation trees as shown

16

A hypergraph is a compact data structure to
encode exponentially many trees.
S 0,4
S→￿X0 , X0 ￿

X 0,4
X→￿X0 de X1 , X1 on X0 ￿

X→￿X0 de X1 , X0 ’s X1 ￿
X→￿X0 de X1 , X0 X1 ￿

X→￿X0 de X1 , X1 of X0 ￿

X 0,2
X→￿dianzi shang, the mat￿

dianzi0 shang1

X 3,4
X→￿mao, a cat￿

de2

mao3

(a) A hypergraph encodes four different derivation trees as shown

16

A hypergraph is a compact data structure to
encode exponentially many trees.
S 0,4
S→￿X0 , X0 ￿

X 0,4
X→￿X0 de X1 , X1 on X0 ￿

X→￿X0 de X1 , X0 ’s X1 ￿
X→￿X0 de X1 , X0 X1 ￿

X→￿X0 de X1 , X1 of X0 ￿

X 0,2
X→￿dianzi shang, the mat￿

dianzi0 shang1

X 3,4
X→￿mao, a cat￿

de2

mao3

(a) A hypergraph encodes four different derivation trees as shown

17

A hypergraph is a compact data structure to
encode exponentially many trees.
S 0,4
S→￿X0 , X0 ￿

X 0,4
X→￿X0 de X1 , X1 on X0 ￿

X→￿X0 de X1 , X0 ’s X1 ￿
X→￿X0 de X1 , X0 X1 ￿

X→￿X0 de X1 , X1 of X0 ￿

X 0,2
X→￿dianzi shang, the mat￿

dianzi0 shang1

X 3,4
X→￿mao, a cat￿

de2

mao3

(a) A hypergraph encodes four different derivation trees as shown

17

A hypergraph is a compact data structure to
encode exponentially many trees.
S 0,4

Structure sharing

S→￿X0 , X0 ￿

X 0,4
X→￿X0 de X1 , X1 on X0 ￿

X→￿X0 de X1 , X0 ’s X1 ￿
X→￿X0 de X1 , X0 X1 ￿

X→￿X0 de X1 , X1 of X0 ￿

X 0,2
X→￿dianzi shang, the mat￿

dianzi0 shang1

X 3,4
X→￿mao, a cat￿

de2

mao3

(a) A hypergraph encodes four different derivation trees as shown

17

Why Hypergraphs?

•

Contains a much larger hypothesis space
than a k-best list

•

General compact data structure

•

•

special cases include

•
•
•

ﬁnite state machine (e.g., lattice),
and/or graph
packed forest

can be used for speech, parsing,
tree-based MT systems, and many more

18

S 0,4
S→￿X0 , X0 ￿

X 0,4
X→￿X0 de X1 , X1 on X0 ￿

X→￿X0 de X1 , X0 ’s X1 ￿
X→￿X0 de X1 , X0 X1 ￿

X→￿X0 de X1 , X1 of X0 ￿

X 0,2

X 3,4

X→￿dianzi shang, the mat￿

X→￿mao, a cat￿

dianzi0 shang1

de2

S→￿X0 , X0 ￿

mao3

X→￿X0 de X1 , X0 X1 ￿

(a) A hypergraph encodes four different derivation trees as shown
in the four ﬁgures below. Rectangles represent items (or nodes),
where each item is identiﬁed by the non-terminal symbol and
S→￿X
source span. An item has one or more incoming hyperedges, 0 , X0 ￿
which represent different ways of deriving the item. A hyperX→￿X de
edge consists of a rule, and a pointer to an antecedent item0 for X1 , X0 ’s X1 ￿
each non-terminal symbol in the rule.
X→￿dianzi shang, the mat￿
S→￿X0 , X0 ￿

dianzi0 shang1

X→￿dianzi shang, the mat￿

dianzi0 shang
nslation: a cat on the mat 1

X→￿mao, a cat￿

de2

mao3
(c) Translation: the mat a cat

dianzi0 shang1

X→￿mao, a cat￿

de2

X→￿X0 de X1 , X1 on X0 ￿

X→￿dianzi shang, the mat￿

mao3

X→￿mao, a cat￿

de2

mao3

S→￿X0 , X0 ￿
X→￿X0 de X1 , X1 of X0 ￿
X→￿dianzi shang, the mat￿

dianzi0 shang1

X→￿mao, a cat￿

de2

mao3

19

S 0,4
S→￿X0 , X0 ￿

X 0,4
X→￿X0 de X1 , X1 on X0 ￿

Weighted Hypergraph

X→￿X0 de X1 , X0 ’s X1 ￿
X→￿X0 de X1 , X0 X1 ￿

X→￿X0 de X1 , X1 of X0 ￿

X 0,2

X 3,4

X→￿dianzi shang, the mat￿

X→￿mao, a cat￿

dianzi0 shang1

de2

mao3

X→￿X0 de X1 , X0 X1 ￿

(a) A hypergraph encodes four different derivation trees as shown
in the four ﬁgures below. Rectangles represent items (or nodes),
where each item is identiﬁed by the non-terminal symbol and
S→￿X
source span. An item has one or more incoming hyperedges, 0 , X0 ￿
which represent different ways of deriving the item. A hyperX→￿X de
edge consists of a rule, and a pointer to an antecedent item0 for X1 , X0 ’s X1 ￿
each non-terminal symbol in the rule.

p=3

p=1

X→￿dianzi shang, the mat￿
S→￿X0 , X0 ￿

dianzi0 shang1

dianzi0 shang
nslation: a cat on the mat 1

X→￿mao, a cat￿

de2

mao3
(c) Translation: the mat a cat

X→￿dianzi shang, the mat￿

dianzi0 shang1

X→￿mao, a cat￿

de2

X→￿X0 de X1 , X1 on X0 ￿
X→￿dianzi shang, the mat￿

p=2

S→￿X0 , X0 ￿

mao3

X→￿mao, a cat￿

de2

S→￿X0 , X0 ￿

mao3

p=2

X→￿X0 de X1 , X1 of X0 ￿
X→￿dianzi shang, the mat￿

dianzi0 shang1

X→￿mao, a cat￿

de2

mao3

19

S 0,4

Linear model:

S→￿X0 , X0 ￿

X 0,4

p(d | x) = θ · Φ(d, x)

X→￿X0 de X1 , X1 on X0 ￿

Weighted Hypergraph

X→￿X0 de X1 , X0 ’s X1 ￿
X→￿X0 de X1 , X0 X1 ￿

X→￿X0 de X1 , X1 of X0 ￿

X 0,2

X 3,4

X→￿dianzi shang, the mat￿

X→￿mao, a cat￿

dianzi0 shang1

de2

mao3

X→￿X0 de X1 , X0 X1 ￿

(a) A hypergraph encodes four different derivation trees as shown
in the four ﬁgures below. Rectangles represent items (or nodes),
where each item is identiﬁed by the non-terminal symbol and
S→￿X
source span. An item has one or more incoming hyperedges, 0 , X0 ￿
which represent different ways of deriving the item. A hyperX→￿X de
edge consists of a rule, and a pointer to an antecedent item0 for X1 , X0 ’s X1 ￿
each non-terminal symbol in the rule.

p=3

p=1

X→￿dianzi shang, the mat￿
S→￿X0 , X0 ￿

dianzi0 shang1

dianzi0 shang
nslation: a cat on the mat 1

X→￿mao, a cat￿

de2

mao3
(c) Translation: the mat a cat

X→￿dianzi shang, the mat￿

dianzi0 shang1

X→￿mao, a cat￿

de2

X→￿X0 de X1 , X1 on X0 ￿
X→￿dianzi shang, the mat￿

p=2

S→￿X0 , X0 ￿

mao3

X→￿mao, a cat￿

de2

S→￿X0 , X0 ￿

mao3

p=2

X→￿X0 de X1 , X1 of X0 ￿
X→￿dianzi shang, the mat￿

dianzi0 shang1

X→￿mao, a cat￿

de2

mao3

19

S 0,4

Linear model:

S→￿X0 , X0 ￿

X 0,4

p(d | x) = θ · Φ(d, x)

X→￿X0 de X1 , X1 on X0 ￿

Weighted Hypergraph

X→￿X0 de X1 , X0 ’s X1 ￿
X→￿X0 de X1 , X0 X1 ￿

X→￿X0 de X1 , X1 of X0 ￿

X 0,2

foreign input

X 3,4

X→￿dianzi shang, the mat￿

X→￿mao, a cat￿

dianzi0 shang1

de2

mao3

X→￿X0 de X1 , X0 X1 ￿

(a) A hypergraph encodes four different derivation trees as shown
in the four ﬁgures below. Rectangles represent items (or nodes),
where each item is identiﬁed by the non-terminal symbol and
S→￿X
source span. An item has one or more incoming hyperedges, 0 , X0 ￿
which represent different ways of deriving the item. A hyperX→￿X de
edge consists of a rule, and a pointer to an antecedent item0 for X1 , X0 ’s X1 ￿
each non-terminal symbol in the rule.

p=3

p=1

X→￿dianzi shang, the mat￿
S→￿X0 , X0 ￿

dianzi0 shang1

dianzi0 shang
nslation: a cat on the mat 1

X→￿mao, a cat￿

de2

mao3
(c) Translation: the mat a cat

X→￿dianzi shang, the mat￿

dianzi0 shang1

X→￿mao, a cat￿

de2

X→￿X0 de X1 , X1 on X0 ￿
X→￿dianzi shang, the mat￿

p=2

S→￿X0 , X0 ￿

mao3

X→￿mao, a cat￿

de2

S→￿X0 , X0 ￿

mao3

p=2

X→￿X0 de X1 , X1 of X0 ￿
X→￿dianzi shang, the mat￿

dianzi0 shang1

X→￿mao, a cat￿

de2

mao3

19

S 0,4

Linear model:

S→￿X0 , X0 ￿

X 0,4

p(d | x) = θ · Φ(d, x)

X→￿X0 de X1 , X1 on X0 ￿

Weighted Hypergraph

X→￿X0 de X1 , X0 ’s X1 ￿
X→￿X0 de X1 , X0 X1 ￿

X→￿X0 de X1 , X1 of X0 ￿

X 0,2

derivation
foreign input

X 3,4

X→￿dianzi shang, the mat￿

X→￿mao, a cat￿

dianzi0 shang1

de2

mao3

X→￿X0 de X1 , X0 X1 ￿

(a) A hypergraph encodes four different derivation trees as shown
in the four ﬁgures below. Rectangles represent items (or nodes),
where each item is identiﬁed by the non-terminal symbol and
S→￿X
source span. An item has one or more incoming hyperedges, 0 , X0 ￿
which represent different ways of deriving the item. A hyperX→￿X de
edge consists of a rule, and a pointer to an antecedent item0 for X1 , X0 ’s X1 ￿
each non-terminal symbol in the rule.

p=3

p=1

X→￿dianzi shang, the mat￿
S→￿X0 , X0 ￿

dianzi0 shang1

dianzi0 shang
nslation: a cat on the mat 1

X→￿mao, a cat￿

de2

mao3
(c) Translation: the mat a cat

X→￿dianzi shang, the mat￿

dianzi0 shang1

X→￿mao, a cat￿

de2

X→￿X0 de X1 , X1 on X0 ￿
X→￿dianzi shang, the mat￿

p=2

S→￿X0 , X0 ￿

mao3

X→￿mao, a cat￿

de2

S→￿X0 , X0 ￿

mao3

p=2

X→￿X0 de X1 , X1 of X0 ￿
X→￿dianzi shang, the mat￿

dianzi0 shang1

X→￿mao, a cat￿

de2

mao3

19

S 0,4

Linear model:

S→￿X0 , X0 ￿

X 0,4

p(d | x) = θ · Φ(d, x)

X→￿X0 de X1 , X1 on X0 ￿

Weighted Hypergraph

X→￿X0 de X1 , X0 ’s X1 ￿
X→￿X0 de X1 , X0 X1 ￿

derivation
foreign input

X→￿X0 de X1 , X1 of X0 ￿

features

X 0,2

X 3,4

X→￿dianzi shang, the mat￿

X→￿mao, a cat￿

dianzi0 shang1

de2

mao3

X→￿X0 de X1 , X0 X1 ￿

(a) A hypergraph encodes four different derivation trees as shown
in the four ﬁgures below. Rectangles represent items (or nodes),
where each item is identiﬁed by the non-terminal symbol and
S→￿X
source span. An item has one or more incoming hyperedges, 0 , X0 ￿
which represent different ways of deriving the item. A hyperX→￿X de
edge consists of a rule, and a pointer to an antecedent item0 for X1 , X0 ’s X1 ￿
each non-terminal symbol in the rule.

p=3

p=1

X→￿dianzi shang, the mat￿
S→￿X0 , X0 ￿

dianzi0 shang1

dianzi0 shang
nslation: a cat on the mat 1

X→￿mao, a cat￿

de2

mao3
(c) Translation: the mat a cat

X→￿dianzi shang, the mat￿

dianzi0 shang1

X→￿mao, a cat￿

de2

X→￿X0 de X1 , X1 on X0 ￿
X→￿dianzi shang, the mat￿

p=2

S→￿X0 , X0 ￿

mao3

X→￿mao, a cat￿

de2

S→￿X0 , X0 ￿

mao3

p=2

X→￿X0 de X1 , X1 of X0 ￿
X→￿dianzi shang, the mat￿

dianzi0 shang1

X→￿mao, a cat￿

de2

mao3

19

S 0,4

Linear model:

S→￿X0 , X0 ￿

X 0,4

p(d | x) = θ · Φ(d, x)

X→￿X0 de X1 , X1 on X0 ￿

Weighted Hypergraph

X→￿X0 de X1 , X0 ’s X1 ￿
X→￿X0 de X1 , X0 X1 ￿

X→￿X0 de X1 , X1 of X0 ￿

weights
X 0,2

derivation
foreign input

features

X 3,4

X→￿dianzi shang, the mat￿

X→￿mao, a cat￿

dianzi0 shang1

de2

mao3

X→￿X0 de X1 , X0 X1 ￿

(a) A hypergraph encodes four different derivation trees as shown
in the four ﬁgures below. Rectangles represent items (or nodes),
where each item is identiﬁed by the non-terminal symbol and
S→￿X
source span. An item has one or more incoming hyperedges, 0 , X0 ￿
which represent different ways of deriving the item. A hyperX→￿X de
edge consists of a rule, and a pointer to an antecedent item0 for X1 , X0 ’s X1 ￿
each non-terminal symbol in the rule.

p=3

p=1

X→￿dianzi shang, the mat￿
S→￿X0 , X0 ￿

dianzi0 shang1

dianzi0 shang
nslation: a cat on the mat 1

X→￿mao, a cat￿

de2

mao3
(c) Translation: the mat a cat

X→￿dianzi shang, the mat￿

dianzi0 shang1

X→￿mao, a cat￿

de2

X→￿X0 de X1 , X1 on X0 ￿
X→￿dianzi shang, the mat￿

p=2

S→￿X0 , X0 ￿

mao3

X→￿mao, a cat￿

de2

S→￿X0 , X0 ￿

mao3

p=2

X→￿X0 de X1 , X1 of X0 ￿
X→￿dianzi shang, the mat￿

dianzi0 shang1

X→￿mao, a cat￿

de2

mao3

19

S 0,4

Log-linear model:

S→￿X0 , X0 ￿

X 0,4

p(d | x) =

Probabilistic
Hypergraph

X→￿X0 de X1 , X1 on X0 ￿

X→￿X0 de X1 , X0 ’s X1 ￿
X→￿X0 de X1 , X0 X1 ￿

X→￿X0 de X1 , X1 of X0 ￿

X 0,2

eθ·Φ(d,x)
Z(x)

Z=2+1+3+2=8
X 3,4

X→￿dianzi shang, the mat￿

X→￿mao, a cat￿

dianzi0 shang1

de2

S→￿X0 , X0 ￿

mao3

X→￿X0 de X1 , X0 X1 ￿

(a) A hypergraph encodes four different derivation trees as shown
in the four ﬁgures below. Rectangles represent items (or nodes),
where each item is identiﬁed by the non-terminal symbol and
S→￿X
source span. An item has one or more incoming hyperedges, 0 , X0 ￿
which represent different ways of deriving the item. A hyperX→￿X de
edge consists of a rule, and a pointer to an antecedent item0 for X1 , X0 ’s X1 ￿
each non-terminal symbol in the rule.
X→￿dianzi shang, the mat￿
S→￿X0 , X0 ￿

dianzi0 shang1

X→￿dianzi shang, the mat￿

dianzi0 shang
nslation: a cat on the mat 1

X→￿mao, a cat￿

de2

mao3
(c) Translation: the mat a cat

dianzi0 shang1

X→￿mao, a cat￿

de2

X→￿X0 de X1 , X1 on X0 ￿

X→￿dianzi shang, the mat￿

mao3

X→￿mao, a cat￿

de2

mao3

S→￿X0 , X0 ￿
X→￿X0 de X1 , X1 of X0 ￿
X→￿dianzi shang, the mat￿

dianzi0 shang1

X→￿mao, a cat￿

de2

mao3

20

S 0,4

Log-linear model:

S→￿X0 , X0 ￿

X 0,4

p(d | x) =

Probabilistic
Hypergraph

X→￿X0 de X1 , X1 on X0 ￿

X→￿X0 de X1 , X0 ’s X1 ￿
X→￿X0 de X1 , X0 X1 ￿

X→￿X0 de X1 , X1 of X0 ￿

X 0,2

eθ·Φ(d,x)
Z(x)

Z=2+1+3+2=8
X 3,4

X→￿dianzi shang, the mat￿

X→￿mao, a cat￿

dianzi0 shang1

de2

mao3

(a) A hypergraph encodes four different derivation trees as shown
in the four ﬁgures below. Rectangles represent items (or nodes),
where each item is identiﬁed by the non-terminal symbol and
S→￿X
source span. An item has one or more incoming hyperedges, 0 , X0 ￿
which represent different ways of deriving the item. A hyperX→￿X de
edge consists of a rule, and a pointer to an antecedent item0 for X1 , X0 ’s X1 ￿
each non-terminal symbol in the rule.

p=3/8

p=1/8

X→￿dianzi shang, the mat￿

S→￿X0 , X0 ￿

dianzi0 shang1

dianzi0 shang
nslation: a cat on the mat 1

X→￿mao, a cat￿

de2

mao3
(c) Translation: the mat a cat

X→￿X0 de X1 , X0 X1 ￿
X→￿dianzi shang, the mat￿

dianzi0 shang1

X→￿mao, a cat￿

de2

X→￿X0 de X1 , X1 on X0 ￿
X→￿dianzi shang, the mat￿

p=2/8

S→￿X0 , X0 ￿

mao3

X→￿mao, a cat￿

de2

mao3

p=2/8

S→￿X0 , X0 ￿

X→￿X0 de X1 , X1 of X0 ￿
X→￿dianzi shang, the mat￿

dianzi0 shang1

X→￿mao, a cat￿

de2

mao3

20

S 0,4
S→￿X0 , X0 ￿

X 0,4

Probabilistic
Hypergraph

X→￿X0 de X1 , X1 on X0 ￿

X→￿X0 de X1 , X0 ’s X1 ￿
X→￿X0 de X1 , X0 X1 ￿

X→￿X0 de X1 , X1 of X0 ￿

X 0,2

X 3,4

X→￿dianzi shang, the mat￿

X→￿mao, a cat￿

dianzi0 shang1

de2

mao3

(a) A hypergraph encodes four different derivation trees as shown
in the four ﬁgures below. Rectangles represent items (or nodes),
where each item is identiﬁed by the non-terminal symbol and
S→￿X
source span. An item has one or more incoming hyperedges, 0 , X0 ￿
which represent different ways of deriving the item. A hyperX→￿X de
edge consists of a rule, and a pointer to an antecedent item0 for X1 , X0 ’s X1 ￿
each non-terminal symbol in the rule.

p=3/8

p=1/8

X→￿dianzi shang, the mat￿

S→￿X0 , X0 ￿

dianzi0 shang1

dianzi0 shang
nslation: a cat on the mat 1

X→￿mao, a cat￿

de2

mao3
(c) Translation: the mat a cat

X→￿X0 de X1 , X0 X1 ￿
X→￿dianzi shang, the mat￿

dianzi0 shang1

X→￿mao, a cat￿

de2

X→￿X0 de X1 , X1 on X0 ￿
X→￿dianzi shang, the mat￿

p=2/8

S→￿X0 , X0 ￿

mao3

X→￿mao, a cat￿

de2

mao3

p=2/8

S→￿X0 , X0 ￿

X→￿X0 de X1 , X1 of X0 ￿
X→￿dianzi shang, the mat￿

dianzi0 shang1

X→￿mao, a cat￿

de2

mao3

21

S 0,4

The hypergraph deﬁnes a probability
distribution over trees!

S→￿X0 , X0 ￿

X 0,4

Probabilistic
Hypergraph

X→￿X0 de X1 , X1 on X0 ￿

X→￿X0 de X1 , X0 ’s X1 ￿
X→￿X0 de X1 , X0 X1 ￿

X→￿X0 de X1 , X1 of X0 ￿

X 0,2

X 3,4

X→￿dianzi shang, the mat￿

X→￿mao, a cat￿

dianzi0 shang1

de2

mao3

(a) A hypergraph encodes four different derivation trees as shown
in the four ﬁgures below. Rectangles represent items (or nodes),
where each item is identiﬁed by the non-terminal symbol and
S→￿X
source span. An item has one or more incoming hyperedges, 0 , X0 ￿
which represent different ways of deriving the item. A hyperX→￿X de
edge consists of a rule, and a pointer to an antecedent item0 for X1 , X0 ’s X1 ￿
each non-terminal symbol in the rule.

p=3/8

p=1/8

X→￿dianzi shang, the mat￿

S→￿X0 , X0 ￿

dianzi0 shang1

dianzi0 shang
nslation: a cat on the mat 1

X→￿mao, a cat￿

de2

mao3
(c) Translation: the mat a cat

X→￿X0 de X1 , X0 X1 ￿
X→￿dianzi shang, the mat￿

dianzi0 shang1

X→￿mao, a cat￿

de2

X→￿X0 de X1 , X1 on X0 ￿
X→￿dianzi shang, the mat￿

p=2/8

S→￿X0 , X0 ￿

mao3

X→￿mao, a cat￿

de2

mao3

p=2/8

S→￿X0 , X0 ￿

X→￿X0 de X1 , X1 of X0 ￿
X→￿dianzi shang, the mat￿

dianzi0 shang1

X→￿mao, a cat￿

de2

mao3

21

S 0,4

The hypergraph deﬁnes a probability
distribution over trees!
the distribution is parameterized by Θ

S→￿X0 , X0 ￿

X 0,4

Probabilistic
Hypergraph

X→￿X0 de X1 , X1 on X0 ￿

X→￿X0 de X1 , X0 ’s X1 ￿
X→￿X0 de X1 , X0 X1 ￿

X→￿X0 de X1 , X1 of X0 ￿

X 0,2

X 3,4

X→￿dianzi shang, the mat￿

X→￿mao, a cat￿

dianzi0 shang1

de2

mao3

(a) A hypergraph encodes four different derivation trees as shown
in the four ﬁgures below. Rectangles represent items (or nodes),
where each item is identiﬁed by the non-terminal symbol and
S→￿X
source span. An item has one or more incoming hyperedges, 0 , X0 ￿
which represent different ways of deriving the item. A hyperX→￿X de
edge consists of a rule, and a pointer to an antecedent item0 for X1 , X0 ’s X1 ￿
each non-terminal symbol in the rule.

p=3/8

p=1/8

X→￿dianzi shang, the mat￿

S→￿X0 , X0 ￿

dianzi0 shang1

dianzi0 shang
nslation: a cat on the mat 1

X→￿mao, a cat￿

de2

mao3
(c) Translation: the mat a cat

X→￿X0 de X1 , X0 X1 ￿
X→￿dianzi shang, the mat￿

dianzi0 shang1

X→￿mao, a cat￿

de2

X→￿X0 de X1 , X1 on X0 ￿
X→￿dianzi shang, the mat￿

p=2/8

S→￿X0 , X0 ￿

mao3

X→￿mao, a cat￿

de2

mao3

p=2/8

S→￿X0 , X0 ￿

X→￿X0 de X1 , X1 of X0 ￿
X→￿dianzi shang, the mat￿

dianzi0 shang1

X→￿mao, a cat￿

de2

mao3

21

S 0,4

The hypergraph deﬁnes a probability
distribution over trees!
the distribution is parameterized by Θ

S→￿X0 , X0 ￿

X 0,4

Probabilistic
Hypergraph

X→￿X0 de X1 , X1 on X0 ￿

X→￿X0 de X1 , X0 ’s X1 ￿
X→￿X0 de X1 , X0 X1 ￿

X→￿X0 de X1 , X1 of X0 ￿

X 0,2
X→￿dianzi shang, the mat￿

dianzi0 shang1

X 3,4
X→￿mao, a cat￿

de2

mao3

(a) A hypergraph encodes four different derivation trees as shown
in the four ﬁgures below. Rectangles represent items (or nodes),
where each item is identiﬁed by the non-terminal symbol and
source span. An item has one or more incoming hyperedges,
which represent different ways of deriving the item. A hyperedge consists of a rule, and a pointer to an antecedent item for
each non-terminal symbol in the rule.

nslation: a cat on the mat

(c) Translation: the mat a cat

22

S 0,4

The hypergraph deﬁnes a probability
distribution over trees!
the distribution is parameterized by Θ

S→￿X0 , X0 ￿

X 0,4

Probabilistic
Hypergraph

X→￿X0 de X1 , X1 on X0 ￿

X→￿X0 de X1 , X0 ’s X1 ￿
X→￿X0 de X1 , X0 X1 ￿

X→￿X0 de X1 , X1 of X0 ￿

X 0,2
X→￿dianzi shang, the mat￿

dianzi0 shang1

X 3,4
X→￿mao, a cat￿

de2

mao3

(a) A hypergraph encodes four different derivation trees as shown
in the four ﬁgures below. Rectangles represent items (or nodes),
where each item is identiﬁed by the non-terminal symbol and
source span. An item has one or more incoming hyperedges,
which represent different ways of deriving the item. A hyperedge consists of a rule, and a pointer to an antecedent item for
each non-terminal symbol in the rule.

Which translation do we present to a user?

nslation: a cat on the mat

(c) Translation: the mat a cat

Decoding

22

S 0,4

The hypergraph deﬁnes a probability
distribution over trees!
the distribution is parameterized by Θ

S→￿X0 , X0 ￿

X 0,4

Probabilistic
Hypergraph

X→￿X0 de X1 , X1 on X0 ￿

X→￿X0 de X1 , X0 ’s X1 ￿
X→￿X0 de X1 , X0 X1 ￿

X→￿X0 de X1 , X1 of X0 ￿

X 0,2
X→￿dianzi shang, the mat￿

dianzi0 shang1

X 3,4
X→￿mao, a cat￿

de2

mao3

(a) A hypergraph encodes four different derivation trees as shown
in the four ﬁgures below. Rectangles represent items (or nodes),
where each item is identiﬁed by the non-terminal symbol and
source span. An item has one or more incoming hyperedges,
which represent different ways of deriving the item. A hyperedge consists of a rule, and a pointer to an antecedent item for
each non-terminal symbol in the rule.

Which translation do we present to a user?

Decoding

How do we set the parameters Θ?

Training

nslation: a cat on the mat

(c) Translation: the mat a cat

22

S 0,4

The hypergraph deﬁnes a probability
distribution over trees!
the distribution is parameterized by Θ

S→￿X0 , X0 ￿

X 0,4

Probabilistic
Hypergraph

X→￿X0 de X1 , X1 on X0 ￿

X→￿X0 de X1 , X0 ’s X1 ￿
X→￿X0 de X1 , X0 X1 ￿

X→￿X0 de X1 , X1 of X0 ￿

X 0,2
X→￿dianzi shang, the mat￿

dianzi0 shang1

X 3,4
X→￿mao, a cat￿

de2

mao3

(a) A hypergraph encodes four different derivation trees as shown
in the four ﬁgures below. Rectangles represent items (or nodes),
where each item is identiﬁed by the non-terminal symbol and
source span. An item has one or more incoming hyperedges,
which represent different ways of deriving the item. A hyperedge consists of a rule, and a pointer to an antecedent item for
each non-terminal symbol in the rule.

Which translation do we present to a user?

Decoding

How do we set the parameters Θ?

Training

What atomic operations do we need to perform? Atomic Inference

nslation: a cat on the mat

(c) Translation: the mat a cat

22

S 0,4

The hypergraph deﬁnes a probability
distribution over trees!
the distribution is parameterized by Θ

S→￿X0 , X0 ￿

X 0,4

Probabilistic
Hypergraph

X→￿X0 de X1 , X1 on X0 ￿

X→￿X0 de X1 , X0 ’s X1 ￿
X→￿X0 de X1 , X0 X1 ￿

X→￿X0 de X1 , X1 of X0 ￿

X 0,2
X→￿dianzi shang, the mat￿

dianzi0 shang1

training
(e.g., mert)

decoding
(e.g., mbr)

X 3,4
X→￿mao, a cat￿

de2

mao3

(a) A hypergraph encodes four different derivation trees as shown
in the four ﬁgures below. Rectangles represent items (or nodes),
where each item is identiﬁed by the non-terminal symbol and
source span. An item has one or more incoming hyperedges,
which represent different ways of deriving the item. A hyperedge consists of a rule, and a pointer to an antecedent item for
each non-terminal symbol in the rule.

atomic inference operations

(e.g., ﬁnding one-best, k-best or expectation,
inference can be exact or approximate)

Which translation do we present to a user?

Decoding

How do we set the parameters Θ?

Training

What atomic operations do we need to perform? Atomic Inference

nslation: a cat on the mat

(c) Translation: the mat a cat

22

S 0,4

The hypergraph deﬁnes a probability
distribution over trees!
the distribution is parameterized by Θ

S→￿X0 , X0 ￿

X 0,4

Probabilistic
Hypergraph

X→￿X0 de X1 , X1 on X0 ￿

X→￿X0 de X1 , X0 ’s X1 ￿
X→￿X0 de X1 , X0 X1 ￿

X→￿X0 de X1 , X1 of X0 ￿

X 0,2
X→￿dianzi shang, the mat￿

dianzi0 shang1

training
(e.g., mert)

decoding
(e.g., mbr)

X 3,4
X→￿mao, a cat￿

de2

mao3

(a) A hypergraph encodes four different derivation trees as shown
in the four ﬁgures below. Rectangles represent items (or nodes),
where each item is identiﬁed by the non-terminal symbol and
source span. An item has one or more incoming hyperedges,
which represent different ways of deriving the item. A hyperedge consists of a rule, and a pointer to an antecedent item for
each non-terminal symbol in the rule.

atomic inference operations

(e.g., ﬁnding one-best, k-best or expectation,
inference can be exact or approximate)

Which translation do we present to a user?

Decoding

How do we set the parameters Θ?

Training

What atomic operations do we need to perform? Atomic Inference
Why are the problems difﬁcult?

nslation: a cat on the mat

(c) Translation: the mat a cat

22

S 0,4

The hypergraph deﬁnes a probability
distribution over trees!
the distribution is parameterized by Θ

S→￿X0 , X0 ￿

X 0,4

Probabilistic
Hypergraph

X→￿X0 de X1 , X1 on X0 ￿

X→￿X0 de X1 , X0 ’s X1 ￿
X→￿X0 de X1 , X0 X1 ￿

X→￿X0 de X1 , X1 of X0 ￿

X 0,2
X→￿dianzi shang, the mat￿

dianzi0 shang1

training
(e.g., mert)

decoding
(e.g., mbr)

X 3,4
X→￿mao, a cat￿

de2

mao3

(a) A hypergraph encodes four different derivation trees as shown
in the four ﬁgures below. Rectangles represent items (or nodes),
where each item is identiﬁed by the non-terminal symbol and
source span. An item has one or more incoming hyperedges,
which represent different ways of deriving the item. A hyperedge consists of a rule, and a pointer to an antecedent item for
each non-terminal symbol in the rule.

atomic inference operations

(e.g., ﬁnding one-best, k-best or expectation,
inference can be exact or approximate)

Which translation do we present to a user?

Decoding

How do we set the parameters Θ?

Training

What atomic operations do we need to perform? Atomic Inference
Why are the problems difﬁcult?
- brute-force will be too slow as there are exponentially
many trees, so require sophisticated dynamic programs

nslation: a cat on the mat

(c) Translation: the mat a cat

22

S 0,4

The hypergraph deﬁnes a probability
distribution over trees!
the distribution is parameterized by Θ

S→￿X0 , X0 ￿

X 0,4

Probabilistic
Hypergraph

X→￿X0 de X1 , X1 on X0 ￿

X→￿X0 de X1 , X0 ’s X1 ￿
X→￿X0 de X1 , X0 X1 ￿

X→￿X0 de X1 , X1 of X0 ￿

X 0,2
X→￿dianzi shang, the mat￿

dianzi0 shang1

training
(e.g., mert)

decoding
(e.g., mbr)

X 3,4
X→￿mao, a cat￿

de2

mao3

(a) A hypergraph encodes four different derivation trees as shown
in the four ﬁgures below. Rectangles represent items (or nodes),
where each item is identiﬁed by the non-terminal symbol and
source span. An item has one or more incoming hyperedges,
which represent different ways of deriving the item. A hyperedge consists of a rule, and a pointer to an antecedent item for
each non-terminal symbol in the rule.

atomic inference operations

(e.g., ﬁnding one-best, k-best or expectation,
inference can be exact or approximate)

Which translation do we present to a user?

Decoding

How do we set the parameters Θ?

Training

What atomic operations do we need to perform? Atomic Inference

Why are the problems difﬁcult?
- brute-force will be too slow as there are exponentially
many trees, so require sophisticated dynamic programs
- sometimes intractable, mat a cat approximations
nslation: a cat on the mat
(c) Translation: the require

22

Inference, Training and Decoding on Hypergraphs

•

Atomic Inference

•

ﬁnding one-best derivations
Best-ﬁrst
Graph
Topological
no heuristic
with heuristic
with hierarchy
FSA
Viterbi
Dijkstra
A∗
HA∗
Hypergraph
CYK
Knuth
Klein and Manning Generalized A∗

•
•

•

•

ﬁnding 2.2: Algorithms for extracting one-best from an FSA or hypergraph
Table k-best derivations
computing expectations (e.g., of features)

Training problem is the same as the lightest derivation problem deﬁned by Knuth
The above

(1977) for a hypergraph. It is also random ﬁeld (CRF), minimum for an FSA (DiPerceptron, conditional equivalent to the shortest-path problem
jkstra, 1959). Table 2.2 shows the classical algorithms that solve this problem. In general,
error rate training (MERT), minimum risk, and MIRA
the algorithms can be classiﬁed by whether the search follows a certain topological order
or best-ﬁrst. The well-known Viterbi (Viterbi, 1967) algorithm for an FSA and the CockeDecoding (CYK) algorithm (alternatively called CKY) for a hypergraph search the
Younger-Kasami
Viterbi decoding, maximum a posterior (MAP) we can further
graph in a topological order.1 In the best-ﬁrst search category,decoding, andclassify the
minimum Bayes risk (MBR) decoding
algorithms by whether heuristic functions are used for estimating the cost from the current
23
node to the goal node. The algorithms described by Dijkstra (1959) and Knuth (1977) are

•

•

Outline
•
•

Hypergraph as Hypothesis Space
Unsupervised Discriminative Training
‣ minimum imputed risk
‣ contrastive language model estimation

•
•

Variational Decoding
First- and Second-order Expectation Semirings

24

Outline
•
•

Hypergraph as Hypothesis Space
Unsupervised Discriminative Training
‣ minimum imputed risk
‣ contrastive language model estimation

•
•

Variational Decoding
First- and Second-order Expectation Semirings
Held-out Bilingual Data
Bilingual
Data

Generative
Training

Monolingual
English

Generative
Training

Unseen
Sentences

Translation
Models
Language
Models

Decoding

Discriminative
Training

Optimal
Weights

Translation
Outputs

24

Outline
•
•

Hypergraph as Hypothesis Space
Unsupervised Discriminative Training
‣ minimum imputed risk
‣ contrastive language model estimation

•
•

main
focus

Variational Decoding
First- and Second-order Expectation Semirings
Held-out Bilingual Data
Bilingual
Data

Generative
Training

Monolingual
English

Generative
Training

Unseen
Sentences

Translation
Models
Language
Models

Decoding

Discriminative
Training

Optimal
Weights

Translation
Outputs

24

Training Setup
•

Each training example consists of

•
•

a foreign sentence (from which a hypergraph is generated
to represent many possible translations)
S 0,4

a reference translation

S→￿X0 , X0 ￿

X 0,4

x: dianzi shang de mao
y: a cat on the mat

X→￿X0 de X1 , X1 on X0 ￿

X→￿X0 de X1 , X0 ’s X1 ￿
X→￿X0 de X1 , X0 X1 ￿

X 0,2
X→￿dianzi shang, the mat￿

• Training

•

X→￿X0 de X1 , X1 of X

dianzi0 shang1

X 3,4
X→￿mao, a cat￿

de2

mao3

(a) A hypergraph encodes four different derivation trees as show
in the four ﬁgures below. Rectangles represent items (or node
where each item is identiﬁed by the non-terminal symbol a
source span. An item has one or more incoming hyperedge
which represent different ways of deriving the item. A hype
edge consists of a rule, and a pointer to an antecedent item f
each non-terminal symbol in the rule.

adjust the parameters Θ so that the reference
translation is preferred by the model

25

Supervised: Minimum Empirical Risk

26

,, y) is not known. In In practice, one typically empirical risk miniy) is not known. practice, one typically does does empirical ri
Supervised: Minimum empirical distribution p(x,
ans replacing p(x, y) y) above withempirical distribution p(x, y) given
˜
ans replacing p(x, above with the the Empirical Risk
˜
trainingset. Therefore,
trainingMinimum Empirical Risk Training
• set. Therefore,
￿￿
∗∗
θθ == arg min
˜ ˜(x, y)L(δ y)
argθ min p(x,py)L(δθ (x), θ (x), y)
θx,y

x,y

N
￿ N
1
= arg min
L(δ
˜
1 ￿θ (xi ), yi )
N
= argθ min i=1
L(δθ (xi ), yi )
˜

θ

N

i=1

(4.2)

ndexes the supervised training examples (xi , yi ). The optimal θ can be
˜
merical methods.3
ndexes the supervised training examples (xi , yi ). The optimal θ
˜
∗

3

merical methods.

minative Training with Missing Input

minative Training with Missing Input
um Imputed-Risk

o the question of how to
um Imputed-Risk make use of unsupervised data—speciﬁcally,
26

,, y) is not known. In In practice, one typically empirical risk miniy) is not known. practice, one typically does does empirical ri
Supervised: Minimum empirical distribution p(x,
ans replacing p(x, y) y) above withempirical distribution p(x, y) given
˜
ans replacing p(x, above with the the Empirical Risk
˜
trainingset. Therefore,
trainingMinimum Empirical Risk Training
• set. Therefore,
￿￿
∗∗
θθ == arg min
˜ ˜(x, y)L(δ y)
argθ min p(x,py)L(δθ (x), θ (x), y)
θx,y

x,y

N
￿ N
1
= arg min
L(δ
˜
1 ￿θ (xi ), yi )
empirical
N
= distribution i=1
argθ min
L(δθ (xi ), yi )
˜

θ

N

i=1

(4.2)

ndexes the supervised training examples (xi , yi ). The optimal θ can be
˜
merical methods.3
ndexes the supervised training examples (xi , yi ). The optimal θ
˜
∗

3

merical methods.

minative Training with Missing Input

minative Training with Missing Input
um Imputed-Risk

o the question of how to
um Imputed-Risk make use of unsupervised data—speciﬁcally,
26

,, y) is not known. In In practice, one typically empirical risk miniy) is not known. practice, one typically does does empirical ri
Supervised: Minimum empirical distribution p(x,
ans replacing p(x, y) y) above withempirical distribution p(x, y) given
˜
ans replacing p(x, above with the the Empirical Risk
˜
trainingset. Therefore,
trainingMinimum Empirical Risk Training
• set. Therefore,
￿￿
∗∗
θθ == arg min
˜ ˜(x, y)L(δ y)
argθ min p(x,py)L(δθ (x), θ (x), y)
θx,y

x,y

N
￿ N
1
= arg min
L(δ
˜
1 ￿θ (xi ), yi )
empirical
N
= distribution i=1
argθ min
L(δθ (xi ), yi )
˜

θ

N

i=1

(4.2)

ndexes the supervised training examples (xi , yi ). The optimal θ can be
˜
merical methods.3
ndexes the supervised training examples (xi , yi ). The optimal θ
˜
∗

3

merical methods.

minative Training with Missing Input

minative Training with Missing Input
um Imputed-Risk

o the question of how to
um Imputed-Risk make use of unsupervised data—speciﬁcally,
26

,, y) is not known. In In practice, one typically empirical risk miniy) is not known. practice, one typically does does empirical ri
Supervised: Minimum empirical distribution p(x,
ans replacing p(x, y) y) above withempirical distribution p(x, y) given
˜
ans replacing p(x, above with the the Empirical Risk
˜
trainingset. Therefore,
trainingMinimum Empirical Risk Training
• set. Therefore,
￿￿
∗∗
θθ == arg min
˜ ˜(x, y)L(δ y)
argθ min p(x,py)L(δθ (x), θ (x), y)
θx,y

x,y

N
￿ N
1
= arg min
L(δ
˜
1 ￿θ (xi ), yi )
empirical
N
= distribution i=1 xL(δθ (xi ), yi )
argθ min
˜

θ

N

i=1

(4.2)

ndexes the supervised training examples (xi , yi ). The optimal θ can be
˜
merical methods.3
ndexes the supervised training examples (xi , yi ). The optimal θ
˜
∗

3

merical methods.

minative Training with Missing Input

minative Training with Missing Input
um Imputed-Risk

o the question of how to
um Imputed-Risk make use of unsupervised data—speciﬁcally,
26

,, y) is not known. In In practice, one typically empirical risk miniy) is not known. practice, one typically does does empirical ri
Supervised: Minimum empirical distribution p(x,
ans replacing p(x, y) y) above withempirical distribution p(x, y) given
˜
ans replacing p(x, above with the the Empirical Risk
˜
trainingset. Therefore,
trainingMinimum Empirical Risk Training
• set. Therefore,
￿￿
∗∗
θθ == arg min
˜ ˜(x, y)L(δ y)
argθ min p(x,py)L(δθ (x), θ (x), y)

rvised setting—as used in MERT
1.
1 ￿￿
= arg min
˜
(4.2)
empirical 1 L(δ (x ), y )
xL(δ δ y )
ex translationN system (xθ),(x). The
= distribution
arg min
˜
N
ndexes the supervised training examples (x ˜ ). The not
may have any form and, yneedoptimal θ can be
merical methods.
ndexes the supervised training examples (x , y ). The optimal θ
˜
ne a scoring function along with
merical methods.
minative Training with Missing Input
ring translation y.
θx,y

x,y
N

N

θ

θ

θ

i

i

θ

i=1

i

i=1

i

3

3

i

∗

i

i

i

minative Training with Missing Input
um Imputed-Risk

o the question of how to
um Imputed-Risk make use of unsupervised data—speciﬁcally,
26

,, y) is not known. In In practice, one typically empirical risk miniy) is not known. practice, one typically does does empirical ri
Supervised: Minimum empirical distribution p(x,
ans replacing p(x, y) y) above withempirical distribution p(x, y) given
˜
ans replacing p(x, above with the the Empirical Risk
˜
trainingset. Therefore,
trainingMinimum Empirical Risk Training
• set. Therefore,
MT decoder
￿￿
∗∗
θθ == arg min
˜ ˜(x, y)L(δ y)
argθ min p(x,py)L(δθ (x), θ (x), y)

rvised setting—as used in MERT
1.
1 ￿￿
= arg min
˜
(4.2)
empirical 1 L(δ (x ), y )
xL(δ δ y )
ex translationN system (xθ),(x). The
= distribution
arg min
˜
N
ndexes the supervised training examples (x ˜ ). The not
may have any form and, yneedoptimal θ can be
merical methods.
ndexes the supervised training examples (x , y ). The optimal θ
˜
ne a scoring function along with
merical methods.
minative Training with Missing Input
ring translation y.
θx,y

x,y
N

N

θ

θ

θ

i

i

θ

i=1

i

i=1

i

3

3

i

∗

i

i

i

minative Training with Missing Input
um Imputed-Risk

o the question of how to
um Imputed-Risk make use of unsupervised data—speciﬁcally,
26

,,Risk (for Supervised typically empirical risk miniy) is not known. In In practice, one Discriminay) is not known. practice, one typically does does empirical ri
Supervised: Minimum empirical distribution p(x,
ans replacing p(x, y) y) above withempirical distribution p(x, y) given
˜
ans replacing p(x, above with the the Empirical Risk
˜
trainingset. Therefore,
trainingMinimum Empirical Risk Training
• set. Therefore,
MT decoder
￿￿
∗∗
θθ == arg min
˜ ˜(x, y)L(δ y)
argθ min p(x,py)L(δθ (x), θ (x), y)

rvised setting—as used in MERT
ining in the supervised setting—as used in MERT
1. 2.6 on page 31. ￿
ion
1
= arg min
L(δ
˜
(4.2)
1 ￿ (x ), y )
empirical
xL(δ δ ), y
θ of some = distribution
complex system (x θ (x). The
ex translationNtranslation system)δθ (x). The
arg min
˜
N
x to English y, may have any form and need not
ndexes the supervised training examples (x ˜ ). The not
may have any form and, yneedoptimal θ can be
metersmethods. deﬁne training examples (x ,along with
merical the supervised a scoring function y ). The optimal θ
ndexes θ may
˜
acting high-scoring translation along with
ne a ascoring function y.
merical methods.
minative Training with Missing Input
ring translation y.
54
θx,y

x,y
N

N

θ

θ

θ

i

i

θ

i=1

i

i=1

i

3

3

i

∗

i

i

i

minative Training with Missing Input
um Imputed-Risk

o the question of how to
um Imputed-Risk make use of unsupervised data—speciﬁcally,
26

,,Risk (for Supervised typically empirical risk miniy) is not known. In In practice, one Discriminay) is not known. practice, one typically does does empirical ri
Supervised: Minimum empirical distribution p(x,
ans replacing p(x, y) y) above withempirical distribution p(x, y) given
˜
ans replacing p(x, above with the the Empirical Risk
˜
trainingset. Therefore,
trainingMinimum Empirical Risk Training
• set. Therefore,
MT decoder
￿￿
∗∗
θθ == arg min
˜ ˜(x, y)L(δ y)
argθ min p(x,py)L(δθ (x), θ (x), y)

rvised setting—as used in MERT
ining in the supervised setting—as used in MERT
1. 2.6 on page 31. ￿
ion
1
= arg min
L(δ
˜
(4.2)
1 ￿ (x ), y )
empirical
xL(δ δ ), y
θ of some = distribution
complex system (x θ (x). The
ex translationNtranslation system)δθ (x). The
arg min
˜
N
x to English y, may have any form and need not
ndexes the supervised training examples (x ˜ ). The MT output
may have any form and, yneedoptimal θ can be
not
metersmethods. deﬁne training examples (x ,along with
merical the supervised a scoring function y ). The optimal θ
ndexes θ may
˜
acting high-scoring translation along with
ne a ascoring function y.
merical methods.
minative Training with Missing Input
ring translation y.
54
θx,y

x,y
N

N

θ

θ

θ

i

i

θ

i=1

i

i=1

i

3

3

i

∗

i

i

i

minative Training with Missing Input
um Imputed-Risk

o the question of how to
um Imputed-Risk make use of unsupervised data—speciﬁcally,
26

,,Risk (for Supervised typically empirical risk miniy) is not known. In In practice, one Discriminay) is not known. practice, one typically does does empirical ri
Supervised: Minimum empirical distribution p(x,
ans replacing p(x, y) y) above withempirical distribution p(x, y) given
˜
ans replacing p(x, above with the the Empirical Risk
˜
trainingset. Therefore,
trainingMinimum Empirical Risk Training
• set. Therefore,
MT decoder
￿￿
∗∗
θθ == arg min
˜ ˜(x, y)L(δ y)
argθ min p(x,py)L(δθ (x), θ (x), y)

rvised setting—as used in MERT
ining in the supervised setting—as used in MERT
1. 2.6 on page 31. ￿
ion
1
loss
= arg min
L(δ
˜
(4.2)
1 ￿ (x ), y )
empirical
xL(δ δ ), y
θ of some = distribution
complex system (x θ (x). The
ex translationNtranslation system)δθ (x). The y
arg min
˜
N
x to English y, may have any form and need not
ndexes the supervised training examples (x ˜ ). The MT output
may have any form and, yneedoptimal θ can be
not
metersmethods. deﬁne training examples (x ,along with
merical the supervised a scoring function y ). The optimal θ
ndexes θ may
˜
acting high-scoring translation along with
ne a ascoring function y.
merical methods.
minative Training with Missing Input
ring translation y.
54
θx,y

x,y
N

N

θ

θ

θ

i

i

θ

i=1

i

i=1

i

3

3

i

∗

i

i

i

minative Training with Missing Input
um Imputed-Risk

o the question of how to
um Imputed-Risk make use of unsupervised data—speciﬁcally,
26

,,Risk (for Supervised typically empirical risk miniy) is not known. In In practice, one Discriminay) is not known. practice, one typically does does empirical ri
Supervised: Minimum empirical distribution p(x,
ans replacing p(x, y) y) above withempirical distribution p(x, y) given
˜
ans replacing p(x, above with the the Empirical Risk
˜
trainingset. Therefore,
trainingMinimum Empirical Risk Training
• set. Therefore,
MT decoder
￿￿
∗∗
θθ == arg min
˜ ˜(x, y)L(δ y)
argθ min p(x,py)L(δθ (x), θ (x), y) negated BLEU

rvised setting—as used in MERT
ining in the supervised setting—as used in MERT
1. 2.6 on page 31. ￿
ion
1
loss
= arg min
L(δ
˜
(4.2)
1 ￿ (x ), y )
empirical
xL(δ δ ), y
θ of some = distribution
complex system (x θ (x). The
ex translationNtranslation system)δθ (x). The y
arg min
˜
N
x to English y, may have any form and need not
ndexes the supervised training examples (x ˜ ). The MT output
may have any form and, yneedoptimal θ can be
not
metersmethods. deﬁne training examples (x ,along with
merical the supervised a scoring function y ). The optimal θ
ndexes θ may
˜
acting high-scoring translation along with
ne a ascoring function y.
merical methods.
minative Training with Missing Input
ring translation y.
54
θx,y

x,y
N

N

θ

θ

θ

i

i

θ

i=1

i

i=1

i

3

3

i

∗

i

i

i

minative Training with Missing Input
um Imputed-Risk

o the question of how to
um Imputed-Risk make use of unsupervised data—speciﬁcally,
26

,,Risk (for 2001), the lossone one typicallyBLEU score.) To be ri
y) is not known. Supervised Discriminaric (Papineniknown. In practice, typically does empirical risk miniy) is not et al., In practice, might be a negated does empirical
c (Papineni lowal., 2001), withlossempirical distribution pBLEU given
the is,1 might be a negated ˜
to ﬁnd θ with et p(x, y) risk,Minimum Empirical Risk(x, y) score
Bayes above with the empirical distribution p(x,
Supervised:
ans replacing p(x, y) above that the
˜
ans replacing
￿ risk, that is,1
o ﬁnd θ set. Therefore,
low Bayes
training withTherefore, p(x,Risk Training
set. arg Empirical y)L(δ (x), y)
trainingMinimum min
∗
• θ= θ ￿ θ
(4.1)
MT decoder
￿￿
x,y
∗∗ ∗
θθ θ == arg min
= arg min p(x,py)L(δθ (x),(x), y)
argθ min ˜ p(x, y)L(δ2θ θ (x), y) negated BLEU
˜(x, y)L(δ y)
θ

rvised setting—as used in MERT
ining in the supervised setting—as
he true distribution over (input,output) pairs. used in MERT
1. is noton pageIn31. ￿one typically does empirical risk minix, y) 2.6 known.
practice,
ion
1
eans replacing p(x, y) over (input,output) ) distribution p(x, y) given
the
˜ loss
e true distribution above withL(δ empirical
= arg min
˜
(4.2)
1 ￿ (x ), y pairs.
empirical
xL(δ δ ), y
θtraining set. = distribution
oftranslation system (x θ (x).(x). The y
complex
ex issomeTherefore,In Ntranslation system)δθdoes empirical ri
The
arg min
˜
, y) not known.
practice, one typically
N
x toreplacing p(x,may have any form and need not p(x,
English y, y)￿
ans havearg min aboveexamples (x , y ). Thedistribution ˜ be
ndexes the supervised training with and ˜
θ =
p(x, y)L(δ the empirical optimal θ can
˜
may θ may any form (x), y) need MTwith
not
output
metersUniform deﬁne training examples (x ,along optimal θ
merical methods. Empirical scoring function y ). The
ndexes set. Therefore,
˜
trainingthe supervised a Distribution
•
acting high-scoring￿
translation along with
ne a ascoring 1function y.
￿
merical methods.
˜
(4.2)
θ = arg min N
= arg min L(δ(x, y)L(δ (x), y)
p (x ), y )
˜
minative Training with Missing Input
ring translation y.
54
θx,y
x,y
x,y
N

θ

θ

θ

∗

3

θ

i

θ

i=1

2

i

i

i

i=1

i

θ

∗

i

x,y

3

∗

N

i

i

N

θ

θ

θ

i

i

θ

i=1

x,y

minative Training with Missing Input

indexes the supervised training examples (xi , yi ). The optimal θ∗ can be
˜
N
1 ￿
3
um Imputed-Risk
umerical methods. arg min
=
L(δ (x ), y )
˜
θ

i

i

N i=1 of unsupervised data—speciﬁcally,
o the question of how to make use
θ

um Imputed-Risk

26

,,Risk (for 2001), the lossone one typicallyBLEU score.) To be ri
y) is not known. Supervised Discriminaric (Papineniknown. In practice, typically does empirical risk miniy) is not et al., In practice, might be a negated does empirical
c (Papineni lowal., 2001), withlossempirical distribution pBLEU given
the is,1 might be a negated ˜
to ﬁnd θ with et p(x, y) risk,Minimum Empirical Risk(x, y) score
Bayes above with the empirical distribution p(x,
Supervised:
ans replacing p(x, y) above that the
˜
ans replacing
￿ risk, that is,1
o ﬁnd θ set. Therefore,
low Bayes
training withTherefore, p(x,Risk Training
set. arg Empirical y)L(δ (x), y)
trainingMinimum min
∗
• θ= θ ￿ θ
(4.1)
MT decoder
￿￿
x,y
∗∗ ∗
θθ θ == arg min
= arg min p(x,py)L(δθ (x),(x), y)
argθ min ˜ p(x, y)L(δ2θ θ (x), y) negated BLEU
˜(x, y)L(δ y)
θ

rvised setting—as used in MERT
ining in the supervised setting—as
he true distribution over (input,output) pairs. used in MERT
1. is noton pageIn31. ￿one typically does empirical risk minix, y) 2.6 known.
practice,
ion
1
eans replacing p(x, y) over (input,output) ) distribution p(x, y) given
the
˜ loss
e true distribution above withL(δ empirical
= arg min
˜
(4.2)
1 ￿ (x ), y pairs.
empirical
xL(δ δ ), y
θtraining set. = distribution
oftranslation system (x θ (x).(x). The y
complex
ex issomeTherefore,In Ntranslation system)δθdoes empirical ri
The
arg min
˜
, y) not known.
practice, one typically
N
x toreplacing p(x,may have any form and need not p(x,
English y, y)￿
ans havearg min aboveexamples (x , y ). Thedistribution ˜ be
ndexes the supervised training with and ˜
θ =
p(x, y)L(δ the empirical optimal θ can
˜
may θ may any form (x), y) need MTwith
not
output
metersUniform deﬁne training examples (x ,along optimal θ
merical methods. Empirical scoring function y ). The
ndexes set. Therefore,
˜
trainingthe supervised a Distribution
•
acting high-scoring￿
translation along with
ne a ascoring 1function y.
￿
merical methods.
˜
θ = arg min N
= arg min L(δ(x, y)L(δ (x), y) - MERT (4.2)
p (x ), y )
˜
minative Training with Missing Input
ring translation y.
- CRF
54
θx,y
x,y
x,y
N

θ

θ

θ

∗

3

θ

i

θ

i=1

2

i

i

i

i=1

i

θ

∗

i

x,y

3

∗

N

i

i

N

θ

θ

θ

i

i

θ

i=1

x,y

minative Training with Missing Input

indexes the supervised training examples (xi , yi ). The optimal θ∗ can be
˜
N
- Peceptron
1 ￿
3
um Imputed-Risk
umerical methods. arg min
=
L(δ (x ), y )
˜
θ

i

i

N i=1 of unsupervised data—speciﬁcally,
o the question of how to make use
θ

um Imputed-Risk

26

,,Risk (for 2001), the lossone one typicallyBLEU score.) To be ri
y) is not known. Supervised Discriminaric (Papineniknown. In practice, typically does empirical risk miniy) is not et al., In practice, might be a negated does empirical
c (Papineni lowal., 2001), withlossempirical distribution pBLEU given
the is,1 might be a negated ˜
to ﬁnd θ with et p(x, y) risk,Minimum Empirical Risk(x, y) score
Bayes above with the empirical distribution p(x,
Supervised:
ans replacing p(x, y) above that the
˜
ans replacing
￿ risk, that is,1
o ﬁnd θ set. Therefore,
low Bayes
training withTherefore, p(x,Risk Training
set. arg Empirical y)L(δ (x), y)
trainingMinimum min
∗
• θ= θ ￿ θ
(4.1)
MT decoder
￿￿
x,y
∗∗ ∗
θθ θ == arg min
= arg min p(x,py)L(δθ (x),(x), y)
argθ min ˜ p(x, y)L(δ2θ θ (x), y) negated BLEU
˜(x, y)L(δ y)
θ

rvised setting—as used in MERT
ining in the supervised setting—as
he true distribution over (input,output) pairs. used in MERT
1. is noton pageIn31. ￿one typically does empirical risk minix, y) 2.6 known.
practice,
ion
1
eans replacing p(x, y) over (input,output) ) distribution p(x, y) given
the
˜ loss
e true distribution above withL(δ empirical
= arg min
˜
(4.2)
1 ￿ (x ), y pairs.
empirical
xL(δ δ ), y
θtraining set. = distribution
oftranslation system (x θ (x).(x). The y
complex
ex issomeTherefore,In Ntranslation system)δθdoes empirical ri
The
arg min
˜
, y) not known.
practice, one typically
N
x toreplacing p(x,may have any form and need not p(x,
English y, y)￿
ans havearg min aboveexamples (x , y ). Thedistribution ˜ be
ndexes the supervised training with and ˜
θ =
p(x, y)L(δ the empirical optimal θ can
˜
may θ may any form (x), y) need MTwith
not
output
metersUniform deﬁne training examples (x ,along optimal θ
merical methods. Empirical scoring function y ). The
ndexes set. Therefore,
˜
trainingthe supervised a Distribution
•
acting high-scoring￿
translation along with
ne a ascoring 1function y.
￿
merical methods.
˜
θ = arg min N
= arg min L(δ(x, y)L(δ (x), y) - MERT (4.2)
p (x ), y )
˜
minative Training with Missing Input
ring translation y.
- CRF
54
θx,y
x,y
x,y
N

θ

θ

θ

∗

3

θ

i

θ

i=1

2

i

i

i

i=1

i

θ

x,y

3

∗

N

∗

i

i

i

N

θ

θ

θ

i

i

θ

i=1

x,y

minative Training with Missing Input

indexes the supervised training examples (xi , yi ). The optimal θ∗ can be
˜
N
- Peceptron
1 ￿
3
um Imputed-Risk
umerical methods. arg min
=
L(δ (x ), y )
˜

What if the input x is missing?
θ i
i
θ N
o the question of how to makei=1 of unsupervised data—speciﬁcally,
use

um Imputed-Risk

26

vised y) in the Therefore,
rainingis not known. In practice, one typically in MERT
p(x, training set. supervised setting—as used does empirica
Unsupervised: Minimum Imputed Risk
￿
ﬁrst review discriminative training in the supervised setting—as used in MERT
∗
ction 2.6 on page 31. above y)L(δ (x), y)
s means replacing p(x, y)2.6 on(x, with θthe empirical distribution
θ = inarg min
p
˜
3) and those discussed Empirical Riskpage 31.
Section
Minimum
Training
θ
• tune theset. Therefore, complex translation system δθ (x). loss y
sed training complex x,y
x
some
The
sshes of someparameters θ oftranslation system δθ (x). The
θ to
N
￿ any form and need not
￿
θ , which translates Chinese x to English y, may have any form and need not
x to Englishthe arg min 1haveL(δ(x,iy)L(δ (x), y) along with (4.2
∗ = y, may θ may deﬁne), y )
listic. For example, = parameters
scoring
θ
arg min
p θ (x a ˜i θ function
˜
θ N
d decoding heuristics for extracting ai=1
ameters θ may deﬁne θ scoring function y.
a high-scoring translation along with
x,y
1, N ] indexes high-scoring translation y. ˜
xtracting a the supervised training examples (xi , yi). The optimal θ∗ can b
N
￿
54

1
ng numerical methods. arg min
=
θ N
3

L(δθ (xi ), yi )
˜

54
scriminativesupervised trainingMissing Input). The optim
Training with examples (x , y
N ] indexes the
i ˜i
i=1

3

g numerical methods.
inimum Imputed-Risk

turn to the question of how to make use of unsupervised data—speciﬁcally
mples i for which we know only yi but not xi . For such i, we cannot comput
˜
nd L(δθ (xi ), yi ) in (4.2). Instead we propose to replace L(δθ (xi ), yi ) with th
˜
˜

criminative Training with Missing Input

imum

￿
￿
Imputed-Risk

￿

27

vised y) in the Therefore,
rainingis not known. In practice, one typically in MERT
p(x, training set. supervised setting—as used does empirica
Unsupervised: Minimum Imputed Risk
￿
ﬁrst review discriminative training in the supervised setting—as used in MERT
∗
ction 2.6 on page 31. above y)L(δ (x), y)
s means replacing p(x, y)2.6 on(x, with θthe empirical distribution
θ = inarg min
p
˜
3) and those discussed Empirical Riskpage 31.
Section
Minimum
Training
θ
• tune theset. Therefore, complex translation system δθ (x). loss y
sed training complex x,y
x
some
The
sshes of someparameters θ oftranslation system δθ (x). The
θ to
N
￿ any form and need not
￿
θ , which translates Chinese x to English y, may have any form and need not
xis to“reverse ∗predictionmin 1haveL(δ(x,iy)L(δimputey) along with i(4.2
Englishthe arg model” that ˜ a ˜ ) function
may θ
pφ aFor example,= y, arg min may p θ (x ), yi to θ (x), the missing x da
attempts
listic.
deﬁne scoring
θ = parameters
θ N
d decodingofθ mayfor extracting ai=1
g variant (4.2), deﬁne θ scoring function y.
minimum translation along risk
ametersheuristicswhat we calledhigh-scoring imputed empiricalwith(abbr
a x,y
mum indexes the supervised training examples (x , y ). The optimal θ∗ can b
imputed-risk), is4
1, N ]• Minimum Imputed translation i
xtracting a high-scoring Risk Training y. ˜i
N
￿
54

1
ng numerical methods. arg min N
=
˜
￿ ￿ L(δθ (xi ), yi )
1
∗
θ N
θ = arg min
˜
˜
i=1pφ (x | yi )L(δθ (x), yi )
3

θ N
54
i=1 x
scriminativesupervised trainingMissing Input). The optim
Training with examples (x , y
N ] indexes the
i ˜i

3

g numerical methods.
inimum imputed-risk objective of (4.4) could be evaluated by brute force
r minimumImputed-Risk

turn to the question of how to make use of unsupervised data—speciﬁcally
mples i for which we know onlyyy,i use not xi . For such i, we cannot comput
˜ but the reverse model p to impute its p
or each unsupervised example ˜i
φ
nd L(δθ (xi ), yi ) in (4.2). Instead we propose to replace L(δθ (xi ), yi ) with th
˜
˜
everse translations {xi1 , xi2 , . . .}, and add each (xij , yi ) pair (weighted by
˜
￿
￿
i ) ≤ 1) to an imputed training set .
27
￿
imum Imputed-Risk

criminative Training with Missing Input

vised y) in the Therefore,
rainingis not known. In practice, one typically in MERT
p(x, training set. supervised setting—as used does empirica
Unsupervised: Minimum Imputed Risk
￿
ﬁrst review discriminative training in the supervised setting—as used in MERT
∗
ction 2.6 on page 31. above y)L(δ (x), y)
s means replacing p(x, y)2.6 on(x, with θthe empirical distribution
θ = inarg min
p
˜
3) and those discussed Empirical Riskpage 31.
Section
Minimum
Training
θ
• tune theset. Therefore, complex translation system δθ (x). loss y
sed training complex x,y
x
some
The
sshes of someparameters θ oftranslation system δθ (x). The
θ to
N
￿ any form and need not
￿
θ , which translates Chinese x to English y, may have any form and need not
xis to“reverse ∗predictionmin 1haveL(δ(x,iy)L(δimputey) along with i(4.2
Englishthe arg model” that ˜ a ˜ ) function
may θ
pφ aFor example,= y, arg min may p θ (x ), yi to θ (x), the missing x da
attempts
listic.
deﬁne scoring
θ = parameters
θ N
d decodingofθ mayfor extracting ai=1
g variant (4.2), deﬁne θ scoring function y.
minimum translation along risk
ametersheuristicswhat we calledhigh-scoring imputed empiricalwith(abbr
a x,y
mum indexes the supervised training examples (x , y ). The optimal θ∗ can b
imputed-risk), is4
1, N ]• Minimum Imputed translation i
xtracting a high-scoring Risk Training y. ˜i
N
￿
54

1
ng numerical methods. arg min N
=
˜
￿ ￿ L(δθ (xi ), yi )
1
∗
θ N
θ = arg min
˜
˜
i=1pφ (x | yi )L(δθ (x), yi )
3

θ N
54
i=1 x
scriminativesupervised trainingMissing Input). The optim
Training with examples (x , y
N ] indexes the
i ˜i

3

g numerical methods.
inimum imputed-risk objective of (4.4) could be evaluated by brute force
r minimumImputed-Risk

turn to the question of how to make use of unsupervised data—speciﬁcally
mples i for which we know onlyyy,i use not xi . For such i, we cannot comput
˜ but the reverse model p to impute its p
or each unsupervised example ˜i
φ
nd L(δθ (xi ), yi ) in (4.2). Instead we propose to replace L(δθ (xi ), yi ) with th
˜
˜
everse translations {xi1 , xi2 , . . .}, and add each (xij , yi ) pair (weighted by
˜
￿
￿
i ) ≤ 1) to an imputed training set .
27
￿
imum Imputed-Risk

criminative Training with Missing Input

vised y) in the Therefore,
rainingis not known. In practice, one typically in MERT
p(x, training set. supervised setting—as used does empirica
Unsupervised: Minimum Imputed Risk
￿
ﬁrst review discriminative training in the supervised setting—as used in MERT
∗
ction 2.6 on page 31. above y)L(δ (x), y)
s means replacing p(x, y)2.6 on(x, with θthe empirical distribution
θ = inarg min
p
˜
3) and those discussed Empirical Riskpage 31.
Section
Minimum
Training
θ
• tune theset. Therefore, complex translation system δθ (x). loss y
sed training complex x,y
x
some
The
sshes of someparameters θ oftranslation system δθ (x). The
θ to
N
￿ any form and need not
￿
θ , which translates Chinese x to English y, may have any form and need not
xis to“reverse ∗predictionmin 1haveL(δ(x,iy)L(δimputey) along with i(4.2
Englishthe arg model” that ˜ a ˜ ) function
may θ
pφ aFor example,= y, arg min may p θ (x ), yi to θ (x), the missing x da
attempts
listic.
deﬁne scoring
θ = parameters
θ N
d decodingofθ mayfor extracting ai=1
g variant (4.2), deﬁne θ scoring function y.
minimum translation along risk
ametersheuristicswhat we calledhigh-scoring imputed empiricalwith(abbr
a x,y
mum indexes the supervised training examples (x , y ). The optimal θ∗ can b
imputed-risk), is4
1, N ]• Minimum Imputed translation i
xtracting a high-scoring Risk Training y. ˜i
N
￿
54

1
ng numerical methods. arg min N
=
˜
￿ ￿ L(δθ (xi ), yi )
1
∗
θ N
θ = arg min
˜
˜
i=1pφ (x | yi )L(δθ (x), yi )
3

θ N
54
i=1 x
scriminativesupervised trainingMissing Input). The optim
Training with examples (x , y
N ] indexes the
i ˜i

3

g numerical methods.
inimum imputed-risk objective of (4.4) could be evaluated by brute force
r minimumImputed-Risk

turn to the question of how to make use of unsupervised data—speciﬁcally
mples i for which we know onlyyy,i use not xi . For such i, we cannot comput
˜ but the reverse model p to impute its p
or each unsupervised example ˜i
φ
nd L(δθ (xi ), yi ) in (4.2). Instead we propose to replace L(δθ (xi ), yi ) with th
˜
˜
everse translations {xi1 , xi2 , . . .}, and add each (xij , yi ) pair (weighted by
˜
￿
￿
i ) ≤ 1) to an imputed training set .
27
￿
imum Imputed-Risk

criminative Training with Missing Input

as y) is not known. In practice, is typically does empirica
vised minimumsupervised setting—as used in MERT
training the Therefore,
set. imputed-risk), one
raining in
p(x,
Unsupervised: Minimum Imputed Risk
￿
ﬁrst review discriminative training in the supervised setting—as used in MERT
∗
ction 2.6 on page 31. above y)L(δ (x), y)
s means replacing p(x, y)2.6 on(x, with θthe empirical distribution
θ = inarg min
p
˜
3) and those discussed Empirical Riskpage 31.
Section
N ￿
Minimum Therefore,
Training
θ
sed oftune theset.
training parameters θ oftranslation translation system￿The y
1(x).(x). loss
x,y
x
sshes to some complex some complex system δθ δθ The
θ
∗
θ￿ y, arg any form and need not pφ
N
, which translates Chinese x to English =may havemin
￿ any form θand need not
θ
xis to“reverse ∗predictionmin 1haveL(δ(x,iy)L(δimputey) along with i(4.2
Englishthe arg model” that ˜ a ˜ ) function missing
may θ
pφ aFor example,= y, arg min may p θ (x ), yi to θ (x),N i=1 x x da
attempts
the
listic.
deﬁne scoring
θ = parameters
θ N
θ
d decodingof (4.2), what we calledhigh-scoring imputed empirical risk (abbr
g variant heuristics for extracting ai=1
minimum translation y.

•

ameters θ may deﬁne a scoring function along with
x,y
mum indexes the supervised training examples (x , y ). The optimal θ∗ can b
imputed-risk), is4
1, N ]• Minimum Imputed translation i
xtracting a high-scoring Risk Training y. ˜i
N
￿
54

1
ng numerical methods. arg min N
=
˜
￿ ￿ L(δθ (xi ), yi )
1
∗
θ N
Our θminimum imputed-risk yobjective) of
= arg min
˜
i=1pφ (x | ˜i )L(δθ (x), yi
3

(4.4) c

θ N
54
i=1 x
lows.
scriminativesupervised trainingMissing Input). The optim
Training with examples (x , y
N ] indexes the
i ˜i

3

g numerical methods.
inimum For each objective of (4.4) could be evaluated ,by brute force
r minimumImputed-Risk
1. imputed-risk unsupervised example yi use the
˜

turn to the question of how to make use ofi1 , xi2 , . . .}, and add e
reverse translations {x unsupervised data—speciﬁcally
mples i for which we know onlyyy,i use not xi . For such i, we cannot comput
˜ but the reverse model p to impute its p
or each unsupervised example imputed training set .
yyii) in (4.2). to an ˜we propose to replace L(δθφ(xi), yi) with th
˜ ) ≤ 1) Instead i
nd L(δθ (xi ), ˜
˜
everse translations {xi1 , xi2 , . . .}, and add each (xij , yi ) pair (weighted by
˜
￿
￿
i ) ≤ 1) to an imputed training set .
27
￿
imum Imputed-Risk

criminative Training with Missing Input

as y) is not known. In practice, is typically does empirica
vised minimumsupervised setting—as used in MERT
training the Therefore, when the one
set. imputed-risk), empirical
raining in
p(x,
ses (i.e., the empirical risk
Unsupervised: Minimum Imputed Risk
￿
ﬁrst review discriminative training in the supervised setting—as used in MERT
∗
ction 2.6 on page 31. 2.6 on(x, with (x), y)
s means replacing p(x, y)to ourpage 31. θthe5empirical distribution
ed training set).=Speciﬁc aboveMT task, this
θ
p y)L(δ
˜
3) and those discussed Empirical Risk Training
inarg min
Section
N ￿
Minimum Therefore,
θ
sed oftune theset. the target-language sentence system￿The y
training parameters θ oftranslation translation δ1(x).(x). loss
translation (from
x
some
sshes to some complex x,y ∗ complex system θ δθ The
θ
arg any form and need not pφ
N
, whichlow (expected) loss.1θ ￿ =may havemin
translates Chinese x to English y,
￿
θ
have to English y, may have any form θand need not
a
pφx aFor example,= arg min θ may p(x, iy)L(δimputey) i=1 with i(4.2
is “reverse ∗ the parameters that ˜ θ (x ), yi to θ (x),N along x x da
attempts
the
listic.
deﬁne ˜ )
scoring
θ prediction model” lattice ora hyper-function missing
= a weighted L(δ
arg min
l decodingof (4.2), what weθ calledhigh-scoring imputed empirical risk (abbr
pφ will heuristics for extracting ai=1
generate
N minimum translation y.
θ
d variant
g

•

prediction model” that attempts to impute
ameters θ may deﬁne a scoring function along with
x,y
lations of ˜i . It is
.2),indexesythewe computationally infeasible imputed can b
what supervised training examples (x , y ).
mum imputed-risk),is called minimum The optimal θ em
1, N ]• Minimum Imputed translation
xtracting a high-scoring Risk Training y. ˜
N
￿ in Sec54 1
We will present several approximations
4
ng numerical methods. arg min
risk),Our minimum imputed-risk(xi), yi) of (4.4) c
is
=
˜
￿ ￿ L(δθ
1
θ N
objective
4

3

i

i

∗

N

θ∗ = arg min

i=1pφ (x

| yi )L(δθ (x), yi )
˜
˜

θ N
54
i=1 x
lows.
scriminativesupervised trainingMissing Input). The optim
Training with examples (x , y
N ￿
N ] indexes the
i ˜i
del pφ : reverse model 1 ￿
3

∗ numerical methods.
g
inimum For each objective of (4.4) could φ (x | yi,by brute force
r minimumImputed-Risk
= imputed-risk unsupervised example ˜ )L(δthe
min
p be evaluated use θ (x
1. argmodel” that attempts to impute
rse prediction
θ how to make use of unsupervised data—speciﬁcally
N i=1 {xi1 , xi2 , . . .}, and add e
turn to the question of
reverse translations
odel in advance (before we tune θ),xdoing the
mples i for which we know onlyyy,i use not xi . For such i, we cannot comput
˜ but the reverse model p to impute its p
or each bilingual Training as well as any set
criminative 1)exampledata propose trainingL(δθφ(x.Input th
yyii) in (4.2). toyan ˜we with replace
˜) ≤
ding ourunsupervised(xiInsteadimputed toMissing i ), yi ) with
, ˜i ) i
nd L(δθ (xi ), ˜
˜
everse translations {xi1 , xi2 , . . .}, and add each (xij , yi ) pair (weighted by
˜
ﬁxed when we￿
tune θ.
￿

≤ 1) to an imputed training set .
￿
imum Imputed-Risk

i)

27

as y) is not known. In practice, is typically does empirica
vised minimumsupervised setting—as used in MERT
training the Therefore, when the one
set. imputed-risk), empirical
raining in
p(x,
ses (i.e., the empirical risk
Unsupervised: Minimum Imputed Risk
￿
ﬁrst review discriminative training in the supervised setting—as used in MERT
∗
ction 2.6 on page 31. 2.6 on(x, with (x), y)
s means replacing p(x, y)to ourpage 31. θthe5empirical distribution
ed training set).=Speciﬁc aboveMT task, this
θ
p y)L(δ
˜
3) and those discussed Empirical Risk Training
inarg min
Section
N ￿
Minimum Therefore,
θ
sed θ tune theset. the target-language sentence system￿The y
translation (from θ θ oftranslation translation δ1(x).(x). loss
x
to
sshes of someparameters i x,y ∗ icomplex system θ δθ The
θ training complex some
arg any form and need not pφ
N
, whichlow (expected) loss.1θ ￿ =may havemin
translates Chinese x to English y,
￿
θ
have to English y, may have any form θand need not
a
pφx aFor example,= arg min θ may p(x, iy)L(δimputey) i=1 with i(4.2
is “reverse ∗ the parameters that ˜ θ (x ), yi to θ (x),N along x x da
attempts
the
listic.
deﬁne ˜ )
scoring
θ prediction model” lattice ora hyper-function missing
= a weighted L(δ
arg min
l decodingof (4.2), what weθ calledhigh-scoring imputed empirical risk (abbr
pφ will heuristics for extracting ai=1
generate
N minimum translation y.
θ
d variant
g

ality of (4.2) extends to our minimum
r δ• and L(δ (x ), y ).
˜
model” that attempts attempts to impute
to impute the mi
prediction model” that
ameters θ may deﬁne a scoring function along with
x,y
e called. minimum imputedimputed em
lations of yi we computationally infeasible empirica
.2),indexes˜the It is called minimum). The optimal θ can b
what supervised training examples (x , y
mum imputed-risk), is
1, N ]• Minimum Imputed translation
xtracting a high-scoring Risk Training y. ˜
N
￿ in Sec54 1
We will present several approximations
4
ng numerical methods. arg min
risk),Our minimum imputed-risk(xi), yi) of (4.4) c
is
=
˜
￿ ￿ L(δθ
1
θ N
objective
4

i

3

∗

i

N

θ∗ = arg min

i=1pφ (x

| yi )L(δθ (x), yi )
˜
˜

θ N
54
i=1 x
lows.
scriminativesupervised trainingMissing Input). The optim
Training with examples (x , y
N
N
N ] indexes￿ ￿ ￿ ￿
the
i ˜i
del pφ :1
reverse model 1
3
∗ numerical methods.
g
inimum For each objective of (4.4) could φ (x | yi,by y the
r minimumImputed-Risk p (x | y example(x),brute force
=: arg input
min 1.imputedminunsupervised iimpute θ ˜ )L(δθ (x
˜ )L(δ (4.6)
˜
yrse prediction model” that φ
| x) imputed-risk N attempts to p be evaluated usei )
θ toNreverseθtranslations {xi1 , xi2 , . . .}, and add e
turn the question of how to make use x unsupervised data—speciﬁcally
of
odel in advance (before we i=1 θ), doing the
i=1 know only tune not xi . For such i, we cannot comput
mples i for which we x
yi but
˜

or each bilingual Training asthe reverse model p
criminative 1)exampledatausewithtoas any L(δ (x.Inputitsth
yyi ) in (4.2). toyan imputed Missing impute
˜) ≤
ding ourunsupervised(xiInsteadywe propose training set to), y ) with p
, ˜i ) ˜ ,
well replace
nd L(δ (x ), ˜
˜
i

φ
θ

i
i
i
i
everseθ translations {xi1 , xi2 , . . .}, and add each (xij , yi ) pair (weighted by
˜
ﬁxed when we￿
tune θ.
￿
i ) ≤ 1) to an imputed training set .
27
￿
imum Imputed-Risk

18) on page 32. Note that in practice,

as y) is not known. In practice, is typically does empirica
vised minimumsupervised setting—as used in MERT
training the Therefore, when the one
set. imputed-risk), empirical
raining the empirical risk
in
p(x,
ses (i.e.,to (and
milarly discriminative training in the supervised setting—as al., 2009a).
ﬁrst review
ng the 2.6Unsupervised: Minimum Imputed used in MERT
open-source 31. to our MT task,5empirical distribution
MT ￿
toolkitwith the this et Risk
Joshua (Li
∗ page
ction discussedSpeciﬁc above 31.
s means replacing p(x, y)2.6 on(x, y)L(δθ (x), y)
ed trainingon = inarg min p page
set). Section
˜
3) and those train- Empirical Risk Training
pervised θ
N ￿
Minimum Therefore,
θ
vised tune theset. the target-language sentence system￿ (and
similarly
sed θ discriminativeθ training performs x δ1(x).(x). loss y
training parameters oftranslation translation θ δθ to The
translation (from θ
some
sshes of some complexi x,y ∗ icomplex system
θ to
The
N
lso, a low (expected) loss.English =may havemin and need not pφ
adding unsupervised ￿ y, arg supervised train, which translates Chinese x to 1θ data into the any form
￿
θ
have to English y, may have any form θand need not
pφx aFor example,= arg min θ may p(x, iy)L(δimputey) i=1 with i(4.2
is “reverse ∗ the parameters that ˜ θ (x ), yi to θ (x),N along x x da
attempts
the
listic.
deﬁne ˜ )
scoring
θ prediction model” lattice ora hyper-function missing
= a weighted L(δ
arg min
l decodingof (4.2), what weθ calledhigh-scoring imputed empirical risk (abbr
pφ will heuristics for extracting ai=1
generate
N minimum translation y.
θ
d variant
g

ality of (4.2) extends to our minimum
r δ• and L(δ (x ), y ).
˜
model” that attempts attempts to impute
to impute the mi
prediction model” that
ameters θ may deﬁne scoring function along with
Superviseda Discrimina- em
x,y
e called. minimum imputedimputed
lations of yi we computationally infeasible empirica
.2),indexes˜the It is called minimum). The optimal θ can b
what supervised training examples (x , y
mum imputed-risk), is
1, N ]• Minimum Imputed translation
xtracting a high-scoring Risk Training y. ˜
N
￿ in Sec54 1
scriminaWe will present several approximations
4
ng numerical methods. arg min
risk),Our minimum imputed-risk(xi), yi) of (4.4) c
is
=
˜
￿ ￿ L(δθ
1
θ N
objective
θ (for Supervised yDiscrimina= arg min
p (x | ˜ )L(δ (x), y )
˜
cal Risk
i=1
N
54
lows.
scriminativesupervised trainingMissing Input). The optim
Training with examples (x , y
N
N
N ] indexes￿ ￿ ￿ ￿
the
i ˜i
del pin1
reverse
3
used φ : MERT model 1
∗ numerical methods.
g
ervised setting—as(4.4) yiexample(x),bruteθforce
inimum For each objective of | could be evaluated ,by y )(x
r minimumImputed-Risk p (x usedφin MERT the
=: arg input
min 1.imputedminunsupervised impute θ yi )L(δ
˜ )L(δ (4.6)
˜
yrse prediction model” that φ
| x) imputed-risk N attempts to p (x | ˜ usei
θ
4

i

3

∗

∗

i

N

θ

φ

i=1

i

θ

i

x

ve training in theof how to make setting—as,used in MERT e
θ δ (x). question supervised use x unsupervised data—speciﬁcally
turn toNreverse translations {xi1 , xi2 . . .},loss add
the The system
of
and
m in advance
31.i for 2.6 on page 31. tune not xi. For such i, we cannot comput
odel θ : forward (before we i=1 θ), doing the
i=1 x
mples unsupervisedknow onlyyy,i use the reverse model p to impute its p
which we example ˜˜ but
Section y ) ≤
or each bilingual Training as well as any set
criminative 1) toyandata propose trainingL(δθφ(x.Input th
i
and θ (x y in (x , ˜i ) we with replace
need
ding ouri ),˜ii )not(4.2). iInsteadimputed toMissing i ), yi ) with
nd L(δ
˜
˜
everseθ of some complex translation systempair (weighted
˜
tersalong with tune, xi2 . .}, and
plextranslations￿{xi132., . Noteadd eachδ(xij(x).δθ (x). Theby
translation set . that in ,practice,
system￿ θ yi) The
ﬁxed when we
θ.
on 1)on page training
18)x to Imputed-Risk have any form and need not
an imputed
i) ≤
￿
ese to English y, may
imum
27

as y) is not known. In practice, is typically does empirica
vised minimumsupervised setting—as used in MERT
training the Therefore, when the one
set. imputed-risk), empirical
raining the empirical risk
in
p(x,
ses (i.e.,to (and
milarly discriminative training in the supervised setting—as al., 2009a).
ﬁrst review
ng the 2.6Unsupervised: Minimum Imputed used in MERT
open-source 31. to our MT task,5empirical distribution
MT ￿
toolkitwith the this et Risk
Joshua (Li
∗ page
ction discussedSpeciﬁc above 31.
s means replacing p(x, y)2.6 on(x, y)L(δθ (x), y)
ed trainingon = inarg min p page
set). Section
˜
3) and those train- Empirical Risk Training
pervised θ
N ￿
Minimum Therefore,
θ
vised tune theset. the target-language sentence system￿ (and
similarly
sed θ discriminativeθ training performs x δ1(x).(x). loss y
training parameters oftranslation translation θ δθ to The
translation (from θ
some
sshes of some complexi x,y ∗ icomplex system
θ to
The
N
lso, a low (expected) loss.English =may havemin and need not pφ
adding unsupervised ￿ y, arg supervised train, which translates Chinese x to 1θ data into the any form
￿
θ
have to English y, may have any form θand need not
pφx aFor example,= arg min θ may p(x, iy)L(δimputey) i=1 with i(4.2
is “reverse ∗ the parameters that ˜ θ (x ), yi to θ (x),N along x x da
attempts
the
listic.
deﬁne ˜ )
scoring
θ prediction model” lattice ora hyper-function missing
= a weighted L(δ
arg min
l decodingof (4.2), what weθ calledhigh-scoring imputed empirical risk (abbr
pφ will heuristics for extracting ai=1
generate
N minimum translation y.
θ
d variant
g

ality of (4.2) extends to our minimum
r δ• and L(δ (x ), y ).
˜
model” that attempts attempts to impute
to impute the mi
prediction model” that
ameters θ may deﬁne scoring function along with
Superviseda Discrimina- em
x,y
e called. minimum imputedimputed
lations of yi we computationally infeasible empirica
.2),indexes˜the It is called minimum). The optimal θ can b
what supervised training examples (x , y
mum imputed-risk), is
1, N ]• Minimum Imputed translation
xtracting a high-scoring Risk Training y. ˜
N
￿ in Sec54 1
scriminaWe will present several approximations
4
ng numerical methods. arg min
risk),Our minimum imputed-risk(xi), yi) of (4.4) c
is
=
˜
￿ ￿ L(δθ
1
θ N
objective
θ (for Supervised yDiscrimina= arg min
p (x | ˜ )L(δ (x), y )
˜
cal Risk
i=1
N
54
lows.
scriminativesupervised trainingMissing Input). The optim
Training with examples (x , y
N
N
N ] indexes￿ ￿ ￿ ￿
the
i ˜i
del pin1
reverse
3
used φ : MERT model 1
∗ numerical methods.
g
ervised setting—as(4.4) yiexample(x),bruteθforce
inimum For each objective of | could be evaluated ,by y )(x
r minimumImputed-Risk p (x usedφin MERT the
=: arg input
min 1.imputedminunsupervised impute θ yi )L(δ
˜ )L(δ (4.6)
˜
yrse prediction model” that φ
| x) imputed-risk N attempts to p (x | ˜ usei
θ
4

i

3

∗

∗

i

N

θ

φ

i=1

i

θ

i

x

ve training in theof how to make setting—as,used in MERT e
θ δ (x). question supervised use x unsupervised data—speciﬁcally
turn toNreverse translations {xi1 , xi2 . . .},loss add
the The system
of
and
m in advance
31.i for 2.6 on page 31. tune not xi. For such i, we cannot comput
odel θ : forward (before we i=1 θ), doing the
i=1 x
mples unsupervisedknow onlyyy,i use the reverse model p to impute its p
which we example ˜˜ but
Section y ) ≤
or each bilingual Training as well as any set
criminative 1) toyandata propose trainingL(δθφ(x.Input th
i
and θ (x y in (x , ˜i ) we with replace
need
ding ouri ),˜ii )not(4.2). iInsteadimputed toMissing i ), yi ) with
nd L(δ
˜
˜
everseθ of some complex translation systempair (weighted
˜
tersalong with tune, xi2 . .}, and
plextranslations￿{xi132., . Noteadd eachδ(xij(x).δθ (x). Theby
translation set . that in ,practice,
system￿ θ yi) The
ﬁxed when we
θ.
on 1)on page training
18)x to Imputed-Risk have any form and need not
an imputed
i) ≤
￿
ese to English y, may
imum
27

as y) is not known. In practice, is typically does empirica
vised minimumsupervised setting—as used in MERT
training the Therefore, when the one
set. imputed-risk), empirical
raining the empirical risk
in
p(x,
ses (i.e.,to (and
milarly discriminative training in the supervised setting—as al., 2009a).
ﬁrst review
ng the 2.6Unsupervised: Minimum Imputed used in MERT
open-source 31. to our MT task,5empirical distribution
MT ￿
toolkitwith the this et Risk
Joshua (Li
∗ page
ction discussedSpeciﬁc above 31.
s means replacing p(x, y)2.6 on(x, y)L(δθ (x), y)
ed trainingon = inarg min p page
set). Section
˜
3) and those train- Empirical Risk Training
pervised θ
N ￿
Minimum Therefore,
θ
vised tune theset. the target-language sentence system￿ (and
similarly
sed θ discriminativeθ training performs x δ1(x).(x). loss y
training parameters oftranslation translation θ δθ to The
translation (from θ
some
sshes of some complexi x,y ∗ icomplex system
θ to
The
N
lso, a low (expected) loss.English =may havemin and need not pφ
adding unsupervised ￿ y, arg supervised train, which translates Chinese x to 1θ data into the any form
￿
θ
have to English y, may have any form θand need not
pφx aFor example,= arg min θ may p(x, iy)L(δimputey) i=1 with i(4.2
is “reverse ∗ the parameters that ˜ θ (x ), yi to θ (x),N along x x da
attempts
the
listic.
deﬁne ˜ )
scoring
θ prediction model” lattice ora hyper-function missing
= a weighted L(δ
arg min
l decodingof (4.2), what weθ calledhigh-scoring imputed empirical risk (abbr
pφ will heuristics for extracting ai=1
generate
N minimum translation y.
θ
d variant
g

ality of (4.2) extends to our minimum
r δ• and L(δ (x ), y ).
˜
model” that attempts attempts to impute
to impute the mi
prediction model” that
ameters θ may deﬁne scoring function along with
Superviseda Discrimina- em
x,y
e called. minimum imputedimputed
lations of yi we computationally infeasible empirica
.2),indexes˜the It is called minimum). The optimal θ can b
what supervised training examples (x , y
mum imputed-risk), is
1, N ]• Minimum Imputed translation
xtracting a high-scoring Risk Training y. ˜
N
￿ in Sec54 1
scriminaWe will present several approximations
4
ng numerical methods. arg min
risk),Our minimum imputed-risk(xi), yi) of (4.4) c
is
=
˜
￿ ￿ L(δθ
1
θ N
objective
θ (for Supervised yDiscrimina= arg min
p (x | ˜ )L(δ (x), y )
˜
cal Risk
i=1
N
54
lows.
scriminativesupervised trainingMissing Input). The optim
Training with examples (x , y
N
N
N ] indexes￿ ￿ ￿ ￿
the
i ˜i
del pin1
reverse
3
used φ : MERT model 1
∗ numerical methods.
g
ervised setting—as(4.4) yiexample(x),bruteθforce
inimum For each objective of | could be evaluated ,by y )(x
r minimumImputed-Risk p (x usedφin MERT the
=: arg input
min 1.imputedminunsupervised impute θ yi )L(δ
˜ )L(δ (4.6)
˜
yrse prediction model” that φ
| x) imputed-risk N attempts to p (x | ˜ usei
θ
4

i

3

∗

∗

i

N

θ

φ

i=1

i

θ

i

x

ve training in theof how to make setting—as,used in MERT e
θ δ (x). question supervised use x unsupervised data—speciﬁcally
turn toNreverse translations {xi1 , xi2 . . .},loss add
the The system
of
and
m in advance
31.i for 2.6 on page 31. tune not xi. For such i, we cannot comput
odel θ : forward (before we i=1 θ), doing the
i=1 x
mples unsupervisedknow onlyyy,i use the reverse model p to impute its p
which we example ˜˜ but
Section y ) ≤
or each bilingual Training as well as any set
criminative 1) toyandata propose trainingL(δθφ(x.Input th
i
andouri ),˜tripin (x , ˜i ) we with replace
need translation
ding Round yii )not(4.2). iInsteadimputed toMissing i ), yi ) with
nd L(δθ (x ˜
˜
everseθ of some complex translation systempair (weighted
˜
tersalong with tune, xi2 . .}, and
plextranslations￿{xi132., . Noteadd eachδ(xij(x).δθ (x). Theby
translation set . that in ,practice,
system￿ θ yi) The
ﬁxed when we
θ.
on 1)on page training
18)x to Imputed-Risk have any form and need not
an imputed
i) ≤
￿
ese to English y, may
imum
27

as y) is not known. In practice, is typically does empirica
vised minimumsupervised setting—as used in MERT
training the Therefore, when the one
set. imputed-risk), empirical
raining the empirical risk
in
p(x,
ses (i.e.,to (and
milarly discriminative training in the supervised setting—as al., 2009a).
ﬁrst review
ng the 2.6Unsupervised: Minimum Imputed used in MERT
open-source 31. to our MT task,5empirical distribution
MT ￿
toolkitwith the this et Risk
Joshua (Li
∗ page
ction discussedSpeciﬁc above 31.
s means replacing p(x, y)2.6 on(x, y)L(δθ (x), y)
ed trainingon = inarg min p page
set). Section
˜
3) and those train- Empirical Risk Training
pervised θ
N ￿
Minimum Therefore,
θ
vised tune theset. the target-language sentence system￿ (and
similarly
sed θ discriminativeθ training performs x δ1(x).(x). loss y
training parameters oftranslation translation θ δθ to The
translation (from θ
some
sshes of some complexi x,y ∗ icomplex system
θ to
The
N
lso, a low (expected) loss.English =may havemin and need not pφ
adding unsupervised ￿ y, arg supervised train, which translates Chinese x to 1θ data into the any form
￿
θ
have to English y, may have any form θand need not
pφx aFor example,= arg min θ may p(x, iy)L(δimputey) i=1 with i(4.2
is “reverse ∗ the parameters that ˜ θ (x ), yi to θ (x),N along x x da
attempts
the
listic.
deﬁne ˜ )
scoring
θ prediction model” lattice ora hyper-function missing
= a weighted L(δ
arg min
l decodingof (4.2), what weθ calledhigh-scoring imputed empirical risk (abbr
pφ will heuristics for extracting ai=1
generate
N minimum translation y.
θ
d variant
g

ality of (4.2) extends to our minimum
r δ• and L(δ (x ), y ).
˜
model” that attempts attempts to impute
to impute the mi
prediction model” that
ameters θ may deﬁne scoring function along with
Superviseda Discrimina- em
x,y
e called. minimum imputedimputed
lations of yi we computationally infeasible empirica
.2),indexes˜the It is called minimum). The optimal θ can b
what supervised training examples (x , y
mum imputed-risk), is
1, N ]• Minimum Imputed translation
xtracting a high-scoring Risk Training y. ˜
N
￿ in Sec54 1
scriminaWe will present several approximations
4
ng numerical methods. arg min
risk),Our minimum imputed-risk(xi), yi) of (4.4) c
is
=
˜
￿ ￿ L(δθ
1
θ N
objective
θ (for Supervised yDiscrimina= arg min
p (x | ˜ )L(δ (x), y )
˜
cal Risk
i=1
N
54
lows.
scriminativesupervised trainingMissing Input). The optim
Training with examples (x , y
N
N
N ] indexes￿ ￿ ￿ ￿
the
i ˜i
del pin1
reverse
3
used φ : MERT model 1
∗ numerical methods.
g
ervised setting—as(4.4) yiexample(x),bruteθforce
inimum For each objective of | could be evaluated ,by y )(x
r minimumImputed-Risk p (x usedφin MERT the
=: arg input
min 1.imputedminunsupervised impute θ yi )L(δ
˜ )L(δ (4.6)
˜
yrse prediction model” that φ
| x) imputed-risk N attempts to p (x | ˜ usei
θ
4

i

3

∗

∗

i

N

θ

φ

i=1

i

θ

i

x

ve training in theof how to make setting—as,used in MERT e
θ δ (x). question supervised use x unsupervised data—speciﬁcally
turn toNreverse translations {xi1 , xi2 . . .},loss add
the The system
of
and
m in advance
31.i for 2.6 on page 31. tune not xi. For such i, we cannot comput
odel θ : forward (before we i=1 θ), doing the
i=1 x
mples unsupervisedknow onlyyy,i use the reverse model p to impute its p
which we example ˜˜ but
Section y ) ≤
or each bilingual Training as well as any set
criminative 1) toyandata propose trainingL(δθφ(x.Input th
i
andouri ),˜tripin (x , ˜i ) we with replace
need translation
ding Round yii )not(4.2). iInsteadimputed toMissing i ), yi ) with
nd L(δθ (x ˜
˜
everseθ of some complex translation systempair (weighted
˜
tersalong with tune, xi2 . .}, and
plextranslations￿{xi132., . Noteadd eachδ(xij(x).δθ (x). Theby
translation set . that in ,practice,
system￿ θ yi) The
ﬁxed on recognition?
when we
θ.
on Speechpage training
18)1) to Imputed-Risk have any form and need not
an imputed
i) ≤
￿
ese x to English y, may
imum
27

isk (for Supervised Discriminaverse Prediction Model pφ
Training Reverse Model
N ￿ ￿￿
N
￿
1
ed setting—as φ , yiexample(x), yithe reverse
arg min p (x | ˜ )L(δ | ˜ )L(δ)
For each unsuperviseda p (x MERT θ (x), ˜
gredient hereφis p usedφin θ yi,prediction yi )
“reverse use
mod
˜
N i=1 x i1 i2
ng in theθtranslations {x , x ,used in MERT each (x
supervised setting—as . . .}, and add
Na. i=1 will train this pφ model in advance (b
reverse x
loss
We

2.6 ≤ 1) to 31. imputed training set .
on page an
yi ) available data—including our bilingual
˜
rom complexsystem δθ (x).δθ (x). The
f some
ranslation 6 translation system The
gual do ordinary supervised training (as of we on
oNow x data. Note any form and need not (4.2)) tu
English y, may have that φ is ﬁxed when
have any form andcouldalongevaluated by
needbe with
not
ers θ may deﬁne scoring function
risk objectiveaof (4.4)
data. (4.4) could be evaluated by brute
ve both supervised and unsupervised data
oit high-scoring translation y.
ing aof
a scoring function along with to perform s

n of (4.2) and (4.4). We will do thisuseourto forward-t
in δθ experiment
he second step means that we must
translation y. as well. For example, in a speec
54 of the translations
be used for other tasks against the corresponding tru
e loss
d example yi ,puseathe reverse model pφ tha
˜ φ is kind of speech synthesizer to
ces text, whereas
minimizes the weighted sum of these losses (i.e., the e
yi ,, xi2 , features or phone each (xp, ˜i ) imput
˜ use . .}, and add sequences).
xacoustic . the reverse model ij φyto pair (w
i1
bution p(x, y) is derived from the imputed training se
˜
sk from x to y, one usually does not make use of mon
28

isk (for Supervised Discrimina- φ
φ
x,y
verse Prediction Model pφ
Training Reverse Model
N ￿ ￿￿
N
￿ that it may be tolerable for pφ to impute mediocre
We remark 1
ed setting—as φChinese,(x least iweuseithe reverse
argijminbe p (x |, y example y ,prediction ˜
Forxeach unsupervisedaip at MERT )(x), y δ
gredient hereφis p usedφin θ (x),willθthen learni )θ
“reverse y
mod
˜ )L(δ | ˜ )L(δ
˜
ugh
may imperfect
ng (observed) N
in theθtranslations. {xwe x ,lucky, in MERT other lowsupervised ˜ If , are . . .}, and add each
Na. i=1 will trainysetting—as used this yiadvance (x
reverse
od We x English ithis pφi2
i=1 x i1
loss
model in˜ and
(b

2.6 ≤ page anthe imperfect xij . In set .
on even of
nslations1) to 31. imputed trainingthat case, the imputed (xi
yi ) available data—including our bilingual
˜
rom just out-of-domain training dataδ(i.e., they are for the ta
f some complex translation system θ (x). The
entially
θ
6
gual x data. Note any form and need not (4.2)) tu
oinese-to-true-English ratherthat true-Chinese-to-true-English).
English ordinary supervised training (as of we on
y, may have than φ is ﬁxed when
Now do

ranslation system δ (x). The
have any form andcouldalongevaluated by
needbe with
not
ers θ may deﬁne scoring function
risk objectiveaof (4.4)
data. (4.4) could be evaluated by brute
ve bothgoal is to trainand unsupervised data to perform s
oit high-scoring translation y.
ing aof supervised Translation System
a scoring function alongsystem δθ and
2.3 Our Forward a good forward with
The
n of (4.2) Loss(4.4). We will do thisyin ourto forward-t
The and means thatL(δθmust use δθ experiment
he second step Function we (xi ), ˜i )
translation y. as well. For example, in a speec
54
be used for other tasks

eThe minimum empirical risk objective ofcorrespondingto
loss of the translations against the (4.2) is quite genera
tru
d example yi ,puseathe reverse model pφ tha
˜ φ is kind of speech synthesizer
ces text,trainingweighted sum ofet al., 2001; Collins, 2002;
whereas
minimizes the methods (Lafferty these losses (i.e., the e
pervised
yi ,, xi2 , . theand or phone each be ij φyto pair (w
˜ 2006; features Eisner, 2006) can (xp, ˜i ) in this fr
use . .}, and add sequences).
xacousticSmith is reverse model formalizedimput
i1
al.,
bution p(x, y) derived from the imputed training se
˜
sk from x to y, one usually does not make use of mon
28

n Hφ (for
(4.5)
isk(Y | X)Supervised Discrimina- φ
φ
esponding true translation yi Model p θ
˜
choose
x,y
verse Prediction, and Model theφ
￿
Training Reverse
N p(x, y) log￿risk when the empirical
N |￿
ses
empirical
n − (i.e., ￿1 pφ (x y)
￿ the that it may be tolerable for pφ5 to impute mediocre
We x,y
remark set). Speciﬁc to our MT task, this
ed setting—as usedφin MERT θ (x), yi )
edtraining hereφis pφChinese,(x least iweuseithe reverse
argijmin imperfect ˜ )L(δ | ˜ prediction mod
Forxeach be p (x |, y example y ,)L(δ then ˜
gredient unsupervisedaip at θ (x),will ) learn δθ
“reverse y
˜
ugh
may
translation (from
ng (observed) N the target-language .sentence˜ add other lowinptheθtranslations. {xwe x ,lucky, in. MERT each (x
supervised ˜ If translations thisEven
Na. i=1 towill trainysetting—as usedxijandiadvance (b
od We impute i=1 ithis pφi2 . .},loss y and
ereverse x English
for φ
mediocre i1 , aremodel in
x

have on page 31.
a
(expected) loss.
2.6 ≤low willanthelearn δθ to translate set case, the imputed (xi
nslations1)
imperfect xij . In
at i ) we to of imputed trainingthatback into
ypφ will even thenadata—including . our bilingual
˜leastavailable weighted lattice orithyperel some complex translation system δ (x). The
generate
rom this yiout-of-domain trainingvalues(i.e., good are for the ta
flucky, just˜ and other low-lossθ data θare they
entially
y
slationscase, ithe 6is computationally infeasible are
of y . It imputed (xthan ) training pairs
˜
In that do ordinary supervised training (as of we on
gual x data. Note anyyform and need not (4.2)) tu
oinese-to-true-English ratherthati true-Chinese-to-true-English).
English y, may have ij , ˜ φ is ﬁxed when
Now present several approximations in SecWe will
data (i.e., they are for the task/domain of garbled-

ranslation system δ (x). The
have any form andcouldalongevaluated by
needbe with
not
ers θ may deﬁne scoring function
risk objectiveaof (4.4)
data. (4.4) could be evaluated by brute
e-Chinese-to-true-English).
ve bothgoal is function alongsystem δθ and
oit high-scoring translation y.
ing aof supervised and unsupervised data to perform s
a scoring to train Translation System
2.3 Our Forward a good forward with
The
n of (4.2) Loss(4.4). We will do thisyin ourto forward-t
The and means thatL(δθmust use δθ experiment
he second step Function we (xi ), ˜i )
translation y. as well. and trained separately
delSystem δθ andtasks
pφ and are
on used for other parameterized For example, in a speec
54
be

eThe minimum empirical risk objective ofcorrespondingto
loss of the translations against the (4.2) is quite genera
tru
d example model” thatthe reverse model pφ tha
yi φ is kind of to impute
˜
(xitext,)whereas,puse a attempts speech synthesizer
), yi
˜
ces prediction weighted sum ofet al., 2001; Collins, 2002;
rse
minimizes the methods (Lafferty these losses (i.e., the e
pervised training
yi ,, x advance (before we tuneeach be ij φyto pair (w
˜ 2006; features Eisner, 2006)various p, ˜i in this fr
use .is and general sequences).
odel in i2 , . the reverseandθ), doingformalizedimput
xacousticSmith quiteor phone model popular)training se
.}, derived from can (x the
i1 of p(x, y) is and add
al.,
ctive
(4.2)
bution ˜
the imputed
dingfrombilingual one yi ) data as well as any use of mon
sk our x to y, (xi , ˜
usually does not make
28

isk (for Supervised Discriminaverse Prediction Model p
￿￿
ed setting—as used in MERT )
p (x | y )L(δ (x), y
˜
˜
N

φ
n Hφ (Y | X)
(4.5)
ditional cross-entropy, as follows7 φ
esponding true translation yi , and Model theφ
˜
choose
θ
x,y
￿
Training Reverse
N p(x, empiricaleach when
N |￿
∗
to(i.e., the y) minpH(xrisk X) thexij , evaluforward-translate (Y y) imputed empirical
ses
n−
log φ φ |
θ φ
= arg1 it￿ be tolerable for p to impute mediocre
We x,y
remarktrue translation y our MT task,φthe θ
that φSpeciﬁc to , and choose 5 this
may
esponding set). is ￿ , ˜i example y , use the reverse
ed training here
argijmin φ pφ “reverse )L(δ (x), ˜
|
Forxeach be imperfect Chinese, at θ ˜ prediction y δ
gredient unsupervisedaipφ (x least iwe willθthen learni )θ
mod
i
ugh
may
sesin theθsupervisedtarget-languageempirical
(i.e., the N the
translationarg min − setting—as used y)
= (from
p(x, y) the φsentence
p (x
ng (observed)empiricalyrisk whenloglucky, |in. MERT other lowφ mediocre
od i=1 towill train ithistranslations x5thisEven each (x
ereverse translations. {xwe xi2 , . . .},ijandiadvance (b
forWe x English ˜ x , aremodel in˜ and
pφ
a. trainingimpute i=1x,y If i1pφMT task, loss y add
ed a low (expected) loss. our
set). Speciﬁc to
this
have on page 31.
2.6
nslations1) to of imputed trainingthatback into
imperfect xij . In
at i ) ≤we willanthelearn δθ to translate set case, the imputed (xi
ypφ will even thenadata—including . our bilingual
˜leastavailable weighted lattice orithypertranslation (from
el some complex the target-language sentence The
rom be generateotherplow-lossθy values(i.e., good are for the.ta
ft may this yiout-of-domain training dataδθare they
entially just˜ and translation system (x).translations xij
lucky, tolerable for φ to impute mediocre
have a low yi . It6is computationally infeasible
slationscase, the imputed (xthan ) training pairs are
of (expected) loss. , y true-Chinese-to-true-English).
˜
In that do ordinary supervised is ﬁxed when
gual x data. aleast weijwill then learn δθ to of we on
oinese-to-true-English ratherthati φ training (asnot (4.2)) tu
English y,
mperfect Chinese, atNote any ˜form and need translate it back
Now presentmay have
el will
weighted lattice or hyperWe pφ will generate for the task/domain of garbledseveral approximations in Secdata imaywe are lucky, this yifunction be evaluated by
are
glishθ˜objective aof (4.4) and otheralong with values are
. If deﬁne scoring˜ could low-loss
ers y(i.e., theyIt is computationally infeasible y
risk of yi.
data.
slations ˜ x . In that case, the imputed (x , y ) training pair
e-Chinese-to-true-English).
he a high-scoring translation y.
ij
oit bothgoal several approximationsSystem ˜ to perform s
ingimperfect isijto trainand unsupervised data iθ and
2.3 Our supervised Translation in Sec- δ
The Forward a good forward system
We will present
f-domain training data (i.e., they are for the task/domain of gar
n of (4.2) Loss(4.4). We will do thisyin ourto forward-t
The and means thatL(δθmust use δθ experiment
he second step Function we (xi ), ˜i )
ish rather than are parameterized and trained
delSystem δθtrue-Chinese-to-true-English). separately
pφ and and
on used for other tasks as well. For example, in a speec
54
be of the translations against the corresponding tru
eThe minimum empirical risk objective of (4.2) is quite genera
loss
d example model” thatthe reverse model pφ tha
yi φ is kind of to impute
˜
(xitext,)whereas,puse a attempts speech synthesizer to
), yi
˜
ces prediction weighted sum ofet al., 2001; Collins, 2002;
rse p training
minimizesfixed whenSystem δθ and
pervised is the methods (Lafferty these losses (i.e., the e
del φ
training
wardin advance (before phone each (x the y ) pair (w
Translation or we tune θ), doing φ ˜
i , of (4.2) .}, and add sequences)., i
odel xi2 , . .is and general and can be formalized in this fr
xacousticSmith quiteEisner, 2006) various popular training se
i1
ij
al., 2006; features
ctive p(x, y) is derived from the imputed
bution ˜ L(δθ (xi ), yi )
Function to y, (xi ˜i
28
rse prediction model”, y˜) attempts tonot any
dingfrombilingual one thatdata as well impute use of mon
our x
as make
sk
usually does

ranslation system δ (x). The
have any form and need not
ve of (4.4) could be evaluated
a scoring function along with by brute
translation y.

y , use the reverse model p to imput
˜

sk (for Supervised Discrimina4.2.4 Approximating pφ (x | yi )
˜
Approximating
N ￿ ￿￿
N
￿
1mentioned at the end of Section 4.2.1, it is comp
As
ed setting—as usedφin θ (x), yithe reverse
arg min pφ (xof theexample(imputed θ the reverse
˜
For each unsupervised ix (xXMERTby(x), yi ) m
| y p ∈ | yi,)L(δ)
˜ )L(δ ˜ use
˜
translate each
θ

ij
i
N i=1setting—as,used in MERT each (xi
g in the translations {x , x . . .}, and add
supervised x
reverse
i1
i2

loss
proximations that are more computationally feasible. Ea
x 31.
i=1 page
2.6 ≤ 1) to an imputed training equation (4.4).
on approximation of p (x | y ) in set .
˜i
yi )
˜
φ

some complex translation system The
anslation system δθ (x).δθ (x). The
English ordinary supervised training support other methods
y,One can have any formfunctionneed not (4.2)) on
may manipulate the loss and to (as of
Now doany(Crammer et al., 2006)) need not decoding.
that use deterministic
have MIRA form andcouldalongevaluated by
ers θ objective aof (4.4) this with the minimum risk training of (
isk may deﬁne scoring function be with
One
data. (4.4)should not confuse evaluated by brute
ve high-scoring translation y. the losswith
one may manipulate
ang aof fertyAgain,could use randomized decoding. to support other pr
scoring al., 2001)) that be
function along function
et
e second step means that we must use δθ to forward-t
translation y.
54
8
9

10

58
loss of the translations against the correspondingto
tru
d example yi , use the reverse model pφ
˜
inimizes the weighted sum of these losses (i.e., the e
yi ,, x , . . .}, and add each (xp, ˜i ) impute
˜ use
xution i2(x,the reverse model ij φyto pair (w
i1
p y) is derived from the imputed training se
˜
29

sk (for Supervised Discrimina4.2.4 Approximating pφ (x | yi )
˜
Approximating
N ￿ ￿￿
N
￿
1mentioned at the end of Section 4.2.1, it is comp
As
ed setting—as usedφin θ (x), yithe reverse
arg min pφ (xof theexample(imputed θ the reverse
˜
For each unsupervised ix (xXMERTby(x), yi ) m
| y p ∈ | yi,)L(δ)
˜ )L(δ ˜ use
˜
translate each
θ

ij
i
N i=1setting—as,used in MERT each (xi
g in the translations {x , x . . .}, and add
supervised x
reverse
i1
i2

loss
proximations that are more computationally feasible. Ea
x 31.
i=1 page
2.6 ≤ 1) to an imputed training equation (4.4).
on approximation of p (x | y ) in set .
˜i
yi )
˜
φ

some complex translation system The
anslation system δθ (x).δθ (x). The
English ordinary supervised training support other methods
y,One can have any formfunctionneed not (4.2)) on
may manipulate the loss and to (as of
Now doany(Crammer et al., 2006)) need not decoding.
that use deterministic
have MIRA form andcouldalongevaluated by
ers θ objective aof (4.4) this with the minimum risk training of (
scoring function be with
isk may deﬁne SCFGconfuse
One should not
data. (4.4) could be evaluated by brute
ve high-scoring translation y. the losswith
ang aof fertyAgain, 2001)) that use randomized decoding. to support other pr
scoring al., one may manipulate
function along function
et
e second step means that we must use δθ to forward-t
translation y.
54
8
9

10

58
loss of the translations against the correspondingto
tru
d example yi , use the reverse model pφ
˜
inimizes the weighted sum of these losses (i.e., the e
yi ,, x , . . .}, and add each (xp, ˜i ) impute
˜ use
xution i2(x,the reverse model ij φyto pair (w
i1
p y) is derived from the imputed training se
˜
29

sk (for Supervised Discrimina4.2.4 Approximating pφ (x | yi )
˜
Approximating
N ￿ ￿￿
N
￿
1mentioned at the end of Section 4.2.1, it is comp
As
ed setting—as usedφin θ (x), yithe reverse
arg min pφ (xof theexample(imputed θ the reverse
˜
For each unsupervised ix (xXMERTby(x), yi ) m
| y p ∈ | yi,)L(δ)
˜ )L(δ ˜ use
˜
translate each
θ

ij
i
N i=1setting—as,used in MERT each (xi
g in the translations {x , x . . .}, and add
supervised x
reverse
i1
i2

loss
proximations that are more computationally feasible. Ea
exponentially
x 31.
i=1 page
2.6 ≤ 1) to an imputed training equation (4.4).
on x, stored in of p (x | y ) in set .
yi )
˜ many approximation φ ˜i
some complex translation system δθ (x). The
a hypergraph
θ

anslation system δ (x). The
English ordinary supervised training support other methods
y,One can have any formfunctionneed not (4.2)) on
may manipulate the loss and to (as of
Now doany(Crammer et al., 2006)) need not decoding.
that use deterministic
have MIRA form andcouldalongevaluated by
ers θ objective aof (4.4) this with the minimum risk training of (
scoring function be with
isk may deﬁne SCFGconfuse
One should not
data. (4.4) could be evaluated by brute
ve high-scoring translation y. the losswith
ang aof fertyAgain, 2001)) that use randomized decoding. to support other pr
scoring al., one may manipulate
function along function
et
e second step means that we must use δθ to forward-t
translation y.
54
8
9

10

58
loss of the translations against the correspondingto
tru
d example yi , use the reverse model pφ
˜
inimizes the weighted sum of these losses (i.e., the e
yi ,, x , . . .}, and add each (xp, ˜i ) impute
˜ use
xution i2(x,the reverse model ij φyto pair (w
i1
p y) is derived from the imputed training se
˜
29

sk (for Supervised Discrimina4.2.4 Approximating pφ (x | yi )
˜
Approximating
N ￿ ￿￿
N
￿
1mentioned at the end of Section 4.2.1, it is comp
As
ed setting—as usedφin θ (x), yithe reverse
arg min pφ (xof theexample(imputed θ the reverse
˜
For each unsupervised ix (xXMERTby(x), yi ) m
| y p ∈ | yi,)L(δ)
˜ )L(δ ˜ use
˜
translate each
θ

ij
i
N i=1setting—as,used in MERT each (xi
g in the translations {x , x . . .}, and add
supervised x
reverse
i1
i2

loss
proximations that are more computationally feasible. Ea
exponentially
x 31.
i=1 page
2.6 ≤ 1) to an imputed training equation (4.4).
on x, stored in of p (x | y ) in set .
yi )
˜ many approximation φ ˜i
some complex translation system δθ (x). The
a hypergraph
θ

anslation system δ (x). The
English ordinary supervised training support other methods
y,One can have any formfunctionneed not (4.2)) on
may manipulate the loss and to (as of
Now doany(Crammer et al., 2006)) need not decoding.
that use deterministic
have MIRA form andcouldalongevaluated by
ers θ objective aof (4.4) this with the minimum risk training of (
scoring SCFG
function be with
isk may deﬁne SCFGconfuse
One should not
data. (4.4) could be evaluated by brute
ve high-scoring translation y. the losswith
ang aof fertyAgain, 2001)) that use randomized decoding. to support other pr
scoring al., one may manipulate
function along function
et
e second step means that we must use δθ to forward-t
translation y.
54
8
9

10

58
loss of the translations against the correspondingto
tru
d example yi , use the reverse model pφ
˜
inimizes the weighted sum of these losses (i.e., the e
yi ,, x , . . .}, and add each (xp, ˜i ) impute
˜ use
xution i2(x,the reverse model ij φyto pair (w
i1
p y) is derived from the imputed training se
˜
29

sk (for Supervised Discrimina4.2.4 Approximating pφ (x | yi )
˜
Approximating
N ￿ ￿￿
N
￿
1mentioned at the end of Section 4.2.1, it is comp
As
ed setting—as usedφin θ (x), yithe reverse
arg min pφ (xof theexample(imputed θ the reverse
˜
For each unsupervised ix (xXMERTby(x), yi ) m
| y p ∈ | yi,)L(δ)
˜ )L(δ ˜ use
˜
translate each
θ

ij
i
N i=1setting—as,used in MERT each (xi
g in the translations {x , x . . .}, and add
supervised x
reverse
i1
i2

loss
proximations that are more computationally feasible. Ea
exponentially
x 31.
i=1 page
2.6 ≤ 1) to an imputed training equation (4.4).
on x, stored in of p (x | y ) in set .
yi )
˜ many approximation φ ˜i
some complex translation system δθ (x). The
a hypergraph
θ

anslation system δ (x). The
English ordinary supervised training support other methods
y,One can have any formfunctionneed not (4.2)) on
may manipulate the loss and to (as of
Now doany(Crammer et al., 2006)) need not decoding.
that use deterministic
have MIRA form andcouldalongevaluated by
ers θ objective aof (4.4) this with the minimum risk training of (
scoring SCFG
function be with
isk may deﬁne SCFGconfuse
One should not
data. (4.4) could be evaluated by brute
ve high-scoring translation y. the losswith
one is manipulate
ang aof fertyAgain,CFGmaynotuse randomized decoding. to support other pr
scoring al., 2001)) that closed under composition!
function along function
et
e second step means that we must use δθ to forward-t
translation y.
54
8
9

10

58
loss of the translations against the correspondingto
tru
d example yi , use the reverse model pφ
˜
inimizes the weighted sum of these losses (i.e., the e
yi ,, x , . . .}, and add each (xp, ˜i ) impute
˜ use
xution i2(x,the reverse model ij φyto pair (w
i1
p y) is derived from the imputed training se
˜
29

sk (for Supervised Discrimina4.2.4 Approximating pφ (x | yi )
˜
Approximating
N ￿ ￿￿
N
￿
1mentioned at the end of Section 4.2.1, it is comp
As
ed setting—as usedφin θ (x), yithe reverse
arg min pφ (xof theexample(imputed θ the reverse
˜
For each unsupervised ix (xXMERTby(x), yi ) m
| y p ∈ | yi,)L(δ)
˜ )L(δ ˜ use
˜
translate each
θ

ij
i
N i=1setting—as,used in MERT each (xi
g in the translations {x , x . . .}, and add
supervised x
reverse
i1
i2

loss
proximations that are more computationally feasible. Ea
exponentially
x 31.
i=1 page
2.6 ≤ 1) to an imputed training equation (4.4).
on x, stored in of p (x | y ) in set .
yi )
˜ many approximation φ ˜i
some complex translation system δθ (x). The
a hypergraph
θ

anslation system δ (x). The
English ordinary supervised training support other methods
y,One can have any formfunctionneed not (4.2)) on
may manipulate the loss and to (as of
Now doany(Crammer et al., 2006)) need not decoding.
that use deterministic
have MIRA form andcouldalongevaluated by
ers θ objective aof (4.4) this with the minimum risk training of (
scoring SCFG
function be with
isk may deﬁne SCFGconfuse
One should not
data. (4.4) could be evaluated by brute
ve high-scoring translation y. the losswith
one is manipulate
ang aof fertyAgain,CFGmaynotuse randomized decoding. to support other pr
scoring al., 2001)) that closed under composition!
function along function
et
• Approximations
e second step means that we must use δθ to forward-t
translation y.
- k-best
54
8
9

10

58
loss of the translations against the correspondingto
tru
d example yi , use the reverse model pφ
˜
- sampling
inimizes the weighted sum of these losses (i.e., the e
yi ,, x , . .lattice add each (xp, ˜i ) impute
˜ use .}, and
xution i2(x,-the reverse model ij φyto pair (w
i1
p y) is derived from the imputed training se
˜
29

sk (for Supervised Discrimina4.2.4 Approximating pφ (x | yi )
˜
Approximating
N ￿ ￿￿
N
￿
1mentioned at the end of Section 4.2.1, it is comp
As
ed setting—as usedφin θ (x), yithe reverse
arg min pφ (xof theexample(imputed θ the reverse
˜
For each unsupervised ix (xXMERTby(x), yi ) m
| y p ∈ | yi,)L(δ)
˜ )L(δ ˜ use
˜
translate each
θ

ij
i
N i=1setting—as,used in MERT each (xi
g in the translations {x , x . . .}, and add
supervised x
reverse
i1
i2

loss
proximations that are more computationally feasible. Ea
exponentially
x 31.
i=1 page
2.6 ≤ 1) to an imputed training equation (4.4).
on x, stored in of p (x | y ) in set .
yi )
˜ many approximation φ ˜i
some complex translation system δθ (x). The
a hypergraph
θ

anslation system δ (x). The
English ordinary supervised training support other methods
y,One can have any formfunctionneed not (4.2)) on
may manipulate the loss and to (as of
Now doany(Crammer et al., 2006)) need not decoding.
that use deterministic
have MIRA form andcouldalongevaluated by
ers θ objective aof (4.4) this with the minimum risk training of (
scoring SCFG
function be with
isk may deﬁne SCFGconfuse
One should not
data. (4.4) could be evaluated by brute
ve high-scoring translation y. the losswith
one is manipulate
ang aof fertyAgain,CFGmaynotuse randomized decoding. to support other pr
scoring al., 2001)) that closed under composition!
function along function
et
• Approximations
e second step means that we must use δθ to forward-t
translation y.
- k-best
54
8
9

10

58
loss of the translations against the correspondingto
tru
d example yi , use the variational approximationφ
˜
reverse model p
- sampling
inimizes the weighted sum of these losses (i.e., the e
+, ˜
yi ,, x , . .lattice add each (xpφyto pair (w
˜ use .}, and
impute
xution i2(x,-the reverse model ij(Dyeri )training se
i1
p y) is derived from the imputed et al., 2008)
˜
lattice decoding
29

Supervised Discriminaof some complex translationSystem δθ (x). The
N ￿ The Forward system
￿
￿English y, may have any form and need not
o 1
ng—as used (x MERT θ (x), yi )
nunsupervised ipφin θ (x), yithe reverse model pφ to impute its
˜
pφ (x | y example yi,)L(δ)
˜ )L(δ | ˜ use
˜

N mayxdeﬁne scoring function ij , yi ) with
ters θi=1 {xi1 , xi2 .a. .},loss add
upervised setting—as,used in MERT each (x alongpair (weighted b
ranslations
and
˜
x
ge 31.a high-scoring translation y.
ting imputed training set .
to an
(4.4
mplex translation system The
(4.4)
ion system δθ (x).δθ (x). The
y, may have any form and
ordinary supervisedneed need not (4.2)) on the (weighted) impute
training (as of
ny form andcouldalongevaluated by brute force as fol
not
54 aof (4.4)
deﬁne
ective scoring function be with
4.4) could be y.
-scoring translation evaluated
ng function along with by brute force as fol-

step means
ation y. that we must use δθ to forward-translate each imputed x

he translations against the correspondingto impute its yi , and ch
true translation ˜
ple yi , use the reverse model pφ
˜
possible
the weighted sum of these losses (i.e., the empirical risk when the
. the reverse model ij φyto pair (weighted by pφ (xij
.y) is and add each (xp , ˜i )training set).its possible MT
.}, derived from the imputed impute Speciﬁc to our
x,
add each (xij , yi ) pairtranslation (from thepφ (xij |
˜
g set .
hat probabilistic “round-trip” (weighted by target-language
nguage and back again) will have a low (expected) loss.
raining (as of reverse on thep(weighted) imputed training
is that a typical (4.2)) model φ will generate a weighted lattice
30
ding exponentially many translations of y . It is computationally
˜

Supervised DiscriminaDeterministic translation system
of some complexDecoding System δθ (x). The
N ￿ The Forward
￿
￿English y, may have any form and need not
o 1 A simple translation rule(x), y ) model p to impute its
ng—as used (x MERT θ wouldi deﬁne φ
nunsupervised ipφin θ (x), yithe reverse
˜
pφ (x | y example yi,)L(δ)
˜ )L(δ | ˜ use
˜

N mayxdeﬁne scoring function ij , yi ) with
ters θi=1 {xi1 , xi2 .a. .},loss add
upervised setting—as,used in MERT each (x alongpair (weighted b
ranslations
and
˜
x
ge 31.a high-scoring translation y. θ (x) = argmax pθ (y | x)
δ
ting imputed training set .
to an
(4.4
y
mplexsystem δθ (x).δθ (x). The
translation system The
(4.4)
ion“reverse prediction model” that attempts to impute the missing xi data.
sa
y, may have any form and need not (4.2)) on the (weighted)
ordinary (4.2),andx)trainingminimum imputeddeﬁned inrisk impute
variant of supervisedneed not of model as empirical (2.18) on p
what we called (as
(abbrev
ny where (4.4)|4 couldalongevaluated by brute force as fol
formpθ (y functiona log-linear
is
54 Deterministic Decoding
deﬁne scoring
ectiveaof
be with
m imputed-risk),be is intractable due to spurious ambiguity and we
is evaluated
solving (4.6) y. translation by brute force as fol4.4) use one-best
-scoring translation
ng • could along with
function
N
￿ ￿ forward-translate each 1 ￿N
7
step means
ation y. ∗ that weamust1usemethodφfor this)L(δθ (x), yi) − N i=1 log
Empirically, min
standard δθ top (x | y is to minimize imputed x
θ = arg
˜i
˜
he translations against the correspondingdev impute its yi , and ch
true translation ˜
θ N
regularization coefﬁcient σ 2 is tuned on a to set.
ple y , use the reversei=1 x p
˜
model
possible

•

i

φ

the weighted sum of these losses (i.e., the empirical risk when the
. the reverse model ij φyto pair (weighted by pφ (xij
.y) is and add each (xp , ˜i )training set).its possible MT
.}, derived from the imputed impute Speciﬁc to our
x,
57
add each (xij ,objective of translationbe evaluatedtarget-language
˜
inimum.imputed-risk yi ) pair(4.4) could (from thepby(xij force as
g set
hat probabilistic “round-trip” (weighted by φ brute |
nguage and back again) will have a low (expected) loss.
raining (as of reverse on, thep(weighted) imputed training
is that a typical example yi use φ reverse model p to impute its pos
each unsupervised (4.2)) model thewill generate aφweighted lattice
˜
30
ding exponentially many translations of y . It is computationally
˜

Supervised DiscriminaDeterministic translation system
of some complexDecoding System δθ (x). The
N ￿ The Forward
￿
￿English y, may have any form and need not
o 1 A simple translation rule(x), y ) model p to impute its
ng—as used (x MERT θ wouldi deﬁne φ
nunsupervised ipφin θ (x), yithe reverse
˜
pφ (x | y example yi,)L(δ)
˜ )L(δ | ˜ use
˜

N mayxdeﬁne scoring function ij , yi ) with
ters θi=1 {xi1 , xi2 .a. .},loss add
upervised setting—as,used in MERT each (x alongpair (weighted b
ranslations
and
˜
x
ge 31.a high-scoring translation y. θ (x) = argmax pθ (y | x)
δ
ting imputed training set .
to an
(4.4
y
mplexsystem δθ (x).δθ (x). The
translation system The
(4.4)
ion“reverse prediction model” that attempts to impute the missing xi data.
sa
y, may have any form and need not (4.2)) on the (weighted)
ordinary (4.2),andx)trainingminimum imputeddeﬁned inrisk impute
variant of supervisedneed not of model as empirical (2.18) on p
what we called (as
(abbrev
ny where (4.4)|4 couldalongevaluated by brute force as fol
formpθ (y functiona log-linear
is
54 Deterministic Decoding
deﬁne scoring
ectiveaof
be with
the
is
m imputed-risk),be is intractable due to spurious ambiguity and we
is evaluated by bruteobjectiveas not
4.4)solving (4.6) y. translation
force foluse one-best
-scoring translation
ng • could along with
function
differentiable
N
￿ ￿ forward-translate each 1 ￿N
7
step means
ation y. ∗ that weamust1usemethodφfor this)L(δθ (x), yi) − N i=1 log
Empirically, min
standard δθ top (x | y is to minimize imputed x
θ = arg
˜i
˜
he translations against the correspondingdev impute its yi , and ch
true translation ˜
θ N
regularization coefﬁcient σ 2 is tuned on a to set.
ple y , use the reversei=1 x p
˜
model
possible

•

☹

i

φ

the weighted sum of these losses (i.e., the empirical risk when the
. the reverse model ij φyto pair (weighted by pφ (xij
.y) is and add each (xp , ˜i )training set).its possible MT
.}, derived from the imputed impute Speciﬁc to our
x,
57
add each (xij ,objective of translationbe evaluatedtarget-language
˜
inimum.imputed-risk yi ) pair(4.4) could (from thepby(xij force as
g set
hat probabilistic “round-trip” (weighted by φ brute |
nguage and back again) will have a low (expected) loss.
raining (as of reverse on, thep(weighted) imputed training
is that a typical example yi use φ reverse model p to impute its pos
each unsupervised (4.2)) model thewill generate aφweighted lattice
˜
30
ding exponentially many translations of y . It is computationally
˜

Supervised Discriminaowever, this would not Decoding
Deterministic yield a differentiable objective function. Inﬁnitesima
of some complex translationSystem δθ (x). The
The Forward
N in discrete changes to the systemoutput string δ (x), and hen
ould result ￿
winning
￿
￿English y, may have any form and need not
θ
1
o θ (x), yiA simple translation rule(x), y ) line search toto imputeopt
ng—as Och (2003) θ (x), yiθspecialized
(δ p (x used (x MERT ) would deﬁne
˜ ). | y example yi, use
nunsupervised ipφin developed athe reverse model pφ solve the its
˜i
˜ )L(δ | ˜ )L(δ
˜
φ

N is {xi1 , xi2 . scoring function ij yi ) with
em. Thismay deﬁne used in MERT each large, number of parameters.
ters θi=1 not scalable whenloss add
upervised setting—as, .a .}, the model has a (x alongpair (weighted b
ranslations x
and
˜
x
ge 31.a high-scoring translation y. θ (x) = argmax pθ (y | x)
δ
ting imputed training set .
to an
(4.4
y
omized Decoding system The
mplexsystem δθ (x).δθ (x). The
translation
(4.4)
ion“reverse prediction model” that attempts to impute the missing xi data.
sa
y, may have
form and
ordinaryusingany | x)max ofneed not (4.2)) on the (weighted) impute
supervisedneed not of model as deﬁned inrisk (abbrevp
(as
variantof (4.2),the arg trainingminimum imputed empirical (2.18)θon
of
what is a log-linear
called(4.6),
steadwhere pθ (y weDecodingwe assume during training that δ (x)
ny form andcouldalongevaluated by brute force as fol
54 Deterministic
deﬁne scoring function be
ectiveaof (4.4)4 randomly with a translation, choosing y with pr
is evaluated by bruteobjectiveas not
m imputed-risk),be is intractable due to spurious ambiguity and we
quantity: the system
outputs the force is fol4.4)solving (4.6)will modify our
use one-best
-scoringatranslation y. translation
ng • could wealong with objective function to take yet another ex
function
differentiable
x). As result,
N
￿ ￿ forward-translate each 1 ￿N
7
step means
n unknown that weamust1usemethod yi in is to minimize imputed x
ation y. ∗quantity, replacing L(δθθ (x),φfor) this(4.4) θwith yi) − N i=1 log
Empirically, min
standard δ top ˜ | y )L(δ (x), ˜
θ = arg
(x ˜i
he translations against the￿ is tuned on a dev impute its yi , and ch
true translation ˜
θ N σ2
regularization coefﬁcient correspondingto set.
x
ple yi , use the reversei=1pθ (y | x)pL(y, yi)
˜
model φ ˜
possible
the weighted sum of these ylosses (i.e., the empirical risk when the
impute
. theRandomized Decoding˜to pair (weighted by pφ (xij
.y) is reverse model ij φyi )training set).its possible MT
.}, and add each (xp ,
x,
derived from the imputed
Speciﬁc to our
57
our ﬁnalimputed-risk yi ) pair(4.4) could
•
add eachdistribution ofofobjective of (4.4) becomes,φ (xij force as
˜
inimumuse a (xij ,objective translations be evaluated by brute |
g set . minimum imputed-risk (weighted by p

•

☹

•

hat probabilistic “round-trip” translation (from the target-language
N ￿
￿
￿have a low (expected) loss.
nguage and back again)1will
∗
θ = argmin
pφ (x | yi )
˜
pθ (y | x)L(y, yi )
˜
raining (as of reverseNyii=1 xp(weighted) imputed training
is that a typical examplemodel φ reverse model p to impute its pos
θ
each unsupervised (4.2)) on thethewill generate aφweighted lattice
˜ , use
y
30
ding exponentially many translations of y . It is computationally
˜

Supervised Discriminaowever, this would not Decoding
Deterministic yield a differentiable objective function. Inﬁnitesima
of some complex translationSystem δθ (x). The
The Forward
N in discrete changes to the systemoutput string δ (x), and hen
ould result ￿
winning
￿
￿English y, may have any form and need not
θ
1
o θ (x), yiA simple translation rule(x), y ) line search toto imputeopt
ng—as Och (2003) θ (x), yiθspecialized
(δ p (x used (x MERT ) would deﬁne
˜ ). | y example yi, use
nunsupervised ipφin developed athe reverse model pφ solve the its
˜i
˜ )L(δ | ˜ )L(δ
˜
φ

N is {xi1 , xi2 . scoring function ij yi ) with
em. Thismay deﬁne used in MERT each large, number of parameters.
ters θi=1 not scalable whenloss add
upervised setting—as, .a .}, the model has a (x alongpair (weighted b
ranslations x
and
˜
x
ge 31.a high-scoring translation y. θ (x) = argmax pθ (y | x)
δ
ting imputed training set .
to an
(4.4
y
omized Decoding system The
mplexsystem δθ (x).δθ (x). The
translation
(4.4)
ion“reverse prediction model” that attempts to impute the missing xi data.
sa
y, may have
form and
ordinaryusingany | x)max ofneed not (4.2)) on the (weighted) impute
supervisedneed not of model as deﬁned inrisk (abbrevp
(as
variantof (4.2),the arg trainingminimum imputed empirical (2.18)θon
of
what is a log-linear
called(4.6),
steadwhere pθ (y weDecodingwe assume during training that δ (x)
ny form andcouldalongevaluated by brute force as fol
54 Deterministic
deﬁne scoring function be
ectiveaof (4.4)4 randomly with a translation, choosing y with pr
is evaluated by bruteobjectiveas not
m imputed-risk),be is intractable due to spurious ambiguity and we
quantity: the system
outputs the force is fol4.4)solving (4.6)will modify our
use one-best
-scoringatranslation y. translation
ng • could wealong with objective function to take yet another ex
function
differentiable
x). As result,
N
￿ ￿ forward-translate each 1 ￿N
7
step means
n unknown that weamust1usemethod yi in is to minimize imputed x
ation y. ∗quantity, replacing L(δθθ (x),φfor) this(4.4) θwith yi) − N i=1 log
Empirically, min
standard δ top ˜ | y )L(δ (x), ˜
θ = arg
(x ˜i
he translations against the￿ is tuned on a dev impute its yi , and ch
true translation ˜
θ N σ2
regularization coefﬁcient correspondingto set.
x
ple yi , use the reversei=1pθ (y | x)pL(y, yi)
˜
model φ ˜
possible
the weighted sum of these ylosses (i.e., the empirical risk when the
impute
. theRandomized Decoding˜to pair (weighted by pφ (xij
.y) is reverse model ij φyi )training set).its possible MT
.}, and add each (xp ,
x,
derived from the imputed
Speciﬁc to our
57
our ﬁnalimputed-risk yi ) pair(4.4) could
•
add eachdistribution ofofobjective of (4.4) becomes,φ (xij force as
˜
inimumuse a (xij ,objective translations be evaluated by brute |
g set . minimum imputed-risk (weighted by p

•

☹

•

hat probabilistic “round-trip” translation (from the target-language
N ￿
￿
￿have a low (expected) loss.
nguage and back again)1will
∗
θ = argmin
pφ (x | yi )
˜
pθ (y | x)L(y, yi )
˜
raining (as of reverseNyii=1 xp(weighted) imputed training
is that a typical examplemodel φ reverse model p to impute its pos
θ
each unsupervised (4.2)) on thethewill generate aφweighted lattice
˜ , use
y
30
ding exponentially many translations of y . It is computationally
˜

Supervised Discriminaowever, this would not Decoding
Deterministic yield a differentiable objective function. Inﬁnitesima
of some complex translationSystem δθ (x). The
The Forward
N in discrete changes to the systemoutput string δ (x), and hen
ould result ￿
winning
￿
￿English y, may have any form and need not
θ
1
o θ (x), yiA simple translation rule(x), y ) line search toto imputeopt
ng—as Och (2003) θ (x), yiθspecialized
(δ p (x used (x MERT ) would deﬁne
˜ ). | y example yi, use
nunsupervised ipφin developed athe reverse model pφ solve the its
˜i
˜ )L(δ | ˜ )L(δ
˜
φ

N is {xi1 , xi2 . scoring function ij yi ) with
em. Thismay deﬁne used in MERT each large, number of parameters.
ters θi=1 not scalable whenloss add
upervised setting—as, .a .}, the model has a (x alongpair (weighted b
ranslations x
and
˜
x
ge 31.a high-scoring translation y. θ (x) = argmax pθ (y | x)
δ
ting imputed training set .
to an
(4.4
y
omized Decoding system The
mplexsystem δθ (x).δθ (x). The
translation
(4.4)
ion“reverse prediction model” that attempts to impute the missing xi data.
sa
y, may have
form and
ordinaryusingany | x)max ofneed not (4.2)) on the (weighted) impute
supervisedneed not of model as deﬁned inrisk (abbrevp
(as
variantof (4.2),the arg trainingminimum imputed empirical (2.18)θon
of
what is a log-linear
called(4.6),
steadwhere pθ (y weDecodingwe assume during training that δ (x)
ny form andcouldalongevaluated by brute force as fol
54 Deterministic
deﬁne scoring function be
ectiveaof (4.4)4 randomly with a translation, choosing y with pr
is evaluated by bruteobjectiveas not
m imputed-risk),be is intractable due to spurious ambiguity and we
quantity: the system
outputs the force is fol4.4)solving (4.6)will modify our
use one-best
-scoringatranslation y. translation
ng • could wealong with objective function to take yet another ex
function
differentiable
x). As result,
N
￿ ￿ forward-translate each 1 ￿N
7
step means
n unknown that weamust1usemethod yi in is to minimize imputed x
ation y. ∗quantity, replacing L(δθθ (x),φfor) this(4.4) θwith yi) − N i=1 log
Empirically, min
standard δ top ˜ | y )L(δ (x), ˜
θ = arg
(x ˜i
he translations against the￿ is tuned on a dev impute its yi , and ch
true translation ˜
θ N σ2
regularization coefﬁcient correspondingto set.
x
ple yi , use the reversei=1pθ (y | x)pL(y, yi)
˜
model φ ˜
possible
the weighted sum of these ylosses (i.e., the empirical risk when the
impute
. theRandomized Decoding˜to pair (weighted by pφ (xij
.y) is reverse model ij φyi )training set).its possible MT
.}, and add each (xp ,
x,
derived from the imputed
Speciﬁc to our
57
our ﬁnalimputed-risk yi ) pair(4.4) could
•
add eachdistribution ofofobjective of (4.4) becomes,φ (xij force as
˜
inimumuse a (xij ,objective translations be evaluated by brute |
g set . minimum imputed-risk (weighted by p

•

☹

•

hat probabilistic “round-trip” translation (from the target-language
expected loss
N ￿
￿
￿have a low (expected) loss.
nguage and back again)1will
∗
θ = argmin
pφ (x | yi )
˜
pθ (y | x)L(y, yi )
˜
raining (as of reverseNyii=1 xp(weighted) imputed training
is that a typical examplemodel φ reverse model p to impute its pos
θ
each unsupervised (4.2)) on thethewill generate aφweighted lattice
˜ , use
y
30
ding exponentially many translations of y . It is computationally
˜

Supervised Discriminaowever, this would not Decoding
Deterministic yield a differentiable objective function. Inﬁnitesima
of some complex translationSystem δθ (x). The
The Forward
N in discrete changes to the systemoutput string δ (x), and hen
ould result ￿
winning
￿
￿English y, may have any form and need not
θ
1
o θ (x), yiA simple translation rule(x), y ) line search toto imputeopt
ng—as Och (2003) θ (x), yiθspecialized
(δ p (x used (x MERT ) would deﬁne
˜ ). | y example yi, use
nunsupervised ipφin developed athe reverse model pφ solve the its
˜i
˜ )L(δ | ˜ )L(δ
˜
φ

N is {xi1 , xi2 . scoring function ij yi ) with
em. Thismay deﬁne used in MERT each large, number of parameters.
ters θi=1 not scalable whenloss add
upervised setting—as, .a .}, the model has a (x alongpair (weighted b
ranslations x
and
˜
x
ge 31.a high-scoring translation y. θ (x) = argmax pθ (y | x)
δ
ting imputed training set .
to an
(4.4
y
omized Decoding system The
mplexsystem δθ (x).δθ (x). The
translation
(4.4)
ion“reverse prediction model” that attempts to impute the missing xi data.
sa
y, may have
form and
ordinaryusingany | x)max ofneed not (4.2)) on the (weighted) impute
supervisedneed not of model as deﬁned inrisk (abbrevp
(as
variantof (4.2),the arg trainingminimum imputed empirical (2.18)θon
of
what is a log-linear
called(4.6),
steadwhere pθ (y weDecodingwe assume during training that δ (x)
ny form andcouldalongevaluated by brute force as fol
54 Deterministic
deﬁne scoring function be
ectiveaof (4.4)4 randomly with a translation, choosing y with pr
is evaluated by bruteobjectiveas not
m imputed-risk),be is intractable due to spurious ambiguity and we
quantity: the system
outputs the force is fol4.4)solving (4.6)will modify our
use one-best
-scoringatranslation y. translation
ng • could wealong with objective function to take yet another ex
function
differentiable
x). As result,
N
￿ ￿ forward-translate each 1 ￿N
7
step means
n unknown that weamust1usemethod yi in is to minimize imputed x
ation y. ∗quantity, replacing L(δθθ (x),φfor) this(4.4) θwith yi) − N i=1 log
Empirically, min
standard δ top ˜ | y )L(δ (x), ˜
θ = arg
(x ˜i
he translations against the￿ is tuned on a dev impute its yi , and ch
true translation ˜
θ N σ2
regularization coefﬁcient correspondingto set.
x
ple yi , use the reversei=1pθ (y | x)pL(y, yi)
˜
model φ ˜
possible
the weighted sum of these ylosses (i.e., the empirical risk when the
impute
. theRandomized Decoding˜to pair (weighted by pφ (xij
.y) is reverse model ij φyi )training set).its possible MT
.}, and add each (xp ,
differentiable our
x,
derived from the imputed
Speciﬁc to
57
our ﬁnalimputed-risk yi ) pair(4.4) could
•
add eachdistribution ofofobjective of (4.4) becomes,φ (xij force as
˜
inimumuse a (xij ,objective translations be evaluated by brute |
g set . minimum imputed-risk (weighted by p

•

☹

•

☺

hat probabilistic “round-trip” translation (from the target-language
expected loss
N ￿
￿
￿have a low (expected) loss.
nguage and back again)1will
∗
θ = argmin
pφ (x | yi )
˜
pθ (y | x)L(y, yi )
˜
raining (as of reverseNyii=1 xp(weighted) imputed training
is that a typical examplemodel φ reverse model p to impute its pos
θ
each unsupervised (4.2)) on thethewill generate aφweighted lattice
˜ , use
y
30
ding exponentially many translations of y . It is computationally
˜

Experiments

•
•
•

Supervised Training

•

require bitext

Unsupervised Training

•

require monolingual English

Semi-supervised Training

•

interpolation of supervised and unsupervised

31

Semi-supervised Training

32

Semi-supervised Training
Training scenario
Test BLEU
Sup, (200, 200*16)
47.6
+Unsup, 100*16 Eng sentences
49.0
+Unsup, 200*16 Eng sentences
48.9

2: BLEU scores for semi-supervised training. The supervised syste
d on a bilingual data set having 200 Chinese sentences and 200*16 E
“+Unsup” means that we add unsupervised data (i.e., monolingual E
or training. For each English sentence, we impute a one-best Chinese
e reverse translation system.

Data size

Test BLEU
Sup Unsup

32

Semi-supervised Training
Training scenario
Test BLEU
Sup, (200, 200*16)
47.6
+Unsup, 100*16 Eng sentences
49.0
+Unsup, 200*16 Eng sentences
48.9

2: BLEU scores for semi-supervised training. The supervised syste
d on a bilingual data set having 200 Chinese sentences and 200*16 E
“+Unsup” means that weHeld-out Bilingual Data
add unsupervised data (i.e., monolingual E
Translation
Bilingual
Generative
Training
Models
Data
or training. For each English sentence, we impute a one-best Chinese
Optimal
Discriminative
Training
Weights
Monolingual
e reverse translationLanguage
system.
Generative
English

Training

Unseen
Sentences

Models

Decoding

Translation
Outputs

Data size

Test BLEU
Sup Unsup

32

Semi-supervised Training
Training scenario
Test BLEU
Sup, (200, 200*16)
47.6
+Unsup, 100*16 Eng sentences
49.0
+Unsup, 200*16 Eng sentences
48.9

2: BLEU scores for semi-supervised training. The supervised syste
40K sent. pairs
d on a bilingual data set having 200 Chinese sentences and 200*16 E
“+Unsup” means that weHeld-out Bilingual Data
add unsupervised data (i.e., monolingual E
Translation
Bilingual
Generative
Training
Models
Data
or training. For each English sentence, we impute a one-best Chinese
Optimal
Discriminative
Training
Weights
Monolingual
e reverse translationLanguage
system.
Generative
English

Training

Unseen
Sentences

Models

Decoding

Translation
Outputs

Data size

Test BLEU
Sup Unsup

32

Semi-supervised Training
Training scenario
Test BLEU
Sup, (200, 200*16)
47.6
+Unsup, 100*16 Eng sentences
49.0
+Unsup, 200*16 Eng sentences
48.9

2: BLEU scores for semi-supervised training. The supervised syste
40K sent. pairs
d on a bilingual data set having 200 Chinese sentences and 200*16 E
“+Unsup” means that weHeld-out Bilingual Data
add unsupervised data (i.e., monolingual E
Translation
Bilingual
Generative
Training
Models
Data
or training. For each English sentence, we impute a one-best Chinese
Optimal
Discriminative
Training
Weights
Monolingual
e reverse translationLanguage
system.
Generative
English

Training

Unseen
Sentences

Models

Decoding

Translation
Outputs

Data size

Test BLEU
Sup Unsup

32

Semi-supervised Training
Training scenario
Test BLEU
Sup, (200, 200*16)
47.6
+Unsup, 100*16 Eng sentences
49.0
+Unsup, 200*16 Eng sentences
48.9

2: BLEU scores for semi-supervised training. The supervised syste
40K sent. pairs
d on a bilingual data set having 200 Chinese sentences and 200*16 E
“+Unsup” means that weHeld-out Bilingual Data
add unsupervised data (i.e., monolingual E
Translation
Bilingual
Generative
Training
Models
Data
or training. For each English sentence, we impute a one-best Chinese
Optimal
Discriminative
Training
Weights
Monolingual
e reverse translationLanguage
system.
Generative
English

Training

Models

551 features
Unseen
Sentences

Decoding

Translation
Outputs

Data size

Test BLEU
Sup Unsup

32

Semi-supervised Training
Test BLEU
Training scenario
47.6
Sup, (200, 200*16)
+Unsup, 100*16 Eng sentences
49.0
+Unsup, 200*16 Eng sentences
48.9

2: BLEU scores for semi-supervised training. The unsupervised
BLEU
supervised syste
syst
Adding
40K sent. pairs
d on a bilingual data set having 200 Chinese data helps! 200*16 E
sentences and
“+Unsup” means that weHeld-out Bilingual Data
add unsupervised data (i.e., monolingual E
Translation
Bilingual
Generative
Training
Models
Data
or training. For each English sentence, we impute a one-best Chinese
Optimal
Discriminative
Training
Weights
Monolingual
e reverse translationLanguage
system.
Generative
Training
English
Models
551 features
Unseen
Sentences

Decoding

Translation
Outputs

Data size

Test BLEU
BLEU
Sup Unsup

32

Supervised vs. Unsupervised
Unsupervised training performs as well as
(and often better than) the supervised one!

Sup
Un-sup
50

BLEU

49
48
47
46
45

100

200

300

400

Data Size

33

Supervised vs. Unsupervised
Unsupervised training performs as well as
(and often better than) the supervised one!
Unsupervised uses 16 times of data
as supervised. For example,

Sup
Un-sup
50

BLEU

49
48
47
46
45

100

200

300

400

Data Size

33

Supervised vs. Unsupervised
Unsupervised training performs as well as
(and often better than) the supervised one!
Unsupervised uses 16 times of data
as supervised. For example,

Sup
Un-sup
50

BLEU

49
48

Chinese

47

Sup

46
45

100

200

300

400

English

100

16*100

Unsup 16*100 16*16*100

Data Size

33

Supervised vs. Unsupervised
Unsupervised training performs as well as
(and often better than) the supervised one!
Unsupervised uses 16 times of data
as supervised. For example,

Sup
Un-sup
50

BLEU

49
48

Chinese

47

Sup

46
45

100

200

300

400

English

100

16*100

Unsup 16*100 16*16*100

Data Size

33

Supervised vs. Unsupervised
Unsupervised training performs as well as
(and often better than) the supervised one!
Unsupervised uses 16 times of data
as supervised. For example,

Sup
Un-sup
50

BLEU

49
48

Chinese

47

Sup

46
45

100

200

300

Data Size

400

English

100

16*100

Unsup 16*100 16*16*100

But, fair comparison!

33

ptimizing the given performance metric.
criminative training be explained as minimizing to (and
performs similarly imputed risk.
Supervised vs. Unsupervised
theoretically sound and can
where pφ a probabilistic “round-trip” translation
o intuitive: it tries to ensureis into the prediction model” that attempts
g unsupervised datathat“reverse supervised train- well as
Unsupervised training performs as
Sup
resulting variant of and back than) called minimum im
nguage sentence to the source language(4.2), what we the supervised one!
(and often better again) will have low
Un-sup

as minimum imputed-risk), is4 16 times of data
Unsupervised uses

50

BLEU

periments by using the open-source MT toolkit Joshuaexample, 2009a).
49
as supervised. For (Li et al.,
N
￿ (and
how that unsupervised discriminative training performs similarly to ￿
1
48
∗
θ =Chinese supervised train-pφ (x |
argimpute the missin
pervised case. Also, adding unsupervised data into themin
prediction model” that attempts attempts to impute the
to θ English
N i=1 x
is a 47
“reverse prediction model” that
erformance.

sk (for(4.2), what minimum imputed16*100 empir
Supervised Discrimina- ris
2), what we called we calledSup 100 imputed
empirical
variant of
minimum
46

4

Unsup 16*100 16*16*100
isk), is 100 200 300is4400
m imputed-risk),Our minimum imputed-risk objective of (4.4) could
um EmpiricalSize (for Supervised DiscriminaRisk
Data
But, fair comparison!
lows.
N ￿ ￿￿
N
aining)
1
1 ￿
∗
g in•the supervised setting—as usedφin θ (x), yithe rever
= arg min
˜
= Moreθexperiments θ unsupervised iexample yi,)L(δθ (x), yi
arg min 1. For each pφ (x | y p (x MERT )
˜ )L(δ | ˜ use
˜
N i=1 x i1 i2
ew discriminative training in the translations {x , x ,used in MERT each
θ Nreverse supervised setting—as . . .}, and add
loss
.6 on page 31. 2.6 on page 31.
i=1 x
ose discussed in Section y ) ≤ 1) to an imputed training set .
˜i
tune the parameters θ translation translation system The
some complex of some complexsystem δθ (x).δθ (x). The
translates Chinese x toNow do ordinary supervised training (asnot (4.2))
English y, may have
2. have any form any form and need of
English y, parameters θ may deﬁne a scoring function along with
may
and need not
or example, the
45

33

ptimizing the given performance metric.
criminative training be explained as minimizing to (and
performs similarly imputed risk.
Supervised vs. Unsupervised
theoretically sound and can
where pφ a probabilistic “round-trip” translation
o intuitive: it tries to ensureis into the prediction model” that attempts
g unsupervised datathat“reverse supervised train- well as
Unsupervised training performs as
Sup
resulting variant of and back than) called minimum im
nguage sentence to the source language(4.2), what we the supervised one!
(and often better again) will have low
Un-sup

as minimum imputed-risk), is4 16 times of data
Unsupervised uses

50

BLEU

periments by using the open-source MT toolkit Joshuaexample, 2009a).
49
as supervised. For (Li et al.,
N
￿ (and
how that unsupervised discriminative training performs similarly to ￿
1
48
∗
θ =Chinese supervised train-pφ (x |
argimpute the missin
pervised case. Also, adding unsupervised data into themin
prediction model” that attempts attempts to impute the
to θ English
N i=1 x
is a 47
“reverse prediction model” that
erformance.

sk (for(4.2), what minimum imputed16*100 empir
Supervised Discrimina- ris
2), what we called we calledSup 100 imputed
empirical
variant of
minimum
46

4

Unsup 16*100 16*16*100
isk), is 100 200 300is4400
m imputed-risk),Our minimum imputed-risk objective of (4.4) could
um EmpiricalSize (for Supervised DiscriminaRisk
Data
But, fair comparison!
lows.
N ￿ ￿￿
N
aining)
1
1 ￿
∗
g in•the supervised setting—as usedφin θ (x), yithe rever
= arg min
˜
= Moreθexperiments θ unsupervised iexample yi,)L(δθ (x), yi
arg min 1. For each pφ (x | y p (x MERT )
˜ )L(δ | ˜ use
˜
ew discriminative training in the translations {x , x ,used in MERT each
θ k-best size supervised setting—as . . .}, and add
Nreverse x N i=1 x i1 i2
loss
.6 on page 31. 2.6 on page 31.
• different
i=1
ose discussed in Section y ) ≤ 1) to an imputed training set .
˜i
tune the parameters θ translation translation system The
some complex of some complexsystem δθ (x).δθ (x). The
translates Chinese x toNow do ordinary supervised training (asnot (4.2))
English y, may have
2. have any form any form and need of
English y, parameters θ may deﬁne a scoring function along with
may
and need not
or example, the
45

33

ptimizing the given performance metric.
criminative training be explained as minimizing to (and
performs similarly imputed risk.
Supervised vs. Unsupervised
theoretically sound and can
where pφ a probabilistic “round-trip” translation
o intuitive: it tries to ensureis into the prediction model” that attempts
g unsupervised datathat“reverse supervised train- well as
Unsupervised training performs as
Sup
resulting variant of and back than) called minimum im
nguage sentence to the source language(4.2), what we the supervised one!
(and often better again) will have low
Un-sup

as minimum imputed-risk), is4 16 times of data
Unsupervised uses

50

BLEU

periments by using the open-source MT toolkit Joshuaexample, 2009a).
49
as supervised. For (Li et al.,
N
￿ (and
how that unsupervised discriminative training performs similarly to ￿
1
48
∗
θ =Chinese supervised train-pφ (x |
argimpute the missin
pervised case. Also, adding unsupervised data into themin
prediction model” that attempts attempts to impute the
to θ English
N i=1 x
is a 47
“reverse prediction model” that
erformance.

sk (for(4.2), what minimum imputed16*100 empir
Supervised Discrimina- ris
2), what we called we calledSup 100 imputed
empirical
variant of
minimum
46

4

Unsup 16*100 16*16*100
isk), is 100 200 300is4400
m imputed-risk),Our minimum imputed-risk objective of (4.4) could
um EmpiricalSize (for Supervised DiscriminaRisk
Data
But, fair comparison!
lows.
N ￿ ￿￿
N
aining)
1
1 ￿
∗
g in•the supervised setting—as usedφin θ (x), yithe rever
= arg min
˜
= Moreθexperiments θ unsupervised iexample yi,)L(δθ (x), yi
arg min 1. For each pφ (x | y p (x MERT )
˜ )L(δ | ˜ use
˜
ew discriminative training in the translations {x , x ,used in MERT each
θ k-best size supervised setting—as . . .}, and add
Nreverse x N i=1 x i1 i2
loss
.6 on page 31. 2.6 on page 31.
• different
i=1
ose discussed in Section y ) ≤ 1) to an imputed training set .
˜i model
• different θ translation translation system The
tune the parametersreverse
some complex of some complexsystem δθ (x).δθ (x). The
translates Chinese x toNow do ordinary supervised training (asnot (4.2))
English y, may have
2. have any form any form and need of
English y, parameters θ may deﬁne a scoring function along with
may
and need not
or example, the
45

33

Outline
•
•

Hypergraph as Hypothesis Space
Unsupervised Discriminative Training
‣ minimum imputed risk
‣ contrastive language model estimation

•
•

Variational Decoding
First- and Second-order Expectation Semirings

34

Language Modeling

35

Language Modeling
f (y)·θ
e
• Language Model pθ (y) = Z(∗)
y
• assign a probability to an English sentence (y)·θ
f
• typically use an n-gram model ￿ e
=
f (y ￿ )·θ
￿ ∈Σ∗ e
y

is a feature vector depending on y, and θ is the corr
d as a whole-sentence maximum entropy language m
1
01).
with the above log-linear model requires computing th
atively called the partition function), which is compu
∗
sum over all the possible sequence y ∈ Σ . In our
he sum over all possible English sentences with any l
35

Normalized Language Model

Language Modeling
f (y)·θ
e p(y), it is instructive to
rastive Language Model p (y) = model
model can be used as a language
θ
• other language modeling techniques. The most commonly used
od to
Z(∗) y
probability to
• assign a parameterized an English sentence (y)·θ
d n-gram model
as follows,
f
an
• typically use￿ n-gram model ￿ e
= c (y)
f (y ￿ )·θ
p(y) =
p(r(w) | h(w))
(5.7
￿ ∈Σ∗ e
y
w

w∈Wn

is a feature vector depending on y, and θ is the corr
of n-gram types. Each w ∈ Wn is an n-gram, which occurs cw (y) time
dw may whole-sentence maximum entropy language m
as a be divided into an (n − 1)-gram preﬁx h(w) (the history) and
d
1
01). rightmost or current word). For example, for a trigram “on th
w) (the
is “on the” and the current word is “table”.requires computing th
with the above log-linear model
model of (5.8), p(r(w) | h(w)) is a proper probability distribution nor
atively called the partition locally normalized. In contrast, ou
function), which is compu
story h(w), and thus the model is
∗
sum over all the possible sequence y ∈ Σ . In our
ge model is globally normalized.
parameters θ in all possible English p(r(w) | h(w)) directly l
he sum over the model? One can treat sentences with any a

t we aim to learn. A usual way to estimate such parameters is to use
35

Normalized Language Model

Language Modeling
f (y)·θ
e p(y), it is instructive to
rastive Language Model p (y) = model
model can be used as a language
θ
• other language modeling techniques. The most commonly used
od to
Z(∗) y
probability to
• assign a parameterized an English sentence (y)·θ
d n-gram model
as follows,
f
an
• typically use￿ n-gram model ￿ e
= c (y)
f (y ￿ )·θ
p(y) =
p(r(w) | h(w))
(5.7
￿ ∈Σ∗ e
y
w

w∈Wn

a set of n-grams occurred in y

is a feature vector depending on y, and θ is the corr
of n-gram types. Each w ∈ Wn is an n-gram, which occurs cw (y) time
dw may whole-sentence maximum entropy language m
as a be divided into an (n − 1)-gram preﬁx h(w) (the history) and
d
1
01). rightmost or current word). For example, for a trigram “on th
w) (the
is “on the” and the current word is “table”.requires computing th
with the above log-linear model
model of (5.8), p(r(w) | h(w)) is a proper probability distribution nor
atively called the partition locally normalized. In contrast, ou
function), which is compu
story h(w), and thus the model is
∗
sum over all the possible sequence y ∈ Σ . In our
ge model is globally normalized.
parameters θ in all possible English p(r(w) | h(w)) directly l
he sum over the model? One can treat sentences with any a

t we aim to learn. A usual way to estimate such parameters is to use
35

Normalized Language Model

Language Modeling
f (y)·θ
e p(y), it is instructive to
rastive Language Model p (y) = model
model can be used as a language
θ
• other language modeling techniques. The most commonly used
od to
Z(∗) y
probability to
• assign a parameterized an English sentence (y)·θ
d n-gram model
as follows,
f
an
• typically use￿ n-gram model ￿ e
= c (y)
f (y ￿ )·θ
p(y) =
p(r(w) | h(w))
(5.7
￿ ∈Σ∗ e
y
w

w∈Wn

a set of n-grams occurred in y

Locally
is a feature vector depending on y, and θ is the corr
normalized
of n-gram types. Each w ∈ Wn is an n-gram, which occurs cw (y) time
dw may whole-sentence maximum entropy language m
as a be divided into an (n − 1)-gram preﬁx h(w) (the history) and
d
1
01). rightmost or current word). For example, for a trigram “on th
w) (the
is “on the” and the current word is “table”.requires computing th
with the above log-linear model
model of (5.8), p(r(w) | h(w)) is a proper probability distribution nor
atively called the partition locally normalized. In contrast, ou
function), which is compu
story h(w), and thus the model is
∗
sum over all the possible sequence y ∈ Σ . In our
ge model is globally normalized.
parameters θ in all possible English p(r(w) | h(w)) directly l
he sum over the model? One can treat sentences with any a
t we aim to learn. A usual way to estimate such parameters is to use
35

Normalized Language Model
ge model pθ (y) (parameterized by θ) over the examples. The model
Language Modeling
f (y)·θ
e p(y), it is instructive to
probability to any English sentence. We can obtain such a model
rastive Language Model p (y) = model
model can be used as a language
θ
• other training modeling as follows,
hood of thelanguage examples techniques. The most commonly used
od to
Z(∗)
￿ an English sentence y
assign a probability to
• θ∗model parameterizedpas(˜ )
d n-gram = arg max
follows,
(5.1)
f (y)·θ
θ yi

•

e
typically use anθ n-gram model
i
￿
= cw￿
(y)
f (y ￿ )·θ
p(y) = p (˜ )?
p(r(w) | h(w))
(5.7
￿ ∈Σ∗ e
hat is the form of θ yi
y
w∈Wn
a set of n-grams occurred in y
Locally
is a feature vector depending on y, and θ is the corr
normalized
of n-gram types. Each w ∈ Wn is an n-gram, which occurs cw (y) time
ence Maximum Entropy Language Model language m
dw may whole-sentence maximum entropy history) and
as a be divided into an (n − 1)-gram preﬁx h(w) (the
d
Global Log-linear Model
1
a (the rightmost or current word). For example, for
01). (whole-sentence maximum-entropy LM)follows,a trigram “on th
w)globally normalized log-linear model as (Rosenfeld et al., 2001)
is “on the” and the current word is “table”.requires computing th
ef (y)·θ
with the(y) = log-linear model
above
p (5.8), p(r(w) | h(w)) is a proper probability distribution nor
(5.2)
model of θ
Z(∗)
atively called the partition locally normalized. In contrast, ou
function), which is compu
story h(w), and thus the model is
f (y)·θ
∗
e
sum over all the possible sequence y ∈ Σ . In (5.3)
our
ge model is globally normalized.
= ￿
f (y ￿ )·θ
parameters θ in the model? ∗ One can treat sentences with any a
y ￿ ∈Σ e English p(r(w) | h(w)) directly l
he sum over all possible
t we aim to learn. A usual way is the corresponding weight vector.
to estimate such parameters is to use
35
vector depending on y, and θ

•

Normalized Language Model
ge model pθ (y) (parameterized by θ) over the examples. The model
Language Modeling
f (y)·θ
e p(y), it is instructive to
probability to any English sentence. We can obtain such a model
rastive Language Model p (y) = model
model can be used as a language
θ
• other training modeling as follows,
hood of thelanguage examples techniques. The most commonly used
od to
Z(∗)
￿ an English sentence y
assign a probability to
• θ∗model parameterizedpas(˜ )
d n-gram = arg max
follows,
(5.1)
f (y)·θ
θ yi

•

e
typically use anθ n-gram model
i
￿
= cw￿
(y)
f (y ￿ )·θ
p(y) = p (˜ )?
p(r(w) | h(w))
(5.7
￿ ∈Σ∗ e
hat is the form of θ yi
y
w∈Wn
a set of n-grams occurred in y
Locally
is a feature vector depending on y, and θ is the corr
normalized
of n-gram types. Each w ∈ Wn is an n-gram, which occurs cw (y) time
ence Maximum Entropy Language Model language m
dw may whole-sentence maximum entropy history) and
as a be divided into an (n − 1)-gram preﬁx h(w) (the
d
Global Log-linear Model
1
a (the rightmost or current word). For example, for
01). (whole-sentence maximum-entropy LM)follows,a trigram “on th
w)globally normalized log-linear model as (Rosenfeld et al., 2001)
is “on the” and the current word is “table”.requires computing th
ef (y)·θ
with the(y) = log-linear model
above
p (5.8), p(r(w) | h(w)) is a proper probability distribution nor
(5.2)
model of θ
Z(∗)
atively called the partition locally normalized. In contrast, ou
function), which is compu
story h(w), and thus the model is
f (y)·θ
Globally
∗
e
sum normalized the possible sequence y ∈ Σ . In (5.3)
over all ￿
our
ge model is globally normalized.
=
f (y ￿ )·θ
parameters θ in the model? ∗ One can treat sentences with any a
y ￿ ∈Σ e English p(r(w) | h(w)) directly l
he sum over all possible
t we aim to learn. A usual way is the corresponding weight vector.
to estimate such parameters is to use
35
vector depending on y, and θ

•

Normalized Language Model
ge model pθ (y) (parameterized by θ) over the examples. The model
Language Modeling
f (y)·θ
e p(y), it is instructive to
probability to any English sentence. We can obtain such a model
rastive Language Model p (y) = Now, the question is: what is
model can be used as a language model
θ
• other training modeling as follows,
hood of thelanguage examples techniques. The most commonly used
od to
Z(∗)
￿ an English sentence y
assign a probability to
• θ∗model parameterizedpas(˜ )
d n-gram = arg max
follows,
(5.1)
f (y)·θ
θ yi

•

typically use anθ n-gram model 5.1.1 e Whole-sentence
i
￿
= cw￿
(y)
f (y ￿ )·θ
p(y) = p (˜ )?
p(r(w) | h(w))
￿ ∈Σ∗ specify p as (5.7
hat is the form of θ yi
We can e
a gl
y
θ
w∈Wn
a set of n-grams occurred in y
Locally
is a feature vector depending on y, and θ is the corr
normalized
of n-gram types. Each w ∈ Wn is an n-gram, which occurs cw (y) time
ence Maximum Entropy Language Model language m
dw may whole-sentence maximum entropy history) and
as a be divided into an (n − 1)-gram preﬁx h(w) (the
d
Global Log-linear Model
1
a (the rightmost or current word). For example, for
01). (whole-sentence maximum-entropy LM)follows,a trigram “on th
w)globally normalized log-linear model as (Rosenfeld et al., 2001)
where f (y) is a feature vect
is “on the” and the current word is “table”.requires computing th
def
ef (y)·θ
with the(y) = log-linear model = ￿ ￿ ∗ ef (y￿ )·θ(5.2)
above
Z(∗)
p (5.8), p(r(w) | h(w)) is a proper probability∈Σ
y distributionis a
model of θ
nor
Z(∗)
atively called the partition locally normalized. In contrast, ou
function), which is compu
maximum entropy language
story h(w), and thus the model is
f (y)·θ
Globally
∗
e
sum normalized the possible sequence y ∈ Σ theIn (5.3)
over all ￿
our
ge model is globally normalized.
Training with . above l
=
f (y ￿ )·θ
parameters θ in the model? ∗ One can treat sentences with any a
Z(∗) (alternatively directly l
y ￿ ∈Σ e English p(r(w) | h(w)) called th
he sum over all possible
t we aim to learn. A usual way is the correspondingsum over all35th
to estimate such parameters is vector.
it requires to weight to use
vector depending on y, and θ

•

Normalized Language Model
ge model pθ (y) (parameterized by θ) over the examples. The model
Language Modeling
f (y)·θ
e p(y), it is instructive to
probability to any English sentence. We can obtain such a model
rastive Language Model p (y) = Now, the question is: what is
model can be used as a language model
θ
• other training modeling as follows,
hood of thelanguage examples techniques. The most commonly used
od to
Z(∗)
￿ an English sentence y
assign a probability to
• θ∗model parameterizedpas(˜ )
d n-gram = arg max
follows,
(5.1)
f (y)·θ
θ yi

•

typically use anθ n-gram model 5.1.1 e Whole-sentence
i
￿
= cw￿
(y)
f (y ￿ )·θ
p(y) = p (˜ )?
p(r(w) | h(w))
￿ ∈Σ∗ specify p as (5.7
hat is the form of θ yi
We can e
a gl
y
θ
w∈Wn
a set of n-grams occurred in y
Locally
is a feature vector depending on y, and θ is the corr
normalized
of n-gram types. Each w ∈ Wn is an n-gram, which occurs cw (y) time
ence Maximum Entropy Language Model language m
dw may whole-sentence maximum entropy history) and
as a be divided into an (n − 1)-gram preﬁx h(w) (the
d
Global Log-linear Model
1
a (the rightmost or current word). For example, for
01). (whole-sentence maximum-entropy LM)follows,a trigram “on th
w)globally normalized log-linear model as (Rosenfeld et al., 2001)
where f (y) is a feature vect
is “on the” and the current word is “table”.requires computing th
def
ef (y)·θ
with the(y) = log-linear model = ￿ ￿ ∗ ef (y￿ )·θ(5.2)
above
Z(∗)
p (5.8), p(r(w) | h(w)) is a proper probability∈Σ
y distributionis a
model of θ
nor
Z(∗)
atively called the partition locally normalized. In contrast, ou
function), which is compu
maximum entropy language
story h(w), and thus the model is
f (y)·θ
Globally
e All English sentences with Σ∗ .length! l
sum normalized the possible sequence y ∈ anytheIn (5.3)
over all ￿
our
ge model is globally normalized.
Training with
above
=
f (y ￿ )·θ
parameters θ in the model? ∗ One can treat sentences with any a
Z(∗) (alternatively directly l
y ￿ ∈Σ e English p(r(w) | h(w)) called th
he sum over all possible
t we aim to learn. A usual way is the correspondingsum over all35th
to estimate such parameters is vector.
it requires to weight to use
vector depending on y, and θ

•

Normalized Language Model
ge model pθ (y) (parameterized by θ) over the examples. The model
Language Modeling
f (y)·θ
e p(y), it is instructive to
probability to any English sentence. We can obtain such a model
rastive Language Model p (y) = Now, the question is: what is
model can be used as a language model
θ
• other training modeling as follows,
hood of thelanguage examples techniques. The most commonly used
od to
Z(∗)
￿ an English sentence y
assign a probability to
• θ∗model parameterizedpas(˜ )
d n-gram = arg max
follows,
(5.1)
f (y)·θ
θ yi

•

typically use anθ n-gram model 5.1.1 e Whole-sentence
i
￿
= cw￿
(y)
f (y ￿ )·θ
p(y) = p (˜ )?
p(r(w) | h(w))
￿ ∈Σ∗ specify p as (5.7
hat is the form of θ yi
We can e
a gl
y
θ
w∈Wn
a set of n-grams occurred in y
Locally
is a feature vector depending on y, and θ is the corr
normalized
of n-gram types. Each w ∈ Wn is an n-gram, which occurs cw (y) time
ence Maximum Entropy Language Model language m
dw may whole-sentence maximum entropy history) and
as a be divided into an (n − 1)-gram preﬁx h(w) (the
d
Global Log-linear Model
1
a (the rightmost or current word). For example, for
01). (whole-sentence maximum-entropy LM)follows,a trigram “on th
w)globally normalized log-linear model as (Rosenfeld et al., 2001)
where f (y) is a feature vect
is “on the” and the current word is “table”.requires computing th
def
ef (y)·θ
with the(y) = log-linear model = ￿ ￿ ∗ ef (y￿ )·θ(5.2)
above
Z(∗)
p (5.8), p(r(w) | h(w)) is a proper probability∈Σ
y distributionis a
model of θ
nor
Z(∗)
atively called the partition locally normalized. In contrast, ou
function), which is compu
maximum entropy language
story h(w), and thus the model is
f (y)·θ
Globally
e All English sentences with Σ∗ .length! l
sum normalized the possible sequence y ∈ anytheIn (5.3)
over all ￿
our
ge model is globally normalized.
Training with
above
=
f (y ￿ )·θ
parameters θ in the model? ∗ One can treat sentences with any a
Z(∗) (alternatively directly l
y ￿ ∈Σ e English p(r(w) | h(w)) called th
he sum over all possible
Sampling
t we aim to learn. A usual way is the correspondingsum over all35th
to estimate such parameters is vector.
it requires to weight to use
vector depending on y, and θ

•

Normalized Language Model
ge model pθ (y) (parameterized by θ) over the examples. The model
Language Modeling
f (y)·θ
e p(y), it is instructive to
probability to any English sentence. We can obtain such a model
rastive Language Model p (y) = Now, the question is: what is
model can be used as a language model
θ
• other training modeling as follows,
hood of thelanguage examples techniques. The most commonly used
od to
Z(∗)
￿ an English sentence y
assign a probability to
• θ∗model parameterizedpas(˜ )
d n-gram = arg max
follows,
(5.1)
f (y)·θ
θ yi

•

typically use anθ n-gram model 5.1.1 e Whole-sentence
i
￿
= cw￿
(y)
f (y ￿ )·θ
p(y) = p (˜ )?
p(r(w) | h(w))
￿ ∈Σ∗ specify p as (5.7
hat is the form of θ yi
We can e
a gl
y
θ
w∈Wn
a set of n-grams occurred in y
Locally
is a feature vector depending on y, and θ is the corr
normalized
of n-gram types. Each w ∈ Wn is an n-gram, which occurs cw (y) time
ence Maximum Entropy Language Model language m
dw may whole-sentence maximum entropy history) and
as a be divided into an (n − 1)-gram preﬁx h(w) (the
d
Global Log-linear Model
1
a (the rightmost or current word). For example, for
01). (whole-sentence maximum-entropy LM)follows,a trigram “on th
w)globally normalized log-linear model as (Rosenfeld et al., 2001)
where f (y) is a feature vect
is “on the” and the current word is “table”.requires computing th
def
ef (y)·θ
with the(y) = log-linear model = ￿ ￿ ∗ ef (y￿ )·θ(5.2)
above
Z(∗)
p (5.8), p(r(w) | h(w)) is a proper probability∈Σ
y distributionis a
model of θ
nor
Z(∗)
atively called the partition locally normalized. In contrast, ou
function), which is compu
maximum entropy language
story h(w), and thus the model is
f (y)·θ
Globally
e All English sentences with Σ∗ .length! l
sum normalized the possible sequence y ∈ anytheIn (5.3)
over all ￿
our
ge model is globally normalized.
Training with
above
=
f (y ￿ )·θ
parameters θ in the model? ∗ One canslow sentences with any a
treat (alternatively directly l
Z(∗)
y ￿ ∈Σ e English p(r(w) | h(w)) called th
he sum over all possible
Sampling
t we aim to learn. A usual way is the correspondingsum over all35th
to estimate such parameters is vector.
it requires to weight to use
vector depending on y, and θ

•

☹

Contrastive Estimation
nce Maximum Entropy Language Model

• Global Log-linear Model model as follows, et al., 2001)
(Rosenfeld
a globally normalized log-linear
(whole-sentence maximum-entropy LM)

where f (y) is a feature vecto
f (y)·θ
def ￿
e
f (y ￿ )·θ
Z(∗) =
is a
pθ (y) =
(5.2)
y ￿ ∈Σ∗ e
Z(∗)
maximum entropy language m
ef (y)·θ
Training with the above lo
￿
=
(5.3)
￿ )·θ
f (y
￿ ∈Σ∗ e
Z(∗) (alternatively called the
y
it requires to weight vector.
ector depending on y, and θ is the correspondingsum over all the
computing the sum over all
-sentence maximum entropy language model (Rosenfeld, Chen,po
To address the computati
compute Z(∗) by using a set
e log-linear model requires computing the normalization constant o
the partition function), which is computationally challenging as
∗
l the possible sequence y ∈ Σ . In our case, Contrastive Esti
this corresponds to
5.1.2
ll possible English sentences with any length!
Smith and Eisner (2005)36
utational difﬁculty issue, Rosenfeld et al. (2001) approximatelyp

utational difﬁculty issue, Rosenfeld et al. (2001) approximately
Contrastive Estimation
set of sentences thatEntropy Language Model
are sampled from Σ∗ .
nce Maximum
ntrastive Estimation

• Global Log-linear Model estimation (CE), whichetuses a small ne
(Rosenfeld
aEisner (2005) propose contrastive model as follows, al., 2001)
globally normalized log-linear

(whole-sentence maximum-entropy LM)
Estimation space Σ∗ ) to approximate the computation of normaliza
where f (y) is a feature vecto
ead of the full
f (y)·θ
def ￿
e
f (y ￿ )·θ
er CE, pθ (y) =
the contrastive estimation (CE),Z(∗) uses ￿ ∈Σ∗ neigh-(5.2)
˜
is a
5) propose likelihood of an observed sequence= is, aysmalle
which y
Z(∗)
∗
ull space Σ ) to approximatef the computation of entropy language m
maximum normalization
(˜)·θ
y
e
f (y)·θ
kelihood of an observedesequence y is, Training with the above lo(
˜
pθEstimation (CE) (Smith and Eisner, 2005)
(˜) =
y
Contrastive ￿
=
(5.3)
Z(˜)·θ
y)
￿
f (y
￿ ∈Σ∗ e
Z(∗) (alternatively called the
f (˜)·θ
y y
f (˜)·θ
y
e
e
(5.3)
y
it (y￿ )·θ
requires to weight vector. (
θ (˜) =
ectorpdepending Z(˜) = ￿ is the correspondingsum over all the
on y, and θ ￿
y
ef
y ∈N (˜) computing the sum over all po
y
-sentence maximum ef (˜)·θ language model (Rosenfeld, Chen,
entropy
y
To address of (5.9), where the
computati
not confuse this with a regular maximum entropy language model the (5.4)
= ￿
f (y ￿ )·θ
ne for each n-gram history∈N (˜)locally normalized).
y ￿ (i.e., e
y
compute Z(∗) by using a set o

•

e log-linear model requires computing the normalization constant
the partition maximum entropy language model of (5.9), where the noris with a regular function), which is computationally challenging as
67
ram history (i.e., locally normalized). ∗ . In our case, this corresponds to
l the possible sequence y ∈ Σ
5.1.2 Contrastive Esti
ll possible English sentences with any length!
67issue, Rosenfeld et al. (2001) approximatelyp
Smith and Eisner (2005)36
utational difﬁculty

utational difﬁculty issue, Rosenfeld et al. (2001) approximately
Contrastive Estimation
set of sentences thatEntropy Language Model
are sampled from Σ∗ .
nce Maximum
ntrastive Estimation

• Global Log-linear Model estimation (CE), whichetuses a small ne
(Rosenfeld
aEisner (2005) propose contrastive model as follows, al., 2001)
globally normalized log-linear

(whole-sentence maximum-entropy LM)
Estimation space Σ∗ ) to approximate the computation of normaliza
where f (y) is a feature vecto
ead of the full
f (y)·θ
def ￿
e
f (y ￿ )·θ
er CE, pθ (y) =
the contrastive estimation (CE),Z(∗) uses ￿ ∈Σ∗ neigh-(5.2)
˜
is a
5) propose likelihood of an observed sequence= is, aysmalle
which y
Z(∗)
∗
ull space Σ ) to approximatef the computation of entropy language m
maximum normalization
(˜)·θ
y
e
f (y)·θ
kelihood of an observedesequence y is, Training with the above lo(
˜
pθEstimation (CE) (Smith and Eisner, 2005)
(˜) =
y
Contrastive ￿
=
(5.3)
Z(˜)·θ
y)
￿
f (y
￿ ∈Σ∗ e
Z(∗) (alternatively called the
f (˜)·θ
y y
f (˜)·θ
y
e
e
(5.3)
y
it (y￿ )·θ
requires to weight vector. (
θ (˜) =
ectorpdepending Z(˜) = ￿ is the correspondingsum over all the
on y, and θ ￿
y
ef
y ∈N (˜) computing the sum over all po
y
-sentence maximum ef (˜)·θ language model (Rosenfeld, Chen,
entropy
y
To address of (5.9), where the
computati
not confuse this with a regular maximum entropy language model the (5.4)
= ￿
f (y ￿ )·θ
ne for each n-gram history∈N (˜)locally normalized).
y ￿ (i.e., e
y
compute Z(∗) by using a set o

•

e log-linear model requires computing the normalization constant
the partition maximum entropy language model of (5.9), where the noris with a regular function), which is computationally challenging as
67
ram history (i.e., locally normalized). ∗ . In our case, this corresponds to
l the possible sequence y ∈ Σ
5.1.2 Contrastive Esti
ll possible English sentences with any length!
67issue, Rosenfeld et al. (2001) approximatelyp
Smith and Eisner (2005)36
utational difﬁculty

utational difﬁculty issue, Rosenfeld et al. (2001) approximately
Contrastive Estimation
set of sentences thatEntropy Language Model
are sampled from Σ∗ .
nce Maximum
ntrastive Estimation

• Global Log-linear Model estimation (CE), whichetuses a small ne
(Rosenfeld
aEisner (2005) propose contrastive model as follows, al., 2001)
globally normalized log-linear

(whole-sentence maximum-entropy LM)
Estimation space Σ∗ ) to approximate the computation of normaliza
where f (y) is a feature vecto
ead of the full
f (y)·θ
def ￿
e
f (y ￿ )·θ
er CE, pθ (y) =
the contrastive estimation (CE),Z(∗) uses ￿ ∈Σ∗ neigh-(5.2)
˜
is a
5) propose likelihood of an observed sequence= is, aysmalle
which y
Z(∗)
∗
ull space Σ ) to approximatef the computation of entropy language m
maximum normalization
(˜)·θ
y
e
f (y)·θ
kelihood of an observedesequence y is, Training with the above lo(
˜
pθEstimation (CE) (Smith and Eisner, 2005)
(˜) =
y
Contrastive ￿
=
(5.3)
Z(˜)·θ
y)
￿
f (y
￿ ∈Σ∗ e
Z(∗) (alternatively called the
f (˜)·θ
y y
f (˜)·θ
y
e
e
(5.3)
y
it (y￿ )·θ
requires to weight vector. (
θ (˜) =
ectorpdepending Z(˜) = ￿ is the correspondingsum over all the
on y, and θ ￿
y
ef
y ∈N (˜) computing the sum over all po
y
-sentence maximum ef (˜)·θ language model (Rosenfeld, Chen,
entropy
y
To address of (5.9), where the
computati
not confuse this with a regular maximum entropy language model the (5.4)
= ￿
f (y ￿ )·θ
ne for each n-gram history∈N (˜)locally normalized).
y ￿ (i.e., e
y
compute Z(∗) by using a set o

•

e log-linear model requires computing the normalization constant
uencea y is, function), which is computationally challenging as
˜ maximum entropy language model of (5.9), where the northe partition
is with regular
67
ram history (i.e., locally normalized). ∗ . In our case, this corresponds to
l the possible sequence y ∈ Σ
5.1.2 Contrastive Esti
ll possible English sentences with any length!
67issue, Rosenfeld et al. (2001) approximatelyp
Smith and Eisner (2005)36
utational difﬁculty

utational difﬁculty issue, Rosenfeld et al. (2001) approximately
Contrastive Estimation
set of sentences thatEntropy Language Model
are sampled from Σ∗ .
nce Maximum
ntrastive Estimation

• Global Log-linear Model estimation (CE), whichetuses a small ne
(Rosenfeld
aEisner (2005) propose contrastive model as follows, al., 2001)
globally normalized log-linear

(whole-sentence maximum-entropy LM)
Estimation space Σ∗ ) to approximate the computation of normaliza
where f (y) is a feature vecto
ead of the full
f (y)·θ
def ￿
e
f (y ￿ )·θ
er CE, pθ (y) =
the contrastive estimation (CE),Z(∗) uses ￿ ∈Σ∗ neigh-(5.2)
˜
is a
5) propose likelihood of an observed sequence= is, aysmalle
which y
Z(∗)
∗
ull space Σ ) to approximatef the computation of entropy language m
maximum normalization
(˜)·θ
y
e
f (y)·θ
kelihood of an observedesequence y is, Training with the above lo(
˜
pθEstimation (CE) (Smith and Eisner, 2005)
(˜) =
y
Contrastive ￿
=
(5.3)
Z(˜)·θ
y)
￿
f (y
￿ ∈Σ∗ e
Z(∗) (alternatively called the
f (˜)·θ
y y
f (˜)·θ
y
e
e
(5.3)
y
it (y￿ )·θ
requires to weight vector. (
θ (˜) =
ectorpdepending Z(˜) = ￿ is the correspondingsum over all the
on y, and θ ￿
y
ef
y ∈N (˜) computing the sum over all po
y
-sentence maximum ef (˜)·θ language model (Rosenfeld, Chen,
entropy
y
To address of (5.9), where the
computati
not confuse this with a regular maximum entropy language model the (5.4)
= ￿
f (y ￿ )·θ
ne for each n-gram history∈N (˜)locally normalized).
y ￿ (i.e., e
y
compute Z(∗) by using a set
e log-linear model requires computing the normalization constant o
Neighborhood
uencea y is, function), which is computationally challenging as
˜ maximum entropy language )model of (5.9), where the norwhere N (˜
y
the partition Function
is with regular
implicit
67
ram history (i.e., locally normalized). ∗ negative

•

l the possible sequence y ∈ Σ . In our case, Contrastive Esti
this corresponds to
5.1.2
ll possible English sentences with any length!
67issue, Rosenfeld et al. (2001) approximatelyp
Smith and Eisner (2005)36
utational difﬁculty

utational difﬁculty issue, Rosenfeld et al. (2001) approximately
Contrastive Estimation
set of sentences thatEntropy Language Model
are sampled from Σ∗ .
nce Maximum
ntrastive Estimation

• Global Log-linear Model estimation (CE), whichetuses a small ne
(Rosenfeld
aEisner (2005) propose contrastive model as follows, al., 2001)
globally normalized log-linear

(whole-sentence maximum-entropy LM)
Estimation space Σ∗ ) to approximate the computation of normaliza
where f (y) is a feature vecto
ead of the full
f (y)·θ
def ￿
e
f (y ￿ )·θ
er CE, pθ (y) =
the contrastive estimation (CE),Z(∗) uses ￿ ∈Σ∗ neigh-(5.2)
˜
is a
5) propose likelihood of an observed sequence= is, aysmalle
which y
Z(∗)
∗
ull space Σ ) to approximatef the computation of entropy language m
maximum normalization
(˜)·θ
y
e
f (y)·θ
kelihood of an observedesequence y is, Training with the above lo(
˜
pθEstimation (CE) (Smith and Eisner, 2005)
(˜) =
y
Contrastive ￿
=
(5.3)
Z(˜)·θ
y)
￿
f (y
￿ ∈Σ∗ e
Z(∗) (alternatively called the
f (˜)·θ
y y
f (˜)·θ
y
e
e
(5.3)
y
it (y￿ )·θ
requires to weight vector. (
θ (˜) =
ectorpdepending Z(˜) = ￿ is the correspondingsum over all the
on y, and θ ￿
y
ef
y ∈N (˜) computing the sum over all po
y
-sentence maximum ef (˜)·θ language model (Rosenfeld, Chen,
entropy
y
To address of (5.9), where the
computati
not confuse this with a regular maximum entropy language model the (5.4)
= ￿
f (y ￿ )·θ
ne for each n-gram history∈N (˜)locally normalized).
y ￿ (i.e., e
y
compute Z(∗) by using a set
e log-linear model requires computing the normalization constant o
Neighborhood
uencea y is, function), which is computationally challenging as
˜ maximum entropy language )model of (5.9), where the norwhere N (˜
y
the partition Function
is with regular
implicit
67
ram history (i.e., locally normalized). ∗ negative

•

l the possible sequence y ∈ Σ . In our case, Contrastive Esti
this corresponds to
5.1.2
neighborhood or
ll possible English sentences with any length!
67issue,contrastive set al. (2001) approximatelyp
utational difﬁculty
Rosenfeld et Smith and Eisner (2005)36

utational difﬁculty issue, Rosenfeld et al. (2001) approximately
Contrastive Estimation
set of sentences thatEntropy Language Model
are sampled from Σ∗ .
nce Maximum
ntrastive Estimation

• Global Log-linear Model estimation (CE), whichetuses a small ne
(Rosenfeld
aEisner (2005) propose contrastive model as follows, al., 2001)
globally normalized log-linear

(whole-sentence maximum-entropy LM)
Estimation space Σ∗ ) to approximate the computation of normaliza
where f (y) is a feature vecto
ead of the full
f (y)·θ
def ￿
e
f (y ￿ )·θ
er CE, pθ (y) =
the contrastive estimation (CE),Z(∗) uses ￿ ∈Σ∗ neigh-(5.2)
˜
is a
5) propose likelihood of an observed sequence= is, aysmalle
which y
Z(∗)
∗
ull space Σ ) to approximatef the computation of entropy language m
maximum normalization
(˜)·θ
y
e
f (y)·θ
kelihood of an observedesequence y is, Training with the above lo(
˜
pθEstimation (CE) (Smith and Eisner, 2005)
(˜) =
y
Contrastive ￿
=
(5.3)
Z(˜)·θ
y)
￿
f (y
￿ ∈Σ∗ e
Z(∗) (alternatively called the
f (˜)·θ
y y
f (˜)·θ
y
e
e
(5.3)
y
it (y￿ )·θ
requires to weight vector. (
θ (˜) =
ectorpdepending Z(˜) = ￿ is the correspondingsum over all the
on y, and θ ￿
y
ef
y ∈N (˜) computing the sum over all po
y
-sentence maximum ef (˜)·θ language model (Rosenfeld, Chen,
entropy
y
To address of (5.9), where the
computati
not confuse this with a regular maximum entropy language model the (5.4)
= ￿
f (y ￿ )·θ
ne for each n-gram history∈N (˜)locally normalized).
y ￿ (i.e., e
y
compute Z(∗) by using a set
e log-linear model requires computing the normalization constant o
Neighborhood
uencea y is, function), which is computationally challenging as
˜ maximum entropy language )model of (5.9), where the norwhere N (˜
y
the partition Function
is with regular
implicit
67
ram history (i.e., locally normalized). ∗ negative

•

l the possible sequence y ∈ Σ . In our case, Contrastive Esti
this corresponds to
5.1.2
neighborhood or
a set of alternate
ll possible English sentences with any length!
Eng. sentences 67 y is, Rosenfeld et Smith and Eisner (2005)
of
bserved sequence issue,contrastive set al. (2001) approximatelyp
36
utational difﬁculty ˜

utational difﬁculty issue, Rosenfeld et al. (2001) approximately
Contrastive Estimation
set of sentences thatEntropy Language Model
are sampled from Σ∗ .
nce Maximum
ntrastive Estimation

• Global Log-linear Model estimation (CE), whichetuses a small ne
(Rosenfeld
aEisner (2005) propose contrastive model as follows, al., 2001)
globally normalized log-linear

(whole-sentence maximum-entropy LM)
Estimation space Σ∗ ) to approximate the computation of normaliza
where f (y) is a feature vecto
ead of the full
f (y)·θ
def ￿
e
f (y ￿ )·θ
er CE, pθ (y) =
the contrastive estimation (CE),Z(∗) uses ￿ ∈Σ∗ neigh-(5.2)
˜
is a
5) propose likelihood of an observed sequence= is, aysmalle
which y
Z(∗)
∗
ull space Σ ) to approximatef the computation of entropy language m
maximum normalization
(˜)·θ
y
e
f (y)·θ
kelihood of an observedesequence y is, Training with the above lo(
˜
pθEstimation (CE) (Smith and Eisner, 2005)
(˜) =
y
Contrastive ￿
=
(5.3)
Z(˜)·θ
y)
￿
f (y
￿ ∈Σ∗ e
Z(∗) (alternatively called the
f (˜)·θ
y y
f (˜)·θ
y
e
e
(5.3)
y
it (y￿ )·θ
requires to weight vector. (
θ (˜) =
ectorpdepending Z(˜) = ￿ is the correspondingsum over all the
on y, and θ ￿
y
ef
y ∈N (˜) computing the sum over all po
y
-sentence maximum ef (˜)·θ language model (Rosenfeld, Chen,
entropy
y
To address of (5.9), where the
computati
not confuse this with aloss maximum entropy language model the (5.4)
= ￿ regular f (y￿ )·θ
ne for each n-gram history∈N (˜)locally normalized).
y ￿ (i.e., e
y
compute Z(∗) by using a set
e log-linear model requires computing the normalization constant o
Neighborhood
uencea y is, function), which is computationally challenging as
˜ maximum entropy language )model of (5.9), where the norwhere N (˜
y
the partition Function
is with regular
implicit
67
ram history (i.e., locally normalized). ∗ negative

•

l the possible sequence y ∈ Σ . In our case, Contrastive Esti
this corresponds to
5.1.2
neighborhood or
a set of alternate
ll possible English sentences with any length!
Eng. sentences 67 y is, Rosenfeld et Smith and Eisner (2005)
of
bserved sequence issue,contrastive set al. (2001) approximatelyp
36
utational difﬁculty ˜

utational difﬁculty issue, Rosenfeld et al. (2001) approximately
Contrastive Estimation
set of sentences thatEntropy Language Model
are sampled from Σ∗ .
nce Maximum
ntrastive Estimation

• Global Log-linear Model estimation (CE), whichetuses a small ne
(Rosenfeld
aEisner (2005) propose contrastive model as follows, al., 2001)
globally normalized log-linear

(whole-sentence maximum-entropy LM)
Estimation space Σ∗ ) to approximate the computation of normaliza
where f (y) is a feature vecto
ead of the full
f (y)·θ
def ￿
e
f (y ￿ )·θ
er CE, pθ (y) =
the contrastive estimation (CE),Z(∗) uses ￿ ∈Σ∗ neigh-(5.2)
˜
is a
5) propose likelihood of an observed sequence= is, aysmalle
which y
Z(∗)
∗
ull space Σ ) to approximatef the computation of entropy language m
maximum normalization
(˜)·θ
y
e
f (y)·θ
kelihood of an observedesequence y is, Training with the above lo(
˜
pθEstimation (CE) (Smith and Eisner, 2005)
(˜) =
y
Contrastive ￿
=
(5.3)
Z(˜)·θ
y)
￿
f (y
￿ ∈Σ∗ e
Z(∗) (alternatively called the
f (˜)·θ
y y
f (˜)·θ
y
e
e
(5.3)
y
it (y￿ )·θ
requires to weight vector. (
θ (˜) =
ectorpdepending Z(˜) = ￿ is the correspondingsum over all the
on y, and θ ￿
y
ef
y ∈N (˜) computing the sum over all po
y
-sentence maximum ef (˜)·θ language model (Rosenfeld, Chen,
entropy
y
To address of (5.9), where the
computati
not confuse this with aloss maximum entropy language model the (5.4)
= ￿ regular f (y￿ )·θ
ne for each n-gram history∈N (˜)locally normalized).
y ￿ (i.e., e
y
compute Z(∗) by using a set
e log-linear model requires computing the normalization constant o
Neighborhood
uencea y is, function), which is computationally challenging as
˜ maximum entropy language )model of (5.9), where the norwhere N (˜
y
the partition Function
is with regular
implicit
67
ram history (i.e., locally normalized). ∗ negative

•

l the possible sequence y ∈ Σ . In our case, Contrastive Esti
this corresponds to
5.1.2
neighborhood or
a set of alternate
ll possible English sentences with any length!
Eng. sentences 67 y is, Rosenfeld et Smith and Eisner (2005)
of
bserved sequence issue,contrastive set al. (2001) approximatelyp
36
utational difﬁculty ˜

utational difﬁculty issue, Rosenfeld et al. (2001) approximately
Contrastive Estimation
set of sentences thatEntropy Language Model
are sampled from Σ∗ .
nce Maximum
ntrastive Estimation

• Global Log-linear Model estimation (CE), whichetuses a small ne
(Rosenfeld
aEisner (2005) propose contrastive model as follows, al., 2001)
globally normalized log-linear

(whole-sentence maximum-entropy LM)
Estimation space Σ∗ ) to approximate the computation of normaliza
where f (y) is a feature vecto
ead of the full
f (y)·θ
def ￿
e
f (y ￿ )·θ
er CE, pθ (y) =
the contrastive estimation (CE),Z(∗) uses ￿ ∈Σ∗ neigh-(5.2)
˜
is a
5) propose likelihood of an observed sequence= is, aysmalle
which y
Z(∗)
∗
ull space Σ ) to approximatef the computation of entropy language m
maximum normalization
(˜)·θ
y
e
f (y)·θ
kelihood of an observedesequence y is, Training with the above lo(
˜
pθEstimation (CE) (Smith and Eisner, 2005)
(˜) =
y
Contrastive ￿
=
(5.3)
Z(˜)·θ
y)
￿
f (y
￿ ∈Σ∗ e
Z(∗) (alternatively called the
f (˜)·θ
y y
f (˜)·θ
y
e
e
(5.3)
pdepending on y, = ￿ is the correspondingsum over all the
y
it (y￿ )·θ improve both speed (
requires to weight vector.
θ (˜) =
ector
and θ ￿
Z(˜)
y
ef
and accuracy
y ∈N (˜) computing the sum over all po
y
-sentence maximum ef (˜)·θ language model (Rosenfeld, Chen,
entropy
y
To address of (5.9), where the
computati
not confuse this with aloss maximum entropy language model the (5.4)
= ￿ regular f (y￿ )·θ
ne for each n-gram history∈N (˜)locally normalized).
y ￿ (i.e., e
y
compute Z(∗) by using a set
e log-linear model requires computing the normalization constant o
Neighborhood
uencea y is, function), which is computationally challenging as
˜ maximum entropy language )model of (5.9), where the norwhere N (˜
y
the partition Function
is with regular
implicit
67
ram history (i.e., locally normalized). ∗ negative

•

l the possible sequence y ∈ Σ . In our case, Contrastive Esti
this corresponds to
5.1.2
neighborhood or
a set of alternate
ll possible English sentences with any length!
Eng. sentences 67 y is, Rosenfeld et Smith and Eisner (2005)
of
bserved sequence issue,contrastive set al. (2001) approximatelyp
36
utational difﬁculty ˜

utational difﬁculty issue, Rosenfeld et al. (2001) approximately
Contrastive Estimation
set of sentences thatEntropy Language Model
are sampled from Σ∗ .
nce Maximum
ntrastive Estimation

• Global Log-linear Model estimation (CE), whichetuses a small ne
(Rosenfeld
aEisner (2005) propose contrastive model as follows, al., 2001)
globally normalized log-linear

(whole-sentence maximum-entropy LM)
Estimation space Σ∗ ) to approximate the computation of normaliza
where f (y) is a feature vecto
ead of the full
f (y)·θ
def ￿
e
f (y ￿ )·θ
er CE, pθ (y) =
the contrastive estimation (CE),Z(∗) uses ￿ ∈Σ∗ neigh-(5.2)
˜
is a
5) propose likelihood of an observed sequence= is, aysmalle
which y
Z(∗)
∗
ull space Σ ) to approximatef the computation of entropy language m
maximum normalization
(˜)·θ
y
e
f (y)·θ
kelihood of an observedesequence y is, Training with the above lo(
˜
pθEstimation (CE) (Smith and Eisner, 2005)
(˜) =
y
Contrastive ￿
=
(5.3)
Z(˜)·θ
y)
￿
f (y
￿ ∈Σ∗ e
Z(∗) (alternatively called the
f (˜)·θ
y y
f (˜)·θ
y
e
e
(5.3)
pdepending on y, = ￿ is the correspondingsum over all the
y
it (y￿ )·θ improve both speed (
requires to weight vector.
θ (˜) =
ector
and θ ￿
Z(˜)
y
ef
and accuracy
y ∈N (˜) computing the sum over all po
y
-sentence maximum ef (˜)·θ language model (Rosenfeld, Chen,
entropy
y
To address the (5.4)
computati
not confuse this with aloss maximum entropy languagenot proposed for
model of (5.9), where the
= ￿ regular f (y￿ )·θ
ne for each n-gram history∈N (˜)locally normalized).
y ￿ (i.e., e
y
compute language modelingset
Z(∗) by using a
e log-linear model requires computing the normalization constant o
Neighborhood
uencea y is, function), which is computationally challenging as
˜ maximum entropy language )model of (5.9), where the norwhere N (˜
y
the partition Function
is with regular
implicit
67
ram history (i.e., locally normalized). ∗ negative

•

l the possible sequence y ∈ Σ . In our case, Contrastive Esti
this corresponds to
5.1.2
neighborhood or
a set of alternate
ll possible English sentences with any length!
Eng. sentences 67 y is, Rosenfeld et Smith and Eisner (2005)
of
bserved sequence issue,contrastive set al. (2001) approximatelyp
36
utational difﬁculty ˜

utational difﬁculty issue, Rosenfeld et al. (2001) approximately
Contrastive Estimation
set of sentences thatEntropy Language Model
are sampled from Σ∗ .
nce Maximum
ntrastive Estimation

• Global Log-linear Model estimation (CE), whichetuses a small ne
(Rosenfeld
aEisner (2005) propose contrastive model as follows, al., 2001)
globally normalized log-linear

(whole-sentence maximum-entropy LM)
Estimation space Σ∗ ) to approximate the computation of normaliza
where f (y) is a feature vecto
ead of the full
f (y)·θ
def ￿
e
f (y ￿ )·θ
er CE, pθ (y) =
the contrastive estimation (CE),Z(∗) uses ￿ ∈Σ∗ neigh-(5.2)
˜
is a
5) propose likelihood of an observed sequence= is, aysmalle
which y
Z(∗)
∗
ull space Σ ) to approximatef the computation of entropy language m
maximum normalization
(˜)·θ
y
e
f (y)·θ
kelihood of an observedesequence y is, Training with the above lo(
˜
pθEstimation (CE) (Smith and Eisner, 2005)
(˜) =
y
Contrastive ￿
=
(5.3)
Z(˜)·θ
y)
￿
f (y
￿ ∈Σ∗ e
Z(∗) (alternatively called the
f (˜)·θ
y y
f (˜)·θ
y
e
e
(5.3)
pdepending on y, = ￿ is the correspondingsum over all the
y
it (y￿ )·θ improve both speed (
requires to weight vector.
θ (˜) =
ector
and θ ￿
Z(˜)
y
ef
and accuracy
y ∈N (˜) computing the sum over all po
y
-sentence maximum ef (˜)·θ language model (Rosenfeld, Chen,
entropy
y
To address the (5.4)
computati
not confuse this with aloss maximum entropy languagenot proposed for
model of (5.9), where the
= ￿ regular f (y￿ )·θ
ne for each n-gram history∈N (˜)locally normalized).
y ￿ (i.e., e
y
compute language modelingset
Z(∗) by using a
e log-linear model requires computing the normalization constant o
Neighborhood
uencea y is, function), which is computationally challenging as
˜ maximum entropy language )model of (5.9), where the norwhere N (˜
y
the partition Function
is with regular
train to recover the
implicit negative
67 ∗
ram history (i.e., locally normalized).

•

l the possible sequence y ∈ Σ . In our case, Contrastive Esti
this corresponds to
original English as much
5.1.2
neighborhood or
a set of alternate
ll possible English sentences with any length! as possible
Eng. sentences 67 y is, Rosenfeld et Smith and Eisner (2005)
of
bserved sequence issue,contrastive set al. (2001) approximatelyp
36
utational difﬁculty ˜

Contrastive Language Model Estimation

37

Contrastive Language Model Estimation

•

•

Step-1: extract a confusion grammar (CG)
an English-to-English SCFG

37

Contrastive Language Model Estimation

•

•

Step-1: extract a confusion grammar (CG)
an English-to-English SCFG

neighborhood function

37

Contrastive Language Model Estimation

•

•

•

Step-1: extract a confusion grammar (CG)
an English-to-English SCFG

neighborhood function

Step-2: for each English sentence, generate a
contrastive set (or neighborhood) using the CG

37

Contrastive Language Model Estimation

•

•

Step-1: extract a confusion grammar (CG)
an English-to-English SCFG

neighborhood function

•

Step-2: for each English sentence, generate a
contrastive set (or neighborhood) using the CG

•

Step-3: discriminative training
37

Contrastive Language Model Estimation

•

•

Step-1: extract a confusion grammar (CG)
an English-to-English SCFG

neighborhood function

•

Step-2: for each English sentence, generate a
contrastive set (or neighborhood) using the CG

•

Step-3: discriminative training
37

Contrastive Language Model Estimation

•

•

Step-1: extract a confusion grammar (CG)

sides are English. Several example rules in the CG are as following,
an English-to-English SCFG
neighborhood function
X → ￿ lead to , result in ￿ ,
X → ￿ X0 at beijing , beijing ’s X0 ￿ ,
X → ( X0 of X1 , X0 of the X1 ) ,
X → ( X0 ’s X1 , X1 of X0 ) .

CFG, a CG contains rules with different arities. Also, there might be
rule as shown in the last example. These rules captures the confusion that
Step-2: for each English sentence, generate a
may have contrastive set (or neighborhood) using the CG
in choosing different senses or reordering patterns for a given
uestion is how we get such a grammar. Below we present two ways.

•
•

Step-3: from Bilingual Grammar
usion Grammar discriminative training

e a confusion grammar from an existing bilingual grammar. For a particu-

37

Contrastive Language Model Estimation

•

•

Step-1: extract a confusion grammar (CG)

sides are English. Several example rules in the CG are as following,
an English-to-English SCFG
neighborhood function
X → ￿ lead to , result in ￿ ,

paraphrase

X → ￿ X0 at beijing , beijing ’s X0 ￿ ,
X → ( X0 of X1 , X0 of the X1 ) ,
X → ( X0 ’s X1 , X1 of X0 ) .

CFG, a CG contains rules with different arities. Also, there might be
rule as shown in the last example. These rules captures the confusion that
Step-2: for each English sentence, generate a
may have contrastive set (or neighborhood) using the CG
in choosing different senses or reordering patterns for a given
uestion is how we get such a grammar. Below we present two ways.

•
•

Step-3: from Bilingual Grammar
usion Grammar discriminative training

e a confusion grammar from an existing bilingual grammar. For a particu-

37

Contrastive Language Model Estimation

grammar (CG)
• Step-1: extract a confusion rules in the CG are as following,
rget sides are English. Several example

•

sides are English. Several example rules in the CG are as following,
an English-to-English SCFG
neighborhood function

X ￿→ ￿ lead to , in ￿ , in ￿ ,
result
X → lead to , result

paraphrase

X X ￿ beijing , beijing ’s X0 ,
X → ￿→0 atX0 at beijing , beijing￿’s X0 ￿ ,
X X ( X , X , the of ) ,
X → (→0 ofX01 ofX0 1of X0 X1the X1 ) ,
X X ( 1 ’s X , 1 of
X → (→0 ’s X0 , X1 1of X0 ) . X0 ) .

ar SCFG, a contains rules rules different arities. arities. there might be
CFG, a CG CG contains with with different Also, Also, there might b
rule ruleStep-2:thein the last example. These rules captures a confusion tha
the as shown in for each English sentence, generate the that
as shown last example. These rules captures the confusion
may may have in choosing different senses or reorderingthe CG for a give
in choosing set (or neighborhood) patterns for a given
m have contrastivedifferent senses or reordering using patterns
uestion is how how we get such a grammar. we present two ways.
he question is we get such a grammar. Below Below we present two ways.

•
•

Step-3: from Bilingual Grammar
usion Grammar discriminative training
Confusion Grammar from Bilingual Grammar

e a confusion grammar from an existing bilingual grammar. For a particu-

37

erive a confusion grammar from an existing bilingual grammar. For a particu

Contrastive Language Model Estimation

grammar (CG)
• Step-1: extract a confusion rules in the CG are as following,
rget sides are English. Several example

•

sides are English. Several example rules in the CG are as following,
rget sides are English. Several example rules in the CG are as following,
an English-to-English SCFG
neighborhood function

X ￿→ ￿ lead to , in ￿ , in ￿ ,
result
X X → lead to ,to , result in ￿ ,
→
￿ lead result

paraphrase

￿ at at beijing , beijing
X XX → X￿ X0X0 beijing , beijing X0X’s￿ X0 ￿ ,
→ ￿→0 at beijing , beijing ’s ’s ￿0, ,
( of , , 0 the of ) ,
X XX →→￿ X0X01X1 XX, X0the 1X1 ￿X1 ) ,
→ ( X0 of X ofX0 1of of X the ,
(
’s , 1
of
X XX→→0 X0X0X1 XX, of 0X0 ￿X0 ) .
→ ( X￿ ’s ’s1 , X1 1of X1 ) . .

r SCFG, CG contains with with different
Also, there might be
ar SCFG,CGCG contains rules with different arities.Also, there might be
CFG, a a a contains rules rules different arities. arities. Also, there might b
the ruleshown in thein the last example. These rules captures a confusion tha
rule ruleStep-2: in the last example. These rules captures the confusion that
the as as shown for each English sentence, generate the that
as shown last example. These rules captures the confusion
m may have in choosing different or or or reorderingthe CG for a
may mayhave choosing different sensessensesreordering patterns for a given give
in in choosing different senses reordering patterns for a given
m have contrastive set (or neighborhood) using patterns
e question how get such such a grammar. we present two ways.
uestion is isis how we get a a grammar. Below we present two ways.
hequestionhow we we get suchgrammar. Below Below we present two ways.

•
•

Step-3: from Bilingual Grammar
onfusion Grammar from Bilingual Grammar
usion Grammar discriminative training
Confusion Grammar from Bilingual Grammar

erive a confusion grammar from an existing bilingual grammar. For particue a confusion grammar from an existing bilingual grammar. For a a particu-

37

erive a confusion grammar from an existing bilingual grammar. For a particu

Contrastive Language Model Estimation

grammar (CG)
• Step-1: extract a confusion rules in the CG are as following,
rget sides are English. Several example

•

sides are English. Several example rules in the CG are as following,
rget sides are English. Several example rules in the CG are as following,
an English-to-English SCFG
neighborhood function

X ￿→ ￿ lead to , in ￿ , in ￿ ,
result
X X → lead to ,to , result in ￿ ,
→
￿ lead result

￿ at at beijing , beijing
X XX → X￿ X0X0 beijing , beijing X0X’s￿ X0 ￿ ,
→ ￿→0 at beijing , beijing ’s ’s ￿0, ,
( of , , 0 the of ) ,
X XX →→￿ X0X01X1 XX, X0the 1X1 ￿X1 ) ,
→ ( X0 of X ofX0 1of of X the ,

paraphrase
insertion

(
’s , 1
of
X XX→→0 X0X0X1 XX, of 0X0 ￿X0 ) .
→ ( X￿ ’s ’s1 , X1 1of X1 ) . .

r SCFG, CG contains with with different
Also, there might be
ar SCFG,CGCG contains rules with different arities.Also, there might be
CFG, a a a contains rules rules different arities. arities. Also, there might b
the ruleshown in thein the last example. These rules captures a confusion tha
rule ruleStep-2: in the last example. These rules captures the confusion that
the as as shown for each English sentence, generate the that
as shown last example. These rules captures the confusion
m may have in choosing different or or or reorderingthe CG for a
may mayhave choosing different sensessensesreordering patterns for a given give
in in choosing different senses reordering patterns for a given
m have contrastive set (or neighborhood) using patterns
e question how get such such a grammar. we present two ways.
uestion is isis how we get a a grammar. Below we present two ways.
hequestionhow we we get suchgrammar. Below Below we present two ways.

•
•

Step-3: from Bilingual Grammar
onfusion Grammar from Bilingual Grammar
usion Grammar discriminative training
Confusion Grammar from Bilingual Grammar

erive a confusion grammar from an existing bilingual grammar. For particue a confusion grammar from an existing bilingual grammar. For a a particu-

37

erive a confusion grammar from an existing bilingual grammar. For a particu

Contrastive Language Model Estimation

grammar (CG)
• Step-1: extract a confusion rules in the CG are as following,
rget sides are English. Several example

•

sides are English. Several example rules in thethe CG arefollowing,
sides are an English-to-English SCFG in CG are as as following,
English. Several example rules
rget sides are English. Severalexample rules in the CG are as following,
neighborhood function

X ￿→ ￿ lead to , in ￿ , in ￿ ,
result
X X → lead to toresult in ￿ ,,
result
X → → ￿ ￿ lead ,to,, result ￿
lead

￿ beijing , beijing ’s
at beijing , beijing￿
X XX → X￿ X0X0 beijing,,beijing X0X’s￿￿, , 0 ￿ ,
→ ￿→Xatat beijing beijing ’s X,0 X
X → ￿ 0 0 at
’s 0
( X of , 0 of X the
X XX →→Xofof0X 1,XX, of0the1X1 ￿￿,, 1 ) ,
→ ( X￿ X0X 1X X0 1of X the X, X
of , X the of )
0
X → ￿
0

1

0

1

paraphrase
insertion

(
’s , 1
of
X XX→→0 X0X0X1 XX, of 0X0 ￿X0 ) .
→ ( X￿ ’s ’s1 , X1 1of X1 ) . .

X → ￿ X0 ’s X1 , X1 of X0 ￿ .
r SCFG, CG contains with with different
Also, there might be
ar SCFG,CGCG contains rules with different arities.Also, there might be
CFG, a a a contains rules rules different arities. arities. Also, there might b
SCFG, aas shown in the last example.different arities. Also, confusion that be
CG contains rules with These rules captures the there might
the ruleshown in thein the last example. These rules captures a confusion tha
rule ruleStep-2: for each English sentence, generate the that
the as as shown last example. These rules captures the confusion
rule as in in in the last example. These
m may shown choosing different senses or reordering patternsconfusion that
may mayhave choosing different sensessensesrules captures thefor a a given give
patterns CG
m havehave in choosing different or reordering using the for given a
or reordering patterns for
contrastive set (or neighborhood)
may have how we we get suchgrammar. Below we present two ways. a given
e question is choosing get a a grammar. or reordering present for
how get such such senses Below we present two ways.
uestion is in is how we different a grammar. Below wepatterns two ways.
he question
uestion is how we get such a grammar. Below we present two ways.

•
•

Step-3: from Bilingual Grammar
onfusion Grammar from Bilingual Grammar
usion Grammar discriminative training
Confusion Grammar from Bilingual Grammar

fusion Grammar from Bilingual Grammar
erive a confusion grammar from an existing bilingual grammar. For particue a confusion grammar from an existing bilingual grammar. For a a particu- 37
erive a confusion grammar from an existing bilingual grammar. For a particu

Contrastive Language Model Estimation

grammar (CG)
• Step-1: extract a confusion rules in the CG are as following,
rget sides are English. Several example

•

sides are English. Several example rules in thethe CG arefollowing,
sides are an English-to-English SCFG in CG are as as following,
English. Several example rules
rget sides are English. Severalexample rules in the CG are as following,
neighborhood function

X ￿→ ￿ lead to , in ￿ , in ￿ ,
result
X X → lead to toresult in ￿ ,,
result
X → → ￿ ￿ lead ,to,, result ￿
lead

￿ beijing , beijing ’s
at beijing , beijing￿
X XX → X￿ X0X0 beijing,,beijing X0X’s￿￿, , 0 ￿ ,
→ ￿→Xatat beijing beijing ’s X,0 X
X → ￿ 0 0 at
’s 0
( X of , 0 of X the
X XX →→Xofof0X 1,XX, of0the1X1 ￿￿,, 1 ) ,
→ ( X￿ X0X 1X X0 1of X the X, X
of , X the of )
0
X → ￿
0

1

0

1

paraphrase
insertion

(
’s , 1
of
X XX→→0 X0X0X1 XX, of 0X0 ￿X0 ) .
→ ( X￿ ’s ’s1 , X1 1of X1 ) . .

X → ￿ X0 ’s X1 , X1 of X0 ￿ .
re-ordering
r SCFG, CG contains with with different
Also, there might be
ar SCFG,CGCG contains rules with different arities.Also, there might be
CFG, a a a contains rules rules different arities. arities. Also, there might b
SCFG, aas shown in the last example.different arities. Also, confusion that be
CG contains rules with These rules captures the there might
the ruleshown in thein the last example. These rules captures a confusion tha
rule ruleStep-2: for each English sentence, generate the that
the as as shown last example. These rules captures the confusion
rule as in in in the last example. These
m may shown choosing different senses or reordering patternsconfusion that
may mayhave choosing different sensessensesrules captures thefor a a given give
patterns CG
m havehave in choosing different or reordering using the for given a
or reordering patterns for
contrastive set (or neighborhood)
may have how we we get suchgrammar. Below we present two ways. a given
e question is choosing get a a grammar. or reordering present for
how get such such senses Below we present two ways.
uestion is in is how we different a grammar. Below wepatterns two ways.
he question
uestion is how we get such a grammar. Below we present two ways.

•
•

Step-3: from Bilingual Grammar
onfusion Grammar from Bilingual Grammar
usion Grammar discriminative training
Confusion Grammar from Bilingual Grammar

fusion Grammar from Bilingual Grammar
erive a confusion grammar from an existing bilingual grammar. For particue a confusion grammar from an existing bilingual grammar. For a a particu- 37
erive a confusion grammar from an existing bilingual grammar. For a particu

Step-1: Extracting a Confusion Grammar (CG)

38

•

Step-1: Extracting a Confusion Grammar (CG)

•

Deriving a CG from a bilingual grammar
use Chinese side as pivots

38

•

Step-1: Extracting a Confusion Grammar (CG)

•

Deriving a CG from a bilingual grammar
use Chinese side as pivots

Bilingual Rule

Confusion Rule

38

→ ￿ X0 of X1 , X0 of the X1 ￿ ,
Step-1: Extracting a Confusion Grammar (CG)
→ ￿ X0 at beijing , beijing ’s X0 ￿ ,
Deriving a X1 X1 a X0 ￿ .
X • → ￿ X0 ’s CG,fromof bilingual grammar
as pivots
• use Chinese1side 0 of the X1 ￿ ,
→ ￿ X0 of X , X
X → ￿ mao, the cat￿.
Bilingual Rule
Confusion Rule
X → ￿ X0 ’s X1 , X1 of X0 ￿ .
X → ￿ mao, a cat￿.
X → ￿ mao, the cat￿.
contains rules with different arities. Also, there might be
n in the last example. cat￿. rules captures the confusion that
X → ￿ mao, a These
choosing different senses or reordering patterns for a given
G containssuch a with different arities.present two ways. be
w we get rules grammar. Below we Also, there might
wn in the last example. These rules captures the confusion that
choosing different senses or reordering patterns for a given
mar from Bilingual Grammar
ow we get such a grammar. Below we present two ways.
grammar from an existing bilingual grammar. For a particu-38

XX → ￿ XXof’s X1 ,X0 1 of X0X1 ￿
→ ￿ 0 0 X1 , X of the ￿ .
→ ￿ X0 of X1 , X0 of the X1 ￿ ,
Step-1: Extracting a Confusion Grammar (CG)
X ,X ￿ ￿ ’s X , X1 of
→ ￿ X0 at beijing , beijing ’s X0 ￿ → →X0mao, 1the cat￿. X0 ￿ .
Deriving a X1 X1 a X0 ￿ .
X • → ￿ X0 ’s CG,fromof bilingual grammar
as pivots
X → mao, a cat￿.
• use Chinese1side 0 of the X1 ￿ , X → ￿￿mao, the cat￿.
→ ￿ X0 of X , X
X → ￿ mao, the cat￿.
Bilingual Rule
Confusion Rulecat￿.
XX → ￿ mao,thecat￿.
→ ￿ a cat, a
X → ￿ X0 ’s X1 , X1 of X0 ￿ .
X → ￿ a cat, the cat￿.
X → ￿ mao, a cat￿.

X → ￿ the cat, a cat￿.
X → ￿ mao, the cat￿.
contains rules with different arities. Also, there might be
X → ￿ the cat, a cat￿.
n in the last example. cat￿. rules captures the confusion that
X → ￿ mao, a These
X → ￿ patterns X1 a X0 ￿,
choosing different senses or reorderingX0 de X1 , for ofgiven
G containssuch a with different arities.present two ,ways. X0 ￿,
w we get rules grammar. BelowX → ￿Also, thereX1 of be
we X0 de X1 might
wn in the last example. These rules captures theX , X ’s X ￿,
confusion that
X → ￿ X0 de 1 0
1
choosing different senses or reordering X de X ,for a given
patterns X ’s X ￿,
X → ￿ rules with different ari
1
0
Like a regular SCFG, a CG contains 0
mar fromsuch a grammar. Below we present two ways. 1
Bilingual Grammar
ow we get
reordering in theSCFG, a CG contains rules with different ar
Like a regular rule as shown in the last example. These rules
grammar from anmay havebilingualthe last example. These rule
an MT system existingshown in grammar. For a particu-38
reordering in the rule as in choosing different senses or reor

XX → ￿ XXof’s X1 ,X0 1 of X0X1 ￿
→ ￿ 0 0 X1 , X of the ￿ .
→ ￿ X0 of X1 , X0 of the X1 ￿ ,
Step-1: Extracting a Confusion as following,
English. Several example rules in the CG areGrammar (CG)
X → → ￿ ’s X , X1 of
→ ￿ X0 at beijing , beijing ’s X0 ￿CG areX0mao, 1the cat￿. X0 ￿ .
,X ￿ as following,
Deriving ’s Xto X1 a bilingual
XEnglish.￿SeveralCG,,fromof in ￿in . grammar
• → →X0 leadexample rules0 ,￿ the
X
￿ a 1 result X
side as pivots ,
X → mao, a cat￿.
•X use Chinese1to Xresultthe￿X1 ￿ , X → ￿￿mao, the cat￿.
→ ￿ → 0 atlead , , , beijing ’s X ￿ ,
￿ X ￿ beijing 0 of in
X of X
X →X →0 ￿ mao, the cat￿.
0
Bilingual Rule
Confusion Rulecat￿.
XX → ￿ mao,thecat￿.
→ ￿ a cat, a
X → ￿ ￿￿X0 of X1 , , ,Xofof X X￿ ,￿ ,
X → X0 at beijingXbeijing ’s 1 .
X → X0 ’s X1 0 1 the X￿ 0
0
X → ￿ a cat, the cat￿.
X → ￿ mao, a cat￿.

XX → ￿ XXof’s X1 , X1 of X0 ￿1. ￿ ,
→ ￿ 0 0 X1 , X0 of the X
X → ￿ the cat, a cat￿.
X → Xmao, the cat￿.X ￿ .
￿ ’s X , X of
X → rules with different
X ￿
1
contains → ￿0mao, 1the cat￿. 0 arities. Also, there might be
X → ￿ the cat, a cat￿.
n inX → ￿ →0 ￿de X1 , a cat￿. X1rules captures the confusion that
the X→ example. cat￿. ￿,
X mao, a 0 on
Xlast ￿ mao, XThese

choosing ￿ X de X senses X ￿, X → ￿ X0 de X1 , for ofgiven
different , X of or reordering patterns X1 a X0 ￿,
X→
0
1
1
0
G contains￿ X on withX of X ￿, arities.present two ,ways. X0 ￿,
rules X , different
w we get such a grammar. BelowX → ￿Also, thereX1 of be
we X0 de X1 might
X→
0
1
1
0
wn in the last example. These rules captures theX , X ’s X ￿,
confusion that
X → ￿ X0 de 1 0
1
X → X0 de X X0 ’s or
choosing￿different1 ,sensesX1 ￿,reordering X de X ,for a given
patterns X ’s X ￿,
￿ 0
0
Xfromregularwith Grammar X →Also, there1 might be 1 ari
→a￿ X0 of XSCFG, a CG contains rules with different
1 , X1 on X0 ￿,
Like
mar get such a grammar. Below we present two ways.
Bilingual different arities.
CG contains rules
ow we
Like alast example. different arities. last there mightthat
CG reordering in theSCFG, a CG contains rules with different ar
owncontainsregular rule as shown in theAlso,example. These rules
in the rules with These rules captures the confusion be
grammar last example. These in choosingpatterns For a given rule
grammar. for a particuan MT from anmay havebilingualthe last example. These reor
system existingshown captures the confusion that 38
own in the
reordering in the rule as reordering different senses or
in choosing different senses or rules in

X ￿ , → Several X , X of X ￿ .
source and X0 ofsides X1 English. XXof X1 1 ,X0 1rules inX1 ￿
target the areX → ￿ ￿ 0 0 ’s example of the 0 the
→ ￿ X0 of X1 ,
Step-1: Extracting a Confusion as following, in ￿ ,
English. Several example rules in the X → Grammar (CG)
CG are ￿ lead to , result
X → → ￿ lead X , ,resultof X
￿
’s to X
→ ￿ X0 at beijing , beijing ’s X0 ￿XX → X0mao, 1the cat￿. ￿ , 0 ￿ .
, are ￿ following,1 in
as
Deriving ’s Xto X1 a bilingual grammar
a 1 result X X
XEnglish.￿SeveralCG,,fromof in ￿in . → ￿ X0 at beijing , beijing ’s X0 ￿
→ →X0 leadexample rules0 ,￿ the CG
X
￿
side as pivots ,X
￿ 0
X → ￿ mao, the cat￿. X
mao, ,a cat￿.
•X use Chinese1to Xresultthe￿X1X → X X→at￿beijingXbeijing ’sX1 0￿ ￿, ,
→ ￿ → 0 atlead , , , beijing ’s X ￿￿,,→ ￿ X0 of X1 , 0 of the
￿ X ￿ beijing 0 of in
X of X
X →X →0 ￿ mao, the cat￿.
0
X → X X0 of mao,Xa cat￿. X1 ￿ ,
￿ → a cat, 0 the
Bilingual Rule
X Confusion,1Rule1cat￿. 0 ￿ .
→ X￿ ’s X theofof X
￿ 0 X1 , X
X → ￿ ￿￿X0 of X1 , , ,Xofof X X￿ ,X, → ￿
X → X0 at beijingXbeijing ’s 1 . X → ￿ X ’s X , X of X ￿ .
X → X0 ’s X1 0 1 the X￿ 0 ￿
0
1
X → ￿ 0a cat, , the1on X10￿,
X ￿ XXof’s X, a X of the X1. ￿ , X → ￿ X0 de X1 X0 cat￿.
→ 0￿ mao, , cat￿. X ￿
XX → ￿ 0 X1 1 X0 1 of 0
→
X → ￿ X0 de X1 , X0 on X1 ￿,

•

X → ￿ the cat, a cat￿.
X → Xmao, the cat￿.X ￿ .
￿ ’s X , X of
X → rules with different
X ￿
1
contains → ￿0mao, 1the cat￿. 0 arities.→ Also, there might ￿,
be
XX → ￿0 the X1 , Xa of X0
￿ X on cat, 1 cat￿.
n inX → ￿ →0 ￿de X1 , a cat￿. X1rules captures0the X1 , X1 of X0that
the X→ example. cat￿. ￿,
X → ￿ X on confusion ￿,
X mao, a 0 on
Xlast ￿ mao, XThese
→￿
X a X0
choosing ￿ X de X senses X ￿, X X → X0 de X1,, X 1on X ￿,￿,
different , X of or reordering￿ patterns for ofgiven
X0 of X1 1
X→
0
0
1
1
0
G contains￿ X on withX of X ￿, arities.present two ,X1might ￿, ￿,
rules X , different
→ X of X , on
w we get such a grammar. BelowXX →￿Also, thereX1 of X0
we ￿X00 de X11 ways.X0 be
X → Like0 a regular SCFG,0a CG contains rules with different aritie
1
1
wn in the last example. SCFG, a CG contains rules X , X ’s X ￿,
These rules captures the with different aritie
confusion that
Like a regular
1
reordering in X0 rule as shown in theX0 de 1 0
the ’s X1 ￿, X → ￿ last example. These rules c
X → ￿different1 ,
X senses or reordering patterns
choosinganX0 desystem may have in X in the Xdifferent ,for a given ca
reordering in the rule as shown → ￿ last de X senses orrules
These reorde
MT XSCFG, a CG contains rules withX0 ’s X1 ￿, ari
choosing 0 example. different
X → ￿ regular 1 , X1 on X0 ￿,
X0 of
Like aan Bilingual different arities. present two might be
mar fromsuch a with Grammar choosing different1senses or reorde
CG contains rulessystem question isin we Also, there ways. Below w
ow we get MT grammar.have how we get such a grammar.
input. Now the may Below
Like alast example. question arities. get thethere mightthat
CG reordering in theSCFG, a CG how we Also,example. different ar
rules with These rules captures such grammar. Below w
owncontainsregular the differentis contains rulesawith These rules
in theinput. Now rule as shown in the last confusion be
grammar last example. These in choosingpatterns For a given rule
grammar. for particuan MT from anmay havebilingualthe last Bilinguala These reor
system existingshown captures the confusion that 38
own in the
reordering in the senses or Grammar different senses or
rule as reordering
in choosing different Confusion rules in from example.Grammar
Extracting

X ￿ , → Several X , X of X ￿ .
source and X0 ofsides X1 English. XXof X1 1 ,X0 1rules inX1 ￿
target the areX → ￿ ￿ 0 0 ’s example of the 0 the
→ ￿ X0 of X1 ,
Step-1: Extracting a Confusion as following, in ￿ ,
English. Several example rules in the X → Grammar (CG)
CG are ￿ lead to , result
X → → ￿ lead X , ,resultof X
￿
’s to X
→ ￿ X0 at beijing , beijing ’s X0 ￿XX → X0mao, 1the cat￿. ￿ , 0 ￿ .
, are ￿ following,1 in
as
Deriving ’s Xto X1 a bilingual grammar
a 1 result X X
XEnglish.￿SeveralCG,,fromof in ￿in . → ￿ X0 at beijing , beijing ’s X0 ￿
→ →X0 leadexample rules0 ,￿ the CG
X
￿
side as pivots ,X
￿ 0
X → ￿ mao, the cat￿. X
mao, ,a cat￿.
•X use Chinese1to Xresultthe￿X1X → X X→at￿beijingXbeijing ’sX1 0￿ ￿, ,
→ ￿ → 0 atlead , , , beijing ’s X ￿￿,,→ ￿ X0 of X1 , 0 of the
￿ X ￿ beijing 0 of in
X of X
X →X →0 ￿ mao, the cat￿.
0
X → X X0 of mao,Xa cat￿. X1 ￿ ,
￿ → a cat, 0 the
Bilingual Rule
X Confusion,1Rule1cat￿. 0 ￿ .
→ X￿ ’s X theofof X
￿ 0 X1 , X
X → ￿ ￿￿X0 of X1 , , ,Xofof X X￿ ,X, → ￿
X → X0 at beijingXbeijing ’s 1 . X → ￿ X ’s X , X of X ￿ .
X → X0 ’s X1 0 1 the X￿ 0 ￿
0
1
X → ￿ 0a cat, , the1on X10￿,
X ￿ XXof’s X, a X of the X1. ￿ , X → ￿ X0 de X1 X0 cat￿.
→ 0￿ mao, , cat￿. X ￿
XX → ￿ 0 X1 1 X0 1 of 0
→
X → ￿ X0 de X1 , X0 on X1 ￿,

•

X → ￿ the cat, a cat￿.
X → Xmao, the cat￿.X ￿ .
￿ ’s X , X of
X → rules with different
X ￿
1
contains → ￿0mao, 1the cat￿. 0 arities.→ Also, there might ￿,
be
XX → ￿0 the X1 , Xa of X0
￿ X on cat, 1 cat￿.
n inX → ￿ →0 ￿de X1 , a cat￿. X1rules captures0the X1 , X1 of X0that
the X→ example. cat￿. ￿,
X → ￿ X on confusion ￿,
X mao, a 0 on
Xlast ￿ mao, XThese
→￿
X a X0
choosing ￿ X de X senses X ￿, X X → X0 de X1,, X 1on X ￿,￿,
different , X of or reordering￿ patterns for ofgiven
X0 of X1 1
X→
0
0
1
1
0
G contains￿ X on withX of X ￿, arities.present two ,X1might ￿, ￿,
rules X , different
→ X of X , on
w we get such a grammar. BelowXX →￿Also, thereX1 of X0
we ￿X00 de X11 ways.X0 be
X → Like0 a regular SCFG,0a CG contains rules with different aritie
1
1
wn in the last example. the confusioncontainssystemwithX ’s X ￿,
CG a regular SCFG, ashownan MT lasttheXwill different aritie
captures These rules captures de confusion that c
Like
1
reordering in X0 rule as CG in theX0 example. 0
the ’s X1 ￿, X → ￿ rules 1 , These rules
X → ￿different1 ,
X senses or reordering patterns
choosinganX0 desystemwhen have in X inan ￿ Xdifferent ,for a given ca
have may
reordering in the rule translating→ input.example. These rules
shown the
MT XSCFG, a as￿, contains last de withX0 ’s or 1 ￿, ari
0
Xfromregularwith Grammar choosingrules X1 might be reorde
→a￿ X0 of 1 , Xdifferent arities. Also, there senses X
on 0
Like an Bilingual 1may have
different
mar get such a grammar.XCGin choosing different senses or reorde
CG contains rulessystem question is how wepresent two ways. Below w
ow we input. Now the
Below we get such a grammar.
MT
Like alast example. question arities. get thethere mightthat
CG reordering in theSCFG, a CG how we Also,example. different ar
rules with These rules captures such grammar. Below w
owncontainsregular the differentis contains rulesawith These rules
in theinput. Now rule as shown in the last confusion be
grammar last example. These in choosingpatterns For a given rule
grammar. for particuan MT from anmay havebilingualthe last Bilinguala These reor
system existingshown captures the confusion that 38
own in the
reordering in the senses or Grammar different senses or
rule as reordering
in choosing different Confusion rules in from example.Grammar
Extracting

X ￿ , → Several X , X of X ￿ .
source and X0 ofsides X1 English. XXof X1 1 ,X0 1rules inX1 ￿
target the areX → ￿ ￿ 0 0 ’s example of the 0 the
→ ￿ X0 of X1 ,
Step-1: Extracting a Confusion as following, in ￿ ,
English. Several example rules in the X → Grammar (CG)
CG are ￿ lead to , result
X → → ￿ lead X , ,resultof X
￿
’s to X
→ ￿ X0 at beijing , beijing ’s X0 ￿XX → X0mao, 1the cat￿. ￿ , 0 ￿ .
, are ￿ following,1 in
as
Deriving ’s Xto X1 a bilingual grammar
a 1 result X X
XEnglish.￿SeveralCG,,fromof in ￿in . → ￿ X0 at beijing , beijing ’s X0 ￿
→ →X0 leadexample rules0 ,￿ the CG
X
￿
side as pivots ,X
￿ 0
X → ￿ mao, the cat￿. X
mao, ,a cat￿.
•X use Chinese1to Xresultthe￿X1X → X X→at￿beijingXbeijing ’sX1 0￿ ￿, ,
→ ￿ → 0 atlead , , , beijing ’s X ￿￿,,→ ￿ X0 of X1 , 0 of the
￿ X ￿ beijing 0 of in
X of X
X →X →0 ￿ mao, the cat￿.
0
X → X X0 of mao,Xa cat￿. X1 ￿ ,
￿ → a cat, 0 the
Bilingual Rule
X Confusion,1Rule1cat￿. 0 ￿ .
→ X￿ ’s X theofof X
￿ 0 X1 , X
X → ￿ ￿￿X0 of X1 , , ,Xofof X X￿ ,X, → ￿
X → X0 at beijingXbeijing ’s 1 . X → ￿ X ’s X , X of X ￿ .
X → X0 ’s X1 0 1 the X￿ 0 ￿
0
1
X → ￿ 0a cat, , the1on X10￿,
X ￿ XXof’s X, a X of the X1. ￿ , X → ￿ X0 de X1 X0 cat￿.
→ 0￿ mao, , cat￿. X ￿
XX → ￿ 0 X1 1 X0 1 of 0
→
X → ￿ X0 de X1 , X0 on X1 ￿,

•

X → ￿ the cat, a cat￿.
X → Xmao, the cat￿.X ￿ .
￿ ’s X , X of
X → rules with different
X ￿
1
contains → ￿0mao, 1the cat￿. 0 arities.→ Also, there might ￿,
be
XX → ￿0 the X1 , Xa of X0
￿ X on cat, 1 cat￿.
n inX → ￿ →0 ￿de X1 , a cat￿. X1rules captures0the X1 , X1 of X0that
the X→ example. cat￿. ￿,
X → ￿ X on confusion ￿,
X mao, a 0 on
Xlast ￿ mao, XThese
→￿
X a X0
choosing ￿ X de X senses X ￿, X X → X0 de X1,, X 1on X ￿,￿,
different , X of or reordering￿ patterns for ofgiven
X0 of X1 1
X→
0
0
1
1
0
G contains￿ X on withX of X ￿, arities.present two ,X1might ￿, ￿,
rules X , different
→ X of X , on
w we get such a grammar. BelowXX →￿Also, thereX1 of X0
we ￿X00 de X11 ways.X0 be
X → Like0 a regular SCFG,0a CG contains rules with different aritie
1
1
wn in the last example. the confusioncontainssystemwithX ’s X ￿,
CG a regular SCFG, ashownan MT lasttheXwill different aritie
captures These rules captures de confusion that c
Like
1
reordering in X0 rule as CG in theX0 example. 0
the ’s X1 ￿, X → ￿ rules 1 , These rules
X → ￿different1 ,
X senses or reordering patterns
choosinganX0 desystemwhen have in X inan ￿ Xdifferent ,for a given ca
have may
reordering in the rule translating→ input.example. These rules
shown the
MT XSCFG, a as￿, contains last de withX0 ’s or 1 ￿, ari
0
Xfromregularwith Grammar choosingrules X1 might be reorde
→a￿ X0 of 1 , Xdifferent arities. Also, there senses X
on 0
Like an Bilingual 1may have
different
mar get such a grammar.XCGin choosing different senses or reorde
CG contains rulessystem question is how wepresent two ways. Below w
ow we input. Now the
Below we get such a grammar.
MT
Our last Now rule as shown captures suchMT-speciﬁc. w
Like aneighborhood question is learned and confusion that
CG reordering in theSCFG, a CG how we Also,example. different ar
rules with function arities. last
owncontainsregular the differentis contains rulesawith These rules
in theinput.example. These rules in theget thethere might be
grammar. Below
grammar last example. These in choosingpatterns For a given rule
grammar. for particuan MT from anmay havebilingualthe last Bilinguala These reor
system existingshown captures the confusion that 38
own in the
reordering in the senses or Grammar different senses or
rule as reordering
in choosing different Confusion rules in from example.Grammar
Extracting

Step-2: Generating Contrastive Sets

39

Step-2: Generating Contrastive Sets
a cat on the mat

39

Step-2: Generating Contrastive Sets
a cat on the mat

CG

39

X → ￿ X0 on X1 , X0 ’s X1 ￿
X → ￿ X0 on X1 , X1 on X0 ￿
X → ￿ X0 on X1 , X1 of X0 ￿
cat → ￿ the X0 ￿
S on X0 , mat

Step-2: Generating Contrastive Sets

a

CG

(a) An example confusion grammar.
S 0,5
S→￿X0 , X0 ￿

X 0,5
X → ￿ X0 on X1 , X1 on X0 ￿

X → ￿ X0 on X1 , X0 ’s X1 ￿

X → ￿ X0 on X1 , X0 X1 ￿

X 0,2

X 3,5

X → ￿ a cat , the cat ￿

a0 cat1

X → ￿ X0 on X1 , X1 of X0 ￿

X → ￿ the mat , the mat ￿

on2

the3 mat4
39

X → ￿ X0 on X1 , X0 ’s X1 ￿
X → ￿ X0 on X1 , X1 on X0 ￿
X → ￿ X0 on X1 , X1 of X0 ￿
cat → ￿ the X0 ￿
S on X0 , mat

Step-2: Generating Contrastive Sets

a

Contrastive set:

CG

(a) An example confusion grammar.
S 0,5
S→￿X0 , X0 ￿

the cat the mat
the cat ’s the mat
the mat on the cat
the mat of the cat

X 0,5
X → ￿ X0 on X1 , X1 on X0 ￿

X → ￿ X0 on X1 , X0 ’s X1 ￿

X → ￿ X0 on X1 , X0 X1 ￿

X 0,2

X 3,5

X → ￿ a cat , the cat ￿

a0 cat1

X → ￿ X0 on X1 , X1 of X0 ￿

X → ￿ the mat , the mat ￿

on2

the3 mat4
39

X → ￿ X0 on X1 , X0 ’s X1 ￿
X → ￿ X0 on X1 , X1 on X0 ￿
X → ￿ X0 on X1 , X1 of X0 ￿
cat → ￿ the X0 ￿
S on X0 , mat

Step-2: Generating Contrastive Sets

a

Contrastive set:

CG

(a) An example confusion grammar.
S 0,5
S→￿X0 , X0 ￿

the cat the mat
the cat ’s the mat
the mat on the cat
the mat of the cat

X 0,5
X → ￿ X0 on X1 , X1 on X0 ￿

X → ￿ X0 on X1 , X0 ’s X1 ￿

X → ￿ X0 on X1 , X0 X1 ￿

X → ￿ X0 on X1 , X1 of X0 ￿

S→￿X0 , X0 ￿

X → ￿ X0 on X1 , X1 of X0 ￿
X → ￿ a cat , the cat ￿

X 0,2

X 3,5

X → ￿ a cat , the cat ￿

a0 cat1

a cat

on

X → ￿ the mat , the mat ￿

the mat

X → ￿ the mat , the mat ￿

on2

the3 mat4
39

X → ￿ X0 on X1 , X0 ’s X1 ￿
X → ￿ X0 on X1 , X1 on X0 ￿
X → ￿ X0 on X1 , X1 of X0 ￿
cat → ￿ the X0 ￿
S on X0 , mat

Step-2: Generating Contrastive Sets

a

Contrastive set:
the cat the mat
the cat ’s the mat
the mat on the cat
the mat of the cat

CG

(a) An example confusion grammar.
S 0,5
S→￿X0 , X0 ￿

X 0,5
X → ￿ X0 on X1 , X1 on X0 ￿

X → ￿ X0 on X1 , X0 ’s X1 ￿

X → ￿ X0 on X1 , X0 X1 ￿

S→￿X0 , X0 ￿

X → ￿ X0 on X1 , X1 of X0 ￿

X → ￿ X0 on X1 , X1 of X0 ￿
X → ￿ a cat , the cat ￿

X 0,2

X 3,5

X → ￿ a cat , the cat ￿

a0 cat1

a cat

on

X → ￿ the mat , the mat ￿

the mat

X → ￿ the mat , the mat ￿

on2

the3 mat4

Translating “dianzi shang de mao”?
39

Step-3: Discriminative Training

40

6.5 on page 34)3Step-3: Discriminative Training
that is,
regular discriminative training. We use the

5 on

3

minimum

￿
page ∗34) that is, Risk (˜ )
θ =Objective
arg min
θ yi
Training

•

θ

￿ ￿
￿
i

∗ = arg min
L(y, θ )p i ) ˜
˜ y
θ = arg min
Riskyi(˜θ (y | yi )
θ
θ y∈N (˜i )
i
y
i
￿ ￿

￿ L(Y(d), y )p (d | y )
￿ ˜
= arg min
˜i
i θ
θ
= arg mind∈D(˜ )
L(y, yi )pθ (y | yi )
˜
˜
i
y
θ

i

i y∈N (˜i )
y
￿ described in Section 2.6.
n also use other discriminative training methods￿
= arg min
L(Y(d), yi )pθ (d
˜
θ
71 d∈D(˜i )
i
y

|y
˜

so use other discriminative training methods described in Section
40

6.5 on page 34)3Step-3: Discriminative Training
that is,
regular discriminative training. We use the

5 on

3

minimum

￿
page ∗34) that is, Risk (˜ )
θ =Objective
arg min
θ yi
Training

•

θ

￿ ￿
￿
i

∗ = arg min
L(y, θ )p i ) ˜
˜ y
θ = arg min
Riskyi(˜θ (y | yi )
θ
θ y∈N (˜i )
i
y
i
contrastive set
￿ ￿

￿ L(Y(d), y )p (d | y )
￿ ˜
= arg min
˜i
i θ
θ
= arg mind∈D(˜ )
L(y, yi )pθ (y | yi )
˜
˜
i
y
θ

i

i y∈N (˜i )
y
￿ described in Section 2.6.
n also use other discriminative training methods￿
= arg min
L(Y(d), yi )pθ (d
˜
θ
71 d∈D(˜i )
i
y

|y
˜

so use other discriminative training methods described in Section
40

6.5 on page 34)3Step-3: Discriminative Training
that is,
regular discriminative training. We use the

5 on

3

minimum

￿
page ∗34) that is, Risk (˜ )
expected loss
θ =Objective
arg min
θ yi
Training

•

θ

￿ ￿
￿
i

∗ = arg min
L(y, θ )p i ) ˜
˜ y
θ = arg min
Riskyi(˜θ (y | yi )
θ
θ y∈N (˜i )
i
y
i
contrastive set
￿ ￿

￿ L(Y(d), y )p (d | y )
￿ ˜
= arg min
˜i
i θ
θ
= arg mind∈D(˜ )
L(y, yi )pθ (y | yi )
˜
˜
i
y
θ

i

i y∈N (˜i )
y
￿ described in Section 2.6.
n also use other discriminative training methods￿
= arg min
L(Y(d), yi )pθ (d
˜
θ
71 d∈D(˜i )
i
y

|y
˜

so use other discriminative training methods described in Section
40

6.5 on page 34)3Step-3: Discriminative Training
that is,
regular discriminative training. We use the

5 on

3

minimum

￿
page ∗34) that is, Risk (˜ )
expected loss
θ =Objective
arg min
θ yi
Training

•

θ

￿ ￿
￿
i

∗ = arg min
L(y, θ )p i ) ˜
˜ y
θ = arg min
Riskyi(˜θ (y | yi )
θ
θ y∈N (˜i )
i
y
i
contrastive set
￿ ￿

￿ L(Y(d), y )p (d | y )
￿ ˜
= arg min
˜i
i θ
θ
= arg mind∈D(˜ maximizesL(y, yi )pθ (y | yi )
˜
CEy )
the ˜
i

i
θ conditional likelihood
i y∈N (˜i )
y
￿ described in Section 2.6.
n also use other discriminative training methods￿
= arg min
L(Y(d), yi )pθ (d
˜
θ
71 d∈D(˜i )
i
y

|y
˜

so use other discriminative training methods described in Section
40

6.5 on page 34)3Step-3: Discriminative Training
that is,
regular discriminative training. We use the

5 on

3

minimum

￿
page ∗34) that is, Risk (˜ )
expected loss
θ =Objective
arg min
θ yi
Training

•

θ

￿ ￿
￿
i

∗ = arg min
L(y, θ )p i ) ˜
˜ y
θ = arg min
Riskyi(˜θ (y | yi )
θ
θ y∈N (˜i )
i
y
i
contrastive set
￿ ￿

￿ L(Y(d), y )p (d | y )
￿ ˜
= arg min
˜i
i θ
θ
= arg mind∈D(˜ maximizesL(y, yi )pθ (y | yi )
˜
CEy )
the ˜
i

i
θ conditional likelihood
i y∈N (˜i )
y
￿ described in Section 2.6.
n also use other discriminative training methods￿
Iterative = arg min
Training
L(Y(d), yi )pθ (d
˜
θ
Step-2: for each English71 d∈D(˜ )
isentence, generate a
yi

•

•

|y
˜

contrastive set (or neighborhood) using the CG

so use other discriminative training methods described in Section
• Step-3: discriminative training
40

Applying the Contrastive Model

41

Applying the Contrastive Model

•

We can use the contrastive model as a
regular language model

41

Applying the Contrastive Model

•

We can use the contrastive model as a
regular language model

•

We can incorporate the contrastive model into
an end-to-end MT system as a feature

41

Applying the Contrastive Model

•

We can use the contrastive model as a
regular language model

•

We can incorporate the contrastive model into
an end-to-end MT system as a feature

•

We may also use the contrastive model to generate
paraphrase sentences
(if the loss function measures semantic similarity)

•

the rules in CG are symmetric

41

Test on Synthesized Hypergraphs of
English Data

42

Test on Synthesized Hypergraphs of
English Data
Monolingual
English
Confusion
grammar

42

Test on Synthesized Hypergraphs of
English Data
Monolingual
English

Training

Confusion
grammar

42

Test on Synthesized Hypergraphs of
English Data
Monolingual
English
Confusion
grammar

Training

Contrastive
LM

42

Test on Synthesized Hypergraphs of
English Data
Monolingual
English

Training

Confusion
grammar

English
Sentence

Parsing

Contrastive
LM

Hypergraph
(Neighborhood)

42

Test on Synthesized Hypergraphs of
English Data
Monolingual
English

Training

Confusion
grammar

English
Sentence

Parsing

Hypergraph
(Neighborhood)

Contrastive
LM

Rank

One-best
English

42

Test on Synthesized Hypergraphs of
English Data
Monolingual
English

Training

Confusion
grammar

English
Sentence

Parsing

Hypergraph
(Neighborhood)

Contrastive
LM

Rank

One-best
English

BLEU Score?
42

Test on Synthesized Hypergraphs of
English Data
Monolingual
English

Training

Confusion
grammar

English
Sentence

Parsing

Hypergraph
(Neighborhood)

Contrastive
LM

Rank

One-best
English

BLEU Score?
42

Results on Synthesized Hypergraphs

43

BLEU

Results on Synthesized Hypergraphs
26
24
22
20
18
16
14
12
10

BLM

+WP +RuleBigram
Features

43

BLEU

Results on Synthesized Hypergraphs
26
24
22
20
18
16
14
12
10

baseline LM (5-gram)

BLM

+WP +RuleBigram
Features

43

BLEU

Results on Synthesized Hypergraphs
26
24
22
20
18
16
14
12
10

baseline LM (5-gram)

BLM

+WP +RuleBigram
Features

word penalty

43

BLEU

Results on Synthesized Hypergraphs
26
24
22
20
18
16
14
12
10

baseline LM (5-gram)

BLM

+WP +RuleBigram
Features

word penalty

43

BLEU

Results on Synthesized Hypergraphs
26
24
22
20
18
16
14
12
10

baseline LM (5-gram)

BLM

+WP +RuleBigram
Features

word penalty

43

BLEU

Results on Synthesized Hypergraphs
26
24
22
20
18
16
14
12
10

baseline LM (5-gram)

BLM

+WP +RuleBigram
Features

m word penalty(“RuleBigram”): For each confusion rule, we extract
Features
r the target-side symbols (including non-terminals and terminals).
• Target side of a confusion rule
onfusion rule’s target side is “on the X1 issue of X2 ” where X1
minals (with a position index), we can extract bigram features
, “the X”, “X issue”, “issue of”, and “of X”. Note that the index
nal of the rule has been removed in the features. Moreover, for
ls, we will use their dominant POS tags (instead of the symbol

43

BLEU

Results on Synthesized Hypergraphs
26
24
22
20
18
16
14
12
10

baseline LM (5-gram)

BLM

+WP +RuleBigram
Features

m word penalty(“RuleBigram”): For each confusion rule, (“RuleBigram”): Fo
Features • Target-rule Bigram Features (“RuleBigram”): For e
extract
• Target-rule Bigram Features we (“RuleBigram”): F
• Target-rule confusion rule
Features
Targetfeatures a Bigramtarget-side symbols (includi
side non-terminals and terminals). (including
of overtarget-side symbols
r the target-side symbols (including
• bigram features the
bigram features over the over the target-side symbols (inclu
igram Features (“RuleBigram”): For each confusion rule, we extract
we
bigram
Bigram Features (“RuleBigram”): For each confusion rule,X1 extract
onfusion rule’s targetexample, if theconfusionof X2 ” where side is is “on
side example, if a issue rule’s target
“on a X1
For symbols (includingconfusion rule’sand terminals).is “
For isFor example, if a confusion rule’s target side t
s over the target-side symbols (including non-terminals andtarget side “on
non-terminals terminals).
res over(with a position index), we can extract bigram features
the target-side
minals
and•XRuleside X2non-terminals (withposition index), w
andareside is “on the X (with ofof a ”2a position X1
are features issue
and is “on the X1issue XX where index),
2 Xbigram are non-terminalsa(withpositionX index
2non-terminals
f“the confusionrule’s target of”, and “of X”. Note that the index
a a X”, “X rule’s target
, if confusion issue”, “issue
1
2 ” where 1
,
including: “on “the“the X”, issue”, “issue of”,
including: including: we can extract issue”, “issue of”,
“on
on-terminals (with been removedthe”,the”,the”,extract“Xbigram features of
non-terminals (withaa position index), “on can “the X”, “X issue”, “issue an
position index), we X”, “X bigram for
features
nal of the rule has
features. of the
Moreover, been remove
under in theand“of of the Notehasthe index remov
the nonterminalrulerulerule has been
the the nonterminal X”. has that
been
on the”,will X”, theirissue”, nonterminal (instead of the that the index
“the X”,underunder“issue of”, of the
“X dominant POS tags
the”, “the use “X issue”, “issue of”, and “ofX”. Note symbol removed
ls, we
theremoved we features. Moreover, for
terminal
thebeen removed symbols, we use Moreover, for 43 P
terminal symbols, we use their their dominant
the terminal symbols,in thewillwillwill use dominant PO
onterminal of therule has been
erminal of the rule has
in the features. their dominant

BLEU

Results on Synthesized Hypergraphs
26
24
22
20
18
16
14
12
10

baseline LM (5-gram)

The contrastive LM better
recovers the original
English than a regular
n-gram LM.

BLM

+WP +RuleBigram
Features

m word penalty(“RuleBigram”): For each confusion rule, (“RuleBigram”): Fo
Features • Target-rule Bigram Features (“RuleBigram”): For e
extract
• Target-rule Bigram Features we (“RuleBigram”): F
• Target-rule confusion rule
Features
Targetfeatures a Bigramtarget-side symbols (includi
side non-terminals and terminals). (including
of overtarget-side symbols
r the target-side symbols (including
• bigram features the
bigram features over the over the target-side symbols (inclu
igram Features (“RuleBigram”): For each confusion rule, we extract
we
bigram
Bigram Features (“RuleBigram”): For each confusion rule,X1 extract
onfusion rule’s targetexample, if theconfusionof X2 ” where side is is “on
side example, if a issue rule’s target
“on a X1
For symbols (includingconfusion rule’sand terminals).is “
For isFor example, if a confusion rule’s target side t
s over the target-side symbols (including non-terminals andtarget side “on
non-terminals terminals).
res over(with a position index), we can extract bigram features
the target-side
minals
and•XRuleside X2non-terminals (withposition index), w
andareside is “on the X (with ofof a ”2a position X1
are features issue
and is “on the X1issue XX where index),
2 Xbigram are non-terminalsa(withpositionX index
2non-terminals
f“the confusionrule’s target of”, and “of X”. Note that the index
a a X”, “X rule’s target
, if confusion issue”, “issue
1
2 ” where 1
,
including: “on “the“the X”, issue”, “issue of”,
including: including: we can extract issue”, “issue of”,
“on
on-terminals (with been removedthe”,the”,the”,extract“Xbigram features of
non-terminals (withaa position index), “on can “the X”, “X issue”, “issue an
position index), we X”, “X bigram for
features
nal of the rule has
features. of the
Moreover, been remove
under in theand“of of the Notehasthe index remov
the nonterminalrulerulerule has been
the the nonterminal X”. has that
been
on the”,will X”, theirissue”, nonterminal (instead of the that the index
“the X”,underunder“issue of”, of the
“X dominant POS tags
the”, “the use “X issue”, “issue of”, and “ofX”. Note symbol removed
ls, we
theremoved we features. Moreover, for
terminal
thebeen removed symbols, we use Moreover, for 43 P
terminal symbols, we use their their dominant
the terminal symbols,in thewillwillwill use dominant PO
onterminal of therule has been
erminal of the rule has
in the features. their dominant

BLEU

Results on Synthesized Hypergraphs
26
24
22
20
18
16
14
12
10

baseline LM (5-gram)

The contrastive LM better
recovers the original
English than a regular
n-gram LM.

BLM

+WP +RuleBigram
Features

All the features look at
only the target sides of
confusion rules

m word penalty(“RuleBigram”): For each confusion rule, (“RuleBigram”): Fo
Features • Target-rule Bigram Features (“RuleBigram”): For e
extract
• Target-rule Bigram Features we (“RuleBigram”): F
• Target-rule confusion rule
Features
Targetfeatures a Bigramtarget-side symbols (includi
side non-terminals and terminals). (including
of overtarget-side symbols
r the target-side symbols (including
• bigram features the
bigram features over the over the target-side symbols (inclu
igram Features (“RuleBigram”): For each confusion rule, we extract
we
bigram
Bigram Features (“RuleBigram”): For each confusion rule,X1 extract
onfusion rule’s targetexample, if theconfusionof X2 ” where side is is “on
side example, if a issue rule’s target
“on a X1
For symbols (includingconfusion rule’sand terminals).is “
For isFor example, if a confusion rule’s target side t
s over the target-side symbols (including non-terminals andtarget side “on
non-terminals terminals).
res over(with a position index), we can extract bigram features
the target-side
minals
and•XRuleside X2non-terminals (withposition index), w
andareside is “on the X (with ofof a ”2a position X1
are features issue
and is “on the X1issue XX where index),
2 Xbigram are non-terminalsa(withpositionX index
2non-terminals
f“the confusionrule’s target of”, and “of X”. Note that the index
a a X”, “X rule’s target
, if confusion issue”, “issue
1
2 ” where 1
,
including: “on “the“the X”, issue”, “issue of”,
including: including: we can extract issue”, “issue of”,
“on
on-terminals (with been removedthe”,the”,the”,extract“Xbigram features of
non-terminals (withaa position index), “on can “the X”, “X issue”, “issue an
position index), we X”, “X bigram for
features
nal of the rule has
features. of the
Moreover, been remove
under in theand“of of the Notehasthe index remov
the nonterminalrulerulerule has been
the the nonterminal X”. has that
been
on the”,will X”, theirissue”, nonterminal (instead of the that the index
“the X”,underunder“issue of”, of the
“X dominant POS tags
the”, “the use “X issue”, “issue of”, and “ofX”. Note symbol removed
ls, we
theremoved we features. Moreover, for
terminal
thebeen removed symbols, we use Moreover, for 43 P
terminal symbols, we use their their dominant
the terminal symbols,in thewillwillwill use dominant PO
onterminal of therule has been
erminal of the rule has
in the features. their dominant

Results on MT Test Set

44

BLEU

Results on MT Test Set
50
49
48
47
46
45

Baseline

+CLM

Features

44

BLEU

Results on MT Test Set
50
49
48
47
46
45

Baseline

+CLM

Features
Held-out Bilingual Data
Bilingual
Data

Generative
Training

Monolingual
English

Generative
Training

Unseen
Sentences

Translation
Models
Language
Models

Decoding

Discriminative
Training

Optimal
Weights

Translation
Outputs

44

BLEU

Results on MT Test Set
50
49
48
47
46
45

Baseline

+CLM

Features
Held-out Bilingual Data
Bilingual
Data

Generative
Training

Monolingual
English

Generative
Training

Translation
Models
Language
Models

Discriminative
Training

Optimal
Weights

Add CLM as a
feature
Unseen
Sentences

Decoding

Translation
Outputs

44

BLEU

Results on MT Test Set
50
49
48
47
46
45

The contrastive LM helps to
improve MT performance.
Baseline

+CLM

Features
Held-out Bilingual Data
Bilingual
Data

Generative
Training

Monolingual
English

Generative
Training

Translation
Models
Language
Models

Discriminative
Training

Optimal
Weights

Add CLM as a
feature
Unseen
Sentences

Decoding

Translation
Outputs

44

Adding Features on the CG itself

•

On English Set

•

On MT Set

45

•

Adding Features on the CG itself
On English Set
45
40
BLEU

35
30
25
20
15
10

•

On MT Set

BLM

+WP +RuleBigram +Arity

+CG

Features

45

•

Adding Features on the CG itself
On English Set
45
40
BLEU

35
30
25
20
15
10

+WP +RuleBigram +Arity

+CG

Features

On MT Set
BLEU

•

BLM

50
49
48
47
46
45

Baseline +RuleBigram +Arity

Features

+CG
45

•

Adding Features on the CG itself
On English Set
45
40
BLEU

35
30
25
20
15
10

+WP +RuleBigram +Arity

+CG

Features

On MT Set
BLEU

•

BLM

50
49
48
47
46
45

Baseline +RuleBigram +Arity

Features

+CG
45

•

Adding Features on the CG itself
On English Set
45
40
BLEU

35
30
25
20
15
10

+WP +RuleBigram +Arity

+CG

Features

On MT Set
BLEU

•

BLM

50
49
48
47
46
45

Baseline +RuleBigram +Arity

Features

+CG
45

•

Adding Features on the CG itself
On English Set
45
40
BLEU

35
30
25
20
15
10

On MT Set
BLEU

•

BLM

50
49
48
47
46
45

nology). This can be done as regular MT decoding as we
“translation” model in an English-to-English “translation” s
To make sure that we produce at least one derivation fo
into This standard glue rules as in Hiero (Chiang, 2007).
nology). CG the+Arity
+WP +RuleBigram can be done as regular MT decoding as we c
+CG

“translation” model in an English-to-English “translation” sy
glue rules￿ or ,regular
Features
S one X X0 ￿
To make sure that we produce at confusion 0rules?, for
least→ derivation
into CG the standard glue rules as in Hiero (Chiang, 2007).
S → ￿ S0 X1 , S0 X1 ￿ ,

S → ￿ (OOV) ￿ ,
We also need to add an out of vocabulary X0 , X0rule X →
y and set the cost of such rule at a maximum value such th
˜
only when the CG does not S → how 0to “translate” ,the
know ￿ S X1 , S0 X1 ￿
contain identity rules for English sides occurred in the bili
We also guaranty add an out of vocabulary (OOV) rule X →
not need to the rules in CG alone are able to cover all th
y and set the cost of rules.
˜ +RuleBigram OOV +CG
Baseline need use the+Arity such rule at a maximum value such tha
only when the CG does not know how to “translate” (˜)45 w
Since the CG is an SCFG, the contrastive set N thege
y
Features
hypergraph, encoding not only the alternate sentences of y b
˜

•

Adding Features on the CG itself
On English Set
45
40
BLEU

35
30
25
20

one big feature

15
10

BLM

On MT Set
BLEU

•

nology). This can be done as regular MT decoding as we
“translation” model in an English-to-English “translation” s
To make sure that we produce at least one derivation fo
into This standard glue rules as in Hiero (Chiang, 2007).
nology). CG the+Arity
+WP +RuleBigram can be done as regular MT decoding as we c
+CG

50
49
48
47
46
45

“translation” model in an English-to-English “translation” sy
glue rules￿ or ,regular
Features
S one X X0 ￿
To make sure that we produce at confusion 0rules?, for
least→ derivation
into CG the standard glue rules as in Hiero (Chiang, 2007).
S → ￿ S0 X1 , S0 X1 ￿ ,

S → ￿ (OOV) ￿ ,
We also need to add an out of vocabulary X0 , X0rule X →
y and set the cost of such rule at a maximum value such th
˜
only when the CG does not S → how 0to “translate” ,the
know ￿ S X1 , S0 X1 ￿
contain identity rules for English sides occurred in the bili
We also guaranty add an out of vocabulary (OOV) rule X →
not need to the rules in CG alone are able to cover all th
y and set the cost of rules.
˜ +RuleBigram OOV +CG
Baseline need use the+Arity such rule at a maximum value such tha
only when the CG does not know how to “translate” (˜)45 w
Since the CG is an SCFG, the contrastive set N thege
y
Features
hypergraph, encoding not only the alternate sentences of y b
˜

•

Adding Features on the CG itself
On English Set
45
40
BLEU

35
30
25
20

one big feature

15
10

BLM

On MT Set
BLEU

•

nology). This can be done as regular MT decoding as we
“translation” model in an English-to-English “translation” s
To make sure that we produce at least one derivation fo
into This standard glue rules as in Hiero (Chiang, 2007).
nology). CG the+Arity
+WP +RuleBigram can be done as regular MT decoding as we c
+CG

50
49
48
47
46
45

“translation” model in an English-to-English “translation” sy
glue rules￿ or ,regular
Features
S one X X0 ￿
To make sure that we produce at confusion 0rules?, for
least→ derivation
into CG the standard glue rules as in Hiero (Chiang, 2007).
S → ￿ S0 X1 , S0 X1 ￿ ,

S → ￿ (OOV) ￿ ,
We also need to add an out of vocabulary X0 , X0rule X →
y and set the cost of such rule at a maximum value such th
˜
only when the CG does not S → how 0to “translate” ,the
know ￿ S X1 , S0 X1 ￿
contain identity rules for English sides occurred in the bili
We also guaranty add an out of vocabulary (OOV) rule X →
not need to the rules in CG alone are able to cover all th
y and set the cost of rules.
˜ +RuleBigram OOV +CG
Baseline need use the+Arity such rule at a maximum value such tha
only when the CG does not know how to “translate” (˜)45 w
Since the CG is an SCFG, the contrastive set N thege
y
Features
hypergraph, encoding not only the alternate sentences of y b
˜

•

Adding Features on the CG itself
On English Set

Paraphrasing
model

45
40
BLEU

35
30
25
20

one big feature

15
10

BLM

On MT Set
BLEU

•

nology). This can be done as regular MT decoding as we
“translation” model in an English-to-English “translation” s
To make sure that we produce at least one derivation fo
into This standard glue rules as in Hiero (Chiang, 2007).
nology). CG the+Arity
+WP +RuleBigram can be done as regular MT decoding as we c
+CG

50
49
48
47
46
45

“translation” model in an English-to-English “translation” sy
glue rules￿ or ,regular
Features
S one X X0 ￿
To make sure that we produce at confusion 0rules?, for
least→ derivation
into CG the standard glue rules as in Hiero (Chiang, 2007).
S → ￿ S0 X1 , S0 X1 ￿ ,

S → ￿ (OOV) ￿ ,
We also need to add an out of vocabulary X0 , X0rule X →
y and set the cost of such rule at a maximum value such th
˜
only when the CG does not S → how 0to “translate” ,the
know ￿ S X1 , S0 X1 ￿
contain identity rules for English sides occurred in the bili
We also guaranty add an out of vocabulary (OOV) rule X →
not need to the rules in CG alone are able to cover all th
y and set the cost of rules.
˜ +RuleBigram OOV +CG
Baseline need use the+Arity such rule at a maximum value such tha
only when the CG does not know how to “translate” (˜)45 w
Since the CG is an SCFG, the contrastive set N thege
y
Features
hypergraph, encoding not only the alternate sentences of y b
˜

•

Adding Features on the CG itself
On English Set

Paraphrasing
model

45
40
BLEU

35
30
25
20

one big feature

15
10

BLM

On MT Set
BLEU

•

nology). This can be done as regular MT decoding as we
“translation” model in an English-to-English “translation” s
To make sure that we produce at least one derivation fo
into This standard glue rules as in Hiero (Chiang, 2007).
nology). CG the+Arity
+WP +RuleBigram can be done as regular MT decoding as we c
+CG

50
49
48
47
46
45

“translation” model in an English-to-English “translation” sy
glue rules￿ or ,regular
Features
S one X X0 ￿
To make sure that we produce at confusion 0rules?, for
least→ derivation
into CG the standard glue rules as in Hiero (Chiang, 2007).
S → ￿ S0 X1 , S0 X1 ￿ ,

S → ￿ (OOV) ￿ ,
We also need to add an out of vocabulary X0 , X0rule X →
y and set the cost of such rule at a maximum value such th
˜
only when the CG does not S → how 0to “translate” ,the
know ￿ S X1 , S0 X1 ￿
contain identity rules for English sides occurred in the bili
We also guaranty add an out of vocabulary (OOV) rule X →
not need to the rules in CG alone are able to cover all th
y and set the cost of rules.
˜ +RuleBigram OOV +CG
Baseline need use the+Arity such rule at a maximum value such tha
only when the CG does not know how to “translate” (˜)45 w
Since the CG is an SCFG, the contrastive set N thege
y
Features
hypergraph, encoding not only the alternate sentences of y b
˜

•

Adding Features on the CG itself
On English Set

Paraphrasing
model

45
40
BLEU

35
30
25
20

one big feature

15
10

BLM

On MT Set
BLEU

•

nology). This can be done as regular MT decoding as we
“translation” model in an English-to-English “translation” s
To make sure that we produce at least one derivation fo
into This standard glue rules as in Hiero (Chiang, 2007).
nology). CG the+Arity
+WP +RuleBigram can be done as regular MT decoding as we c
+CG

50
49
48
47
46
45

“translation” model in an English-to-English “translation” sy
glue rules￿ or ,regular
Features
S one X X0 ￿
To make sure that we produce at confusion 0rules?, for
least→ derivation
into CG the standard glue rules as in Hiero (Chiang, 2007).
S → ￿ S0 X1 , S0 X1 ￿ ,

S → ￿ (OOV) ￿ ,
We also need to add an out of vocabulary X0 , X0rule X →
y and set the cost of such rule at a maximum value such th
˜
only when the CG does not S → how 0to “translate” ,the
know ￿ S X1 , S0 X1 ￿
contain identity rules for English sides occurred in the bili
We also guaranty add an out of vocabulary (OOV) rule X →
not need to the rules in CG alone are able to cover all th
y and set the cost of rules.
˜ +RuleBigram OOV +CG
Baseline need use the+Arity such rule at a maximum value such tha
only when the CG does not know how to “translate” (˜)45 w
Since the CG is an SCFG, the contrastive set N thege
y
Features
hypergraph, encoding not only the alternate sentences of y b
˜

•

Adding Features on the CG itself
On English Set

Paraphrasing
model

45
40
BLEU

35
30
25
20

one big feature

15
10

BLM

On MT Set
BLEU

•

nology). This can be done as regular MT decoding as we
“translation” model in an English-to-English “translation” s
To make sure that we produce at least one derivation fo
into This standard glue rules as in Hiero (Chiang, 2007).
nology). CG the+Arity
+WP +RuleBigram can be done as regular MT decoding as we c
+CG

50
49
48
47
46
45

“translation” model in an English-to-English “translation” sy
glue rules￿ or ,regular
Features
S one X X0 ￿
To make sure that we produce at confusion 0rules?, for
least→ derivation
into CG the standard glue rules as in Hiero (Chiang, 2007).
S → ￿ S0 X1 , S0 X1 ￿ ,

S → ￿ (OOV) ￿ ,
We also need to add an out of vocabulary X0 , X0rule X →
y and set the cost of such rule at a maximum value such th
˜
only when the CG does not S → how 0to “translate” ,the
know ￿ S X1 , S0 X1 ￿
contain identity rules for English sides occurred in the bili
We also guaranty add an out of vocabulary (OOV) rule X →
not need to the rules in CG alone are able to cover all th
y and set the cost of rules.
˜ +RuleBigram OOV +CG
Baseline need use the+Arity such rule at a maximum value such tha
only when the CG does not know how to “translate” (˜)45 w
Since the CG is an SCFG, the contrastive set N thege
y
Features
hypergraph, encoding not only the alternate sentences of y b
˜

the open-source MT toolkit Joshua (Li et al., 2009a).
N
￿ (and
sed discriminative training performs similarly to ￿ Training
Summary for Discriminative
1
∗
θ = to impute the missing x θ (x), yi ) T
arg supervised train-pφ (x | yi )L(δ data.
˜
˜
o, adding unsupervised data into themin
model”Supervised: that attempts to impute the missing x da
that setting—as
i
the supervised attempts used in MERT Risk
Minimum θ N
Empirical
prediction model”

setting—as used in MERT

•
upervised Discriminai=1

x

i

n page 31.
called minimum imputedimputed empirical(abbreviat
empirical risk risk (abbr
), what we called minimum loss
require
x
e complex system δθ (x). The
nslation translation system δθ (x). The y
4
sk),Our minimum imputed-risk objectivenot (4.4) could be evaluated by
isbitext have any form and need of
lish y, may
al Risk (for Supervised Discriminaave any form and need Imputed Risk
not
ows. deﬁne a scoring function along with
may Unsupervised: Minimum
N ￿ ￿
N
￿function ￿
1
high-scoring translation y.
1 setting—as used in with
coringeach unsupervised example y ,)L(δthe reverse model p to im
along MERT (x), y )
rvised min
= For
arg
˜i
in 1. require θ pφ (x | yipφ (x θ (x), yiθ
˜ )L(δ | ˜i use )
˜
φ
N
training in the supervised setting—as . . .}, MERT
Nreverse xy. i=1 x
anslationtranslations {xi1, xi2,used inand add each (xij , yi) pair (we
˜
loss
1. monolingual 31.
i=1 page
ectionEnglish to an imputed training set .
2.6 ≤ 1)
on
yi )
˜

•

rs θ translation translation system The
(4
ex of some complexsystem δθ (x).δθ (x). The
e x toNow do ordinary supervised training (asnot (4.2)) on the (weighted
English y, may have
2. have any form any form and need of
may Unsupervised: (4.4) couldalongevaluated by brute force
and needbe with
not
rameters θ may deﬁne scoring function
ted-risk objectiveaof Contrastive LM Estimation
• aof (4.4) could be y.
data.
ective
xtracting high-scoring translation
ﬁne a scoring functionlossevaluated by brute force as f
along with
require
The second step means that we must use δθ to forward-translate each i
ring 54
translation y. Neighborhood
monolingual

y
˜
N (˜)
y
ate the loss of the translationsFunction the correspondingto impute its y
against
true translation ˜
vised example yi , use the reverse model pφ
˜
p
English
hat minimizes the weighted sum of these losses (i.e., the empirical risk w
ple y , use the reverse model p to impute its possib
˜

46
i
ns {xi1 , xi2 , . . .}, and add each (xij φyi ) pair (weighted by p
,˜

the open-source MT toolkit Joshua (Li et al., 2009a).
N
￿ (and
sed discriminative training performs similarly to ￿ Training
Summary for Discriminative
1
∗
θ = to impute the missing x θ (x), yi ) T
arg supervised train-pφ (x | yi )L(δ data.
˜
˜
o, adding unsupervised data into themin
model”Supervised: that attempts to impute the missing x da
that setting—as
i
the supervised attempts used in MERT Risk
Minimum θ N
Empirical
prediction model”

setting—as used in MERT

•
upervised Discriminai=1

x

i

n page 31.
called minimum imputedimputed empirical(abbreviat
empirical risk risk (abbr
), what we called minimum loss
require
x
e complex system δθ (x). The
nslation translation system δθ (x). The y
4
sk),Our minimum imputed-risk objectivenot (4.4) could be evaluated by
isbitext have any form and need of
lish y, may
al Risk (for Supervised Discriminaave any form and need Imputed Risk
not
ows. deﬁne a scoring function along with
may Unsupervised: Minimum
N ￿ ￿
N
￿function ￿
1
high-scoring translation y.
1 setting—as used in with
coringeach unsupervised example y ,)L(δthe reverse model p to im
along MERT (x), y )
rvised min
= For
arg
˜i
in 1. require θ pφ (x | yipφ (x θ (x), yiθ
˜ )L(δ | ˜i use )
˜
φ
N
training in the supervised setting—as . . .}, MERT
Nreverse xy. i=1 x
anslationtranslations {xi1, xi2,used inand add each (xij , yi) pair (we
˜
loss
1. monolingual 31.
i=1 page
ectionEnglish to an imputed training set .
2.6 ≤ 1)
on
yi )
˜

•

rs θ translation translation system The
(4
ex of some complexsystem δθ (x).δθ (x). The
e x toNow do ordinary supervised training (asnot (4.2)) on the (weighted
English y, may have
2. have any form any form and need of
may Unsupervised: (4.4) couldalongevaluated by brute force
and needbe with
not
rameters θ may deﬁne scoring function
ted-risk objectiveaof Contrastive LM Estimation
• aof (4.4) could be y.
data.
ective
xtracting high-scoring translation
ﬁne a scoring functionlossevaluated by brute force as f
along with
require
The second step means that we must use δθ to forward-translate each i
ring 54
translation y. Neighborhood
monolingual

y
˜
N (˜)
y
ate the loss of the translationsFunction the correspondingto impute its y
against
true translation ˜
vised example yi , use the reverse model pφ
˜
p
English
hat minimizes the weighted sum of these losses (i.e., the empirical risk w
ple y , use the reverse model p to impute its possib
˜

46
i
ns {xi1 , xi2 , . . .}, and add each (xij φyi ) pair (weighted by p
,˜

the open-source MT toolkit Joshua (Li et al., 2009a).
N
￿ (and
sed discriminative training performs similarly to ￿ Training
Summary for Discriminative
1
∗
θ = to impute the missing x θ (x), yi ) T
arg supervised train-pφ (x | yi )L(δ data.
˜
˜
o, adding unsupervised data into themin
model”Supervised Training in MERT
that setting—as used θ N
i
the supervised attempts attempts to impute the missing x da
prediction model” that

setting—as used in MERT

•
upervised Discriminai=1

x

i

n page 31.
called minimum imputedimputed empirical(abbreviat
empirical risk risk (abbr
), what we called minimum loss
require
x
e complex system δθ (x). The
nslation translation system δθ (x). The y
4
sk),Our minimum imputed-risk objectivenot (4.4) could be evaluated by
isbitext have any form and need of
lish y, may
al Risk (for Supervised Discriminaave any form and need Imputed Risk
not
ows. deﬁne a scoring function along with
may Unsupervised: Minimum
N ￿ ￿
N
￿function ￿
1
high-scoring translation y.
1 setting—as used in with
coringeach unsupervised example y ,)L(δthe reverse model p to im
along MERT (x), y )
rvised min
= For
arg
˜i
in 1. require θ pφ (x | yipφ (x θ (x), yiθ
˜ )L(δ | ˜i use )
˜
φ
N
training in the supervised setting—as . . .}, MERT
Nreverse xy. i=1 x
anslationtranslations {xi1, xi2,used inand add each (xij , yi) pair (we
˜
loss
1. monolingual 31.
i=1 page
ectionEnglish to an imputed training set .
2.6 ≤ 1)
on
yi )
˜

•

rs θ translation translation system The
(4
ex of some complexsystem δθ (x).δθ (x). The
e x toNow do ordinary supervised training (asnot (4.2)) on the (weighted
English y, may have
2. have any form any form and need of
may Unsupervised: (4.4) couldalongevaluated by brute force
and needbe with
not
rameters θ may deﬁne scoring function
ted-risk objectiveaof Contrastive LM Estimation
• aof (4.4) could be y.
data.
ective
xtracting high-scoring translation
ﬁne a scoring functionlossevaluated by brute force as f
along with
require
The second step means that we must use δθ to forward-translate each i
ring 54
translation y. Neighborhood
monolingual

y
˜
N (˜)
y
ate the loss of the translationsFunction the correspondingto impute its y
against
true translation ˜
vised example yi , use the reverse model pφ
˜
p
English
hat minimizes the weighted sum of these losses (i.e., the empirical risk w
ple y , use the reverse model p to impute its possib
˜

47
i
ns {xi1 , xi2 , . . .}, and add each (xij φyi ) pair (weighted by p
,˜

the open-source MT toolkit Joshua (Li et al., 2009a).
N
￿ (and
sed discriminative training performs similarly to ￿ Training
Summary for Discriminative
1
∗
θ = to impute the missing x θ (x), yi ) T
arg supervised train-pφ (x | yi )L(δ data.
˜
˜
o, adding unsupervised data into themin
model”Supervised Training in MERT
that setting—as used θ N
i
the supervised attempts attempts to impute the missing x da
prediction model” that

setting—as used in MERT

•
upervised Discriminai=1

x

i

n page 31.
called minimum imputedimputed empirical(abbreviat
empirical risk risk (abbr
), what we called minimum loss
require
x
e complex system δθ (x). The
nslation translation system δθ (x). The y
4
sk),Our minimum imputed-risk objectivenot (4.4) could be evaluated by
isbitext have any form and need of
lish y, may
al Risk (for Supervised Discriminaave any form and need Imputed Risk
not
ows. deﬁne a scoring function along with
may Unsupervised: Minimum
N ￿ ￿
N
￿function ￿
1
high-scoring translation y.
1 setting—as used in with
coringeach unsupervised example y ,)L(δthe reversereverse model im
along MERT require a model p to
rvised min
= For
arg
˜
in 1. require θ pφ (x | yipφ (x θ (x), yiθ (x), yi )
˜ )L(δ | ˜i use )
˜
φ
N
training in the supervised setting—as . . .}, MERT
Nreverse xy. i=1 x
anslationtranslations {xi1, xi2,used inand add each (xij , yi) pair (we
˜
loss
1. monolingual 31.
i=1 page
ectionEnglish to an imputed training set .
2.6 ≤ 1)
on
yi )
˜

•

rs θ translation translation system The
(4
ex of some complexsystem δθ (x).δθ (x). The
e x toNow do ordinary supervised training (asnot (4.2)) on the (weighted
English y, may have
2. have any form any form and need of
may Unsupervised: (4.4) couldalongevaluated by brute force
and needbe with
not
rameters θ may deﬁne scoring function
ted-risk objectiveaof Contrastive LM Estimation
• aof (4.4) could be y.
data.
ective
xtracting high-scoring translation
ﬁne a scoring functionlossevaluated by brute force as f
along with
require
The second step means that we must use δθ to forward-translate each i
ring 54
translation y. Neighborhood
monolingual

y
˜
N (˜)
y
ate the loss of the translationsFunction the correspondingto impute its y
against
true translation ˜
vised example yi , use the reverse model pφ
˜
p
English
hat minimizes the weighted sum of these losses (i.e., the empirical risk w
ple y , use the reverse model p to impute its possib
˜

47
i
ns {xi1 , xi2 , . . .}, and add each (xij φyi ) pair (weighted by p
,˜

the open-source MT toolkit Joshua (Li et al., 2009a).
N
￿ (and
sed discriminative training performs similarly to ￿ Training
Summary for Discriminative
1
∗
θ = to impute the missing x θ (x), yi ) T
arg supervised train-pφ (x | yi )L(δ data.
˜
˜
o, adding unsupervised data into themin
model”Supervised Training in MERT
that setting—as used θ N
i
the supervised attempts attempts to impute the missing x da
prediction model” that

setting—as used in MERT

•
upervised Discriminai=1

x

i

n page 31.
called minimum imputedimputed empirical(abbreviat
empirical risk risk (abbr
), what we called minimum loss
require
x
e complex system δθ (x). The
nslation translation system δθ (x). The y
4
sk),Our minimum imputed-risk objectivenot (4.4) could be evaluated by
isbitext have any form and need of
lish y, may
al Risk (for Supervised Discriminaave any form and need Imputed Risk
not
ows. deﬁne a scoring function along with
may Unsupervised: Minimum
N ￿ ￿
N
￿function ￿
1
high-scoring translation y.
1 setting—as used in with
coringeach unsupervised example y ,)L(δthe reversereverse model im
along MERT require a model p to
rvised min
= For
arg
˜
in 1. require θ pφ (x | yipφ (x θ (x), yiθ (x), yi )
˜ )L(δ | ˜i use )
˜
φ
N
training in the supervised setting—as . . .}, MERT
Nreverse xy. i=1 x
anslationtranslations {xi1, xi2,used inand add each (xij , yi) pair (we
˜
loss
1. monolingual 31.
i=1 page
ectionEnglish to an imputed training set .
2.6 ≤ 1)
on
yi )
˜
can have both TM and

•

rs θ translation translation system The
ex of some complexsystem δθ (x).δθ (x). The LM features (4
e x toNow do ordinary supervised training (asnot (4.2)) on the (weighted
English y, may have
2. have any form any form and need of
may Unsupervised: (4.4) couldalongevaluated by brute force
and needbe with
not
rameters θ may deﬁne scoring function
ted-risk objectiveaof Contrastive LM Estimation
• aof (4.4) could be y.
data.
ective
xtracting high-scoring translation
ﬁne a scoring functionlossevaluated by brute force as f
along with
require
The second step means that we must use δθ to forward-translate each i
ring 54
translation y. Neighborhood
monolingual

y
˜
N (˜)
y
ate the loss of the translationsFunction the correspondingto impute its y
against
true translation ˜
vised example yi , use the reverse model pφ
˜
p
English
hat minimizes the weighted sum of these losses (i.e., the empirical risk w
ple y , use the reverse model p to impute its possib
˜

47
i
ns {xi1 , xi2 , . . .}, and add each (xij φyi ) pair (weighted by p
,˜

the open-source MT toolkit Joshua (Li et al., 2009a).
N
￿ (and
sed discriminative training performs similarly to ￿ Training
Summary for Discriminative
1
∗
θ = to impute the missing x θ (x), yi ) T
arg supervised train-pφ (x | yi )L(δ data.
˜
˜
o, adding unsupervised data into themin
model”Supervised Training in MERT
that setting—as used θ N
i
the supervised attempts attempts to impute the missing x da
prediction model” that

setting—as used in MERT

•
upervised Discriminai=1

x

i

n page 31.
called minimum imputedimputed empirical(abbreviat
empirical risk risk (abbr
), what we called minimum loss
require
x
e complex system δθ (x). The
nslation translation system δθ (x). The y
4
sk),Our minimum imputed-risk objectivenot (4.4) could be evaluated by
isbitext have any form and need of
lish y, may
al Risk (for Supervised Discriminaave any form and need Imputed Risk
not
ows. deﬁne a scoring function along with
may Unsupervised: Minimum
N ￿ ￿
N
￿function ￿
1
high-scoring translation y.
1 setting—as used in with
coringeach unsupervised example y ,)L(δthe reversereverse model im
along MERT require a model p to
rvised min
= For
arg
˜
in 1. require θ pφ (x | yipφ (x θ (x), yiθ (x), yi )
˜ )L(δ | ˜i use )
˜
φ
N
training in the supervised setting—as . . .}, MERT
Nreverse xy. i=1 x
anslationtranslations {xi1, xi2,used inand add each (xij , yi) pair (we
˜
loss
1. monolingual 31.
i=1 page
ectionEnglish to an imputed training set .
2.6 ≤ 1)
on
yi )
˜
can have both TM and

•

rs θ translation translation system The
ex of some complexsystem δθ (x).δθ (x). The LM features (4
e x toNow do ordinary supervised training (asnot (4.2)) on the (weighted
English y, may have
2. have any form any form and need of
may Unsupervised: (4.4) couldalongevaluated by brute force
and needbe with
not
rameters θ may deﬁne scoring function
ted-risk objectiveaof Contrastive LM Estimation
• aof (4.4) could be y.
data.
ective
xtracting high-scoring translation
ﬁne a scoring functionlossevaluated by brute force as f
along with
can have LM features
require step means that we must use δ to forward-translate each i
The second
θ
ring 54
translation y. Neighborhood
only
monolingual

y
˜
N (˜)
y
ate the loss of the translationsFunction the correspondingto impute its y
against
true translation ˜
vised example yi , use the reverse model pφ
˜
p
English
hat minimizes the weighted sum of these losses (i.e., the empirical risk w
ple y , use the reverse model p to impute its possib
˜

47
i
ns {xi1 , xi2 , . . .}, and add each (xij φyi ) pair (weighted by p
,˜

the open-source MT toolkit Joshua (Li et al., 2009a).
N
￿ (and
sed discriminative training performs similarly to ￿ Training
Summary for Discriminative
1
∗
θ = to impute the missing x θ (x), yi ) T
arg supervised train-pφ (x | yi )L(δ data.
˜
˜
o, adding unsupervised data into themin
model”Supervised Training in MERT
that setting—as used θ N
i
the supervised attempts attempts to impute the missing x da
prediction model” that

setting—as used in MERT

•
upervised Discriminai=1

x

i

n page 31.
called minimum imputedimputed empirical(abbreviat
empirical risk risk (abbr
), what we called minimum loss
require
x
e complex system δθ (x). The
nslation translation system δθ (x). The y
4
sk),Our minimum imputed-risk objectivenot (4.4) could be evaluated by
isbitext have any form and need of
lish y, may
al Risk (for Supervised Discriminaave any form and need Imputed Risk
not
ows. deﬁne a scoring function along with
may Unsupervised: Minimum
N ￿ ￿
N
￿function ￿
1
high-scoring translation y.
1 setting—as used in with
coringeach unsupervised example y ,)L(δthe reversereverse model im
along MERT require a model p to
rvised min
= For
arg
˜
in 1. require θ pφ (x | yipφ (x θ (x), yiθ (x), yi )
˜ )L(δ | ˜i use )
˜
φ
N
training in the supervised setting—as . . .}, MERT
Nreverse xy. i=1 x
anslationtranslations {xi1, xi2,used inand add each (xij , yi) pair (we
˜
loss
1. monolingual 31.
i=1 page
ectionEnglish to an imputed training set .
2.6 ≤ 1)
on
yi )
˜
can have both TM and

•

rs θ translation translation system The
ex of some complexsystem δθ (x).δθ (x). The LM features (4
e x toNow do ordinary supervised training (asnot (4.2)) on the (weighted
English y, may have
2. have any form any form and need of
may Unsupervised: (4.4) couldalongevaluated by brute force
and needbe with
not
rameters θ may deﬁne scoring function
ted-risk objectiveaof Contrastive LM Estimation
• aof (4.4) could be y.
data.
ective
xtracting high-scoring translation
ﬁne a scoring functionlossevaluated by brute force as f
along with
can have LM features
require step means that we must use δ to forward-translate each i
The second
θ
ring 54
translation y. Neighborhood
only
monolingual

y
˜
N (˜)
y
ate the loss of the translationsFunction the correspondingto impute its y
against
true translation ˜
vised example yi , use the reverse model pφ
˜
p
English
hat minimizes the weighted sum of these losses (i.e., the empirical risk w
ple y , use the reverse model p to impute its possib
˜

47
i
ns {xi1 , xi2 , . . .}, and add each (xij φyi ) pair (weighted by p
,˜

Outline
•
•

Hypergraph as Hypothesis Space
Unsupervised Discriminative Training
‣ minimum imputed risk
‣ contrastive language model estimation

•
•

Variational Decoding
First- and Second-order Expectation Semirings
decoding
(e.g., mbr)

training
(e.g., mert)

atomic inference operations

(e.g., ﬁnding one-best, k-best or expectation,
inference can be exact or approximate)
48

Variational Decoding

49

•

Variational Decoding
We want to do inference under p, but it is intractable

49

Variational Decoding

•

We want to do inference under p, but it is intractable

•

Instead, we derive a simpler distribution q*

49

Variational Decoding

•

We want to do inference under p, but it is intractable

•

Instead, we derive a simpler distribution q*

•

Then, we will use q* as a surrogate for p in inference

49

Variational Decoding

•

We want to do inference under p, but it is intractable

•

Instead, we derive a simpler distribution q*

•

Then, we will use q* as a surrogate for p in inference

intractable MAP decoding

49

Variational Decoding

m A Posterior (MAP) Decoding
We want to do inference under p, but it is intractable

•

a posteriori (MAP)MAP decoding
intractable decision rule is
y

∗

= arg max p(y | x)
y

(2.6)

odness of a translation string y is its posterior probability p(y | x),
Instead, we derive a simpler distribution q*
n from p(d | x) by marginalizing out d. Therefore, the MAP decision

•

y

∗

= argmax
y

•

￿

d∈D(x,y)

p(d | x)

(2.7)

28

Then, we will use q* as a surrogate for p in inference

49

y

= arg max p(y | x)

Variationala translation string y is its posterior p
Decoding
Under MAP, the goodness of
y

m A Posterior (MAP) Decoding
We want to do inference under p, but it is intractable

• which we can obtain from p(d | x) by marginalizing out d. Therefor

a posterioribecomesMAP decoding
intractable decision rule is
rule (MAP)
y

∗

∗
y x) = arg max
= arg max p(y |
y

y

￿

d∈D(x,y)

p(d | x) (2.6)

odness of a translation string y is its posterior probability p(y | x),
Instead, we derive a simpler distribution q*
n from p(d | x) by marginalizing out d. Therefore, the MAP decision
28

•

y

∗

= argmax
y

•

￿

d∈D(x,y)

p(d | x)

(2.7)

28

Then, we will use q* as a surrogate for p in inference

49

y

= arg max p(y | x)

Variationala translation string y is its posterior p
Decoding
Under MAP, the goodness of
y

m A Posterior (MAP) Decoding
We want to do inference under p, but it is intractable

• which we can obtain from p(d | x) by marginalizing out d. Therefor

a posterioribecomesMAP decoding
intractable decision rule is
rule (MAP)
y

∗

(Sima’an 1996)
￿
∗
y x) = arg max
p(d | x) (2.6)
= arg max p(y |
y

y

d∈D(x,y)

odness of a translation string y is its posterior probability p(y | x),
Instead, we derive a simpler distribution q*
n from p(d | x) by marginalizing out d. Therefore, the MAP decision
28

•

y

∗

= argmax
y

•

￿

d∈D(x,y)

p(d | x)

(2.7)

28

Then, we will use q* as a surrogate for p in inference

49

y

= arg max p(y | x)

Variationala translation string y is its posterior p
Decoding
Under MAP, the goodness of
y

m A Posterior (MAP) Decoding
We want to do inference under p, but it is intractable

• which we can obtain from p(d | x) by marginalizing out d. Therefor

a posterioribecomesMAP decoding
intractable decision rule is
rule (MAP)
y

∗

(Sima’an 1996)
￿
∗
y x) = arg max
p(d | x) (2.6)
= arg max p(y |
y

y

d∈D(x,y)

odness of a translation string y is its posterior probability p(y | x),
Instead, we derive a simpler distribution q*
n from p(d | x) by marginalizing out d. Therefore, the MAP decision
28

•

tractable estimation
￿
∗

y q ∗ = arg min KL(p||q) x)
= argmax
p(d |
y

•

(2.7)

q∈Q
d∈D(x,y)

28

Then, we will use q* as a surrogate for p in inference

49

y

= arg max p(y | x)

Variationala translation string y is its posterior p
Decoding
Under MAP, the goodness of
y

m A Posterior (MAP) Decoding
We want to do inference under p, but it is intractable

• which we can obtain from p(d | x) by marginalizing out d. Therefor

a posterioribecomesMAP decoding
intractable decision rule is
rule (MAP)
y

∗

(Sima’an 1996)
￿
∗
y x) = arg max
p(d | x) (2.6)
= arg max p(y |
y

y

d∈D(x,y)

odness of a translation string y is its posterior probability p(y | x),
Instead, we derive a simpler distribution q*
n from p(d | x) by marginalizing out d. Therefore, the MAP decision
28

•

tractable estimation
￿
∗

y q ∗ = arg min KL(p||q) x)
= argmax
p(d |
y

•

P

(2.7)

q∈Q
d∈D(x,y)

28

Then, we will use q* as a surrogate for p in inference

49

y

= arg max p(y | x)

Variationala translation string y is its posterior p
Decoding
Under MAP, the goodness of
y

m A Posterior (MAP) Decoding
We want to do inference under p, but it is intractable

• which we can obtain from p(d | x) by marginalizing out d. Therefor

a posterioribecomesMAP decoding
intractable decision rule is
rule (MAP)
y

∗

(Sima’an 1996)
￿
∗
y x) = arg max
p(d | x) (2.6)
= arg max p(y |
y

y

d∈D(x,y)

odness of a translation string y is its posterior probability p(y | x),
Instead, we derive a simpler distribution q*
n from p(d | x) by marginalizing out d. Therefore, the MAP decision
28

•

tractable estimation
￿
∗

y q ∗ = arg min KL(p||q) x)
= argmax
p(d |
y

•

P

p(y|x)

(2.7)

q∈Q
d∈D(x,y)

28

Then, we will use q* as a surrogate for p in inference

49

y

= arg max p(y | x)

Variationala translation string y is its posterior p
Decoding
Under MAP, the goodness of
y

m A Posterior (MAP) Decoding
We want to do inference under p, but it is intractable

• which we can obtain from p(d | x) by marginalizing out d. Therefor

a posterioribecomesMAP decoding
intractable decision rule is
rule (MAP)
y

∗

(Sima’an 1996)
￿
∗
y x) = arg max
p(d | x) (2.6)
= arg max p(y |
y

y

d∈D(x,y)

odness of a translation string y is its posterior probability p(y | x),
Instead, we derive a simpler distribution q*
n from p(d | x) by marginalizing out d. Therefore, the MAP decision
28

•

tractable estimation
￿
∗

y q ∗ = arg min KL(p||q) x)
= argmax
p(d |
y

•

q∈Q
d∈D(x,y)

P

p(y|x)

(2.7)

Q

28

Then, we will use q* as a surrogate for p in inference

49

y

= arg max p(y | x)

Variationala translation string y is its posterior p
Decoding
Under MAP, the goodness of
y

m A Posterior (MAP) Decoding
We want to do inference under p, but it is intractable

• which we can obtain from p(d | x) by marginalizing out d. Therefor

a posterioribecomesMAP decoding
intractable decision rule is
rule (MAP)
y

∗

(Sima’an 1996)
￿
∗
y x) = arg max
p(d | x) (2.6)
= arg max p(y |
y

y

d∈D(x,y)

odness of a translation string y is its posterior probability p(y | x),
Instead, we derive a simpler distribution q*
n from p(d | x) by marginalizing out d. Therefore, the MAP decision
28

•

tractable estimation
￿
∗

y q ∗ = arg min KL(p||q) x)
= argmax
p(d |
y

•

q∈Q
d∈D(x,y)

P

p(y|x)

Q

q*(y)

(2.7)

28

Then, we will use q* as a surrogate for p in inference

49

y

= arg max p(y | x)

Variationala translation string y is its posterior p
Decoding
Under MAP, the goodness of
y

m A Posterior (MAP) Decoding
We want to do inference under p, but it is intractable

• which we can obtain from p(d | x) by marginalizing out d. Therefor

a posterioribecomesMAP decoding
intractable decision rule is
rule (MAP)
y

∗

(Sima’an 1996)
￿
∗
y x) = arg max
p(d | x) (2.6)
= arg max p(y |
y

y

d∈D(x,y)

odness of a translation string y is its posterior probability p(y | x),
Instead, we derive a simpler distribution q*
n from p(d | x) by marginalizing out d. Therefore, the MAP decision
28

•

tractable estimation
￿
∗

y q ∗ = arg min KL(p||q) x)
= argmax
p(d |
y

•

q∈Q
d∈D(x,y)

P

p(y|x)

Q

q*(y)

(2.7)

28

Then, we will use q* as a surrogate for p in inference

tractable decoding
∗
∗
y = arg max q (y | x)
y

49

Variational Decoding for MT: an Overview

53

Variational Decoding for MT: an Overview
Sentence-speciﬁc decoding

53

Variational Decoding for MT: an Overview
Sentence-speciﬁc decoding
Three steps:

53

Variational Decoding for MT: an Overview
Sentence-speciﬁc decoding
Three steps:

1

Generate a hypergraph for
the foreign sentence

53

Variational Decoding for MT: an Overview
Sentence-speciﬁc decoding
Three steps:

1

Generate a hypergraph for
the foreign sentence

Foreign
sentence x

53

Variational Decoding for MT: an Overview
Sentence-speciﬁc decoding

MAP decoding under P is
intractable

Three steps:

1

Generate a hypergraph for
the foreign sentence

S 0,4
S→￿X0 , X0 ￿

Foreign
sentence x

X 0,4 the · · · cat

SMT

S→￿X0 , X0 ￿

X 0,4 a · · · mat

p(d | x)

X→￿X0 de X1 , X1 on X0 ￿

X→￿X0 de X1 , X0 ’s X1 ￿

X→￿X0 de X1 , X1 of X

X→￿X0 de X1 , X0 X1 ￿

p(y matx)=∑d∈D(x,y) p(d|x)
|
X 0,2 the · · ·

X 3,4 a · · · cat

X→￿dianzi shang, the mat￿

dianzi0 shang1

X→￿mao, a cat￿

de2

mao3
53

1

S 0,4
S→￿X0 , X0 ￿

X 0,4 the · · · cat

S→￿X0 , X0 ￿

X 0,4 a · · · mat

X→￿X0 de X1 , X1 on X0 ￿

p(d | x)
X→￿X0 de X1 , X0 ’s X1 ￿

X→￿X0 de X1 , X1 of X0 ￿

X→￿X0 de X1 , X0 X1 ￿

X 0,2 the · · · mat
X→￿dianzi shang, the mat￿

dianzi0 shang1

Generate a hypergraph

X 3,4 a · · · cat
X→￿mao, a cat￿

de2

mao3

54

1

S 0,4
S→￿X0 , X0 ￿

X 0,4 the · · · cat

S→￿X0 , X0 ￿

X 0,4 a · · · mat

X→￿X0 de X1 , X1 on X0 ￿

p(d | x)
X→￿X0 de X1 , X0 ’s X1 ￿

X→￿X0 de X1 , X1 of X0 ￿

X→￿X0 de X1 , X0 X1 ￿

X 0,2 the · · · mat
X→￿dianzi shang, the mat￿

dianzi0 shang1

Generate a hypergraph

X 3,4 a · · · cat
X→￿mao, a cat￿

de2

mao3

54

1

S 0,4
S→￿X0 , X0 ￿

X 0,4 the · · · cat

S→￿X0 , X0 ￿

X 0,4 a · · · mat

X→￿X0 de X1 , X1 on X0 ￿

p(d | x)
X→￿X0 de X1 , X0 ’s X1 ￿

X→￿X0 de X1 , X1 of X0 ￿

X→￿X0 de X1 , X0 X1 ￿

X 0,2 the · · · mat
X→￿dianzi shang, the mat￿

dianzi0 shang1

Generate a hypergraph

X 3,4 a · · · cat
X→￿mao, a cat￿

de2

mao3

2

54

1

S 0,4
S→￿X0 , X0 ￿

X 0,4 the · · · cat

S→￿X0 , X0 ￿

X 0,4 a · · · mat

X→￿X0 de X1 , X1 on X0 ￿

p(d | x)
X→￿X0 de X1 , X0 ’s X1 ￿

X→￿X0 de X1 , X1 of X0 ￿

X→￿X0 de X1 , X0 X1 ￿

X 0,2 the · · · mat

X 3,4 a · · · cat

X→￿dianzi shang, the mat￿

X→￿mao, a cat￿

dianzi0 shang1

2

Generate a hypergraph

de2

mao3

S 0,4
S→￿X0 , X0 ￿

X 0,4 the · · · cat

S→￿X0 , X0 ￿

X 0,4 a · · · mat

X→￿X0 de X1 , X1 on X0 ￿

p(d | x)
X→￿X0 de X1 , X0 ’s X1 ￿

X→￿X0 de X1 , X1 of X0 ￿

X→￿X0 de X1 , X0 X1 ￿

X 0,2 the · · · mat
X→￿dianzi shang, the mat￿

dianzi0 shang1

X 3,4 a · · · cat
X→￿mao, a cat￿

de2

mao3

54

1

S 0,4
S→￿X0 , X0 ￿

X 0,4 the · · · cat

S→￿X0 , X0 ￿

X 0,4 a · · · mat

X→￿X0 de X1 , X1 on X0 ￿

p(d | x)
X→￿X0 de X1 , X0 ’s X1 ￿

X→￿X0 de X1 , X1 of X0 ￿

X→￿X0 de X1 , X0 X1 ￿

X 0,2 the · · · mat

X 3,4 a · · · cat

X→￿dianzi shang, the mat￿

X→￿mao, a cat￿

dianzi0 shang1

2

de2

mao3

S 0,4
S→￿X0 , X0 ￿

X 0,4 the · · · cat

S→￿X0 , X0 ￿

X 0,4 a · · · mat

X→￿X0 de X1 , X1 on X0 ￿

p(d | x)
X→￿X0 de X1 , X0 ’s X1 ￿

X→￿X0 de X1 , X1 of X0 ￿

X→￿X0 de X1 , X0 X1 ￿

X 0,2 the · · · mat
X→￿dianzi shang, the mat￿

dianzi0 shang1

Generate a hypergraph

Estimate a model
q* is an n-gram model
from the hypergraph over output strings.
by minimizing KL

q*(y | x)

X 3,4 a · · · cat
X→￿mao, a cat￿

de2

mao3

54

1

S 0,4
S→￿X0 , X0 ￿

X 0,4 the · · · cat

S→￿X0 , X0 ￿

X 0,4 a · · · mat

X→￿X0 de X1 , X1 on X0 ￿

p(d | x)
X→￿X0 de X1 , X0 ’s X1 ￿

X→￿X0 de X1 , X1 of X0 ￿

X→￿X0 de X1 , X0 X1 ￿

X 0,2 the · · · mat

X 3,4 a · · · cat

X→￿dianzi shang, the mat￿

X→￿mao, a cat￿

dianzi0 shang1

2

de2

mao3

S 0,4
S→￿X0 , X0 ￿

X 0,4 the · · · cat

S→￿X0 , X0 ￿

X 0,4 a · · · mat

X→￿X0 de X1 , X1 on X0 ￿

p(d | x)
X→￿X0 de X1 , X0 ’s X1 ￿

X→￿X0 de X1 , X1 of X0 ￿

X→￿X0 de X1 , X0 X1 ￿

X 0,2 the · · · mat
X→￿dianzi shang, the mat￿

dianzi0 shang1

Generate a hypergraph

X 3,4 a · · · cat
X→￿mao, a cat￿

de2

mao3

Estimate a model
q* is an n-gram model
from the hypergraph over output strings.
by minimizing KL

q*(y | x)

≈∑d∈D(x,y) p(d|x)

54

1

S 0,4
S→￿X0 , X0 ￿

X 0,4 the · · · cat

S→￿X0 , X0 ￿

X 0,4 a · · · mat

X→￿X0 de X1 , X1 on X0 ￿

p(d | x)
X→￿X0 de X1 , X0 ’s X1 ￿

X→￿X0 de X1 , X1 of X0 ￿

X→￿X0 de X1 , X0 X1 ￿

X 0,2 the · · · mat

X 3,4 a · · · cat

X→￿dianzi shang, the mat￿

X→￿mao, a cat￿

dianzi0 shang1

2

de2

mao3

S 0,4
S→￿X0 , X0 ￿

X 0,4 the · · · cat

S→￿X0 , X0 ￿

X 0,4 a · · · mat

X→￿X0 de X1 , X1 on X0 ￿

p(d | x)
X→￿X0 de X1 , X0 ’s X1 ￿

X→￿X0 de X1 , X1 of X0 ￿

X→￿X0 de X1 , X0 X1 ￿

X 0,2 the · · · mat
X→￿dianzi shang, the mat￿

dianzi0 shang1

Generate a hypergraph

X 3,4 a · · · cat
X→￿mao, a cat￿

de2

mao3

Estimate a model
q* is an n-gram model
from the hypergraph over output strings.
by minimizing KL

q*(y | x)

≈∑d∈D(x,y) p(d|x)

3

54

1

S 0,4
S→￿X0 , X0 ￿

X 0,4 the · · · cat

S→￿X0 , X0 ￿

X 0,4 a · · · mat

X→￿X0 de X1 , X1 on X0 ￿

p(d | x)
X→￿X0 de X1 , X0 ’s X1 ￿

X→￿X0 de X1 , X1 of X0 ￿

X→￿X0 de X1 , X0 X1 ￿

X 0,2 the · · · mat

X 3,4 a · · · cat

X→￿dianzi shang, the mat￿

X→￿mao, a cat￿

dianzi0 shang1

2

de2

mao3

S 0,4
S→￿X0 , X0 ￿

X 0,4 the · · · cat

S→￿X0 , X0 ￿

X 0,4 a · · · mat

X→￿X0 de X1 , X1 on X0 ￿

p(d | x)
X→￿X0 de X1 , X0 ’s X1 ￿

X→￿X0 de X1 , X1 of X0 ￿

X→￿X0 de X1 , X0 X1 ￿

X 0,2 the · · · mat

Estimate a model
q* is an n-gram model
from the hypergraph over output strings.
by minimizing KL

q*(y | x)

X 3,4 a · · · cat

X→￿dianzi shang, the mat￿

≈∑d∈D(x,y) p(d|x)

X→￿mao, a cat￿

dianzi0 shang1

3

Generate a hypergraph

de2

mao3

S 0,4
S→￿X0 , X0 ￿

X 0,4 the · · · cat

S→￿X0 , X0 ￿

X 0,4 a · · · mat

X→￿X0 de X1 , X1 on X0 ￿

q*(y | x)
X→￿X0 de X1 , X0 ’s X1 ￿

X→￿X0 de X1 , X1 of X0 ￿

X→￿X0 de X1 , X0 X1 ￿

X 0,2 the · · · mat
X→￿dianzi shang, the mat￿

dianzi0 shang1

Decode using q*
on the hypergraph

X 3,4 a · · · cat
X→￿mao, a cat￿

de2

mao3

54

1

S 0,4
S→￿X0 , X0 ￿

X 0,4 the · · · cat

S→￿X0 , X0 ￿

X 0,4 a · · · mat

X→￿X0 de X1 , X1 on X0 ￿

p(d | x)
X→￿X0 de X1 , X0 ’s X1 ￿

X→￿X0 de X1 , X1 of X0 ￿

X→￿X0 de X1 , X0 X1 ￿

X 0,2 the · · · mat

X 3,4 a · · · cat

X→￿dianzi shang, the mat￿

X→￿mao, a cat￿

dianzi0 shang1

2

Generate a hypergraph

de2

mao3

S 0,4
S→￿X0 , X0 ￿

X 0,4 the · · · cat

S→￿X0 , X0 ￿

X 0,4 a · · · mat

X→￿X0 de X1 , X1 on X0 ￿

p(d | x)
X→￿X0 de X1 , X0 ’s X1 ￿

X→￿X0 de X1 , X1 of X0 ￿

X→￿X0 de X1 , X0 X1 ￿

X 0,2 the · · · mat

Estimate a model
q* is an n-gram model
from the hypergraph over output strings.
by minimizing KL

q*(y | x)

X 3,4 a · · · cat

Approximate a hypergraph with a lattice!
X→￿dianzi shang, the mat￿

X→￿mao, a cat￿

dianzi0 shang1

3

de2

mao3

≈∑d∈D(x,y) p(d|x)

S 0,4
S→￿X0 , X0 ￿

X 0,4 the · · · cat

S→￿X0 , X0 ￿

X 0,4 a · · · mat

X→￿X0 de X1 , X1 on X0 ￿

q*(y | x)
X→￿X0 de X1 , X0 ’s X1 ￿

X→￿X0 de X1 , X1 of X0 ￿

X→￿X0 de X1 , X0 X1 ￿

X 0,2 the · · · mat
X→￿dianzi shang, the mat￿

dianzi0 shang1

Decode using q*
on the hypergraph

X 3,4 a · · · cat
X→￿mao, a cat￿

de2

mao3

54

Outline
•
•

Hypergraph as Hypothesis Space
Unsupervised Discriminative Training
‣ minimum imputed risk
‣ contrastive language model estimation

•
•

Variational Decoding
First- and Second-order Expectation Semirings
decoding
(e.g., mbr)

training
(e.g., mert)

atomic inference operations

(e.g., ﬁnding one-best, k-best or expectation,
inference can be exact or approximate)
52

S 0,4
S→￿X0 , X0 ￿

X 0,4 the · · · cat

S→￿X0 , X0 ￿

A semiring framework to compute all of these

X 0,4 a · · · mat

Probabilistic
Hypergraph
X→￿X0 de X1 , X1 on X0 ￿

X→￿X0 de X1 , X0 ’s X1 ￿

X→￿X0 de X1 , X1 of X0 ￿

X→￿X0 de X1 , X0 X1 ￿

X 0,2 the · · · mat
X→￿dianzi shang, the mat￿

dianzi0 shang1

X 3,4 a · · · cat
X→￿mao, a cat￿

de2

mao3

• First-order expectations:

- expectation
- entropy
- expected loss
- cross-entropy
- KL divergence
- feature expectations
- ﬁrst-order gradient of Z

• “Decoding” quantities:
- Viterbi
- K-best
- Counting
- ......

• Second-order expectations:

- expectation over product
- interaction between features
- Hessian matrix of Z
- second-order gradient
descent
- gradient of expectation
- gradient of expected loss
or entropy

53

S 0,4
S→￿X0 , X0 ￿

X 0,4 the · · · cat

S→￿X0 , X0 ￿

A semiring framework to compute all of these

X 0,4 a · · · mat

Probabilistic
Hypergraph
X→￿X0 de X1 , X1 on X0 ￿

X→￿X0 de X1 , X0 ’s X1 ￿

X→￿X0 de X1 , X1 of X0 ￿

X→￿X0 de X1 , X0 X1 ￿

X 0,2 the · · · mat
X→￿dianzi shang, the mat￿

dianzi0 shang1

X 3,4 a · · · cat
X→￿mao, a cat￿

de2

mao3

• First-order expectations:

- expectation
- entropy
- expected loss
- cross-entropy
- KL divergence
- feature expectations
- ﬁrst-order gradient of Z

• “Decoding” quantities:
- Viterbi
- K-best
- Counting
- ......

• Second-order expectations:

- expectation over product
- interaction between features
- Hessian matrix of Z
- second-order gradient
descent
- gradient of expectation
- gradient of expected loss
or entropy

53

S 0,4
S→￿X0 , X0 ￿

X 0,4 the · · · cat

S→￿X0 , X0 ￿

A semiring framework to compute all of these

X 0,4 a · · · mat

• “Decoding” quantities:
Probabilistic
- Viterbi
Hypergraph to compute a quantity:
Recipe
X→￿X0 de X1 , X1 on X0 ￿

X→￿X0 de X1 , X0 ’s X1 ￿

X→￿X0 de X1 , X1 of X0 ￿

X→￿X0 de X1 , X0 X1 ￿

X 0,2 the · · · mat

X→￿dianzi shang, the mat￿

dianzi0 shang1

X 3,4 a · · · cat

X→￿mao, a cat￿

de2

mao3

• First-order expectations:

- expectation
- entropy
- expected loss
- cross-entropy
- KL divergence
- feature expectations
- ﬁrst-order gradient of Z

- K-best
- Counting
- ......

• Second-order expectations:

- expectation over product
- interaction between features
- Hessian matrix of Z
- second-order gradient
descent
- gradient of expectation
- gradient of expected loss
or entropy

53

S 0,4
S→￿X0 , X0 ￿

X 0,4 the · · · cat

S→￿X0 , X0 ￿

A semiring framework to compute all of these

X 0,4 a · · · mat

• “Decoding” quantities:
Probabilistic
- Viterbi
Hypergraph to compute a quantity:
Recipe
X→￿X0 de X1 , X1 on X0 ￿

X→￿X0 de X1 , X0 ’s X1 ￿

X→￿X0 de X1 , X1 of X0 ￿

X→￿X0 de X1 , X0 X1 ￿

X 0,2 the · · · mat

X→￿dianzi shang, the mat￿

dianzi0 shang1

•

- K-best
- Counting
Choose a semiring
- ......
X 3,4 a · · · cat

X→￿mao, a cat￿

de2

mao3

• First-order expectations:

- expectation
- entropy
- expected loss
- cross-entropy
- KL divergence
- feature expectations
- ﬁrst-order gradient of Z

• Second-order expectations:

- expectation over product
- interaction between features
- Hessian matrix of Z
- second-order gradient
descent
- gradient of expectation
- gradient of expected loss
or entropy

53

S 0,4
S→￿X0 , X0 ￿

X 0,4 the · · · cat

S→￿X0 , X0 ￿

A semiring framework to compute all of these

X 0,4 a · · · mat

• “Decoding” quantities:
Probabilistic
- Viterbi
Hypergraph to compute a quantity:
Recipe
X→￿X0 de X1 , X1 on X0 ￿

X→￿X0 de X1 , X0 ’s X1 ￿

X→￿X0 de X1 , X1 of X0 ￿

X→￿X0 de X1 , X0 X1 ￿

- K-best
- Counting
Choose a semiring
- ......

X 0,2 the · · · mat

X→￿dianzi shang, the mat￿

dianzi0 shang1

X 3,4 a · · · cat

•
de
mao
• Speciﬁc a semiring weight for each hyperedge
X→￿mao, a cat￿

2

3

• First-order expectations:

- expectation
- entropy
- expected loss
- cross-entropy
- KL divergence
- feature expectations
- ﬁrst-order gradient of Z

• Second-order expectations:

- expectation over product
- interaction between features
- Hessian matrix of Z
- second-order gradient
descent
- gradient of expectation
- gradient of expected loss
or entropy

53

S 0,4
S→￿X0 , X0 ￿

X 0,4 the · · · cat

S→￿X0 , X0 ￿

A semiring framework to compute all of these

X 0,4 a · · · mat

• “Decoding” quantities:
Probabilistic
- Viterbi
Hypergraph to compute a quantity:
Recipe
X→￿X0 de X1 , X1 on X0 ￿

X→￿X0 de X1 , X0 ’s X1 ￿

X→￿X0 de X1 , X1 of X0 ￿

X→￿X0 de X1 , X0 X1 ￿

- K-best
- Counting
Choose a semiring
- ......

X 0,2 the · · · mat

X 3,4 a · · · cat

•
dianzi shang
de
mao
• Speciﬁc a semiring weight for each hyperedge
• First-order expectations: • algorithm
• Run the inside Second-order expectations:

X→￿dianzi shang, the mat￿

0

1

X→￿mao, a cat￿

2

3

- expectation
- entropy
- expected loss
- cross-entropy
- KL divergence
- feature expectations
- ﬁrst-order gradient of Z

- expectation over product
- interaction between features
- Hessian matrix of Z
- second-order gradient
descent
- gradient of expectation
- gradient of expected loss
or entropy

53

Applications of Expectation Semirings: a Summary
Quantity
ke
kroot
Final
Expectation
￿pe , pe re ￿
￿Z, r￿
r/Z
def
Entropy
re = log pe , so ke = ￿pe , pe log pe ￿
￿Z, r￿
log Z − r/Z
Cross￿qe ￿
￿Zq ￿
log Zq − r/Zp
def
entropy
re = log qe , so ke = ￿pe , pe log qe ￿
￿Zp , r￿
def
Bayes risk
re = Le , so ke = ￿pe , pe Le ￿
￿Z, r￿
r/Z
First-order
￿pe , ∇pe ￿
￿Z, ∇Z￿
∇Z
gradient
Covariance
t
r sT
￿pe , pe re , pe se , pe re se ￿
￿Z, r, s, t￿
− Z2
Z
matrix
Hessian
￿pe , ∇pe , ∇pe , ∇2 pe ￿
￿Z, ∇Z, ∇Z, ∇2 Z￿
∇2 Z
matrix
Gradient of
Z∇r−r∇Z
￿pe , pe re , ∇pe , (∇pe )re + pe (∇re )￿ ￿Z, r, ∇Z, ∇r￿
Z2
expectation
Gradient of
￿pe , pe log pe , ∇pe , (1 + log pe )∇pe ￿ ￿Z, r, ∇Z, ∇r￿ ∇Z − Z∇r−r∇Z
Z
Z2
entropy
Gradient of
Z∇r−r∇Z
￿pe , pe Le , ∇pe , Le ∇pe ￿
￿Z, r, ∇Z, ∇r￿
Z2
risk
54

Inference, Training and Decoding on Hypergraphs

•

Unsupervised Discriminative Training
‣ minimum imputed risk (In Preparation)
‣ contrastive language model estimation (In Preparation)

•

Variational Decoding

•

First- and Second-order Expectation Semirings

(Li et al., ACL 2009)

(Li and Eisner, EMNLP 2009)

55

My Other MT Research

• Training methods (supervised)
•

Discriminative forest reranking with Perceptron

•

Discriminative n-gram language models

(Li and Khudanpur, GALE book chapter 2009)
(Li and Khudanpur, AMTA 2008)

• Algorithms
•
•

•

Oracle extraction from hypergraphs
Efﬁcient intersection between n-gram LM and CFG

(Li and Khudanpur, NAACL 2009)

(Li and Khudanpur, ACL SSST 2008)

Others

•
•

System combination (Smith et al., GALE book chapter 2009)
Unsupervised translation induction for Chinese
abbreviations (Li and Yarowsky, ACL 2008)

56

Research other than MT
• Information extraction
•

Relation extraction between formal and informal
phrases (Li and Yarowsky, EMNLP 2008)

• Spoken dialog management
•

Optimal dialog in consumer-rating systems using
a POMDP (Li et al., SIGDial 2008)

57

Joshua project
•
•

An open-source parsing-based MT toolkit (Li et al. 2009)

•

support Hiero (Chiang, 2007) and SAMT (Venugopal et al., 2007)

Team members

•

Zhifei Li, Chris Callison-Burch, Chris Dyer, Sanjeev Khudanpur, Wren
Thornton, Jonathan Weese, Juri Ganitkevitch, Lane Schwartz, and Omar Zaidan
Bilingual
Data

Translation
Model
Training

Monolingual
English

Unseen
Sentences

Language
Model

MT
Decoder

Optimal
Weights

Translation
Outputs

Only rely on word-aligner and SRI LM!
All the methods presented have been implemented in Joshua!

58

Thank you!
XieXie!
谢谢!

59

60

61

62

Decoding over a hypergraph
S 0,4
S→￿X0 , X0 ￿

X 0,4 the · · · cat

S→￿X0 , X0 ￿

X 0,4 a · · · mat

X→￿X0 de X1 , X1 on X0 ￿

X→￿X0 de X1 , X0 ’s X1 ￿

X→￿X0 de X1 , X1 of X0 ￿

X→￿X0 de X1 , X0 X1 ￿

X 0,2 the · · · mat
X→￿dianzi shang, the mat￿

dianzi0 shang1

X 3,4 a · · · cat
X→￿mao, a cat￿

de2

mao3

63

Decoding over a hypergraph
S 0,4
S→￿X0 , X0 ￿

X 0,4 the · · · cat

S→￿X0 , X0 ￿

X 0,4 a · · · mat

X→￿X0 de X1 , X1 on X0 ￿

X→￿X0 de X1 , X0 ’s X1 ￿

X→￿X0 de X1 , X1 of X0 ￿

X→￿X0 de X1 , X0 X1 ￿

X 0,2 the · · · mat
X→￿dianzi shang, the mat￿

dianzi0 shang1

X 3,4 a · · · cat
X→￿mao, a cat￿

de2

mao3

Given a hypergraph of possible translations
(generated for a given foreign sentence by already-trained model)

63

Decoding over a hypergraph
S 0,4
S→￿X0 , X0 ￿

X 0,4 the · · · cat

S→￿X0 , X0 ￿

X 0,4 a · · · mat

X→￿X0 de X1 , X1 on X0 ￿

X→￿X0 de X1 , X0 ’s X1 ￿

X→￿X0 de X1 , X1 of X0 ￿

X→￿X0 de X1 , X0 X1 ￿

X 0,2 the · · · mat
X→￿dianzi shang, the mat￿

dianzi0 shang1

X 3,4 a · · · cat
X→￿mao, a cat￿

de2

mao3

Given a hypergraph of possible translations
(generated for a given foreign sentence by already-trained model)

Pick a single translation to output
(why not just pick the tree with the highest weight?)

63

Spurious Ambiguity

•

Statistical models in MT exhibit spurious
ambiguity

•

•
•

Many different derivations (e.g., trees or
segmentations) generate the same translation string

Tree-based MT systems

•

derivation tree ambiguity

Regular phrase-based MT systems

•

phrase segmentation ambiguity

64

Spurious Ambiguity in Derivation Trees

65

Spurious Ambiguity in Derivation Trees

machine translation software
jiqi fanyi yuanjian
机器 翻译 软件

65

Spurious Ambiguity in Derivation Trees

machine translation software
jiqi fanyi yuanjian
机器 翻译 软件

S->(S0 S1, S0 S1)
S->(S0 S1, S0 S1)
S->(机器, machine)

S->(翻译, translation) S->(软件, software)

65

Spurious Ambiguity in Derivation Trees

machine translation software
jiqi fanyi yuanjian
机器 翻译 软件

S->(S0 S1, S0 S1)
S->(S0 S1, S0 S1)
S->(机器, machine)

S->(翻译, translation) S->(软件, software)
S->(S0 S1, S0 S1)
S->(S0 S1, S0 S1)

S->(机器, machine)

S->(翻译, translation) S->(软件, software)

65

Spurious Ambiguity in Derivation Trees

machine translation software
jiqi fanyi yuanjian
机器 翻译 软件

S->(S0 S1, S0 S1)
S->(S0 S1, S0 S1)
S->(机器, machine)

S->(翻译, translation) S->(软件, software)
S->(S0 S1, S0 S1)
S->(S0 S1, S0 S1)

S->(机器, machine)

S->(翻译, translation) S->(软件, software)

S->(S0 翻译 S1, S0 translation S1)

S->(机器, machine)

翻译

S->(软件, software)

65

Spurious Ambiguity in Derivation Trees

machine translation software
jiqi fanyi yuanjian
Same output:
机器 翻译 软件

•

S->(S0 S1, S0 S1)

•

S->(S0 S1, S0 S1)
S->(机器, machine)

“machine translation software”
Three different derivation trees

S->(翻译, translation) S->(软件, software)
S->(S0 S1, S0 S1)
S->(S0 S1, S0 S1)

S->(机器, machine)

S->(翻译, translation) S->(软件, software)

S->(S0 翻译 S1, S0 translation S1)

S->(机器, machine)

翻译

S->(软件, software)

65

Spurious Ambiguity in Derivation Trees

machine translation software
jiqi fanyi yuanjian
Same output:
机器 翻译 软件

•

S->(S0 S1, S0 S1)

•

S->(S0 S1, S0 S1)
S->(机器, machine)

“machine translation software”
Three different derivation trees

S->(翻译, translation) S->(软件, software)
S->(S0 S1, S0 S1)

Another translation:
machine transfer software

S->(S0 S1, S0 S1)
S->(机器, machine)

S->(翻译, translation) S->(软件, software)

S->(S0 翻译 S1, S0 translation S1)

S->(机器, machine)

翻译

S->(软件, software)

65

MAP, Viterbi and N-best Approximations

66

MAP, Viterbi and N-best Approximations

•

Exact MAP decoding
y

∗

= arg

max

p(y|x)
y∈Trans(x)
￿
= arg max
p(y, d|x)
y∈Trans(x)

d∈D(x,y)

66

MAP, Viterbi and N-best Approximations

•

Exact MAP decoding
y

∗

= arg

max

p(y|x)
NP-hard (Sima’an 1996)
y∈Trans(x)
￿
= arg max
p(y, d|x)
y∈Trans(x)

d∈D(x,y)

66

MAP, Viterbi and N-best Approximations

•

Exact MAP decoding
y

∗

= arg

max

p(y|x)
NP-hard (Sima’an 1996)
y∈Trans(x)
￿
= arg max
p(y, d|x)
y∈Trans(x)

d∈D(x,y)

• Viterbi approximation
y∗

=

arg

max

max p(y, d|x)

y∈Trans(x) d∈D(x,y)

66

MAP, Viterbi and N-best Approximations

•

Exact MAP decoding
y

∗

= arg

max

p(y|x)
NP-hard (Sima’an 1996)
y∈Trans(x)
￿
= arg max
p(y, d|x)
y∈Trans(x)

d∈D(x,y)

• Viterbi approximation
y∗

=

arg

max

max p(y, d|x)

=

Y(arg max p(y, d|x))

y∈Trans(x) d∈D(x,y)
d∈D(x)

66

MAP, Viterbi and N-best Approximations

•

Exact MAP decoding
y

∗

= arg

max

p(y|x)
NP-hard (Sima’an 1996)
y∈Trans(x)
￿
= arg max
p(y, d|x)
y∈Trans(x)

d∈D(x,y)

• Viterbi approximation
y∗

=

arg

max

max p(y, d|x)

=

Y(arg max p(y, d|x))

y∈Trans(x) d∈D(x,y)
d∈D(x)

66

MAP, Viterbi and N-best Approximations

•

Exact MAP decoding
y

∗

= arg

max

p(y|x)
NP-hard (Sima’an 1996)
y∈Trans(x)
￿
= arg max
p(y, d|x)
y∈Trans(x)

d∈D(x,y)

• Viterbi approximation
y∗

=

arg

max

max p(y, d|x)

=

Y(arg max p(y, d|x))

y∈Trans(x) d∈D(x,y)
d∈D(x)

66

MAP, Viterbi and N-best Approximations

•

Exact MAP decoding
y

∗

= arg

max

p(y|x)
NP-hard (Sima’an 1996)
y∈Trans(x)
￿
= arg max
p(y, d|x)
y∈Trans(x)

d∈D(x,y)

• Viterbi approximation
=

arg

=

y∗

max

max p(y, d|x)

Y(arg max p(y, d|x))

y∈Trans(x) d∈D(x,y)
d∈D(x)

• N-best approximation (crunching) (May and
Knight 2006)

y∗

=

arg

max

y∈Trans(x)

￿

d∈D(x,y)∩ND(x)
66

p(y, d|x)

MAP, Viterbi and N-best Approximations

•

Exact MAP decoding
y

∗

= arg

max

p(y|x)
NP-hard (Sima’an 1996)
y∈Trans(x)
￿
= arg max
p(y, d|x)
y∈Trans(x)

d∈D(x,y)

• Viterbi approximation
=

arg

=

y∗

max

max p(y, d|x)

Y(arg max p(y, d|x))

y∈Trans(x) d∈D(x,y)
d∈D(x)

• N-best approximation (crunching) (May and
Knight 2006)

y∗

=

arg

max

y∈Trans(x)

￿

d∈D(x,y)∩ND(x)
66

p(y, d|x)

MAP, Viterbi and N-best Approximations

•

Exact MAP decoding
y

∗

= arg

max

p(y|x)
NP-hard (Sima’an 1996)
y∈Trans(x)
￿
= arg max
p(y, d|x)
y∈Trans(x)

d∈D(x,y)

• Viterbi approximation
=

arg

=

y∗

max

max p(y, d|x)

Y(arg max p(y, d|x))

y∈Trans(x) d∈D(x,y)
d∈D(x)

• N-best approximation (crunching) (May and
Knight 2006)

y∗

=

arg

max

y∈Trans(x)

￿

d∈D(x,y)∩ND(x)
66

p(y, d|x)

MAP vs. Approximations
translation
string

4-best
MAP Viterbi
crunching

red translation

0.28

0.16

0.16

blue translation

0.28

0.14

0.28

green translation

0.44

0.13

0.13

derivation probability

0.16
0.14
0.14
0.13
0.12
0.11
0.10
0.10

67

MAP vs. Approximations
translation
string

4-best
MAP Viterbi
crunching

red translation

0.28

0.16

0.16

blue translation

0.28

0.14

0.28

green translation

0.44

0.13

0.13

•

derivation probability

0.16
0.14
0.14
0.13
0.12
0.11
0.10
0.10

Exact MAP decoding under spurious ambiguity is intractable on HG

67

MAP vs. Approximations
translation
string

4-best
MAP Viterbi
crunching

red translation

0.28

0.16

0.16

blue translation

0.28

0.14

0.28

green translation

0.44

0.13

derivation probability

0.13

•
•

0.16
0.14
0.14
0.13
0.12
0.11
0.10
0.10

Exact MAP decoding under spurious ambiguity is intractable on HG
Viterbi and crunching are efﬁcient, but ignore most derivations

67

MAP vs. Approximations
translation
string

4-best
MAP Viterbi
crunching

red translation

0.28

0.16

0.16

blue translation

0.28

0.14

0.28

green translation

0.44

0.13

derivation probability

0.13

•
•
•

0.16
0.14
0.14
0.13
0.12
0.11
0.10
0.10

Exact MAP decoding under spurious ambiguity is intractable on HG
Viterbi and crunching are efﬁcient, but ignore most derivations
Our goal: develop an approximation that considers all the
derivations but still allows tractable decoding
67

Variational Decoding

68

Variational Decoding
Decoding using Variational approximation
Decoding using a sentence-speciﬁc
approximate distribution

68

Variational Decoding for MT: an Overview

68

Variational Decoding for MT: an Overview
Sentence-speciﬁc decoding

68

Variational Decoding for MT: an Overview
Sentence-speciﬁc decoding
Three steps:

68

Variational Decoding for MT: an Overview
Sentence-speciﬁc decoding
Three steps:

1

Generate a hypergraph for
the foreign sentence

68

Variational Decoding for MT: an Overview
Sentence-speciﬁc decoding
Three steps:

1

Generate a hypergraph for
the foreign sentence

Foreign
sentence x

68

Variational Decoding for MT: an Overview
Sentence-speciﬁc decoding
Three steps:

1

Generate a hypergraph for
the foreign sentence

Foreign
sentence x

SMT

68

Variational Decoding for MT: an Overview
Sentence-speciﬁc decoding

MAP decoding under P is
intractable

Three steps:

1

Generate a hypergraph for
the foreign sentence

S 0,4
S→￿X0 , X0 ￿

Foreign
sentence x

X 0,4 the · · · cat

SMT

S→￿X0 , X0 ￿

X 0,4 a · · · mat

p(y, d | x)

X→￿X0 de X1 , X1 on X0 ￿

X→￿X0 de X1 , X0 ’s X1 ￿

X→￿X0 de X1 , X0 X1 ￿

X→￿X0 de X1 , X1 of X

p(y | x)

X 0,2 the · · · mat
X→￿dianzi shang, the mat￿

dianzi0 shang1

X 3,4 a · · · cat
X→￿mao, a cat￿

de2

mao3
68

1

S 0,4
S→￿X0 , X0 ￿

X 0,4 the · · · cat

S→￿X0 , X0 ￿

X 0,4 a · · · mat

X→￿X0 de X1 , X1 on X0 ￿

p(d | x)
X→￿X0 de X1 , X0 ’s X1 ￿

X→￿X0 de X1 , X1 of X0 ￿

X→￿X0 de X1 , X0 X1 ￿

X 0,2 the · · · mat
X→￿dianzi shang, the mat￿

dianzi0 shang1

Generate a hypergraph

X 3,4 a · · · cat
X→￿mao, a cat￿

de2

mao3

69

1

S 0,4
S→￿X0 , X0 ￿

X 0,4 the · · · cat

S→￿X0 , X0 ￿

X 0,4 a · · · mat

X→￿X0 de X1 , X1 on X0 ￿

p(d | x)
X→￿X0 de X1 , X0 ’s X1 ￿

X→￿X0 de X1 , X1 of X0 ￿

X→￿X0 de X1 , X0 X1 ￿

X 0,2 the · · · mat
X→￿dianzi shang, the mat￿

dianzi0 shang1

Generate a hypergraph

X 3,4 a · · · cat
X→￿mao, a cat￿

de2

mao3

69

1

S 0,4
S→￿X0 , X0 ￿

X 0,4 the · · · cat

S→￿X0 , X0 ￿

X 0,4 a · · · mat

X→￿X0 de X1 , X1 on X0 ￿

p(d | x)
X→￿X0 de X1 , X0 ’s X1 ￿

X→￿X0 de X1 , X1 of X0 ￿

X→￿X0 de X1 , X0 X1 ￿

X 0,2 the · · · mat
X→￿dianzi shang, the mat￿

dianzi0 shang1

Generate a hypergraph

X 3,4 a · · · cat
X→￿mao, a cat￿

de2

mao3

2

69

1

S 0,4
S→￿X0 , X0 ￿

X 0,4 the · · · cat

S→￿X0 , X0 ￿

X 0,4 a · · · mat

X→￿X0 de X1 , X1 on X0 ￿

p(d | x)
X→￿X0 de X1 , X0 ’s X1 ￿

X→￿X0 de X1 , X1 of X0 ￿

X→￿X0 de X1 , X0 X1 ￿

X 0,2 the · · · mat

X 3,4 a · · · cat

X→￿dianzi shang, the mat￿

X→￿mao, a cat￿

dianzi0 shang1

2

Generate a hypergraph

de2

mao3

S 0,4
S→￿X0 , X0 ￿

X 0,4 the · · · cat

S→￿X0 , X0 ￿

X 0,4 a · · · mat

X→￿X0 de X1 , X1 on X0 ￿

p(d | x)
X→￿X0 de X1 , X0 ’s X1 ￿

X→￿X0 de X1 , X1 of X0 ￿

X→￿X0 de X1 , X0 X1 ￿

X 0,2 the · · · mat
X→￿dianzi shang, the mat￿

dianzi0 shang1

X 3,4 a · · · cat
X→￿mao, a cat￿

de2

mao3

69

1

S 0,4
S→￿X0 , X0 ￿

X 0,4 the · · · cat

S→￿X0 , X0 ￿

X 0,4 a · · · mat

X→￿X0 de X1 , X1 on X0 ￿

p(d | x)
X→￿X0 de X1 , X0 ’s X1 ￿

X→￿X0 de X1 , X1 of X0 ￿

X→￿X0 de X1 , X0 X1 ￿

X 0,2 the · · · mat

X 3,4 a · · · cat

X→￿dianzi shang, the mat￿

X→￿mao, a cat￿

dianzi0 shang1

2

de2

mao3

S 0,4
S→￿X0 , X0 ￿

X 0,4 the · · · cat

S→￿X0 , X0 ￿

X 0,4 a · · · mat

X→￿X0 de X1 , X1 on X0 ￿

p(d | x)
X→￿X0 de X1 , X0 ’s X1 ￿

X→￿X0 de X1 , X1 of X0 ￿

X→￿X0 de X1 , X0 X1 ￿

X 0,2 the · · · mat
X→￿dianzi shang, the mat￿

dianzi0 shang1

Generate a hypergraph

Estimate a model
from the hypergraph

q*(y | x)

X 3,4 a · · · cat
X→￿mao, a cat￿

de2

mao3

69

1

S 0,4
S→￿X0 , X0 ￿

X 0,4 the · · · cat

S→￿X0 , X0 ￿

X 0,4 a · · · mat

X→￿X0 de X1 , X1 on X0 ￿

p(d | x)
X→￿X0 de X1 , X0 ’s X1 ￿

X→￿X0 de X1 , X1 of X0 ￿

X→￿X0 de X1 , X0 X1 ￿

X 0,2 the · · · mat

X 3,4 a · · · cat

X→￿dianzi shang, the mat￿

X→￿mao, a cat￿

dianzi0 shang1

2

de2

mao3

S 0,4
S→￿X0 , X0 ￿

X 0,4 the · · · cat

S→￿X0 , X0 ￿

X 0,4 a · · · mat

X→￿X0 de X1 , X1 on X0 ￿

p(d | x)
X→￿X0 de X1 , X0 ’s X1 ￿

X→￿X0 de X1 , X1 of X0 ￿

X→￿X0 de X1 , X0 X1 ￿

X 0,2 the · · · mat
X→￿dianzi shang, the mat￿

dianzi0 shang1

Generate a hypergraph

Estimate a model
from the hypergraph

q* is an n-gram model
over output strings.

q*(y | x)

X 3,4 a · · · cat
X→￿mao, a cat￿

de2

mao3

69

1

S 0,4
S→￿X0 , X0 ￿

X 0,4 the · · · cat

S→￿X0 , X0 ￿

X 0,4 a · · · mat

X→￿X0 de X1 , X1 on X0 ￿

p(d | x)
X→￿X0 de X1 , X0 ’s X1 ￿

X→￿X0 de X1 , X1 of X0 ￿

X→￿X0 de X1 , X0 X1 ￿

X 0,2 the · · · mat

X 3,4 a · · · cat

X→￿dianzi shang, the mat￿

X→￿mao, a cat￿

dianzi0 shang1

2

de2

mao3

S 0,4
S→￿X0 , X0 ￿

X 0,4 the · · · cat

S→￿X0 , X0 ￿

X 0,4 a · · · mat

X→￿X0 de X1 , X1 on X0 ￿

p(d | x)
X→￿X0 de X1 , X0 ’s X1 ￿

X→￿X0 de X1 , X1 of X0 ￿

X→￿X0 de X1 , X0 X1 ￿

X 0,2 the · · · mat
X→￿dianzi shang, the mat￿

dianzi0 shang1

Generate a hypergraph

X 3,4 a · · · cat
X→￿mao, a cat￿

de2

mao3

Estimate a model
from the hypergraph

q* is an n-gram model
over output strings.

q*(y | x)
≈∑d∈D(x,y) p(d|x)

69

1

S 0,4
S→￿X0 , X0 ￿

X 0,4 the · · · cat

S→￿X0 , X0 ￿

X 0,4 a · · · mat

X→￿X0 de X1 , X1 on X0 ￿

p(d | x)
X→￿X0 de X1 , X0 ’s X1 ￿

X→￿X0 de X1 , X1 of X0 ￿

X→￿X0 de X1 , X0 X1 ￿

X 0,2 the · · · mat

X 3,4 a · · · cat

X→￿dianzi shang, the mat￿

X→￿mao, a cat￿

dianzi0 shang1

2

de2

mao3

S 0,4
S→￿X0 , X0 ￿

X 0,4 the · · · cat

S→￿X0 , X0 ￿

X 0,4 a · · · mat

X→￿X0 de X1 , X1 on X0 ￿

p(d | x)
X→￿X0 de X1 , X0 ’s X1 ￿

X→￿X0 de X1 , X1 of X0 ￿

X→￿X0 de X1 , X0 X1 ￿

X 0,2 the · · · mat
X→￿dianzi shang, the mat￿

dianzi0 shang1

Generate a hypergraph

X 3,4 a · · · cat
X→￿mao, a cat￿

de2

mao3

Estimate a model
from the hypergraph

q* is an n-gram model
over output strings.

q*(y | x)
≈∑d∈D(x,y) p(d|x)

3

69

1

S 0,4
S→￿X0 , X0 ￿

X 0,4 the · · · cat

S→￿X0 , X0 ￿

X 0,4 a · · · mat

X→￿X0 de X1 , X1 on X0 ￿

p(d | x)
X→￿X0 de X1 , X0 ’s X1 ￿

X→￿X0 de X1 , X1 of X0 ￿

X→￿X0 de X1 , X0 X1 ￿

X 0,2 the · · · mat

X 3,4 a · · · cat

X→￿dianzi shang, the mat￿

X→￿mao, a cat￿

dianzi0 shang1

2

de2

mao3

S 0,4
S→￿X0 , X0 ￿

X 0,4 the · · · cat

S→￿X0 , X0 ￿

X 0,4 a · · · mat

X→￿X0 de X1 , X1 on X0 ￿

p(d | x)
X→￿X0 de X1 , X0 ’s X1 ￿

X→￿X0 de X1 , X1 of X0 ￿

X→￿X0 de X1 , X0 X1 ￿

X 0,2 the · · · mat

Estimate a model
from the hypergraph

X 3,4 a · · · cat

X→￿dianzi shang, the mat￿

de2

q* is an n-gram model
over output strings.

q*(y | x)
≈∑d∈D(x,y) p(d|x)

X→￿mao, a cat￿

dianzi0 shang1

3

Generate a hypergraph

mao3

S 0,4
S→￿X0 , X0 ￿

X 0,4 the · · · cat

S→￿X0 , X0 ￿

X 0,4 a · · · mat

X→￿X0 de X1 , X1 on X0 ￿

q*(y | x)
X→￿X0 de X1 , X0 ’s X1 ￿

X→￿X0 de X1 , X1 of X0 ￿

X→￿X0 de X1 , X0 X1 ￿

X 0,2 the · · · mat
X→￿dianzi shang, the mat￿

dianzi0 shang1

Decode using q*
on the hypergraph

X 3,4 a · · · cat
X→￿mao, a cat￿

de2

mao3

69

Variational Inference

71

•

Variational Inference
We want to do inference under p, but it is intractable

71

•

Variational Inference
We want to do inference under p, but it is intractable
y

∗

=

arg max p(y|x)
y

71

•

Variational Inference
We want to do inference under p, but it is intractable
y

•

∗

=

arg max p(y|x)
y

Instead, we derive a simpler distribution q*

71

•

Variational Inference
We want to do inference under p, but it is intractable
y

•

∗

=

arg max p(y|x)
y

Instead, we derive a simpler distribution q*
q

∗

=

arg min KL(p||q)
q∈Q

71

•

Variational Inference
We want to do inference under p, but it is intractable
y

•

=

arg max p(y|x)
y

Instead, we derive a simpler distribution q*
q

•

∗

∗

=

arg min KL(p||q)
q∈Q

Then, we will use q* as a surrogate for p in inference

71

•

Variational Inference
We want to do inference under p, but it is intractable
y

•

=

arg max p(y|x)
y

Instead, we derive a simpler distribution q*
q

•

∗

∗

=

arg min KL(p||q)
q∈Q

Then, we will use q* as a surrogate for p in inference
y

∗

=

arg max q (y | x)
y

∗

71

•

Variational Inference
We want to do inference under p, but it is intractable
y

•

=

arg max p(y|x)
y

Instead, we derive a simpler distribution q*
q

•

∗

∗

=

arg min KL(p||q)

P

q∈Q

Then, we will use q* as a surrogate for p in inference
y

∗

=

arg max q (y | x)
y

∗

71

•

Variational Inference
We want to do inference under p, but it is intractable
y

•

=

arg max p(y|x)
y

Instead, we derive a simpler distribution q*
q

•

∗

∗

=

arg min KL(p||q)

P

p

q∈Q

Then, we will use q* as a surrogate for p in inference
y

∗

=

arg max q (y | x)
y

∗

71

•

Variational Inference
We want to do inference under p, but it is intractable
y

•

∗

=

arg max p(y|x)
y

Instead, we derive a simpler distribution q*
q

∗

=

arg min KL(p||q)

P

p

q∈Q

Q

•

Then, we will use q* as a surrogate for p in inference
y

∗

=

arg max q (y | x)
y

∗

71

•

Variational Inference
We want to do inference under p, but it is intractable
y

•

∗

=

arg max p(y|x)
y

Instead, we derive a simpler distribution q*
q

∗

=

arg min KL(p||q)

P

p

q∈Q

Q

•

Then, we will use q* as a surrogate for p in inference
y

∗

=

arg max q (y | x)
y

∗

71

•

Variational Inference
We want to do inference under p, but it is intractable
y

•

∗

=

arg max p(y|x)
y

Instead, we derive a simpler distribution q*
q

∗

=

arg min KL(p||q)

P

p

q∈Q

Q

•

q*

Then, we will use q* as a surrogate for p in inference
y

∗

=

arg max q (y | x)
y

∗

71

•

Variational Approximation

q*: an approximation having minimum distance to p
q∗

=

arg min KL(p||q)
q∈Q

a family of distributions

72

•

Variational Approximation

q*: an approximation having minimum distance to p
q∗

=

=

arg min KL(p||q)
q∈Q

arg min
q∈Q

￿

y∈Trans(x)

a family of distributions

p
plog
q

72

•

Variational Approximation

q*: an approximation having minimum distance to p
q∗

=

=

=

arg min KL(p||q)
q∈Q

arg min
q∈Q

arg min
q∈Q

￿

a family of distributions

y∈Trans(x)

￿

y∈Trans(x)

p
plog
q

(plogp − plogq)

72

•

Variational Approximation

q*: an approximation having minimum distance to p
q∗

=

=

=

arg min KL(p||q)
q∈Q

arg min
q∈Q

arg min
q∈Q

￿

a family of distributions

y∈Trans(x)

￿

y∈Trans(x)

p
plog
q

(plogp − plogq)

constant

72

•

Variational Approximation

q*: an approximation having minimum distance to p
q∗

=

=

=
=

arg min KL(p||q)
q∈Q

￿

arg min
q∈Q

y∈Trans(x)

arg min
q∈Q

￿

y∈Trans(x)

arg max
q∈Q

a family of distributions

￿

p
plog
q

(plogp − plogq)
plogq

constant

y∈Trans(x)

72

•

Variational Approximation

q*: an approximation having minimum distance to p
q∗

=

=

=
=

•

arg min KL(p||q)
q∈Q

￿

arg min
q∈Q

y∈Trans(x)

arg min
q∈Q

Three questions

￿

y∈Trans(x)

arg max
q∈Q

a family of distributions

￿

p
plog
q

(plogp − plogq)
plogq

constant

y∈Trans(x)

72

Variational Approximation

•

q*: an approximation having minimum distance to p
q∗

=

=

=
=

•

arg min KL(p||q)
q∈Q

arg min
q∈Q

y∈Trans(x)

arg min
q∈Q

q∈Q

￿

y∈Trans(x)

arg max

Three questions

•

￿

a family of distributions

￿

p
plog
q

(plogp − plogq)
plogq

constant

y∈Trans(x)

how to parameterize q?

72

Variational Approximation

•

q*: an approximation having minimum distance to p
q∗

=

=

=
=

•

arg min KL(p||q)
q∈Q

arg min
q∈Q

y∈Trans(x)

￿

arg min
q∈Q

y∈Trans(x)

arg max

Three questions

•
•

￿

q∈Q

a family of distributions

￿

p
plog
q

(plogp − plogq)
plogq

constant

y∈Trans(x)

how to parameterize q?
how to estimate q*?

72

Variational Approximation

•

q*: an approximation having minimum distance to p
q∗

=

=

=
=

•

arg min KL(p||q)
q∈Q

arg min
q∈Q

y∈Trans(x)

￿

arg min
q∈Q

y∈Trans(x)

arg max

Three questions

•
•
•

￿

q∈Q

a family of distributions

￿

p
plog
q

(plogp − plogq)
plogq

constant

y∈Trans(x)

how to parameterize q?
how to estimate q*?
how to use q* for decoding?
72

Variational Approximation

•

q*: an approximation having minimum distance to p
q∗

=

=

=
=

•

arg min KL(p||q)
q∈Q

arg min
q∈Q

y∈Trans(x)

￿

arg min
q∈Q

y∈Trans(x)

arg max

Three questions

•
•
•

￿

q∈Q

a family of distributions

￿

p
plog
q

(plogp − plogq)
plogq

constant

y∈Trans(x)

how to parameterize q?

an n-gram model

how to estimate q*?
how to use q* for decoding?
72

Variational Approximation

•

q*: an approximation having minimum distance to p
q∗

=

=

=
=

•

arg min KL(p||q)
q∈Q

arg min
q∈Q

y∈Trans(x)

￿

arg min
q∈Q

y∈Trans(x)

arg max

Three questions

•
•
•

￿

q∈Q

a family of distributions

￿

p
plog
q

(plogp − plogq)
plogq

constant

y∈Trans(x)

how to parameterize q?

an n-gram model

how to estimate q*?

compute expected n-gram
counts and normalize

how to use q* for decoding?
72

Variational Approximation

•

q*: an approximation having minimum distance to p
q∗

=

=

=
=

•

arg min KL(p||q)
q∈Q

arg min
q∈Q

y∈Trans(x)

￿

arg min
q∈Q

y∈Trans(x)

arg max

Three questions

•
•
•

￿

q∈Q

a family of distributions

￿

p
plog
q

(plogp − plogq)
plogq

constant

y∈Trans(x)

how to parameterize q?

an n-gram model

how to estimate q*?

compute expected n-gram
counts and normalize

how to use q* for decoding? score the hypergraph
with the n-gram model

72

KL divergences under different variational models
q

∗

=

arg min KL(p||q) = H(p, q) − H(p)
q∈Q

73

KL divergences under different variational models
q

∗

=

Measure
bits/word
MT’04
MT’05

arg min KL(p||q) = H(p, q) − H(p)
q∈Q

H(p)
1.36
1.37

∗
q1

0.97
0.94

73

KL(p||·)
∗
∗
q2
q3
0.32 0.21
0.32 0.21

∗
q4

0.17
0.17

KL divergences under different variational models
q

∗

=

Measure
bits/word
MT’04
MT’05

arg min KL(p||q) = H(p, q) − H(p)
q∈Q

H(p)
1.36
1.37

∗
q1

0.97
0.94

KL(p||·)
∗
∗
q2
q3
0.32 0.21
0.32 0.21

∗
q4

0.17
0.17

• Larger n ==> better approximation q_n ==>
smaller KL divergence from p

• The reduction of KL divergence happens
mostly when switching from unigram to
bigram
73

BLEU Results on Chinese-English
NIST MT 2004 Tasks
(Kumar and Byrne, 2004)
(May and Knight, 2006)
New!

•

Decoding scheme
Viterbi
MBR (K=1000)
Crunching (N =10000)
Crunching+MBR (N =10000)
Variational (1to4gram+wp+vt)

BLEU
35.4
35.8
35.7
35.8
36.6

variational decoding improves over Viterbi, MBR, and crunching

74

•

Variational Inference
We want to do inference under p, but it is intractable
y

•

∗

=

arg max p(y|x)
y

Instead, we derive a simpler distribution q*
q

∗

=

arg min KL(p||q)

P

p

q∈Q

Q

•

q*

Then, we will use q* as a surrogate for p in inference
y

∗

=

arg max q (y | x)
y

∗

75

Variational Inference

• We want to do inference under p, but it is intractable
intractable
y

•

∗

=

arg max p(y|x)
y

Instead, we derive a simpler distribution q*
q

∗

=

arg min KL(p||q)

P

p

q∈Q

Q

•

q*

Then, we will use q* as a surrogate for p in inference
y

∗

=

arg max q (y | x)
y

∗

75

Variational Inference

• We want to do inference under p, but it is intractable
intractable
y

•

∗

=

arg max p(y|x)
y

Instead, we derive a simpler distribution q*

tractable

q

∗

=

arg min KL(p||q)

P

p

q∈Q

Q

q*

•

Then, we will use q* as a surrogate for p in inference
tractable ∗
∗
y

=

arg max q (y | x)
y

75

Outline
•
•

Hypergraph as Hypothesis Space
Unsupervised Discriminative Training
‣ minimum imputed risk
‣ contrastive language model estimation

•
•

Variational Decoding
First- and Second-order Expectation Semirings
decoding
(e.g., mbr)

training
(e.g., mert)

atomic inference operations

(e.g., ﬁnding one-best, k-best or expectation,
inference can be exact or approximate)
76

S 0,4
S→￿X0 , X0 ￿

X 0,4 the · · · cat

S→￿X0 , X0 ￿

A semiring framework to compute all of these

X 0,4 a · · · mat

Probabilistic
Hypergraph
X→￿X0 de X1 , X1 on X0 ￿

X→￿X0 de X1 , X0 ’s X1 ￿

X→￿X0 de X1 , X1 of X0 ￿

X→￿X0 de X1 , X0 X1 ￿

X 0,2 the · · · mat
X→￿dianzi shang, the mat￿

dianzi0 shang1

X 3,4 a · · · cat
X→￿mao, a cat￿

de2

mao3

• First-order quantities:

- expectation
- entropy
- Bayes risk
- cross-entropy
- KL divergence
- feature expectations
- ﬁrst-order gradient of Z

• “decoding” quantities:
- Viterbi
- K-best
- Counting
- ......

• Second-order quantities:

- expectation over product
- interaction between features
- Hessian matrix of Z
- second-order gradient
descent
- gradient of expectation
- gradient of entropy or
Bayes risk

77

Compute Quantities on a Hypergraph: a Recipe

•
v5

v5
e7

e7

e8

e3

e8

v4

v4

v3

v3
e4

Semiring-weighted inside algorithm
• three steps:

e4
e5

v1
e1
dianzi0 shang1

e5

e6

e6

v2

v2

e2

e2

de2

mao3

78

Compute Quantities on a Hypergraph: a Recipe

•
v5

v5
e7

e7

e8

e4

e3

e8

v4

v4

v3

v3

Semiring-weighted inside algorithm
• three steps:
‣ choose a semiring

e4
e5

v1
e1
dianzi0 shang1

e5

e6

e6

v2

v2

e2

e2

de2

mao3

78

Compute Quantities on a Hypergraph: a Recipe

•
v5

v5
e7

e7

e8

e4

e3

e8

v4

v4

v3

v3

Semiring-weighted inside algorithm
• three steps:
‣ choose a semiring

e4
e5

v1
e1
dianzi0 shang1

e5

e6

e6

v2

v2

e2

e2

de2

mao3

‣ specify a weight for each hyperedge

78

Compute Quantities on a Hypergraph: a Recipe

•
v5

v5
e7

e7

e8

e4

e3

e8

v4

v4

v3

v3

Semiring-weighted inside algorithm
• three steps:
‣ choose a semiring

e4
e5

v1
e1
dianzi0 shang1

e5

e6

e6

v2

v2

e2

e2

de2

mao3

‣ specify a weight for each hyperedge
‣ run the inside algorithm
78

Compute Quantities on a Hypergraph: a Recipe

•
v5

v5
e7

e7

e8

e4

e3

e8

v4

v4

v3

v3

Semiring-weighted inside algorithm
• three steps:
‣ choose a semiring

e4
e5

v1
e1
dianzi0 shang1

e5

e6

￿K, ⊕, ⊗￿

e6

v2

v2

e2

a set with plus and times operations

e2

de2

mao3

‣ specify a weight for each hyperedge
‣ run the inside algorithm
78

Compute Quantities on a Hypergraph: a Recipe

•
v5

v5
e7

e7

e8

e4

e3

e8

v4

v4

v3

v3

Semiring-weighted inside algorithm
• three steps:
‣ choose a semiring

e4
e5

v1
e1
dianzi0 shang1

e5

e6

￿K, ⊕, ⊗￿

e6

v2

v2

e2

e2

de2

a set with plus and times operations
e.g., integer numbers with regular + and ×

mao3

‣ specify a weight for each hyperedge
‣ run the inside algorithm
78

Compute Quantities on a Hypergraph: a Recipe

•
v5

v5
e7

e7

e8

e4

e3

e8

v4

v4

v3

v3

Semiring-weighted inside algorithm
• three steps:
‣ choose a semiring

e4
e5

v1
e1
dianzi0 shang1

e5

e6

￿K, ⊕, ⊗￿

e6

v2

v2

e2

e2

de2

a set with plus and times operations
e.g., integer numbers with regular + and ×

mao3

‣ specify a weight for each hyperedge
each weight is a semiring member

‣ run the inside algorithm
78

Compute Quantities on a Hypergraph: a Recipe

•
v5

v5
e7

e7

e8

e4

e3

e8

v4

v4

v3

v3

Semiring-weighted inside algorithm
• three steps:
‣ choose a semiring

e4
e5

v1
e1
dianzi0 shang1

e5

e6

￿K, ⊕, ⊗￿

e6

v2

v2

e2

e2

de2

a set with plus and times operations
e.g., integer numbers with regular + and ×

mao3

‣ specify a weight for each hyperedge
each weight is a semiring member

‣ run the inside algorithm
complexity is O(hypergraph size)
78

Semirings
•

“Decoding” time semirings

•

•

counting,Viterbi, K-best, etc.

“Training” time semirings

•
•

•

(Goodman, 1999)

ﬁrst-order expectation semirings (Eisner, 2002)
second-order expectation semirings (new)

Applications of the Semirings (new)

•

entropy, risk, gradient of them, and many more

79

v5

v5
e7

e4

e3

v1

e4
e5

v1

e1

e8

v4

v4

v3

v3
e3

e7

e8

e1
dianzi0 shang1

e5

e6

e6

v2

v2

e2

e2

de2

mao3
80

v5

v5
e7

e4

e3

v1

e4
e5

v1

e1

e8

v4

v4

v3

v3
e3

e7

e8

How many trees?

e1
dianzi0 shang1

e5

e6

e6

v2

v2

e2

e2

de2

mao3
80

v5

v5
e7

e4

e3

v1

e4
e5

v1

e1

e8

e1
dianzi0 shang1

four☺

v4

v4

v3

v3
e3

e7

e8

How many trees?

e5

e6

e6

v2

v2

e2

e2

de2

mao3
80

v5

v5
e7

e4

e3

v1

e4
e5

v1

e1

e8

e1
dianzi0 shang1

four☺

v4

v4

v3

v3
e3

e7

e8

How many trees?

e5

e6

e6

compute it
use a semiring?

v2

v2

e2

e2

de2

mao3
80

e

Compute the Number of Derivation Trees
Three steps:
v5

v5
e7

e7

e8

e4

e3

v4

v4

v3

v3

e8

e4
e5

v1
e1
dianzi0 shang1

e5

e6

e6

v2

v2

e2

e2

de2

mao3

81

e

Compute the Number of Derivation Trees
Three steps:
‣ choose a semiring

v5

v5
e7

e7

e8

e4

e3

v4

v4

v3

v3

e8

e4
e5

v1
e1
dianzi0 shang1

e5

e6

e6

v2

v2

e2

e2

de2

mao3

81

e

Compute the Number of Derivation Trees
Three steps:
‣ choose a semiring

counting semiring:
e7
ordinary integers with
v3
regular + and x

v5

v5

e4

e7

e8

v4

v4

v3
e3

e8

e4
e5

v1
e1
dianzi0 shang1

e5

e6

e6

v2

v2

e2

e2

de2

mao3

81

e

Compute the Number of Derivation Trees
Three steps:
‣ choose a semiring

counting semiring:
e7
ordinary integers with
v3
regular + and x

‣ specify a weight for each

v5

v5

e4

e8

v4

v4

v3
e3

hyperedge

e7

e8

e4
e5

v1
e1
dianzi0 shang1

e5

e6

e6

v2

v2

e2

e2

de2

mao3

81

e

Compute the Number of Derivation Trees
Three steps:
‣ choose a semiring

counting semiring:
e7
ordinary integers with
v3
regular + and x

‣ specify a weight for each

v5

e4

v5
e8 1 e7

1

hyperedge

1
v4

v4

v3
e3

e8

e4
1
e5

v1
e1 1
dianzi0 shang1

1
e5

e6

1

e6

v2

v2

e2

1 e2

de2

mao3

81

e

Compute the Number of Derivation Trees
Three steps:
‣ choose a semiring

counting semiring:
e7
ordinary integers with
v3
regular + and x

‣ specify a weight for each

v5

e4

e8 1 e7

1

e8

1
v4

v4

v3
e3

hyperedge

‣ run the inside algorithm

v5

e4
1
e5

v1
e1 1
dianzi0 shang1

1
e5

e6

1

e6

v2

v2

e2

1 e2

de2

mao3

81

e

v5
e7

v5
e8 1 e7

e4

Bottom-up
process in
computing the
number of trees

e3

1

1
v4

v4

v3

v3

e8

e4
1
e5

v1
e1 1
dianzi0 shang1

1
e5

e6

1

e6

v2

v2

e2

1 e2

de2

mao3

82

e

k(v1)= k(e1)

v5
e7

v5
e8 1 e7

e4

Bottom-up
process in
computing the
number of trees

e3

1
v1

1
v4

v4

v3

v3

e8

e4
1
e5

k(v1)=1

e1 1
dianzi0 shang1

1
e5

e6

1

e6

v2

v2

e2

1 e2

de2

mao3

82

e

k(v1)= k(e1)

k(v2)= k(e2)

v5
e7

v5
e8 1 e7

e4

Bottom-up
process in
computing the
number of trees

e3

1
v1

1
v4

v4

v3

v3

e8

e4
1
e5

k(v1)=1

e1 1
dianzi0 shang1

1
e5

e6

v2
e2
de2

1

e6

v2 k(v2)=1

1 e2
mao3

82

e

k(v1)= k(e1)
k(v3)= k(e3)

k(v2)= k(e2)

⊗ k(v ) ⊗ k(v ) ⊕ k(e ) ⊗ k(v )⊗ k(v )
1

2

v5
e7

4

e8 1 e7

Bottom-up
process in
computing the
number of trees

e3

1
v1

2

v5
e8

1
v4

v4

k(v
v3 3)=2 v3
e4

1

e4
1
e5

k(v1)=1

e1 1
dianzi0 shang1

1
e5

e6

v2
e2
de2

1

e6

v2 k(v2)=1

1 e2
mao3

82

e

k(v1)= k(e1)

k(v2)= k(e2)

⊗ k(v ) ⊗ k(v ) ⊕ k(e ) ⊗ k(v )⊗ k(v )
k(v )= k(e ) ⊗ k(v ) ⊗ k(v ) ⊕ k(e ) ⊗ k(v )⊗ k(v )

k(v3)= k(e3)
4

5

1

2

4

1

2

1

2

6

1

2

v5
e7

v5
e8 1 e7

Bottom-up
process in
computing the
number of trees

e3

1
v1

1

e4
1
e5

k(v1)=1

e1 1
dianzi0 shang1

k(v4)=2

v4

v4

k(v
v3 3)=2 v3
e4

e8

1
e5

e6

v2
e2
de2

1

e6

v2 k(v2)=1

1 e2
mao3

82

e

k(v1)= k(e1)

k(v2)= k(e2)

⊗ k(v ) ⊗ k(v ) ⊕ k(e ) ⊗ k(v )⊗ k(v )
k(v )= k(e ) ⊗ k(v ) ⊗ k(v ) ⊕ k(e ) ⊗ k(v )⊗ k(v )
k(v )= k(e ) ⊗ k(v ) ⊕ k(e ) ⊗ k(v )

k(v3)= k(e3)

1

2

4

1

2

2

6

1

2

4

5

1

5

7

3

8

v5

e7

4

e8 1 e7

Bottom-up
process in
computing the
number of trees

e3

1
v1

e8

1

e4
1
e5

k(v1)=1

e1 1
dianzi0 shang1

k(v4)=2

v4

v4

k(v
v3 3)=2 v3
e4

k(v5)=4

v5

1
e5

e6

v2
e2
de2

1

e6

v2 k(v2)=1

1 e2
mao3

82

e

k(v1)= k(e1)

k(v2)= k(e2)

⊗ k(v ) ⊗ k(v ) ⊕ k(e ) ⊗ k(v )⊗ k(v )
k(v )= k(e ) ⊗ k(v ) ⊗ k(v ) ⊕ k(e ) ⊗ k(v )⊗ k(v )
k(v )= k(e ) ⊗ k(v ) ⊕ k(e ) ⊗ k(v )

k(v3)= k(e3)

1

2

4

1

2

2

6

1

2

4

5

1

5

7

3

8

v5

e7

4

e8 1 e7

Bottom-up
process in
computing the
number of trees

e3

1
v1

e8

1

e4
1
e5

k(v1)=1

e1 1
dianzi0 shang1

k(v4)=2

v4

v4

k(v
v3 3)=2 v3
e4

k(v5)=4

v5

1
e5

e6

v2
e2
de2

1

e6

v2 k(v2)=1

1 e2
mao3

82

S 0,4
S→￿X0 , X0 ￿

S→￿X0 , X0 ￿

X 0,4 a · · · mat

X 0,4 the · · · cat

X→￿X0 de X1 , X1 on X0 ￿

X→￿X0 de X1 , X0 ’s X1 ￿

X→￿X0 de X1 , X1 of X0 ￿

X→￿X0 de X1 , X0 X1 ￿

X 0,2 the · · · mat

X 3,4 a · · · cat

X→￿dianzi shang, the mat￿

X→￿mao, a cat￿

dianzi0 shang1

de2

mao3

p=3/8
S→￿X0 , X0 ￿

X→￿dianzi shang, the mat￿

dianzi0 shang1

S→￿X0 , X0 ￿

X→￿dianzi shang, the mat￿

dianzi0 shang1

X→￿mao, a cat￿

de2

mao3

a cat on the mat

X→￿dianzi shang, the mat￿

X→￿mao, a cat￿

de2

mao3

the mat a cat

X→￿mao, a cat￿

de2

mao3

the mat ’s a cat

X→￿X0 de X1 , X1 on X0 ￿

X→￿X0 de X1 , X0 X1 ￿

dianzi0 shang1

X→￿X0 de X1 , X0 ’s X1 ￿

p=1/8

p=2/8

S→￿X0 , X0 ￿

p=2/8

S→￿X0 , X0 ￿

X→￿X0 de X1 , X1 of X0 ￿
X→￿dianzi shang, the mat￿

dianzi0 shang1

X→￿mao, a cat￿

de2

mao3

a cat of the mat

83

expected translation length?

S 0,4
S→￿X0 , X0 ￿

S→￿X0 , X0 ￿

X 0,4 a · · · mat

X 0,4 the · · · cat

X→￿X0 de X1 , X1 on X0 ￿

X→￿X0 de X1 , X0 ’s X1 ￿

X→￿X0 de X1 , X1 of X0 ￿

X→￿X0 de X1 , X0 X1 ￿

X 0,2 the · · · mat

X 3,4 a · · · cat

X→￿dianzi shang, the mat￿

X→￿mao, a cat￿

dianzi0 shang1

de2

mao3

p=3/8
S→￿X0 , X0 ￿

X→￿dianzi shang, the mat￿

dianzi0 shang1

S→￿X0 , X0 ￿

X→￿dianzi shang, the mat￿

dianzi0 shang1

X→￿mao, a cat￿

de2

mao3

a cat on the mat

X→￿dianzi shang, the mat￿

X→￿mao, a cat￿

de2

mao3

the mat a cat

X→￿mao, a cat￿

de2

mao3

the mat ’s a cat

X→￿X0 de X1 , X1 on X0 ￿

X→￿X0 de X1 , X0 X1 ￿

dianzi0 shang1

X→￿X0 de X1 , X0 ’s X1 ￿

p=1/8

p=2/8

S→￿X0 , X0 ￿

p=2/8

S→￿X0 , X0 ￿

X→￿X0 de X1 , X1 of X0 ￿
X→￿dianzi shang, the mat￿

dianzi0 shang1

X→￿mao, a cat￿

de2

mao3

a cat of the mat

83

expected translation length?

S 0,4
S→￿X0 , X0 ￿

S→￿X0 , X0 ￿

X 0,4 a · · · mat

X 0,4 the · · · cat

X→￿X0 de X1 , X1 on X0 ￿

X→￿X0 de X1 , X0 ’s X1 ￿

X→￿X0 de X1 , X1 of X0 ￿

X→￿X0 de X1 , X0 X1 ￿

X 0,2 the · · · mat

X 3,4 a · · · cat

X→￿dianzi shang, the mat￿

X→￿mao, a cat￿

dianzi0 shang1

de2

mao3

X→￿dianzi shang, the mat￿

dianzi0 shang1

dianzi0 shang1

X→￿mao, a cat￿

de2

mao3

a cat on the mat 5

X→￿mao, a cat￿

de2

mao3

the mat a cat

4

X→￿mao, a cat￿

de2

mao3

p=2/8

S→￿X0 , X0 ￿

the mat ’s a cat 5

X→￿X0 de X1 , X1 on X0 ￿
X→￿dianzi shang, the mat￿

X→￿dianzi shang, the mat￿

dianzi0 shang1

X→￿X0 de X1 , X0 ’s X1 ￿

S→￿X0 , X0 ￿

X→￿X0 de X1 , X0 X1 ￿

p=3/8
S→￿X0 , X0 ￿

p=1/8

p=2/8

S→￿X0 , X0 ￿

X→￿X0 de X1 , X1 of X0 ￿

X→￿dianzi shang, the mat￿

dianzi0 shang1

X→￿mao, a cat￿

de2

mao3

a cat of the mat 5

83

expected translation length?

S 0,4
S→￿X0 , X0 ￿

S→￿X0 , X0 ￿

2/8×4	 +	 6/8×5	 =	 4.75

X 0,4 a · · · mat

X 0,4 the · · · cat

X→￿X0 de X1 , X1 on X0 ￿

X→￿X0 de X1 , X0 ’s X1 ￿

X→￿X0 de X1 , X1 of X0 ￿

X→￿X0 de X1 , X0 X1 ￿

X 0,2 the · · · mat

X 3,4 a · · · cat

X→￿dianzi shang, the mat￿

X→￿mao, a cat￿

dianzi0 shang1

de2

mao3

X→￿dianzi shang, the mat￿

dianzi0 shang1

dianzi0 shang1

X→￿mao, a cat￿

de2

mao3

a cat on the mat 5

X→￿mao, a cat￿

de2

mao3

the mat a cat

4

X→￿mao, a cat￿

de2

mao3

p=2/8

S→￿X0 , X0 ￿

the mat ’s a cat 5

X→￿X0 de X1 , X1 on X0 ￿
X→￿dianzi shang, the mat￿

X→￿dianzi shang, the mat￿

dianzi0 shang1

X→￿X0 de X1 , X0 ’s X1 ￿

S→￿X0 , X0 ￿

X→￿X0 de X1 , X0 X1 ￿

p=3/8
S→￿X0 , X0 ￿

p=1/8

p=2/8

S→￿X0 , X0 ￿

X→￿X0 de X1 , X1 of X0 ￿

X→￿dianzi shang, the mat￿

dianzi0 shang1

X→￿mao, a cat￿

de2

mao3

a cat of the mat 5

83

expected translation length?

S 0,4
S→￿X0 , X0 ￿

S→￿X0 , X0 ￿

2/8×4	 +	 6/8×5	 =	 4.75

X 0,4 a · · · mat

X 0,4 the · · · cat

X→￿X0 de X1 , X1 on X0 ￿

X→￿X0 de X1 , X0 ’s X1 ￿

X→￿X0 de X1 , X1 of X0 ￿

X→￿X0 de X1 , X0 X1 ￿

X 0,2 the · · · mat

variance?

X 3,4 a · · · cat

X→￿dianzi shang, the mat￿

X→￿mao, a cat￿

dianzi0 shang1

de2

mao3

X→￿dianzi shang, the mat￿

dianzi0 shang1

dianzi0 shang1

X→￿mao, a cat￿

de2

mao3

a cat on the mat 5

X→￿mao, a cat￿

de2

mao3

the mat a cat

4

X→￿mao, a cat￿

de2

mao3

p=2/8

S→￿X0 , X0 ￿

the mat ’s a cat 5

X→￿X0 de X1 , X1 on X0 ￿
X→￿dianzi shang, the mat￿

X→￿dianzi shang, the mat￿

dianzi0 shang1

X→￿X0 de X1 , X0 ’s X1 ￿

S→￿X0 , X0 ￿

X→￿X0 de X1 , X0 X1 ￿

p=3/8
S→￿X0 , X0 ￿

p=1/8

p=2/8

S→￿X0 , X0 ￿

X→￿X0 de X1 , X1 of X0 ￿

X→￿dianzi shang, the mat￿

dianzi0 shang1

X→￿mao, a cat￿

de2

mao3

a cat of the mat 5

83

expected translation length?

S 0,4
S→￿X0 , X0 ￿

S→￿X0 , X0 ￿

2/8×4	 +	 6/8×5	 =	 4.75

X 0,4 a · · · mat

X 0,4 the · · · cat

X→￿X0 de X1 , X1 on X0 ￿

X→￿X0 de X1 , X0 ’s X1 ￿

X→￿X0 de X1 , X1 of X0 ￿

X→￿X0 de X1 , X0 X1 ￿

variance?

2/8×(4-4.75)^2	 +	 6/8×(5-4.75)^2	 ≈	 0.19
X 0,2 the · · · mat

X 3,4 a · · · cat

X→￿dianzi shang, the mat￿

X→￿mao, a cat￿

dianzi0 shang1

de2

mao3

X→￿dianzi shang, the mat￿

dianzi0 shang1

dianzi0 shang1

X→￿mao, a cat￿

de2

mao3

a cat on the mat 5

X→￿mao, a cat￿

de2

mao3

the mat a cat

4

X→￿mao, a cat￿

de2

mao3

p=2/8

S→￿X0 , X0 ￿

the mat ’s a cat 5

X→￿X0 de X1 , X1 on X0 ￿
X→￿dianzi shang, the mat￿

X→￿dianzi shang, the mat￿

dianzi0 shang1

X→￿X0 de X1 , X0 ’s X1 ￿

S→￿X0 , X0 ￿

X→￿X0 de X1 , X0 X1 ￿

p=3/8
S→￿X0 , X0 ￿

p=1/8

p=2/8

S→￿X0 , X0 ￿

X→￿X0 de X1 , X1 of X0 ￿

X→￿dianzi shang, the mat￿

dianzi0 shang1

X→￿mao, a cat￿

de2

mao3

a cat of the mat 5

83

First- and Second-order Expectation Semirings
First-order:

•

(Eisner, 2002)

each member is a 2-tuple: ￿p, r ￿
￿p1 , r1 ￿ ⊗ ￿p2 , r2 ￿
￿p1 , r1 ￿ ⊕ ￿p2 , r2 ￿

￿ p1 p2 , p 1 r 2 + p2 r1 ￿
￿ p 1 + p2 , r 1 + r 2 ￿

Second-order:

•

each member is a 4-tuple: ￿p, r, s, t￿

￿p1 , r1 , s1 , t1 ￿ ⊗ ￿p2 , r2 , s2 , t2 ￿
￿p1 , r1 , s1 , t1 ￿ ⊕ ￿p2 , r2 , s2 , t2 ￿

￿p1 p2 , p1 r2 + p2 r1 , p1 s2 + p2 s1 ,
p1 t2 + p2 t1 + r1 s2 + r2 s1 ￿

￿p1 + p2 , r1 + r2 , s1 + s2 , t1 + t2 ￿
84

e

k(v1)= k(e1)

k(v2)= k(e2)

⊗ k(v ) ⊗ k(v ) ⊕ k(e ) ⊗ k(v )⊗ k(v )
k(v )= k(e ) ⊗ k(v ) ⊗ k(v ) ⊕ k(e ) ⊗ k(v )⊗ k(v )
k(v )= k(e ) ⊗ k(v ) ⊕ k(e ) ⊗ k(v )

k(v3)= k(e3)

1

2

4

1

2

2

6

1

2

4

5

1

5

7

3

8

4

v5 k(v5)=〈8,	 4.75〉

v5

e7
8
e7
e8 〈1,	 0〉e〈1,	 0〉
k(v3)=〈0,	 1〉

v3

v3

v4

v4

k(v4)=〈1,	 2〉

First-order:
e4
e4 e3 〈3,	 3〉 〈1,	 1〉
〈2,	 2〉
each semiring member 〈2,	 0〉 e5 e6
e5 e6
is a 2-tuple

k(v2)=〈1,	 2〉

v1 k(v1)=〈1,	 2〉
v2

Fake

e〈1,	 2〉
1
dianzi0 shang1

e2

v2
〈1,	 2〉 2
e

de2

mao3

85

e

k(v1)= k(e1)

k(v2)= k(e2)

⊗ k(v ) ⊗ k(v ) ⊕ k(e ) ⊗ k(v )⊗ k(v )
k(v )= k(e ) ⊗ k(v ) ⊗ k(v ) ⊕ k(e ) ⊗ k(v )⊗ k(v )
k(v )= k(e ) ⊗ k(v ) ⊕ k(e ) ⊗ k(v )

k(v3)= k(e3)

1

2

4

1

2

2

6

1

2

4

5

1

5

7

3

8

4

v5 k(v5)=〈8,4.5,4.5,5〉

v5

e8
e7
e8 e7
〈1,	 0,0,0〉 〈1,0,0,0〉
k(v3)=〈1,1,1,1〉v
v4 k(v4)=〈1,2,1,3〉
v4

v3

3

〈1,1,1,1〉
Second-order:
e4 e3 e4
〈2,2,2,2〉
each semiring member
〈3,3,3,3〉 e5 e6
〈2,0,0,0〉e5 e6
is a 4-tuple

v1 k(v1)=〈1,2,2,4〉
v2
〈1,2,2,4〉
e1

dianzi0 shang1

v2 k(v2)=〈1,2,2,4〉

〈1,2,2,4〉 2
e
e2

de2

mao3

86

e

k(v1)= k(e1)

k(v2)= k(e2)

⊗ k(v ) ⊗ k(v ) ⊕ k(e ) ⊗ k(v )⊗ k(v )
k(v )= k(e ) ⊗ k(v ) ⊗ k(v ) ⊕ k(e ) ⊗ k(v )⊗ k(v )
k(v )= k(e ) ⊗ k(v ) ⊕ k(e ) ⊗ k(v )

k(v3)= k(e3)

1

2

4

1

2

2

6

1

2

4

5

1

5

7

3

8

4

v5 k(v5)=〈8,4.5,4.5,5〉

v5

e8
e7
e8 e7
〈1,	 0,0,0〉 〈1,0,0,0〉
k(v3)=〈1,1,1,1〉v
v4 k(v4)=〈1,2,1,3〉
v4

v3

3

〈1,1,1,1〉
Second-order:
e4 e3 e4
〈2,2,2,2〉
each semiring member
〈3,3,3,3〉 e5 e6
〈2,0,0,0〉e5 e6
is a 4-tuple

v1 k(v1)=〈1,2,2,4〉
v2

Fake

〈1,2,2,4〉
e1

dianzi0 shang1

v2 k(v2)=〈1,2,2,4〉

〈1,2,2,4〉 2
e
e2

de2

mao3

86

Expectations on Hypergraphs

87

Expectations on Hypergraphs
•

Expectation over a hypergraph

87

Expectations on Hypergraphs
•

Expectation over a hypergraph
def

r = Ep [r] =

￿

p(d)r(d)

d∈HG

•

r(d) is a function over a derivation d

e.g., the length of the translation yielded by d

87

Expectations on Hypergraphs
•

Expectation over a hypergraph
def

r = Ep [r] =

￿

p(d)r(d)

d∈HG

•

exponential size

r(d) is a function over a derivation d

e.g., the length of the translation yielded by d

87

Expectations on Hypergraphs
•

Expectation over a hypergraph
def

r = Ep [r] =

￿

p(d)r(d)

d∈HG

exponential size

•

r(d) is a function over a derivation d

•

r(d) is additively decomposed
￿
def
r(d) =
re

e.g., the length of the translation yielded by d

e∈d

e.g., translation length is additively decomposed!
87

Second-order Expectations on Hypergraphs

•

Expectation of products over a hypergraph
def

t = Ep [r · s] =

•

￿

p(d)r(d)s(d)

d∈HG

exponential size

r and s are additively decomposed
￿
def
r(d) =
re
e∈d

￿
def
s(d) =
se
e∈d

r and s can be identical or different functions.
88

Compute expectation using expectation semiring:

89

Compute expectation using expectation semiring:
def

ke = ￿pe , pe re ￿

pe : transition probability or log-linear score at edge e
re ?

89

Compute expectation using expectation semiring:
def

ke = ￿pe , pe re ￿

pe : transition probability or log-linear score at edge e
re ?
def
re = log pe
Entropy:

89

Compute expectation using expectation semiring:
def

ke = ￿pe , pe re ￿

pe : transition probability or log-linear score at edge e
re ?
def
re = log pe
Entropy:

Why?

89

Compute expectation using expectation semiring:
def

ke = ￿pe , pe re ￿

pe : transition probability or log-linear score at edge e
re ?
def
re = log pe
Entropy:

Why?

entropy is an expectation

89

Compute expectation using expectation semiring:
def

ke = ￿pe , pe re ￿

pe : transition probability or log-linear score at edge e
re ?
def
re = log pe
Entropy:

Why?

entropy is an expectation
H(p) = Ep [− log p] = −

￿

p(d) log p(d)

d∈HG
89

Compute expectation using expectation semiring:
def

ke = ￿pe , pe re ￿

pe : transition probability or log-linear score at edge e
re ?
def
re = log pe
Entropy:

Why?

entropy is an expectation
H(p) = Ep [− log p] = −

￿

p(d) log p(d)

d∈HG

log p(d) is additively decomposed!
89

Compute expectation using expectation semiring:
def

ke = ￿pe , pe re ￿

pe : transition probability or log-linear score at edge e
re ?
def
re = log pe
Entropy:
Cross-entropy:

def

re = log qe

Why?

90

Compute expectation using expectation semiring:
def

ke = ￿pe , pe re ￿

pe : transition probability or log-linear score at edge e
re ?
def
re = log pe
Entropy:
Cross-entropy:

Why?

def

re = log qe

cross-entropy is an expectation

90

Compute expectation using expectation semiring:
def

ke = ￿pe , pe re ￿

pe : transition probability or log-linear score at edge e
re ?
def
re = log pe
Entropy:
Cross-entropy:

Why?

def

re = log qe

cross-entropy is an expectation
H(p, q) = Ep (− log q) = −

￿

p(d) log q(d)

d∈HG

90

Compute expectation using expectation semiring:
def

ke = ￿pe , pe re ￿

pe : transition probability or log-linear score at edge e
re ?
def
re = log pe
Entropy:
Cross-entropy:

Why?

def

re = log qe

cross-entropy is an expectation
H(p, q) = Ep (− log q) = −

￿

p(d) log q(d)

d∈HG

log q(d) is additively decomposed!
90

Compute expectation using expectation semiring:
def

ke = ￿pe , pe re ￿

pe : transition probability or log-linear score at edge e
re ?
def
re = log pe
Entropy:
Cross-entropy:
Bayes risk:

def

re = log qe
def

re = loss at edge e

Why?

91

Compute expectation using expectation semiring:
def

ke = ￿pe , pe re ￿

pe : transition probability or log-linear score at edge e
re ?
def
re = log pe
Entropy:
Cross-entropy:
Bayes risk:
Why?

def

re = log qe
def

re = loss at edge e
Bayes risk is an expectation

91

Compute expectation using expectation semiring:
def

ke = ￿pe , pe re ￿

pe : transition probability or log-linear score at edge e
re ?
def
re = log pe
Entropy:
Cross-entropy:
Bayes risk:
Why?

def

re = log qe
def

re = loss at edge e
Bayes risk is an expectation
Risk = Ep (L) = −

￿

d∈HG

p(d) · L(Y (d))
91

Compute expectation using expectation semiring:
def

ke = ￿pe , pe re ￿

pe : transition probability or log-linear score at edge e
re ?
def
re = log pe
Entropy:
Cross-entropy:
Bayes risk:
Why?

def

re = log qe
def

re = loss at edge e
Bayes risk is an expectation
Risk = Ep (L) = −

￿

d∈HG

p(d) · L(Y (d))

L(Y(d)) is additively decomposed!
91

Compute expectation using expectation semiring:
def

ke = ￿pe , pe re ￿

pe : transition probability or log-linear score at edge e
re ?
def
re = log pe
Entropy:
Cross-entropy:
Bayes risk:
Why?

def

re = log qe
def

re = loss at edge e
Bayes risk is an expectation
Risk = Ep (L) = −

￿

d∈HG

p(d) · L(Y (d))

L(Y(d)) is additively decomposed!

(Tromble et al. 2008)
91

Applications of Expectation Semirings: a Summary
Quantity
ke
kroot
Final
Expectation
￿pe , pe re ￿
￿Z, r￿
r/Z
def
Entropy
re = log pe , so ke = ￿pe , pe log pe ￿
￿Z, r￿
log Z − r/Z
Cross￿qe ￿
￿Zq ￿
log Zq − r/Zp
def
entropy
re = log qe , so ke = ￿pe , pe log qe ￿
￿Zp , r￿
def
Bayes risk
re = Le , so ke = ￿pe , pe Le ￿
￿Z, r￿
r/Z
First-order
￿pe , ∇pe ￿
￿Z, ∇Z￿
∇Z
gradient
Covariance
t
r sT
￿pe , pe re , pe se , pe re se ￿
￿Z, r, s, t￿
− Z2
Z
matrix
Hessian
￿pe , ∇pe , ∇pe , ∇2 pe ￿
￿Z, ∇Z, ∇Z, ∇2 Z￿
∇2 Z
matrix
Gradient of
Z∇r−r∇Z
￿pe , pe re , ∇pe , (∇pe )re + pe (∇re )￿ ￿Z, r, ∇Z, ∇r￿
Z2
expectation
Gradient of
￿pe , pe log pe , ∇pe , (1 + log pe )∇pe ￿ ￿Z, r, ∇Z, ∇r￿ ∇Z − Z∇r−r∇Z
Z
Z2
entropy
Gradient of
Z∇r−r∇Z
￿pe , pe Le , ∇pe , Le ∇pe ￿
￿Z, r, ∇Z, ∇r￿
Z2
risk
92

S 0,4
S→￿X0 , X0 ￿

X 0,4 the · · · cat

S→￿X0 , X0 ￿

A semiring framework to compute all of these

X 0,4 a · · · mat

Probabilistic
Hypergraph
X→￿X0 de X1 , X1 on X0 ￿

X→￿X0 de X1 , X0 ’s X1 ￿

X→￿X0 de X1 , X1 of X0 ￿

X→￿X0 de X1 , X0 X1 ￿

X 0,2 the · · · mat
X→￿dianzi shang, the mat￿

dianzi0 shang1

X 3,4 a · · · cat
X→￿mao, a cat￿

de2

mao3

• First-order quantities:

- expectation
- entropy
- Bayes risk
- cross-entropy
- KL divergence
- feature expectations
- ﬁrst-order gradient of Z

• “decoding” quantities:
- Viterbi
- K-best
- Counting
- ......

• Second-order quantities:

- Expectation over product
- interaction between features
- Hessian matrix of Z
- second-order gradient
descent
- gradient of expectation
- gradient of entropy or
Bayes risk

93

