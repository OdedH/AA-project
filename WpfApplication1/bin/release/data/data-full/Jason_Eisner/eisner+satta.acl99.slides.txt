Efficient Parsing for
•Bilexical CF Grammars
•Head Automaton Grammars
Jason Eisner
U. of Pennsylvania

Giorgio Satta
U. of Padova, Italy

When’s a grammar bilexical?
If it has rules / entries that mention 2
specific words in a dependency relation:
convene - meeting
eat - blintzes
ball - bounces
joust - with

U. of Rochester

Bilexical Grammars
Instead of
or even

Bilexical CF grammars

VP → V NP
VP → solved NP

use detailed rules that mention

Every rule has one of these forms:

2 heads:

S[solved] → NP[Peggy] VP[solved]
VP[solved] → V[solved] NP[puzzle]
NP[puzzle] → Det[a] N[puzzle]

so we can exclude, or reduce probability of,
VP[solved] → V[solved] NP[goat]
NP[puzzle] → Det[two] N[puzzle]

Bilexicalism at Work
Not just selectional but adjunct preferences:
Peggy [solved a puzzle] from the library.
Peggy solved [a puzzle from the library].

Hindle & Rooth

(1993) - PP attachment

A[x] → B[x] C[y]
A[x] → B[y] C[x]
A[x] → x

so head of LHS
is inherited from
a child on RHS.
(rules could also have probabilities)

B[x], B[y], C[x], C[y], ... many nonterminals
A, B, C ... are “traditional nonterminals”
x, y ... are words

Bilexicalism at Work
Bilexical parsers that fit the CF formalism:
Alshawi (1996)
Charniak (1997)
Collins (1997)
Eisner (1996)

-

head automata
Treebank grammars
context-free grammars
dependency grammars

Other superlexicalized parsers that don’t:
Jones & Eisner (1992)
Lafferty et al. (1992)
Magerman (1995)
Ratnaparkhi (1997)
Chelba & Jelinek (1998)

-

bilexical LFG parser
stochastic link parsing
decision-tree parsing
maximum entropy parsing
shift-reduce parsing

1

How bad is bilex CF parsing?
A[x] → B[x] C[y]

The CKY-style algorithm
[ Mary ]

loves

[ [ the]

girl

[ outdoors ] ]

Grammar size = O(t3 V2)
where t = |{A, B, ...}|

V = |{x, y ...}|

So CKY takes O(t3 V2 n3)
Reduce to O(t3 n5) since relevant V = n
This is terrible ... can we do better?
Recall: regular CKY is O(t3 n3)

Why CKY is O(n5) not O(n3)

Idea #1

visiting relatives
visiting relatives
C

... advocate
... hug
B

Combine B with what C?
must try different-width
C’s (vary k)

i

h

j

j+1

h’

k
O(n3 combinations)

A

O(n5

combinations)

B
i h

C
j

j+1 h’ k

A
i

h

(Alshawi 1996)

[Good old Peggy] solved [the puzzle] [with her teeth] !

B

C

The head automaton for solved:

h’
(the old CKY way)

i h

j

A

C

C

C
h’

j

j+1 h’ k

i

j

j+1 h’ k

A

A
i

h’ k

Head Automaton Grammars

Idea #1

i h

Separate these!

k
i

B

must try differentlyheaded C’s (vary h’)

h’ k

i

a finite-state device
can consume words adjacent to it on either side
does so after they’ve consumed their dependents
[Peggy] solved [puzzle] [with]
[Peggy] solved [with]
[Peggy] solved
solved

(state = V)
(state = VP)
(state = VP)
(state = S; halt)

h’ k

2

Formalisms too powerful?

Transform the grammar

So we have Bilex CFG and HAG in O(n4).
HAG is quite powerful - head c can require an c bn:

Absent such cycles,
we can transform to a “split grammar”:
Each head eats all its right dependents first
I.e., left dependents are more oblique.
A A

... [...a3...] [...a2...] [...a1...] c [...b1...] [...b2...] [...b3...] ...
not center-embedding, [a3 [[a2 [[a1] b1]] b2]] b3

Linguistically unattested and unlikely
Possible only if the HA has a left-right cycle
Absent such cycles, can we parse faster?

This allows
i

h h

(for both HAG and equivalent Bilexical CFG)

k

A
i

Idea #2

k

Idea #2
Combine what B and C?
must try different-width
C’s (vary k)

B
(the old CKY way)

C

i h

j

j+1 h’

A
B
i h

C
j

j+1 h’ k

i h

i h

Idea #2

(the old CKY way)

C
j

j+1 h’ k

i

h’

h

h’ k

A
i h

k

k

[ Mary ]

C

h

j

[ [ the]

girl

[ outdoors ] ]

j+1 h’

A
C
h

loves

h’

C
h’ k

A

A
i h

j+1 h’ k

The O(n3) half-tree algorithm
B

i h

j

C

C

A

h’ k

B

C

Separate these!

A
i

B

must try different
midpoints j

k

h

k

3

Theoretical Speedup

Reality check

n = input length
g = polysemy
t = traditional nonterms or automaton states

Constant factor
Pruning may do just as well

Naive: O(n5 g2 t)
New: O(n4 g2 t)
Even better for split grammars:
Eisner (1997): O(n3 g3 t2)
New: O(n3 g2 t)

“visiting relatives”: 2 plausible NP hypotheses
i.e., both heads survive to compete - common??

Amdahl’s law
much of time spent smoothing probabilities
fixed cost per parse if we cache probs for reuse

all independent of vocabulary size!

Experimental Speedup

3 parsers (pruned)

(not in paper)

6000

5000

Used Eisner (1996) Treebank WSJ parser
and its split bilexical grammar

4000

NAIVE
IWPT-97

Time

Parsing with pruning:
Both old and new O(n3) methods
give 5x speedup over the O(n5) - at 30 words

ACL-99

3000

2000

Exhaustive parsing (e.g., for EM):

1000

Old O(n3) method (Eisner 1997)
gave 3x speedup over O(n5) - at 30 words

0
0

New O(n3) method gives 19x speedup

10

20

30

40

50

60

Sentence Length

3 parsers (pruned): log-log plot

3 parsers (exhaustive)
80000

10000

70000
y = cx3.8
60000

1000

100

NAIVE
IWPT-97

Time

Time

NAIVE

50000

y = cx2.7

IWPT-97
ACL-99

40000

ACL-99

30000
20000

10

10000
1

0
10

100
Sentence Length

0

10

20

30

40

50

60

Sentence Length

4

3 parsers (exhaustive): log-log plot

3 parsers
80000

100000
y = cx5.2

70000
y = cx4.2

10000

60000

IWPT-97

Time

ACL-99

Time

NAIVE

y = cx3.3

1000

50000
NAIVE
IWPT-97

40000

ACL-99
NAIVE

100

IWPT-97

30000

ACL-99

20000
10
10000
1

0
10

0

100

10

Sentence Length

20

30

40

50

60

Sentence Length

3 parsers: log-log plot

Summary

100000

10000

Simple bilexical CFG notion A[x] → B[x] C[y]
Covers several existing stat NLP parsers

exhaustive

1000
Time

pruned

NAIVE
IWPT-97
ACL-99
NAIVE

100

IWPT-97
ACL-99

10

Fully general O(n4) algorithm - not O(n5)
Faster O(n3) algorithm for the “split” case
Demonstrated practical speedup

Extensions: TAGs and post-transductions

1
10

100
Sentence Length

5

