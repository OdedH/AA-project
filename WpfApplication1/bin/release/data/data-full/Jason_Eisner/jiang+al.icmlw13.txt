Prioritized Asynchronous Belief Propagation

Jiarong Jiang
University of Maryland, College Park, MD 20740

jiarong@umiacs.umd.edu

Taesun Moon
University of Maryland, College Park, MD 20740

tsmoon@umiacs.umd.edu

Hal Daum´ III
e
University of Maryland, College Park, MD 20740

hal@umiacs.umd.edu

Jason Eisner
Johns Hopkins University, Baltimore, MD 21218

jason@cs.jhu.edu

Abstract
Message scheduling is shown to be very
eﬀective in belief propagation (BP) algorithms. However, most existing scheduling algorithms use ﬁxed heuristics regardless of the structure of the graphs or properties of the distribution. On the other
hand, designing diﬀerent scheduling heuristics for all graph structures are not feasible.
In this paper, we propose a reinforcement
learning based message scheduling framework
(RLBP) to learn the heuristics automatically
which generalizes to any graph structures and
distributions. In the experiments, we show
that the learned problem-speciﬁc heuristics
largely outperform other baselines in speed.

1. Introduction
Probabilistic graphical models (Pearl, 1988; Wainwright & Jordan, 2008; Jordan, 1999) play an important role in representing complex distributions and dependencies between random variables for many real
world applications. Many approximate inference algorithms have been proposed to solve the problem
eﬃciently. One of the most common methods is
message-passing algorithms such as loopy belief propagation (Murphy et al., 1999; Ihler et al., 2005; Yedidia
et al., 2000). The idea behind this is to pass messages
between adjacent nodes until the ﬁxed points of the
beliefs are achieved.
Inferning Workshop at 30 th International Conference on
Machine Learning, Atlanta, Georgia, USA, 2013. Copyright 2013 by the author(s).

It is shown in Elidan (2006) that asynchronous propagation can achieve better convergence compared to
synchronous methods and thus initiated interest in
studying message scheduling schemes. However, most
existing research only focuses on manually designed
heuristics. For example, Wainwright et al. proposed a
tree reparameterization algorithm as message scheduling for asynchronous propagation. Elidan (2006) orders the messages in the order of the diﬀerences between two consecutive values of the message. Yedidia
et al. (2005); Vila Casado et al. (2010) also design speciﬁc scheduling for LDPC decoding problems.
In contrast with standard scheduling algorithms, in
this paper, we explore automatically scheduling messages for any graph structure and distribution. To
achieve this, we formalized the learning of the ordering
as a Markov Decision Process (MDP) and associate the
ﬁnal reward with the convergence speed. The ordering
can be viewed as a linear policy of the MDP. The goal
is trying to learn a good policy such that the corresponding scheduling can achieve a better convergence
speed. In the experiments, we show that that for the
graphs that the heuristics are automatically learned,
the learned heuristics performs better than the baselines. We also notice that the learned weights reveal
some useful features to consider in the scheduling.

2. Background
In this paper, we consider the problem of marginal
inference on undirected and discrete graphical models.
Formally, we denote a graph G = (X, E) and let x =
{x1 , ..., xm } to be the random variables associated with
nodes in X. The exponential family distribution p over

Prioritized Asynchronous Belief Propagation

the random variables is deﬁned as follows:
pθ (x) = exp[ θ, φ(x) − A(θ)]

(1)

where φ(x) is the suﬃcient statistics of x and θ is
the canonical or exponential parameters. A(θ) =
log x exp[ θ, φ(x) ] is the log-partition function. In
this paper, we will focus on solving the marginal problem which is inferring marginal distribution p(x) for
all x.
2.1. Belief Propagation
Belief propagation (Murphy et al., 1999; Ihler et al.,
2005; Yedidia et al., 2000) (or sum-product) algorithm
is the standard message-passing algorithm for inferring
marginal distributions over random variables. It is a
ﬁxed point iteration algorithm where the ﬁxed point
is the exact solution to trees or polytrees structures
(Pearl, 1988). On loopy graphs, they are not guaranteed to converge, but if they do, the ﬁnal estimates are
shown to be reasonably good (Yedidia et al., 2000).
The message Mts passed from node t to one of its
neighbors s is deﬁned as:
Mts (xs ) ← κ

exp[θst (xs , xt ) + θt (xt )]
xt ∈Xt

(2)

the corresponding ordering can improve the speed of
convergence. Note that in this paper, we will only consider ordering the nodes in the graph, but the learning
framework can be easily generalized to ordering each
message in the graph.
3.1. MDP Formulation
A Markov Decision Process (MDP) (Bellman, 1957;
Puterman, 2009) can be viewed as a memoryless search
process. It consists of a state space S, an action space
A, a transition function T , a reward function R and
the environment E. An agent repeatedly observes the
current state s ∈ S and takes an action a ∈ A. The
environment E then samples the new state s from
the transition function T (s |s, a) and gives a reward
R(s |a, s). A policy π(a|s) describes how the agent
chooses an action based on its current state.
More speciﬁcally, for the schedule learning problem,
the state space S is the graph and its messages/beliefs.
The action to take at each step is choosing a node
x in the graph and compute its messages. Assume
the features associated with node x is φ(x) and the
transition function T is deterministic. The reward is
deﬁned with regard to the convergence speed. Here we
consider a linear policy (priority) which is

Mut (xt )

πω (s) = arg max ω · φ(x)
x

u∈N (t)\s

where κ is the normalization constant. N (t)\s are the
neighbors of t excluding s. When the messages converge, the belief/psuedomarginal distribution for the
node s is given by
µs (xs ) = κ exp{θs (xs )}

Mts (xs )

(3)

t∈N (s)

For synchronous message-passing, only the old messages from the previous iteration are used to compute
the messages for the current iteration. However, asynchronous message-passing uses the latest messages for
updates. In Elidan (2006), it is shown that any reasonable asynchronous BP algorithm converges to a unique
ﬁxed point under the assumptions that are similar to
the synchronous version. They use the residues (differences in beliefs between updates) as a priority to
schedule the messages. In this paper, instead of using
the greedy residual schedule, we learn a proper schedule automatically by a reinforcement learner.

3. Message Scheduling with
Reinforcement Learner
We formulate the schedule learning problem as a MDP
and the goal is to learn a policy for the MDP such that

(4)

where ω is the feature weight vector. The goal is to
maximize the expected reward R = Er∼πω [R(τ )] =
T
Er∼πω [ t=0 rt ] where τ = (s0 , a0 , r0 , ..., sT , aT , rT ) is
a trajectory. In the current situation, this is equivalent
to minimizing the number of messages passed.
At test time, the transition function is deterministic so
that we always choose the node that has the highest
priority in the current state s. However, at the training
time, we allow the agent to explore the space by a
stochastic policy:
πω (xt ) =

1
exp
Z

1
ω · φ(x)
T

(5)

where T is temperature and Z is a normalization constant. When T → ∞, πω achieves a random choice
over all the nodes while T → 0 only exploits the deterministic policy.
To solve this, we apply the standard policy gradient
algorithm (Sutton et al., 2000).
ω Eτ [R(τ )]

=Eτ

ω pω (τ )
(R(τ ) − b)
pω (τ )

(6)

T

=Eτ (R(τ ) − b)

ω
t=0

log π(xt )

Prioritized Asynchronous Belief Propagation

where b is some arbitrary baseline. In the last term,
the gradient of each policy decision is:

1
φ(xt ) −
ω log πω (xt ) =
T


πω (xt )φ(xt )

(7)

xt

Here we also use the optimal baseline
T
t=0

2
ω

log πω (xt )

b=
T
t=0

R(τ )
2

ω

(8)

log πω (xt )

and the averaged gradient
T

g=

ω

log πω (xt ) (R(τ ) − b)

(9)

t=0

3.2. Algorithm
A detailed training algorithm is described in Algorithm 1. In the training, assume node and edge potentials are given for a set of training graphs. The
goal is to learn the policy (feature weights) so that the
ordering of the nodes will help the model convergence.
One run on a graph is treated as a trajectory for the
learner.
A priority queue is maintained to order the nodes
of the graph. For each iteration, a node is popped
from the queue according to a Boltzmann exploration
policy. Then the relevant messages are updated for
the node and the features (and their priorities) of the
neighboring nodes are also updated. One iteration
is over when there are no more nodes in the priority
queue. If the beliefs are not converged, then push all
the nodes to the priority queue and repeat. Continue
this process until the gradient converges and update
the weight vector ω. Return the ﬁnal weight vector ω
when the gradient → 0.
During the test time, the learned deterministic policy
will be used to prioritize the nodes and the node with
the highest priority is updated for each step.

Algorithm 1 Training Algorithm
Input: Training set with graphs and given potentials.
Initialize the policy parameter ω.
for each graph in the training set do
Initialize the messages and priority queue Q with
all the nodes in the graph.
while the beliefs are not converged or the maximum number of iterations is not reached do
Pop a node x from Q according to (5).
Compute its messages by (3).
Compute the derivative ω log π(xt |Qt ) by (7).
Update the dynamic features of its neighbors.
if the priority queue Q is empty and algorithm
not converged then
Enqueue all the nodes.
end if
end while
Compute the reward of the current trajectory.
Compute the baseline by (8) and gradient by (9).
Update the policy when the gradient converges.
end for

3.3.1. Static Features
The static features are:
• the degrees of the node,
• the dimension of the node,
• the max/min degrees of the neighboring nodes,
• the max/min dimension of the neighboring nodes
3.3.2. Dynamic Features
Dynamic features are those features that change between iterations or even within a single iteration. Here
we use
• KL divergence and residual between the current
learned belief and previous belief
• the diﬀerence between the current and last incoming messages
• the diﬀerence between the outgoing messages.

3.3. Features
The features we use here include both static features
and dynamic features. The static features are mostly
extracted according to the graph properties once per
inference run. Dynamic features are associated with
real-time updates and a change in the values of dynamic features for a node can impact the feature values
for other nodes in the graph.

4. Experiments
To empirically evaluate the model, we randomly generate Ising models according to the experiment setups
in (Elidan, 2006)1 : The graphical representation of an
1
As ongoing work, we only show the preliminary results
here.

Prioritized Asynchronous Belief Propagation

We generate 40 5×5 grid graphs with C = 5. We train
our scheduling policy on the same graphs for which
we report convergence results. LBP converges on 19
graphs and the remaining 3 algorithms converge on 27
graphs. We only try to learn the scheduling on the
27 graphs where either RBP or KLBP converges. In
table 1, we show results in terms of time reduction over
LBP per model. For clarity, LBP is shown as having
0% time reduction over itself. It is clear that RLBP
outperforms all other models by a wide margin.
0%
39.62%

RBP
RLBP

34.70%
55.10%

Table 1. Time reduction in percentage over LBP by propagation framework

q

120

q

q
q

q
q

q

q

40

60

80

(10)

We evaluate our reinforcement learning based message
scheduling framework (RLBP) against the standard
loopy belief propagation (LBP) and residual belief propogation (RBP). We also propose and evaluate against
a novel variant of the RBP where instead of measuring
the L2 diﬀerences between belief updates, we use a KL
divergence type of diﬀerence (KLBP).

LBP
KLBP

q

20

where λ ∼ U [−0.5, 0.5] and C is a constant.

q

q

0

if xs = xt
if xs = xt

iterations till convergence

φs,t (Xs , Xt ) =

exp(λC)
exp(−λC)

q
q

100

Ising model is a N ×N random grid with a corresponding number of binary variables. The node potentials
are drawn from U [0, 1] and the pairwised potentials

RBP

BP

KLBP

RLBP

Figure 1. Plot of iterations per model until convergence

here on learning the prioritization for the nodes in the
graph. However, in the general case, we can also apply
the same learning framework to learning or pruning
the speciﬁc messages in the belief propagation. If the
graphical model is large with many variables or variables sitting in a high dimension space, the edge-based
scheduling becomes harder. More complicated models
should be considered such as merging variables and
performing inference on diﬀerent parts of the graph,
coarse-to-ﬁne message passing, etc.

References
Figure 1 shows the number of iterations taken for each
of the 27 graphs. Note some of the results for LBP are
missing because it does not converge. In order to make
the plot easy to read, we order the graphs on the x-axis
by the number of iterations RLBP (red line) takes.
It can be seen that RBP and KLBP also show large reductions in the time to convergence and sometimes one
scheme is better than the other. However, the learned
heuristics (RLBP) performs the best of all most of
time. One interesting observation in the ﬁnal feature
weights is that the degrees of the node here is a very
indicative feature besides the diﬀerence or divergence
of the beliefs, which supports the fact that diﬀerent
graph structures should have diﬀerent scheduling for
the messages.

5. Discussion and Future Work
In this paper, we presented a novel, eﬀective algorithm
for learning how to schedule messages automatically
with a reinforcement learner. We restricted our focus

Bellman, Richard. A markovian decision process.
Technical report, DTIC Document, 1957.
Elidan, Gal. Residual belief propagation: Informed
scheduling for asynchronous message passing. In
in Proceedings of the Twenty-second Conference on
Uncertainty in AI (UAI, 2006.
Ihler, Alexander T., Iii, John W. Fisher, Willsky,
Alan S., and Chickering, Maxwell. Loopy belief
propagation: Convergence and eﬀects of message
errors. Journal of Machine Learning Research, 6:
905–936, 2005.
Jordan, Michael I. An introduction to variational
methods for graphical models. In Machine Learning, pp. 183–233. MIT Press, 1999.
Murphy, Kevin P., Weiss, Yair, and Jordan, Michael I.
Loopy belief propagation for approximate inference:
An empirical study. In In Proceedings of Uncertainty
in AI, pp. 467–475, 1999.

Prioritized Asynchronous Belief Propagation

Pearl, Judea. Probabilistic Reasoning in Intelligent
Systems: Networks of Plausble Inference. Morgan
Kaufmann Pub, 1988.
Puterman, Martin L. Markov decision processes: discrete stochastic dynamic programming, volume 414.
Wiley-Interscience, 2009.
Sutton, Richard S, McAllester, David, Singh, Satinder,
and Mansour, Yishay. Policy gradient methods for
reinforcement learning with function approximation.
Advances in neural information processing systems,
12(22), 2000.
Vila Casado, Andres I, Griot, Miguel, and Wesel,
Richard D. Ldpc decoders with informed dynamic
scheduling. Communications, IEEE Transactions
on, 58(12):3470–3479, 2010.
Wainwright, Martin J and Jordan, Michael I. Graphical models, exponential families, and variational
inference. Foundations and Trends R in Machine
Learning, 1(1-2):1–305, 2008.
Wainwright, MJ, Jaakkola, T, and Willsky, AS. Treebased reparameterization for approximate estimation on loopy graphs. Advances in Neural Information Processing Systems, 14.
Yedidia, Jonathan S., Freeman, William T., and
Weiss, Yair. Generalized belief propagation. In IN
NIPS 13, pp. 689–695. MIT Press, 2000.
Yedidia, Jonathan S., Wang, Yige, Wang, Yige, Zhang,
Juntan, Zhang, Juntan, Fossorier, Marc, and Fossorier, Marc. Reduced latency iterative decoding of
ldpc codes. In Proc. of the IEEE Global Communications Conf, 2005.

