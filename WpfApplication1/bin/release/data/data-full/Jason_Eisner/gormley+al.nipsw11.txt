Shared Components Topic Models with Application
to Selectional Preference
Matthew R. Gormley

Mark Dredze
Benjamin Van Durme
Center for Language and Speech Processing
Human Language Technology Center of Excellence
Department of Computer Science
Johns Hopkins University, Baltimore, MD
{mrg,mdredze,vandurme,jason}@cs.jhu.edu

Jason Eisner

Introduction Predicate argument selectional preference1 is the notion that the roles, or argument
positions, of a given predicate tend to prefer some arguments to others. Automatically inferring
these preferences has been a topic of interest within the computational linguistics community since
the early 1990’s, with Resnik [3] giving examples such as: Mary drank some {wine, gasoline,
pencils, sadness}, where the provided nouns in the syntactic object position of the verb drink are of
various levels of semantic acceptability. Here we are motivated by statements such as the following
by Resnik [3]:
Although selectional preference is traditionally formalized in terms of feature agreement using notations
like [+Animate], such formalizations often fail to specify the set of allowable features, or to capture the
gradedness of qualitative difference [. . .],

Like Resnik, we would like a preference model that retained this notion of feature agreement, but
also allowed for the gradedness that contemporary computational linguists have come to assume. A
related intuition can be found in recent work on category learning of Grifﬁths and colleagues, such
as [4], which is partially motivated by the notion of linguistic ontologies [5].
We introduce the Shared Components Topic Model (SCTM), which expresses selectional preferences as soft disjunctions of conjunctions of semantic features. The model assumes there exist
underlying semantic features, e.g. [+liquid], [+solid], each of which deﬁnes a distribution
over nouns. The model conjoins these features into semantic classes, such as [+liquid] &
[+comestible] ; each class gives a distribution over nouns that all possess the same set of semantic features. In this way, we aim to model the acceptable nouns of eat as ( [+comestible] &
[+solid] OR [+meal] ). In the SCTM, because the features are distributions, the classes are
constructed as products (the soft variant of conjunctions) and preferences are encoded by mixtures
(the soft variant of disjunctions).
Model Latent Dirichlet Allocation (LDA) [6] has been used to learn selectional preferences as
soft disjunctions over ﬂat semantic classes [7, 8, 9]. Our model, the SCTM, also learns the structure
of each class as a soft conjunction of high-level semantic features. Figure 1a provides the generative
process for topic modeling and a mapping of terminology to selectional preference. Here, we further
describe the SCTM by analogy to LDA. In both LDA and the SCTM, each verb is modeled as
a mixture over K semantic classes; for each noun token, this mixture generates a semantic class
assignment, and the noun’s type is sampled from that semantic class. In LDA, a semantic class is a
multinomial over words sampled from a shared Dirichlet prior. By contrast, the SCTM models each
semantic class as a normalized product of a subset of C underlying semantic features (which we call
components).

1
The term selectional preference has a variety of (near) synonyms. Chomsky [1] used the term selectional
rules, giving the alternatives: selectional restrictions and restrictions of cooccurrence. Semanticists such as
Thomason [2] referred to the problem as sortal (in)correctness, with Thomason providing variants including:
category mistake, selectionally incorrect, type crossing, semi-grammatical, and semantically anomalous.

1

Topic Modeling ↔Selectional Preference
document ↔verb, word ↔noun,
topic ↔class, component ↔feature

(a) The SCTM generative process

model
CPM

Figure 1
1000

C

800
700
600
500

SCTM

400
300
10

20

30

40

50

Number of Components

60

(b) Topic Modeling

70

K
10
11
20
21
30
40

LDA
(K=C)

LDA
SCTM

900

Perplexity

For each component c ∈ {1, . . . , C}:
φc ∼ Dir(β)
γ
πc ∼ Beta( C , 1)
For each topic k ∈ {1, . . . , K}:
bkc ∼ Bernoulli(πc )
For each document m ∈ {1, . . . , M }
θ m ∼ Dir(α)
For each word n ∈ {1, . . . , Nm }
zmn ∼ Mult(1, θ m )
xmn ∼ p(· |bzmn , φ) from Eq. (1)

10
10
10
10
20
20
20
20

10
20
40
80
20
40
80
160

PwA
89.99
77.98
78.10
81.92
82.51
83.84
85.28
75.01
78.91
80.97
81.90
78.23
83.04
85.19
86.52

Perp
194.34
597.59
583.42
462.80
460.32
406.42
363.80
684.32
543.80
461.40
432.35
615.09
423.67
334.36
283.39

(c) Selectional Preference

The kth semantic class in the SCTM is a Product of Experts (PoE) [10] model, where the subset
of semantic features included in the product is determined by a binary vector generated by a betabernoulli model, the ﬁnite counterpart of the Indian Buffet Process (IBP) [11].
C
φbkc
p(x|bk , φ) = V c=1C cx b
(1)
kc
v=1
c=1 φcv
Here, φc is the cth semantic feature, a distribution over words. bk is the binary vector deﬁning the
structure of this semantic class. The model is closely related to SAGE [12] and the IOMM [13].
Learning To perform parameter estimation, we use an algorithm that follows the outline of the
Monte Carlo EM (MCEM) algorithm [14]. In the Monte Carlo E-step, we sample the class assignments zmn and the binary vectors bkc based on current parameters φ and observed data X. In the
M-step, we ﬁnd new components φ. Since these are the parameters of the PoEs, we replace the usual
maximization of data log-likelihood with a contrastive divergence (CD) objective [15], popular for
PoE training. Normally, CD only estimates the parameters of the product distributions. However,
in our model, which features are included in the product change based on the E-step. Since we can
generate multiple samples in the E-step, we modify the CD objective to compute the gradient for
each E-step sample and take the average to approximate the expectation under bkc and zmn . This
approximate algorithm is much more efﬁcient than a pure MCEM (or a pure MCMC) approach.
Experiments We present results on two tasks: selectional preference and topic modeling. For
selectional preference, our data comes from the part-of-speech (POS) tagged n-grams corpus of
[16]. Using POS tag patterns we produce a corpus of selectional preference examples of the form
(verbdependency type , noun). For example, the pattern VBD (PRP$|DT) NN would match sold the
car. Figure 1c presents pseudo-word accuracy (PwA) and per-noun perplexity (Perp) on test data.
We consider the tradeoff of model compactness vs. accuracy. In the case where C=K=10, the
performance of LDA is better than the SCTM in both PwA and Perp. Yet, if we increase the size of
both models and consider the case of LDA with K=11 and of the SCTM with C=10, K=40, then
the PwA of the former is 78.10 and the latter is 80.97. Yet, LDA has added a multinomial the size
of the vocabulary (1000) while the SCTM has added only a few binary vectors of length C.
We also apply the SCTM to topic modeling to explore its potential as a more compact representation
of topics. We use 1,000 randomly selected articles from the 20 Newsgroups dataset.2 We evaluate the
average perplexity per word on held out test data using the left-to-right approximation of [17]. Figure
1b shows the results for LDA and the SCTM for the same number of components and varying K
(SCTM). For LDA (K=C), this is a single (dashed) line. For SCTM, the x markers each correspond
to a different K at the shown C. The shaded region shows the full SCTM perplexity range for
different K. Observe that for each number of components, LDA falls within the upper portion of the
shaded region. This shows that, while for some (small) values of K for the SCTM, LDA does better,
the SCTM can easily include more K (requiring few new parameters) to achieve better results.
Discussion The main goal of this work is to learn features which can compactly describe selectional preference and the ways in which they combine into semantic classes. While quantitatively
our model performs as well as and learns classes (topics) that are similar in appearance to LDA,
our current work is focussed on biasing the SCTM to prefer more interpretable underlying features
(components).
2

http://people.csail.mit.edu/jrennie/20Newsgroups/

2

References
[1] Noam Chomsky. Aspects of the Theory of Syntax. 1965.
[2] R. H. Thomason. A semantic theory of sortal incorrectness. Journal of Philosophical Logic, 1:209–258,
1972.
[3] Philip Resnik. Semantic classes and syntactic ambiguity. In Proceedings of the workshop on Human
Language Technology, pages 278–283, 1993.
[4] K.R. Canini and T.L. Grifﬁths. A nonparametric bayesian model of Multi-Level category learning. In
Proceedings of the 25th AAAI Conference on Artiﬁcial Intelligence., 2011.
[5] F.C. Keil. Semantic and conceptual development: An ontological perspective. Harvard University Press,
Cambridge, MA, 1979.
[6] David Blei, Andrew Ng, and Michael Jordan. Latent dirichlet allocation. Journal of Machine Learning
Research, 3, 2003.
[7] Benjamin Van Durme and Daniel Gildea. Topic models for corpus-centric knowledge generalization.
Technical Report, (TR-946), June 2009.
[8] Alan Ritter, Mausam, and Oren Etzioni. A latent dirichlet allocation method for selectional preferences.
In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 424–
434, Uppsala, Sweden, July 2010. Association for Computational Linguistics.
´ e
[9] Diarmuid O S´ aghdha. Latent variable models of selectional preference. In Proceedings of the 48th
Annual Meeting of the Association for Computational Linguistics, pages 435–444, Uppsala, Sweden, July
2010. Association for Computational Linguistics.
[10] Geoffrey Hinton.
(ICANN), 1999.

Products of experts.

In International Conference on Artiﬁcial Neural Networks

[11] Thomas Grifﬁths and Zoubin Ghahramani. Inﬁnite latent feature models and the indian buffet process. In
Advances in Neural Information Processing Systems (NIPS), volume 18, 2006.
[12] Jacob Eisenstein, Amr Ahmed, and Eric P. Xing. Sparse additive generative models of text. In International Conference on Machine Learning (ICML), 2011.
[13] K.A. Heller and Z. Ghahramani. A nonparametric bayesian approach to modeling overlapping clusters.
In Proceedings of the Eleventh International Conference on Artiﬁcial Intelligence and Statistics (AISTATS07), page 187194, 2007.
[14] Greg Wei and Martin Tanner. A monte carlo implementation of the EM algorithm and the poor man’s
data augmentation algorithms. Journal of the American Statistical Association, 85(411):699–704, 1990.
[15] Geoffrey Hinton. Training products of experts by minimizing contrastive divergence. Neural Computation, 14(8):1771–1800, 2002.
[16] Dekang Lin, Kenneth Church, Heng Ji, Satoshi Sekine, David Yarowsky, Shane Bergsma, Kailash Patil,
Emily Pitler, Rachel Lathbury, Vikram Rao, Kapil Dalwani, and Sushant Narsale. New tools for web-scale
n-grams. In Language Resources and Evaluation (LREC), 2010.
[17] Hanna Wallach, Ian Murray, Ruslan Salakhutdinov, and David Mimno. Evaluation methods for topic
models. In International Conference on Machine Learning (ICML), pages 1105–1112, 2009.

3

