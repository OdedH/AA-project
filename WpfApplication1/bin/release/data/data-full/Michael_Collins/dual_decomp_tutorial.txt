Dual Decomposition
for Natural Language Processing
Alexander M. Rush and Michael Collins

Decoding complexity

focus: decoding problem for natural language tasks
y ∗ = arg max f (y )
y

motivation:
•

richer model structure often leads to improved accuracy

•

exact decoding for complex models tends to be intractable

Decoding tasks
many common problems are intractable to decode exactly
high complexity
• combined parsing and part-of-speech tagging (Rush et al.,
2010)
• “loopy” HMM part-of-speech tagging
• syntactic machine translation (Rush and Collins, 2011)
NP-Hard
• symmetric HMM alignment (DeNero and Macherey, 2011)
• phrase-based translation (Chang and Collins, 2011)
• higher-order non-projective dependency parsing (Koo et al.,
2010)
in practice:
• approximate decoding methods (coarse-to-ﬁne, beam search,
cube pruning, gibbs sampling, belief propagation)
• approximate models (mean ﬁeld, variational models)

Motivation
cannot hope to ﬁnd exact algorithms (particularly when NP-Hard)
aim: develop decoding algorithms with formal guarantees
method:
•

derive fast algorithms that provide certiﬁcates of optimality

•

show that for practical instances, these algorithms often yield
exact solutions

•

provide strategies for improving solutions or ﬁnding
approximate solutions when no certiﬁcate is found

dual decomposition helps us develop algorithms of this form

Lagrangian relaxation

(Held and Karp, 1971)

important method from combinatorial optimization
initially used for traveling salesman problems
optimal tour - NP-Hard
1

5
3

7
4

6

2

optimal 1-tree - easy (MST)
1

5
3

7
4

2

6

Dual decomposition

(Komodakis et al., 2010; Lemar´chal, 2001)
e

goal: solve complicated optimization problem
y ∗ = arg max f (y )
y

method: decompose into subproblems, solve iteratively
beneﬁt: can choose decomposition to provide “easy” subproblems
aim for simple and eﬃcient combinatorial algorithms
•

dynamic programming

•

minimum spanning tree

•

shortest path

•

min-cut

•

bipartite match

•

etc.

Related work

there are related methods used NLP with similar motivation
related methods:
•

belief propagation (particularly max-product) (Smith and
Eisner, 2008)

•

factored A* search (Klein and Manning, 2003)

•

exact coarse-to-ﬁne (Raphael, 2001)

aim to ﬁnd exact solutions without exploring the full search space

Tutorial outline
focus:
•

developing dual decomposition algorithms for new NLP tasks

•

understanding formal guarantees of the algorithms

•

extensions to improve exactness and select solutions

outline:
1. worked algorithm for combined parsing and tagging
2. important theorems and formal derivation
3. more examples from parsing, sequence labeling, MT
4. practical considerations for implementing dual decomposition
5. relationship to linear programming relaxations
6. further variations and advanced examples

1. Worked example

aim: walk through a dual decomposition algorithm for combined
parsing and part-of-speech tagging
•

introduce formal notation for parsing and tagging

•

give assumptions necessary for decoding

•

step through a run of the dual decomposition algorithm

Combined parsing and part-of-speech tagging
S
VP

NP
N

V

United

ﬂies

NP
D

A

N

some

large

jet

goal: ﬁnd parse tree that optimizes
score(S → NP VP) + score(VP → V NP) +
... + score(N → V) + score(N → United) + ...

Constituency parsing
notation:
• Y is set of constituency parses for input
• y ∈ Y is a valid parse
• f (y ) scores a parse tree
goal:
arg max f (y )
y ∈Y

example: a context-free grammar for constituency parsing
S
NP
N

VP
V

United ﬂies

NP
D

A

N

some large jet

Part-of-speech tagging
notation:
•
•
•

Z is set of tag sequences for input
z ∈ Z is a valid tag sequence

g (z) scores of a tag sequence

goal:
arg max g (z)
z∈Z

example: an HMM for part-of speech tagging
N

V

United1 ﬂies2

D

A

some3 large4

N
jet5

Identifying tags
notation: identify the tag labels selected by each model
•

y (i, t) = 1 when parse y selects tag t at position i

•

z(i, t) = 1 when tag sequence z selects tag t at position i

example: a parse and tagging with y (4, A) = 1 and z(4, A) = 1
S
NP
N

VP
V

N

NP

United ﬂies

D

A

N

some large jet

y

V

United1 ﬂies2

D

A

some3 large4

z

N
jet5

Combined optimization
goal:
arg max f (y ) + g (z)
y ∈Y,z∈Z

such that for all i = 1 . . . n, t ∈ T ,
y (i, t) = z(i, t)
i.e. ﬁnd the best parse and tagging pair that agree on tag labels
equivalent formulation:
arg max f (y ) + g (l(y ))
y ∈Y

where l : Y → Z extracts the tag sequence from a parse tree

Dynamic programming intersection
can solve by solving the product of the two models
example:
•

parsing model is a context-free grammar

•

tagging model is a ﬁrst-order HMM

•

can solve as CFG and ﬁnite-state automata intersection
S

replace S → NP VP
with
SN,N → NPN,N VPV,N

NP
N

VP
V

United ﬂies

NP
D

A

N

some large jet

Parsing assumption
the structure of Y could be CFG, TAG, etc.
assumption: optimization with u can be solved eﬃciently
arg max f (y ) +
y ∈Y

u(i, t)y (i, t)
i,t

generally benign since u can be incorporated into the structure of f
example: CFG with rule scoring function h
f (y ) =
X →Y Z ∈y

h(X → Y Z ) +

(i,X )∈y

h(X → wi )

where
arg maxy ∈Y
arg maxy ∈Y

f (y ) +

u(i, t)y (i, t) =
i,t

X →Y Z ∈y

h(X → Y Z ) +

(i,X )∈y

(h(X → wi ) + u(i, X ))

Tagging assumption
we make a similar assumption for the set Z
assumption: optimization with u can be solved eﬃciently
arg max g (z) −
z∈Z

u(i, t)z(i, t)
i,t

example: HMM with scores for transitions T and observations O
g (z) =
t→t ∈z

T (t → t ) +

(i,t)∈z

O(t → wi )

where
arg maxz∈Z

g (z) −

arg maxz∈Z
t→t ∈z

u(i, t)z(i, t) =
i,t

T (t → t ) +

(i,t)∈z

(O(t → wi ) − u(i, t))

Dual decomposition algorithm
Set u (1) (i, t) = 0 for all i, t ∈ T
For k = 1 to K
y (k) ← arg max f (y ) +
y ∈Y

z (k) ← arg max g (z) −
z∈Z

u (k) (i, t)y (i, t) [Parsing]
i,t

u (k) (i, t)z(i, t) [Tagging]
i,t

If y (k) (i, t) = z (k) (i, t) for all i, t Return (y (k) , z (k) )
Else u (k+1) (i, t) ← u (k) (i, t) − αk (y (k) (i, t) − z (k) (i, t))

CKY Parsing

Penalties
u(i, t) = 0 for all i,t

y ∗ = arg max(f (y ) +
y ∈Y

u(i, t)y (i, t))
i,t

Viterbi Decoding
United1 ﬂies2 some3 large4

z ∗ = arg max(g (z) −
z∈Z

jet5

u(i, t)z(i, t))
i,t

Key
f (y )
Y
y (i, t) = 1

⇐
⇐
if

CFG
Parse Trees
y contains tag t at position i

g (z)
Z

⇐
⇐

HMM
Taggings

CKY Parsing

Penalties

S
NP

u(i, t) = 0 for all i,t

VP

A

N

D

A

V

United

ﬂies

some

large

jet

y ∗ = arg max(f (y ) +
y ∈Y

u(i, t)y (i, t))
i,t

Viterbi Decoding
United1 ﬂies2 some3 large4

z ∗ = arg max(g (z) −
z∈Z

jet5

u(i, t)z(i, t))
i,t

Key
f (y )
Y
y (i, t) = 1

⇐
⇐
if

CFG
Parse Trees
y contains tag t at position i

g (z)
Z

⇐
⇐

HMM
Taggings

CKY Parsing

Penalties

S
NP

u(i, t) = 0 for all i,t

VP

A

N

D

A

V

United

ﬂies

some

large

jet

y ∗ = arg max(f (y ) +
y ∈Y

u(i, t)y (i, t))
i,t

Viterbi Decoding
N

V

D

A

United1 ﬂies2 some3 large4

z ∗ = arg max(g (z) −
z∈Z

N
jet5

u(i, t)z(i, t))
i,t

Key
f (y )
Y
y (i, t) = 1

⇐
⇐
if

CFG
Parse Trees
y contains tag t at position i

g (z)
Z

⇐
⇐

HMM
Taggings

CKY Parsing

Penalties

S
NP

u(i, t) = 0 for all i,t

VP

A

N

D

A

V

United

ﬂies

some

large

jet

y ∗ = arg max(f (y ) +
y ∈Y

u(i, t)y (i, t))
i,t

Viterbi Decoding
N

V

D

A

United1 ﬂies2 some3 large4

z ∗ = arg max(g (z) −
z∈Z

N
jet5

u(i, t)z(i, t))
i,t

Key
f (y )
Y
y (i, t) = 1

⇐
⇐
if

CFG
Parse Trees
y contains tag t at position i

g (z)
Z

⇐
⇐

HMM
Taggings

CKY Parsing

Penalties

S
NP

u(i, t) = 0 for all i,t

VP

A

N

D

A

United

ﬂies

some

large

Iteration 1
u(1, A)

V
jet

-1

u(1, N)
y ∗ = arg max(f (y ) +

u(2, N)

u(i, t)y (i, t))

N

V

D

A

United1 ﬂies2 some3 large4

z ∗ = arg max(g (z) −
z∈Z

N
jet5

u(i, t)z(i, t))
i,t

Key
CFG
Parse Trees
y contains tag t at position i

g (z)
Z

⇐
⇐

HMM
Taggings

1

u(5, V )

i,t

Viterbi Decoding

⇐
⇐
if

-1

u(2, V )

-1

u(5, N)

y ∈Y

f (y )
Y
y (i, t) = 1

1

1

CKY Parsing

Penalties
u(i, t) = 0 for all i,t
Iteration 1
u(1, A)

-1

u(1, N)
y ∗ = arg max(f (y ) +

u(2, N)

u(i, t)y (i, t))

United1 ﬂies2 some3 large4

z ∗ = arg max(g (z) −
z∈Z

jet5

u(i, t)z(i, t))
i,t

Key
CFG
Parse Trees
y contains tag t at position i

g (z)
Z

⇐
⇐

HMM
Taggings

1

u(5, V )

i,t

Viterbi Decoding

⇐
⇐
if

-1

u(2, V )

-1

u(5, N)

y ∈Y

f (y )
Y
y (i, t) = 1

1

1

CKY Parsing

Penalties

S
NP

u(i, t) = 0 for all i,t

VP

N

V

United

ﬂies

Iteration 1
u(1, A)

NP
D
some

y ∗ = arg max(f (y ) +

A

N

large

u(i, t)y (i, t))

-1

u(2, V )

United1 ﬂies2 some3 large4

z ∗ = arg max(g (z) −
z∈Z

jet5

u(i, t)z(i, t))
i,t

Key
CFG
Parse Trees
y contains tag t at position i

g (z)
Z

⇐
⇐

HMM
Taggings

1

u(5, V )

i,t

Viterbi Decoding

⇐
⇐
if

1

u(2, N)

jet

-1

u(5, N)

y ∈Y

f (y )
Y
y (i, t) = 1

-1

u(1, N)

1

CKY Parsing

Penalties

S
NP

u(i, t) = 0 for all i,t

VP

N

V

United

ﬂies

Iteration 1
u(1, A)

NP
D
some

y ∗ = arg max(f (y ) +

A

N

large

u(i, t)y (i, t))

-1

u(2, V )

A

N

D

A

United1 ﬂies2 some3 large4

z ∗ = arg max(g (z) −
z∈Z

N
jet5

u(i, t)z(i, t))
i,t

Key
CFG
Parse Trees
y contains tag t at position i

g (z)
Z

⇐
⇐

HMM
Taggings

1

u(5, V )

i,t

Viterbi Decoding

⇐
⇐
if

1

u(2, N)

jet

-1

u(5, N)

y ∈Y

f (y )
Y
y (i, t) = 1

-1

u(1, N)

1

CKY Parsing

Penalties

S
NP

u(i, t) = 0 for all i,t

VP

N

V

United

ﬂies

Iteration 1
u(1, A)

NP
D
some

y ∗ = arg max(f (y ) +

A

N

large

u(i, t)y (i, t))

1

u(2, N)

jet

-1

u(2, V )

Viterbi Decoding
A

N

D

A

United1 ﬂies2 some3 large4

-1
1

Iteration 2
u(5, V )

i,t

1

u(5, V )
u(5, N)

y ∈Y

-1

N
jet5

u(5, N)
z ∗ = arg max(g (z) −
z∈Z

u(i, t)z(i, t))
i,t

Key
f (y )
Y
y (i, t) = 1

⇐
⇐
if

-1

u(1, N)

CFG
Parse Trees
y contains tag t at position i

g (z)
Z

⇐
⇐

HMM
Taggings

1

CKY Parsing

Penalties
u(i, t) = 0 for all i,t
Iteration 1
u(1, A)

-1

u(1, N)
y ∗ = arg max(f (y ) +

u(2, N)

u(i, t)y (i, t))

1
-1

u(2, V )

i,t

Viterbi Decoding
United1 ﬂies2 some3 large4

1

u(5, V )

-1

u(5, N)

y ∈Y

1

Iteration 2
u(5, V )

jet5

u(5, N)
z ∗ = arg max(g (z) −
z∈Z

u(i, t)z(i, t))
i,t

Key
f (y )
Y
y (i, t) = 1

⇐
⇐
if

CFG
Parse Trees
y contains tag t at position i

g (z)
Z

⇐
⇐

HMM
Taggings

-1
1

CKY Parsing

Penalties

S
NP

u(i, t) = 0 for all i,t

VP

N

V

United

ﬂies

Iteration 1
u(1, A)

NP
D
some

y ∗ = arg max(f (y ) +

A

N

large

u(i, t)y (i, t))

1

u(2, N)

jet

-1

u(2, V )

i,t

Viterbi Decoding
United1 ﬂies2 some3 large4

1

u(5, V )

-1

u(5, N)

y ∈Y

1

Iteration 2
u(5, V )

jet5

u(5, N)
z ∗ = arg max(g (z) −
z∈Z

u(i, t)z(i, t))
i,t

Key
f (y )
Y
y (i, t) = 1

⇐
⇐
if

-1

u(1, N)

CFG
Parse Trees
y contains tag t at position i

g (z)
Z

⇐
⇐

HMM
Taggings

-1
1

CKY Parsing

Penalties

S
NP

u(i, t) = 0 for all i,t

VP

N

V

United

ﬂies

Iteration 1
u(1, A)

NP
D
some

y ∗ = arg max(f (y ) +

A

N

large

u(i, t)y (i, t))

1

u(2, N)

jet

-1

u(2, V )

i,t

Viterbi Decoding
N

V

D

A

United1 ﬂies2 some3 large4

1

u(5, V )

-1

u(5, N)

y ∈Y

1

N

Iteration 2
u(5, V )

jet5

u(5, N)
z ∗ = arg max(g (z) −
z∈Z

u(i, t)z(i, t))
i,t

Key
f (y )
Y
y (i, t) = 1

⇐
⇐
if

-1

u(1, N)

CFG
Parse Trees
y contains tag t at position i

g (z)
Z

⇐
⇐

HMM
Taggings

-1
1

CKY Parsing

Penalties

S
NP

u(i, t) = 0 for all i,t

VP

N

V

United

ﬂies

Iteration 1
u(1, A)

NP
D
some

y ∗ = arg max(f (y ) +

A

N

large

-1

u(1, N)

u(i, t)y (i, t))

1

u(2, N)

jet

-1

u(2, V )

i,t

Viterbi Decoding
N

V

D

A

United1 ﬂies2 some3 large4

1

u(5, V )

-1

u(5, N)

y ∈Y

1

N

Iteration 2
u(5, V )

jet5

u(5, N)
z ∗ = arg max(g (z) −
z∈Z

-1
1

u(i, t)z(i, t))

Converged

i,t

y ∗ = arg max f (y ) + g (y )

Key
f (y )
Y
y (i, t) = 1

⇐
⇐
if

CFG
Parse Trees
y contains tag t at position i

g (z)
Z

⇐
⇐

HMM
Taggings

y ∈Y

Main theorem

theorem: if at any iteration, for all i, t ∈ T
y (k) (i, t) = z (k) (i, t)
then (y (k) , z (k) ) is the global optimum
proof: focus of the next section

Convergence
100

% examples converged

80

60

40

20

0

50
<=

20
<=

10
<=

4
<=

3
<=

2
<=

1
<=

number of iterations

2. Formal properties
aim: formal derivation of the algorithm given in the previous
section
•

derive Lagrangian dual

•

prove three properties
upper bound
convergence
optimality

•

describe subgradient method

Lagrangian
goal:
arg max f (y ) + g (z) such that y (i, t) = z(i, t)
y ∈Y,z∈Z

Lagrangian:
L(u, y , z) = f (y ) + g (z) +
i,t

redistribute terms


L(u, y , z) = f (y ) +

i,t

u(i, t) (y (i, t) − z(i, t))




u(i, t)y (i, t) + g (z) −

i,t



u(i, t)z(i, t)

Lagrangian dual
Lagrangian:


L(u, y , z) = f (y ) +

i,t

Lagrangian dual:

L(u) =



u(i, t)y (i, t) + g (z) −

max L(u, y , z)


y ∈Y,z∈Z

= max f (y ) +
y ∈Y



max g (z) −
z∈Z





i,t

u(i, t)y (i, t) +

i,t

u(i, t)z(i, t)



i,t



u(i, t)z(i, t)

Theorem 1. Upper bound

deﬁne:
•

y ∗ , z ∗ is the optimal combined parsing and tagging solution
with y ∗ (i, t) = z ∗ (i, t) for all i, t

theorem: for any value of u
L(u) ≥ f (y ∗ ) + g (z ∗ )
L(u) provides an upper bound on the score of the optimal solution
note: upper bound may be useful as input to branch and bound or
A* search

Theorem 1. Upper bound (proof)

theorem: for any value of u, L(u) ≥ f (y ∗ ) + g (z ∗ )
proof:
L(u) =
≥
=

max L(u, y , z)

y ∈Y,z∈Z

(1)

max

L(u, y , z)

(2)

max

f (y ) + g (z)

(3)

y ∈Y,z∈Z:y =z

y ∈Y,z∈Z:y =z
∗
∗

= f (y ) + g (z )

(4)

Formal algorithm (reminder)
Set u (1) (i, t) = 0 for all i, t ∈ T
For k = 1 to K
y (k) ← arg max f (y ) +
y ∈Y

z (k) ← arg max g (z) −
z∈Z

u (k) (i, t)y (i, t) [Parsing]
i,t

u (k) (i, t)z(i, t) [Tagging]
i,t

If y (k) (i, t) = z (k) (i, t) for all i, t Return (y (k) , z (k) )
Else u (k+1) (i, t) ← u (k) (i, t) − αk (y (k) (i, t) − z (k) (i, t))

Theorem 2. Convergence
notation:
•
•
•

u (k+1) (i, t) ← u (k) (i, t) + αk (y (k) (i, t) − z (k) (i, t)) is update
u (k) is the penalty vector at iteration k
αk is the update rate at iteration k

theorem: for any sequence α1 , α2 , α3 , . . . such that
lim αt = 0 and

t→∞

∞
t=1

αt = ∞,

we have
lim L(u t ) = min L(u)

t→∞

u

i.e. the algorithm converges to the tightest possible upper bound
proof: by subgradient convergence (next section)

Dual solutions
deﬁne:
•

for any value of u
yu = arg max f (y ) +
y ∈Y

and



zu = arg max g (z) −
z∈Z

•



i,t

i,t



u(i, t)y (i, t)


u(i, t)z(i, t)

yu and zu are the dual solutions for a given u

Theorem 3. Optimality
theorem: if there exists u such that
yu (i, t) = zu (i, t)
for all i, t then
f (yu ) + g (zu ) = f (y ∗ ) + g (z ∗ )
i.e. if the dual solutions agree, we have an optimal solution
(yu , zu )

Theorem 3. Optimality (proof)
theorem: if u such that yu (i, t) = zu (i, t) for all i, t then
f (yu ) + g (zu ) = f (y ∗ ) + g (z ∗ )
proof: by the deﬁnitions of yu and zu
L(u) = f (yu ) + g (zu ) +
i,t

u(i, t)(yu (i, t) − zu (i, t))

= f (yu ) + g (zu )
since L(u) ≥ f (y ∗ ) + g (z ∗ ) for all values of u
f (yu ) + g (zu ) ≥ f (y ∗ ) + g (z ∗ )
but y ∗ and z ∗ are optimal
f (yu ) + g (zu ) ≤ f (y ∗ ) + g (z ∗ )

Dual optimization
Lagrangian dual:
L(u) =

max L(u, y , z)


y ∈Y,z∈Z

= max f (y ) +
y ∈Y



max g (z) −
z∈Z



i,t

u(i, t)y (i, t) +

i,t

u(i, t)z(i, t)



goal: dual problem is to ﬁnd the tightest upper bound
min L(u)
u

Dual subgradient
L(u)

=



max f (y ) +
y ∈Y

i,t





u(i, t)y (i, t) + max g (z) −
z∈Z

i,t



u(i, t)z(i, t)

properties:
• L(u) is convex in u (no local minima)
• L(u) is not diﬀerentiable (because of max operator)
handle non-diﬀerentiability by using subgradient descent

deﬁne: a subgradient of L(u) at u is a vector gu such that for all v
L(v ) ≥ L(u) + gu · (v − u)

Subgradient algorithm
L(u)

=



max f (y ) +
y ∈Y

i,t





u(i, t)y (i, t) + max g (z) −
z∈Z

i,j

u(i, t)z(i, t)

recall, yu and zu are the argmax’s of the two terms
subgradient:
gu (i, t)



= yu (i, t) − zu (i, t)

subgradient descent: move along the subgradient
u (i, t) = u(i, t) − α (yu (i, t) − zu (i, t))
guaranteed to ﬁnd a minimum with conditions given earlier for α

3. More examples

aim: demonstrate similar algorithms that can be applied to other
decoding applications
•

context-free parsing combined with dependency parsing

•

corpus-level part-of-speech tagging

•

combined translation alignment

Combined constituency and dependency parsing
(Rush et al., 2010)

setup: assume separate models trained for constituency and
dependency parsing
problem: ﬁnd constituency parse that maximizes the sum of the
two models
example:
•

combine lexicalized CFG with second-order dependency parser

Lexicalized constituency parsing
notation:
• Y is set of lexicalized constituency parses for input
• y ∈ Y is a valid parse
• f (y ) scores a parse tree
goal:
arg max f (y )
y ∈Y

example: a lexicalized context-free grammar
S(ﬂies)
NP(United)

VP(ﬂies)

N

V

United

ﬂies

NP(jet)
D

A

N

some large jet

Dependency parsing
deﬁne:
•
•
•

Z is set of dependency parses for input
z ∈ Z is a valid dependency parse

g (z) scores a dependency parse

example:

*0

United1 ﬂies2

some3 large4

jet5

Identifying dependencies
notation: identify the dependencies selected by each model
• y (i, j) = 1 when word i modiﬁes of word j in constituency
parse y
• z(i, j) = 1 when word i modiﬁes of word j in dependency
parse z
example: a constituency and dependency parse with y (3, 5) = 1
and z(3, 5) = 1
S(ﬂies)
NP(United)

VP(ﬂies)

N

V

NP(jet)

United

ﬂies

D

A

N

some large jet

y

*0

United1 ﬂies2

some3 large4

z

jet5

Combined optimization

goal:
arg max f (y ) + g (z)
y ∈Y,z∈Z

such that for all i = 1 . . . n, j = 0 . . . n,
y (i, j) = z(i, j)

CKY Parsing

Penalties
u(i, j) = 0 for all i,j

y ∗ = arg max(f (y ) +
y ∈Y

u(i, j)y (i, j))
i,j

Dependency Parsing
*0

United1 ﬂies2 some3 large4

z ∗ = arg max(g (z) −
z∈Z

jet5

u(i, j)z(i, j))
i,j

Key
f (y )
Y
y (i, j) = 1

⇐
⇐
if

CFG
Parse Trees
y contains dependency i, j

g (z)
Z

⇐
⇐

Dependency Model
Dependency Trees

CKY Parsing

Penalties

S(ﬂies)
NP

u(i, j) = 0 for all i,j

VP(ﬂies)

N

V

D

United

ﬂies

some

NP(jet)

y = arg max(f (y ) +
y ∈Y

N

large
∗

A

jet

u(i, j)y (i, j))
i,j

Dependency Parsing
*0

United1 ﬂies2 some3 large4

z ∗ = arg max(g (z) −
z∈Z

jet5

u(i, j)z(i, j))
i,j

Key
f (y )
Y
y (i, j) = 1

⇐
⇐
if

CFG
Parse Trees
y contains dependency i, j

g (z)
Z

⇐
⇐

Dependency Model
Dependency Trees

CKY Parsing

Penalties

S(ﬂies)
NP

u(i, j) = 0 for all i,j

VP(ﬂies)

N

V

D

United

ﬂies

some

NP(jet)

y = arg max(f (y ) +
y ∈Y

N

large
∗

A

jet

u(i, j)y (i, j))
i,j

Dependency Parsing
*0

United1 ﬂies2 some3 large4

z ∗ = arg max(g (z) −
z∈Z

jet5

u(i, j)z(i, j))
i,j

Key
f (y )
Y
y (i, j) = 1

⇐
⇐
if

CFG
Parse Trees
y contains dependency i, j

g (z)
Z

⇐
⇐

Dependency Model
Dependency Trees

CKY Parsing

Penalties

S(ﬂies)
NP

u(i, j) = 0 for all i,j

VP(ﬂies)

N

V

D

United

ﬂies

some

NP(jet)

y = arg max(f (y ) +
y ∈Y

N

large
∗

A

jet

u(i, j)y (i, j))
i,j

Dependency Parsing
*0

United1 ﬂies2 some3 large4

z ∗ = arg max(g (z) −
z∈Z

jet5

u(i, j)z(i, j))
i,j

Key
f (y )
Y
y (i, j) = 1

⇐
⇐
if

CFG
Parse Trees
y contains dependency i, j

g (z)
Z

⇐
⇐

Dependency Model
Dependency Trees

CKY Parsing

Penalties

S(ﬂies)
NP

u(i, j) = 0 for all i,j

VP(ﬂies)

N

V

D

United

ﬂies

some

A
large

y ∗ = arg max(f (y ) +
y ∈Y

Iteration 1
u(2, 3)

NP(jet)
N

u(5, 3)

jet

u(i, j)y (i, j))
i,j

Dependency Parsing
*0

United1 ﬂies2 some3 large4

z ∗ = arg max(g (z) −
z∈Z

jet5

u(i, j)z(i, j))
i,j

Key
f (y )
Y
y (i, j) = 1

⇐
⇐
if

CFG
Parse Trees
y contains dependency i, j

g (z)
Z

⇐
⇐

Dependency Model
Dependency Trees

-1
1

CKY Parsing

Penalties
u(i, j) = 0 for all i,j
Iteration 1
u(2, 3)
u(5, 3)

y ∗ = arg max(f (y ) +
y ∈Y

u(i, j)y (i, j))
i,j

Dependency Parsing
*0

United1 ﬂies2 some3 large4

z ∗ = arg max(g (z) −
z∈Z

jet5

u(i, j)z(i, j))
i,j

Key
f (y )
Y
y (i, j) = 1

⇐
⇐
if

CFG
Parse Trees
y contains dependency i, j

g (z)
Z

⇐
⇐

Dependency Model
Dependency Trees

-1
1

CKY Parsing

Penalties

S(ﬂies)
NP

u(i, j) = 0 for all i,j

VP(ﬂies)

N

V

United

ﬂies

Iteration 1
u(2, 3)

NP(jet)
D

A

some

large

y ∗ = arg max(f (y ) +
y ∈Y

N

u(5, 3)

jet

u(i, j)y (i, j))
i,j

Dependency Parsing
*0

United1 ﬂies2 some3 large4

z ∗ = arg max(g (z) −
z∈Z

jet5

u(i, j)z(i, j))
i,j

Key
f (y )
Y
y (i, j) = 1

⇐
⇐
if

CFG
Parse Trees
y contains dependency i, j

g (z)
Z

⇐
⇐

Dependency Model
Dependency Trees

-1
1

CKY Parsing

Penalties

S(ﬂies)
NP

u(i, j) = 0 for all i,j

VP(ﬂies)

N

V

United

ﬂies

Iteration 1
u(2, 3)

NP(jet)
D

A

some

large

y ∗ = arg max(f (y ) +
y ∈Y

N

u(5, 3)

jet

u(i, j)y (i, j))
i,j

Dependency Parsing
*0

United1 ﬂies2 some3 large4

z ∗ = arg max(g (z) −
z∈Z

jet5

u(i, j)z(i, j))
i,j

Key
f (y )
Y
y (i, j) = 1

⇐
⇐
if

CFG
Parse Trees
y contains dependency i, j

g (z)
Z

⇐
⇐

Dependency Model
Dependency Trees

-1
1

CKY Parsing

Penalties

S(ﬂies)
NP

u(i, j) = 0 for all i,j

VP(ﬂies)

N

V

United

ﬂies

Iteration 1
u(2, 3)

NP(jet)
D

A

some

large

y ∗ = arg max(f (y ) +
y ∈Y

N

u(5, 3)

jet

u(i, j)y (i, j))
∗

y = arg max f (y ) + g (y )

United1 ﬂies2 some3 large4

z ∗ = arg max(g (z) −
z∈Z

jet5

u(i, j)z(i, j))
i,j

Key
f (y )
Y
y (i, j) = 1

⇐
⇐
if

1

Converged

i,j

Dependency Parsing
*0

-1

CFG
Parse Trees
y contains dependency i, j

g (z)
Z

⇐
⇐

Dependency Model
Dependency Trees

y ∈Y

Convergence
100

% examples converged

80

60

40

20

0

50
<=

20
<=

10
<=

4
<=

3
<=

2
<=

1
<=

number of iterations

Integrated Constituency and Dependency Parsing: Accuracy
92

Collins
Dep
Dual

91

90

89

88

87

F1 Score
Collins (1997) Model 1
Fixed, First-best Dependencies from Koo (2008)
Dual Decomposition

Corpus-level tagging

setup: given a corpus of sentences and a trained sentence-level
tagging model
problem: ﬁnd best tagging for each sentence, while at the same
time enforcing inter-sentence soft constraints
example:
•

test-time decoding with a trigram tagger

•

constraint that each word type prefer a single POS tag

Corpus-level tagging
N

English

is

my

ﬁrst

langauge

He

studies

language

arts

now

Language

makes

us

human

beings

Sentence-level decoding
notation:
• Yi is set of tag sequences for input sentence i
• Y = Y1 × . . . × Ym is set of tag sequences for the input corpus
• Y ∈ Y is a valid tag sequence for the corpus
• F (Y ) =
f (Yi ) is the score for tagging the whole corpus
i

goal:
arg max F (Y )
Y ∈Y

example: decode each sentence with a trigram tagger
N

V

P

A

N

English

is

my

ﬁrst

language

P

V

A

N

R

He

studies

language

arts

now

Inter-sentence constraints
notation:
•
•
•

Z is set of possible assignments of tags to word types
z ∈ Z is a valid tag assignment

g (z) is a scoring function for assignments to word types

example: an MRF model that encourages words of the same type
to choose the same tag
z1

z2

N

N

N

N

N

N

A

N

language

language

language

language

language

language

g (z1 ) > g (z2 )

Identifying word tags

notation: identify the tag labels selected by each model
•
•

Ys (i, t) = 1 when the tagger for sentence s at position i
selects tag t
z(s, i, t) = 1 when the constraint assigns at sentence s
position i the tag t

example: a parse and tagging with Y1 (5, N) = 1 and
z(1, 5, N) = 1

English

is

my

ﬁrst

language

He

studies

language

arts

now

language

Y

language

z

language

Combined optimization

goal:
arg

max

Y ∈Y,z∈Z

F (Y ) + g (z)

such that for all s = 1 . . . m, i = 1 . . . n, t ∈ T ,
Ys (i, t) = z(s, i, t)

Penalties
Tagging

u(s, i, t) = 0 for all s,i,t

MRF

Key
F (Y )
Y
Ys (i, t) = 1

⇐
⇐
if

Tagging model
Sentence-level tagging
sentence s has tag t at position i

g (z)
Z

⇐
⇐

MRF
Inter-sentence constraints

Penalties
Tagging

u(s, i, t) = 0 for all s,i,t
N

V

P

A

N

English

is

my

ﬁrst

language

P

V

A

N

R

He

studies

language

arts

now

N

V

P

N

N

Language

makes

us

human

beings

MRF

Key
F (Y )
Y
Ys (i, t) = 1

⇐
⇐
if

Tagging model
Sentence-level tagging
sentence s has tag t at position i

g (z)
Z

⇐
⇐

MRF
Inter-sentence constraints

Penalties
Tagging

u(s, i, t) = 0 for all s,i,t
N

V

P

A

N

English

is

my

ﬁrst

language

P

V

A

N

R

He

studies

language

arts

now

N

V

P

N

N

Language

makes

us

human

beings

MRF
A

A

A

A

language

language

language

Key
F (Y )
Y
Ys (i, t) = 1

⇐
⇐
if

Tagging model
Sentence-level tagging
sentence s has tag t at position i

g (z)
Z

⇐
⇐

MRF
Inter-sentence constraints

Penalties
Tagging

u(s, i, t) = 0 for all s,i,t
N

V

P

A

N

English

is

my

ﬁrst

language

P

V

A

N

R

He

studies

language

arts

now

N

V

P

N

N

Language

makes

us

human

beings

MRF
A

A

A

A

language

language

language

Key
F (Y )
Y
Ys (i, t) = 1

⇐
⇐
if

Tagging model
Sentence-level tagging
sentence s has tag t at position i

g (z)
Z

⇐
⇐

MRF
Inter-sentence constraints

Penalties
Tagging

u(s, i, t) = 0 for all s,i,t
Iteration 1
u(1, 5, N) -1

N

V

P

A

N

English

is

my

ﬁrst

language

u(1, 5, A)

1

P

V

A

N

R

u(3, 1, N)

-1

He

studies

language

arts

now

u(3, 1, A)

1

N

V

P

N

N

Language

makes

us

human

beings

MRF
A

A

A

A

language

language

language

Key
F (Y )
Y
Ys (i, t) = 1

⇐
⇐
if

Tagging model
Sentence-level tagging
sentence s has tag t at position i

g (z)
Z

⇐
⇐

MRF
Inter-sentence constraints

Penalties
Tagging

u(s, i, t) = 0 for all s,i,t
Iteration 1
u(1, 5, N) -1
u(1, 5, A)

1

u(3, 1, N)

-1

u(3, 1, A)

1

MRF

Key
F (Y )
Y
Ys (i, t) = 1

⇐
⇐
if

Tagging model
Sentence-level tagging
sentence s has tag t at position i

g (z)
Z

⇐
⇐

MRF
Inter-sentence constraints

Penalties
Tagging

u(s, i, t) = 0 for all s,i,t
Iteration 1
u(1, 5, N) -1

N

V

P

A

N

English

is

my

ﬁrst

language

u(1, 5, A)

1

P

V

A

N

R

u(3, 1, N)

-1

He

studies

language

arts

now

u(3, 1, A)

1

N

V

P

N

N

Language

makes

us

human

beings

MRF

Key
F (Y )
Y
Ys (i, t) = 1

⇐
⇐
if

Tagging model
Sentence-level tagging
sentence s has tag t at position i

g (z)
Z

⇐
⇐

MRF
Inter-sentence constraints

Penalties
Tagging

u(s, i, t) = 0 for all s,i,t
Iteration 1
u(1, 5, N) -1

N

V

P

A

N

English

is

my

ﬁrst

language

u(1, 5, A)

1

P

V

A

N

R

u(3, 1, N)

-1

He

studies

language

arts

now

u(3, 1, A)

1

N

V

P

N

N

Language

makes

us

human

beings

MRF
N

N

N

N

language

language

language

Key
F (Y )
Y
Ys (i, t) = 1

⇐
⇐
if

Tagging model
Sentence-level tagging
sentence s has tag t at position i

g (z)
Z

⇐
⇐

MRF
Inter-sentence constraints

Penalties
Tagging

u(s, i, t) = 0 for all s,i,t
Iteration 1
u(1, 5, N) -1

N

V

P

A

N

English

is

my

ﬁrst

language

u(1, 5, A)

1

P

V

A

N

R

u(3, 1, N)

-1

He

studies

language

arts

now

u(3, 1, A)

1

N

V

P

N

N

Language

makes

us

human

beings

MRF
N

N

N

N

language

language

language

Key
F (Y )
Y
Ys (i, t) = 1

⇐
⇐
if

Tagging model
Sentence-level tagging
sentence s has tag t at position i

g (z)
Z

⇐
⇐

MRF
Inter-sentence constraints

Penalties
Tagging

u(s, i, t) = 0 for all s,i,t
Iteration 1
u(1, 5, N) -1

N

V

P

A

N

English

is

my

ﬁrst

language

u(1, 5, A)

1

P

V

A

N

R

u(3, 1, N)

-1

He

studies

language

arts

now

u(3, 1, A)

1

N

V

P

N

N

Language

makes

us

human

beings

Iteration 2
u(1, 5, N)

-1

u(1, 5, A)

1

u(3, 1, N)

-1

MRF
N

u(3, 1, A)

N

N

language

language

1

u(2, 3, A)

N

1

u(2, 3, N)

-1

language

Key
F (Y )
Y
Ys (i, t) = 1

⇐
⇐
if

Tagging model
Sentence-level tagging
sentence s has tag t at position i

g (z)
Z

⇐
⇐

MRF
Inter-sentence constraints

Penalties
Tagging

u(s, i, t) = 0 for all s,i,t
Iteration 1
u(1, 5, N) -1

N

V

P

A

N

English

is

my

ﬁrst

language

u(1, 5, A)

1

P

V

N

N

R

u(3, 1, N)

-1

He

studies

language

arts

now

u(3, 1, A)

1

N

V

P

N

N

Language

makes

us

human

beings

Iteration 2
u(1, 5, N)

-1

u(1, 5, A)

1

u(3, 1, N)

-1

MRF

u(3, 1, A)

1

u(2, 3, N)

1

u(2, 3, A)

-1

Key
F (Y )
Y
Ys (i, t) = 1

⇐
⇐
if

Tagging model
Sentence-level tagging
sentence s has tag t at position i

g (z)
Z

⇐
⇐

MRF
Inter-sentence constraints

Penalties
Tagging

u(s, i, t) = 0 for all s,i,t
Iteration 1
u(1, 5, N) -1

N

V

P

A

N

English

is

my

ﬁrst

language

u(1, 5, A)

1

P

V

N

N

R

u(3, 1, N)

-1

He

studies

language

arts

now

u(3, 1, A)

1

N

V

P

N

N

Language

makes

us

human

beings

Iteration 2
u(1, 5, N)

-1

u(1, 5, A)

1

u(3, 1, N)

-1

MRF
N

u(3, 1, A)

N

N

language

language

1

u(2, 3, A)

N

1

u(2, 3, N)

-1

language

Key
F (Y )
Y
Ys (i, t) = 1

⇐
⇐
if

Tagging model
Sentence-level tagging
sentence s has tag t at position i

g (z)
Z

⇐
⇐

MRF
Inter-sentence constraints

Combined alignment

(DeNero and Macherey, 2011)

setup: assume separate models trained for English-to-French and
French-to-English alignment
problem: ﬁnd an alignment that maximizes the score of both
models
example:
•

HMM models for both directional alignments (assume correct
alignment is one-to-one for simplicity)

English-to-French alignment

deﬁne:
•
•
•

Y is set of all possible English-to-French alignments
y ∈ Y is a valid alignment
f (y ) scores of the alignment

example: HMM alignment
Le1

laid3

chien2

a4

rouge6

fourrure5

The1

ugly2

dog3

has4

red5

fur6

French-to-English alignment

deﬁne:
•
•
•

Z is set of all possible French-to-English alignments
z ∈ Z is a valid alignment
g (z) scores of an alignment

example: HMM alignment
The1

ugly2

dog3

has4

fur6

red5

Le1

chien2

laid3

a4

fourrure5

rouge6

Identifying word alignments
notation: identify the tag labels selected by each model
•

y (i, j) = 1 when e-to-f alignment y selects French word i to
align with English word j

•

z(i, j) = 1 when f-to-e alignment z selects French word i to
align with English word j

example: two HMM alignment models with y (6, 5) = 1 and
z(6, 5) = 1

Combined optimization

goal:
arg max f (y ) + g (z)
y ∈Y,z∈Z

such that for all i = 1 . . . n, j = 1 . . . n,
y (i, j) = z(i, j)

English-to-French

Penalties
u(i, j) = 0 for all i,j

y ∗ = arg max(f (y ) +
y ∈Y

u(i, j)y (i, j))
i,j

French-to-English

z ∗ = arg max(g (z) −
z∈Z

u(i, j)z(i, j))
i,j

Key
f (y )
Y
y (i, j) = 1

⇐
⇐
if

HMM Alignment
English-to-French model
French word i aligns to English word j

g (z)
Z

⇐
⇐

HMM Alignment
French-to-English model

English-to-French

Penalties
u(i, j) = 0 for all i,j

y ∗ = arg max(f (y ) +
y ∈Y

u(i, j)y (i, j))
i,j

French-to-English

z ∗ = arg max(g (z) −
z∈Z

u(i, j)z(i, j))
i,j

Key
f (y )
Y
y (i, j) = 1

⇐
⇐
if

HMM Alignment
English-to-French model
French word i aligns to English word j

g (z)
Z

⇐
⇐

HMM Alignment
French-to-English model

English-to-French

Penalties
u(i, j) = 0 for all i,j

y ∗ = arg max(f (y ) +
y ∈Y

u(i, j)y (i, j))
i,j

French-to-English

z ∗ = arg max(g (z) −
z∈Z

u(i, j)z(i, j))
i,j

Key
f (y )
Y
y (i, j) = 1

⇐
⇐
if

HMM Alignment
English-to-French model
French word i aligns to English word j

g (z)
Z

⇐
⇐

HMM Alignment
French-to-English model

English-to-French

Penalties
u(i, j) = 0 for all i,j

y ∗ = arg max(f (y ) +
y ∈Y

u(i, j)y (i, j))
i,j

French-to-English

z ∗ = arg max(g (z) −
z∈Z

u(i, j)z(i, j))
i,j

Key
f (y )
Y
y (i, j) = 1

⇐
⇐
if

HMM Alignment
English-to-French model
French word i aligns to English word j

g (z)
Z

⇐
⇐

HMM Alignment
French-to-English model

English-to-French

Penalties
u(i, j) = 0 for all i,j
Iteration 1
u(3, 2)
-1
u(2, 2)

y ∗ = arg max(f (y ) +
y ∈Y

-1

u(3, 3)

u(i, j)y (i, j))

1

u(2, 3)

1

i,j

French-to-English

z ∗ = arg max(g (z) −
z∈Z

u(i, j)z(i, j))
i,j

Key
f (y )
Y
y (i, j) = 1

⇐
⇐
if

HMM Alignment
English-to-French model
French word i aligns to English word j

g (z)
Z

⇐
⇐

HMM Alignment
French-to-English model

English-to-French

Penalties
u(i, j) = 0 for all i,j
Iteration 1
u(3, 2)
-1
u(2, 2)

y ∗ = arg max(f (y ) +
y ∈Y

-1

u(3, 3)

u(i, j)y (i, j))

1

u(2, 3)

1

i,j

French-to-English

z ∗ = arg max(g (z) −
z∈Z

u(i, j)z(i, j))
i,j

Key
f (y )
Y
y (i, j) = 1

⇐
⇐
if

HMM Alignment
English-to-French model
French word i aligns to English word j

g (z)
Z

⇐
⇐

HMM Alignment
French-to-English model

English-to-French

Penalties
u(i, j) = 0 for all i,j
Iteration 1
u(3, 2)
-1
u(2, 2)

y ∗ = arg max(f (y ) +
y ∈Y

-1

u(3, 3)

u(i, j)y (i, j))

1

u(2, 3)

1

i,j

French-to-English

z ∗ = arg max(g (z) −
z∈Z

u(i, j)z(i, j))
i,j

Key
f (y )
Y
y (i, j) = 1

⇐
⇐
if

HMM Alignment
English-to-French model
French word i aligns to English word j

g (z)
Z

⇐
⇐

HMM Alignment
French-to-English model

English-to-French

Penalties
u(i, j) = 0 for all i,j
Iteration 1
u(3, 2)
-1
u(2, 2)

y ∗ = arg max(f (y ) +
y ∈Y

-1

u(3, 3)

u(i, j)y (i, j))

1

u(2, 3)

1

i,j

French-to-English

z ∗ = arg max(g (z) −
z∈Z

u(i, j)z(i, j))
i,j

Key
f (y )
Y
y (i, j) = 1

⇐
⇐
if

HMM Alignment
English-to-French model
French word i aligns to English word j

g (z)
Z

⇐
⇐

HMM Alignment
French-to-English model

English-to-French

Penalties
u(i, j) = 0 for all i,j
Iteration 1
u(3, 2)
-1
u(2, 2)

y ∗ = arg max(f (y ) +
y ∈Y

-1

u(3, 3)

u(i, j)y (i, j))

1

u(2, 3)

1

i,j

French-to-English

z ∗ = arg max(g (z) −
z∈Z

u(i, j)z(i, j))
i,j

Key
f (y )
Y
y (i, j) = 1

⇐
⇐
if

HMM Alignment
English-to-French model
French word i aligns to English word j

g (z)
Z

⇐
⇐

HMM Alignment
French-to-English model

English-to-French

Penalties
u(i, j) = 0 for all i,j
Iteration 1
u(3, 2)
-1
u(2, 2)

y ∗ = arg max(f (y ) +
y ∈Y

-1

u(3, 3)

u(i, j)y (i, j))

1

u(2, 3)

1

i,j

French-to-English

z ∗ = arg max(g (z) −
z∈Z

u(i, j)z(i, j))
i,j

Key
f (y )
Y
y (i, j) = 1

⇐
⇐
if

HMM Alignment
English-to-French model
French word i aligns to English word j

g (z)
Z

⇐
⇐

HMM Alignment
French-to-English model

4. Practical issues

aim: overview of practical dual decomposition techniques
•

tracking the progress of the algorithm

•

choice of update rate αk

•

lazy update of dual solutions

•

extracting solutions if algorithm does not converge

Optimization tracking

at each stage of the algorithm there are several useful values
track:
•
•
•
•

y (k) , z (k) are current dual solutions
L(u (k) ) is the current dual value
y (k) , l(y (k) ) is a potential primal feasible solution
f (y (k) ) + g (l(y (k) )) is the potential primal value

Tracking example
-13
-14

Value

-15
-16
-17
-18

Current Primal
Current Dual

-19
0

10

20

30
Round

40

50

60

example run from syntactic machine translation (later in talk)
•

current primal

•

current dual

f (y (k) ) + g (l(y (k) ))
L(u (k) )

Optimization progress

useful signals:
•
•
•

L(u (k) ) − L(u (k−1) ) is the dual change (may be positive)

min L(u (k) ) is the best dual value (tightest upper bound)
k

max f (y (k) ) + g (l(y (k) )) is the best primal value
k

the optimal value must be between the best dual and primal values

Progress example
-13

best primal

-14

max f (y (k) ) + g (l(y (k) ))

Value

-15

k

-16

best dual

-17
-18

Best Primal
Best Dual

-19
0

10

20

30
Round

40

4

50

min L(u (k) )
k

60

Gap

gap

3.5

Value

3

min L(u k ) −

2.5
2

k

1.5

max f (y (k) ) + g (l(y (k) )

1

k

0.5
0
0

10

20

30
Round

40

50

60

Update rate
choice of αk has important practical consequences
•

αk too high causes dual value to ﬂuctuate

•

αk too low means slow progress
-13

0.01
0.005
0.0005

-13.5

Value

-14
-14.5
-15
-15.5
-16
0

5

10

15

20
Round

25

30

35

40

Update rate

practical: ﬁnd a rate that is robust to varying inputs
•
•
•

αk = c (constant rate) can be very fast, but hard to ﬁnd
constant that works for all problems
c
αk = (decreasing rate) often cuts rate too aggressively,
k
lowers value even when making progress
rate based on dual progress

c
where t < k is number of iterations where dual
t +1
value increased
robust in practice, reduces rate when dual value is ﬂuctuating

αk =

Lazy decoding

idea: don’t recompute y (k) or z (k) from scratch each iteration
lazy decoding: if subgradient u (k) is sparse, then y (k) may be
very easy to compute from y (k−1)
use:
•

helpful if y or z factor naturally into independent components

•

can be important for fast decompositions

Lazy decoding example

recall corpus-level tagging
example
at this iteration, only
sentence 2 receives a
weight update
with lazy decoding
(k)

(k−1)

Y1 ← Y1
(k)
(k−1)
Y3 ← Y3

Lazy decoding results

% of Head Automata Recomputed

lazy decoding is critical for the eﬃciency of some applications
30

% recomputed, g+s
% recomputed, sib

25
20
15
10
5
0
0

1000

2000
3000
4000
Iterations of Dual Decomposition

5000

recomputation statistics for non-projective dependency parsing

Approximate solution
upon agreement the solution is exact, but this may not occur
otherwise, there is an easy way to ﬁnd an approximate solution
choose: the structure y (k ) where
k = arg max f (y (k) ) + g (l(y (k) ))
k

is the iteration with the best primal score
guarantee: the solution y k is non-optimal by at most
(min L(u k )) − (f (y (k ) ) + g (l(y (k ) )))
k

there are other methods to estimate solutions, for instance by
averaging solutions (see Nedi´ and Ozdaglar (2009))
c

Choosing best solution
0
Best Primal
-5

Value

-10
-15
-20
-25

Current Primal
Current Dual

-30
0

10

20

30

40
Round

50

60

non-exact example from syntactic translation
best approximate primal solution occurs at iteration 63

70

Early stopping results
early stopping results for constituency and dependency parsing
100

Percentage

90
80
70
60

f score
% certificates
% match K=50

50
0

10
20
30
40
Maximum Number of Dual Decomposition Iterations

50

Early stopping results
early stopping results for non-projective dependency parsing
100

Percentage

90
80
70
60

% validation UAS
% certificates
% match K=5000

50
0

200
400
600
800
Maximum Number of Dual Decomposition Iterations

1000

Tightening

instead of using approximate solution, can tighten the algorithm
may help ﬁnd an exact solution at the cost of added complexity
this technique is the focus of the next section

5. Linear programming

aim: explore the connections between dual decomposition and
linear programming
•

basic optimization over the simplex

•

formal properties of linear programming

•

full example with fractional optimal solutions

•

tightening linear program relaxations

Simplex
deﬁne:
• ∆y ⊂ R|Y| is the simplex over Y where α ∈ ∆y implies
αy ≥ 0 and
•
•
•

αy = 1
y

α is distribution over Y
∆z is the simplex over Z
δy : Y → ∆y maps elements to the simplex

example:

δy (y1 )

Y = {y1 , y2 , y3 }
vertices
•

δy (y1 ) = (1, 0, 0)

•

δy (y2 ) = (0, 1, 0)

•

δy (y3 ) = (0, 0, 1)

∆y
δy (y2 )

δy (y 3)

Theorem 1. Simplex linear program
optimize over the simplex ∆y instead of the discrete sets Y
goal: optimize linear program
max

α∈∆y

αy f (y )
y

theorem:
max f (y ) = max
y ∈Y

α∈∆y

αy f (y )
y

proof: points in Y correspond to the exteme points of simplex
{δy (y ) : y ∈ Y}
linear program has optimum at extreme point
note: ﬁnding the highest scoring distribution α over Y
proof shows that best distribution chooses a single parse

Combined linear program
optimize over the simplices ∆y and ∆z instead of the discrete sets
Y and Z
goal: optimize linear program
max

α∈∆y ,β∈∆z

αy f (y ) +
y

βz g (z)
z

such that for all i, t
αy y (i, t) =
y

βz z(i, t)
z

note: the two distributions must match in expectation of POS tags
the best distributions α∗ ,β ∗ are possibly no longer a single parse
tree or tag sequence

Lagrangian
Lagrangian:
M(u, α, β)

=

αy f (y ) +
y

βz g (z) +

=

i,t

αy f (y ) +
y

u(i, t)

βz g (z) −

αy y (i, t)

y

+

y

i,t

z

αy y (i, t) −

u(i, t)

z

u(i, t)

βz z(i, t)
z

i,t

Lagrangian dual:
M(u) =

max

α∈∆y ,β∈∆z

M(u, α, β)

βz z(i, t)
z

Theorem 2. Strong duality

deﬁne:
•

α∗ , β ∗ is the optimal assignment to α, β in the linear program

theorem:
∗
αy f (y ) +

min M(u) =
u

y

proof: by linear programming duality

∗
βz g (z)
z

Theorem 3. Dual relationship

theorem: for any value of u,
M(u) = L(u)
note: solving the original Lagrangian dual also solves dual of the
linear program

Theorem 3. Dual relationship (proof sketch)
focus on Y term in Lagrangian
L(u)

M(u)

=

=



max f (y ) +
y ∈Y



max 

α∈∆y

i,t



u(i, t)y (i, t) + . . .

αy f (y ) +
y

u(i, t)
i,t

y



αy y (i, t) + . . .

by theorem 1. optimization over Y and ∆y have the same max
similar argument for Z gives L(u) = M(u)

Summary

f (y ) + g (z)
L(u)
y αy f (y ) +
M(u)

z

original primal objective
original dual
LP primal objective
LP dual

βz g (z)

relationship between LP dual, original dual, and LP primal objective
u

u

∗
βz g (z)

∗
αy f (y ) +

min M(u) = min L(u) =
y

z

Primal relationship
deﬁne:
•

Q ⊆ ∆y × ∆z corresponds to feasible solutions of the original
problem
Q = {(δy (y ), δz (z)): y ∈ Y, z ∈ Z,

y (i, t) = z(i, t) for all (i, t)}

•

Q ⊆ ∆y × ∆z is the set of feasible solutions to the LP
Q = {(α, β): α ∈ ∆Y , β ∈ ∆Z ,
y

•

αy y (i, t) =

z

βz z(i, t) for all (i, t)}

Q⊆Q

solutions:
max h(q) ≤ max h(q) for any h
q∈Q

q∈Q

Concrete example
•
•
•

Y = {y1 , y2 , y3 }
Z = {z1 , z2 , z3 }

∆y ⊂ R3 , ∆z ⊂ R3
y1

y3

x

Y

y2
x

x

a

a

b

b

c

c

He is

He is

z1

Z

He is
z2

z3

a

b

b

a

c

c

He

is

He

is

He

is

Simple solution

y1

y3

x

Y

y2
x

x

a

a

b

b

c

c

He is

He is

z1

Z

He is
z2

z3

a

b

b

a

c

c

He

is

He

is

He

is

choose:
• α(1) = (0, 0, 1) ∈ ∆y is representation of y3
• β (1) = (0, 0, 1) ∈ ∆z is representation of z3
conﬁrm:
(1)
(1)
αy y (i, t) =
βz z(i, t)
y

α

(1)

and β

(1)

z

satisfy agreement constraint

Fractional solution
y
y
1

y3

x

Y

2

x

x

a

a

b

b

c

c

He is

He is

z1

Z

He is
z2

z3

a

b

b

a

c

c

He

is

He

is

He

is

choose:
• α(2) = (0.5, 0.5, 0) ∈ ∆y is combination of y1 and y2
• β (2) = (0.5, 0.5, 0) ∈ ∆z is combination of z1 and z2
conﬁrm:
(2)
(2)
βz z(i, t)
αy y (i, t) =
y

z

α(2) and β (2) satisfy agreement constraint, but not integral

Optimal solution
weights:
•
•

the choice of f and g determines the optimal solution
if (f , g ) favors (α(2) , β (2) ), the optimal solution is fractional

example: f = [1 1 2] and g = [1 1 − 2]
•
•

f · α(1) + g · β (1) = 0 vs f · α(2) + g · β (2) = 2

α(2) , β (2) is optimal, even though it is fractional

summary: dual and LP primal optimal:
(2)

u

u

(2)

αy f (y ) +

min M(u) = min L(u) =
y

original primal optimal:
f (y ∗ ) + g (z ∗ ) = 0

βz g (z) = 2
z

round 1

dual values:
y (1)
2.00
(1)
z
1.00
L(u (1) ) 3.00

dual solutions:
y3

z2

x
c

b
He

previous solutions:
y3 z2

a
is

c

He is
5
4
3
2
1
0
0

1

2

3

4

5
Round

6

7

8

9

10

round 2

dual values:
y (2)
2.00
(2)
z
1.00
L(u (2) ) 3.00

dual solutions:
y2

z1

x
b

a
He

previous solutions:
y3 z2
y2 z1

b
is

b

He is
5
4
3
2
1
0
0

1

2

3

4

5
Round

6

7

8

9

10

round 3

dual values:
y (3)
2.50
(3)
z
0.50
L(u (3) ) 3.00

dual solutions:
y1

z1

x
a

a
He

previous solutions:
y3 z2
y2 z1
y1 z1

b
is

a

He is
5
4
3
2
1
0
0

1

2

3

4

5
Round

6

7

8

9

10

round 4

dual values:
y (4)
2.17
(4)
z
0.17
L(u (4) ) 2.33

dual solutions:
y1

z1

x
a

a
He

previous solutions:
y3 z2
y2 z1
y1 z1
y1 z1

b
is

a

He is
5
4
3
2
1
0
0

1

2

3

4

5
Round

6

7

8

9

10

round 5

dual values:
y (5)
2.08
(5)
z
0.08
L(u (5) ) 2.17

dual solutions:
y2

z2

x
b

b
He

previous solutions:
y3 z2
y2 z1
y1 z1
y1 z1
y2 z2

a
is

b

He is
5
4
3
2
1
0
0

1

2

3

4

5
Round

6

7

8

9

10

round 6

dual values:
y (6)
2.12
(6)
z
0.12
L(u (6) ) 2.23

dual solutions:
y1

z1

x
a

a
He

previous solutions:
y3 z2
y2 z1
y1 z1
y1 z1
y2 z2
y1 z1

b
is

a

He is
5
4
3
2
1
0
0

1

2

3

4

5
Round

6

7

8

9

10

round 7

dual values:
y (7)
2.05
(7)
z
0.05
L(u (7) ) 2.10

dual solutions:
y2

z2

x
b

b
He

previous solutions:
y3 z2
y2 z1
y1 z1
y1 z1
y2 z2
y1 z1
y2 z2

a
is

b

He is
5
4
3
2
1
0
0

1

2

3

4

5
Round

6

7

8

9

10

round 8

dual values:
y (8)
2.09
(8)
z
0.09
L(u (8) ) 2.19

dual solutions:
y1

z1

x
a

a
He

previous solutions:
y3 z2
y2 z1
y1 z1
y1 z1
y2 z2
y1 z1
y2 z2
y1 z1

b
is

a

He is
5
4
3
2
1
0
0

1

2

3

4

5
Round

6

7

8

9

10

round 9

dual values:
y (9)
2.03
(9)
z
0.03
L(u (9) ) 2.06

dual solutions:
y2

z2

x
b

b
He

previous solutions:
y3 z2
y2 z1
y1 z1
y1 z1
y2 z2
y1 z1
y2 z2
y1 z1
y2 z2

a
is

b

He is
5
4
3
2
1
0
0

1

2

3

4

5
Round

6

7

8

9

10

Tightening

(Sherali and Adams, 1994; Sontag et al., 2008)

modify:
•
•
•

extend Y, Z to identify bigrams of part-of-speech tags
y (i, t1 , t2 ) = 1 ↔ y (i, t1 ) = 1 and y (i + 1, t2 ) = 1

z(i, t1 , t2 ) = 1 ↔ z(i, t1 ) = 1 and z(i + 1, t2 ) = 1

all bigram constraints: valid to add for all i, t1 , t2 ∈ T
αy y (i, t1 , t2 ) =
y

βz z(i, t1 , t2 )
z

however this would make decoding expensive

Iterative tightening
single bigram constraint: cheaper to implement
αy y (1, a, b) =

βz z(1, a, b)

y

z

the solution α(1) , β (1) trivially passes this constraint, while
α(2) , β (2) violates it
y1

y3

x

Y

y2
x

x

a

a

He is

b

He is

z1

Z

b

c

c

He is

z2

z3

a

b

b

a

c

c

He

is

He

is

He

is

Dual decomposition with tightening
tightened decomposition includes an additional Lagrange multiplier
yu,v = arg max f (y ) +
y ∈Y

zu,v = arg max g (z) −
z∈Z

u(i, t)y (i, t) + v (1, a, b)y (1, a, b)
i,t

i,t

u(i, t)z(i, t) − v (1, a, b)z(1, a, b)

in general, this term can make the decoding problem more diﬃcult
example:
•

for small examples, these penalties are easy to compute

•

for CFG parsing, need to include extra states that maintain
tag bigrams (still faster than full intersection)

round 7

dual values:
y (7)
2.00
(7)
z
1.00
L(u (7) ) 3.00

dual solutions:
y3

z2

x
c

b
He

previous solutions:
y3 z2

a
is

c

He is
5
4
3
2
1
0
7

8

9

10

11
Round

12

13

14

15

16

round 8

dual values:
y (8)
3.00
(8)
z
2.00
L(u (8) ) 5.00

dual solutions:
y2

z3

x
b

c
He

previous solutions:
y3 z2
y2 z3

c
is

b

He is
5
4
3
2
1
0
7

8

9

10

11
Round

12

13

14

15

16

round 9

dual values:
y (9)
3.00
(9)
z
-1.00
L(u (9) ) 2.00

dual solutions:
y1

z2

x
a

b
He

previous solutions:
y3 z2
y2 z3
y1 z2

a
is

a

He is
5
4
3
2
1
0
7

8

9

10

11
Round

12

13

14

15

16

round 10

dual values:
y (10)
2.00
(10)
z
1.00
L(u (10) ) 3.00

dual solutions:
y3

z1

x
c

a
He

previous solutions:
y3 z2
y2 z3
y1 z2
y3 z1

b
is

c

He is
5
4
3
2
1
0
7

8

9

10

11
Round

12

13

14

15

16

round 11

dual values:
y (11)
3.00
(11)
z
2.00
L(u (11) ) 5.00

dual solutions:
y2

z3

x
b

c
He

previous solutions:
y3 z2
y2 z3
y1 z2
y3 z1
y2 z3

c
is

b

He is
5
4
3
2
1
0
7

8

9

10

11
Round

12

13

14

15

16

round 12

dual values:
y (12)
3.00
(12)
z
-1.00
L(u (12) ) 2.00

dual solutions:
y1

z2

x
a

b
He

previous solutions:
y3 z2
y2 z3
y1 z2
y3 z1
y2 z3
y1 z2

a
is

a

He is
5
4
3
2
1
0
7

8

9

10

11
Round

12

13

14

15

16

round 13

dual values:
y (13)
2.00
(13)
z
-1.00
L(u (13) ) 1.00

dual solutions:
y3

z1

x
c

a
He

previous solutions:
y3 z2
y2 z3
y1 z2
y3 z1
y2 z3
y1 z2
y3 z1

b
is

c

He is
5
4
3
2
1
0
7

8

9

10

11
Round

12

13

14

15

16

round 14

dual values:
y (14)
3.00
(14)
z
2.00
L(u (14) ) 5.00

dual solutions:
y2

z3

x
b

c
He

previous solutions:
y3 z2
y2 z3
y1 z2
y3 z1
y2 z3
y1 z2
y3 z1
y2 z3

c
is

b

He is
5
4
3
2
1
0
7

8

9

10

11
Round

12

13

14

15

16

round 15

dual values:
y (15)
3.00
(15)
z
-1.00
L(u (15) ) 2.00

dual solutions:
y1

z2

x
a

b
He

previous solutions:
y3 z2
y2 z3
y1 z2
y3 z1
y2 z3
y1 z2
y3 z1
y2 z3
y1 z2

a
is

a

He is
5
4
3
2
1
0
7

8

9

10

11
Round

12

13

14

15

16

round 16

dual values:
y (16)
2.00
(16)
z
-2.00
L(u (16) ) 0.00

dual solutions:
y3

z3

x
c

c
He

previous solutions:
y3 z2
y2 z3
y1 z2
y3 z1
y2 z3
y1 z2
y3 z1
y2 z3
y1 z2
y3 z3

c
is

c

He is
5
4
3
2
1
0
7

8

9

10

11
Round

12

13

14

15

16

6. Advanced examples

aim: demonstrate some diﬀerent relaxation techniques
•

higher-order non-projective dependency parsing

•

syntactic machine translation

Higher-order non-projective dependency parsing
setup: given a model for higher-order non-projective dependency
parsing (sibling features)
problem: ﬁnd non-projective dependency parse that maximizes the
score of this model
diﬃculty:
•

model is NP-hard to decode

•

complexity of the model comes from enforcing combinatorial
constraints

strategy: design a decomposition that separates combinatorial
constraints from direct implementation of the scoring function

Non-Projective Dependency Parsing
*0

John1

saw2

a3

movie4

today5

that6

he7

liked8

*0

John1

saw2

a3

movie4

today5

that6

he7

liked8

Important problem in many languages.
Problem is NP-Hard for all but the simplest models.

Dual Decomposition
A classical technique for constructing decoding algorithms.
Solve complicated models
y ∗ = arg max f (y )
y

by decomposing into smaller problems.
Upshot: Can utilize a toolbox of combinatorial algorithms.
Dynamic programming
Minimum spanning tree
Shortest path
Min-Cut
...

Non-Projective Dependency Parsing
*0

John1

saw2

a3

movie4

today5

that6

he7

liked8

*0

John1

saw2

a3

movie4

today5

that6

he7

liked8

Starts at the root symbol *
Each word has a exactly one parent word
Produces a tree structure (no cycles)
Dependencies can cross

Arc-Factored

*0

John1

f (y ) =

saw2

a3

movie4

today5

that6

he7

liked8

Arc-Factored

*0

John1

saw2

a3

movie4

f (y ) = score(head =∗0 , mod =saw2 )

today5

that6

he7

liked8

Arc-Factored

*0

John1

saw2

a3

movie4

today5

that6

he7

f (y ) = score(head =∗0 , mod =saw2 ) +score(saw2 , John1 )

liked8

Arc-Factored

*0

John1

saw2

a3

movie4

today5

that6

he7

f (y ) = score(head =∗0 , mod =saw2 ) +score(saw2 , John1 )
+score(saw2 , movie4 )

liked8

Arc-Factored

*0

John1

saw2

a3

movie4

today5

that6

he7

f (y ) = score(head =∗0 , mod =saw2 ) +score(saw2 , John1 )
+score(saw2 , movie4 ) +score(saw2 , today5 )

liked8

Arc-Factored

*0

John1

saw2

a3

movie4

today5

that6

he7

f (y ) = score(head =∗0 , mod =saw2 ) +score(saw2 , John1 )
+score(saw2 , movie4 ) +score(saw2 , today5 )
+score(movie4 , a3 ) + ...

liked8

Arc-Factored

*0

John1

saw2

a3

movie4

today5

that6

he7

f (y ) = score(head =∗0 , mod =saw2 ) +score(saw2 , John1 )
+score(saw2 , movie4 ) +score(saw2 , today5 )
+score(movie4 , a3 ) + ...
e.g. score(∗0 , saw2 ) = log p(saw2 |∗0 )

(generative model)

liked8

Arc-Factored

*0

John1

saw2

a3

movie4

today5

that6

he7

liked8

f (y ) = score(head =∗0 , mod =saw2 ) +score(saw2 , John1 )
+score(saw2 , movie4 ) +score(saw2 , today5 )
+score(movie4 , a3 ) + ...
e.g. score(∗0 , saw2 ) = log p(saw2 |∗0 )
or score(∗0 , saw2 ) = w · φ(saw2 , ∗0 )

(generative model)
(CRF/perceptron model)

Arc-Factored

*0

John1

saw2

a3

movie4

today5

that6

he7

liked8

f (y ) = score(head =∗0 , mod =saw2 ) +score(saw2 , John1 )
+score(saw2 , movie4 ) +score(saw2 , today5 )
+score(movie4 , a3 ) + ...
e.g. score(∗0 , saw2 ) = log p(saw2 |∗0 )
or score(∗0 , saw2 ) = w · φ(saw2 , ∗0 )

(generative model)
(CRF/perceptron model)

y ∗ = arg max f (y ) ⇐ Minimum Spanning Tree Algorithm
y

Sibling Models

*0

John1

f (y ) =

saw2

a3

movie4

today5

that6

he7

liked8

Sibling Models

*0

John1

saw2

a3

movie4

today5

that6

f (y ) = score(head = ∗0 , prev = NULL, mod = saw2 )

he7

liked8

Sibling Models

*0

John1

saw2

a3

movie4

today5

that6

f (y ) = score(head = ∗0 , prev = NULL, mod = saw2 )
+score(saw2 , NULL, John1 )

he7

liked8

Sibling Models

*0

John1

saw2

a3

movie4

today5

that6

he7

liked8

f (y ) = score(head = ∗0 , prev = NULL, mod = saw2 )
+score(saw2 , NULL, John1 ) +score(saw2 , NULL, movie4 )

Sibling Models

*0

John1

saw2

a3

movie4

today5

that6

he7

liked8

f (y ) = score(head = ∗0 , prev = NULL, mod = saw2 )
+score(saw2 , NULL, John1 ) +score(saw2 , NULL, movie4 )
+score(saw2 ,movie4 , today5 ) + ...

Sibling Models

*0

John1

saw2

a3

movie4

today5

that6

he7

liked8

f (y ) = score(head = ∗0 , prev = NULL, mod = saw2 )
+score(saw2 , NULL, John1 ) +score(saw2 , NULL, movie4 )
+score(saw2 ,movie4 , today5 ) + ...
e.g. score(saw2 , movie4 , today5 ) = log p(today5 |saw2 , movie4 )

Sibling Models

*0

John1

saw2

a3

movie4

today5

that6

he7

liked8

f (y ) = score(head = ∗0 , prev = NULL, mod = saw2 )
+score(saw2 , NULL, John1 ) +score(saw2 , NULL, movie4 )
+score(saw2 ,movie4 , today5 ) + ...
e.g. score(saw2 , movie4 , today5 ) = log p(today5 |saw2 , movie4 )
or score(saw2 , movie4 , today5 ) = w · φ(saw2 , movie4 , today5 )

Sibling Models

*0

John1

saw2

a3

movie4

today5

that6

he7

liked8

f (y ) = score(head = ∗0 , prev = NULL, mod = saw2 )
+score(saw2 , NULL, John1 ) +score(saw2 , NULL, movie4 )
+score(saw2 ,movie4 , today5 ) + ...
e.g. score(saw2 , movie4 , today5 ) = log p(today5 |saw2 , movie4 )
or score(saw2 , movie4 , today5 ) = w · φ(saw2 , movie4 , today5 )
y ∗ = arg max f (y ) ⇐ NP-Hard
y

Thought Experiment: Individual Decoding

*0

John1

saw2

a3

movie4

today5

that6

he7

liked8

Thought Experiment: Individual Decoding

*0

John1

saw2

a3

movie4

today5

that6

he7

liked8

score(saw2 , NULL, John1 ) + score(saw2 , NULL, movie4 )
+score(saw2 , movie4 , today5 )

Thought Experiment: Individual Decoding

*0

John1

saw2

a3

movie4

today5

that6

he7

liked8

score(saw2 , NULL, John1 ) + score(saw2 , NULL, movie4 )
+score(saw2 , movie4 , today5 )
score(saw2 , NULL, John1 ) + score(saw2 , NULL, that6 )

Thought Experiment: Individual Decoding

*0

John1

saw2

a3

movie4

today5

that6

he7

liked8

score(saw2 , NULL, John1 ) + score(saw2 , NULL, movie4 )
+score(saw2 , movie4 , today5 )
score(saw2 , NULL, John1 ) + score(saw2 , NULL, that6 )
score(saw2 , NULL, a3 ) + score(saw2 , a3 , he7 )

Thought Experiment: Individual Decoding

*0

John1

saw2

a3

movie4

today5

that6

he7

liked8

score(saw2 , NULL, John1 ) + score(saw2 , NULL, movie4 )
+score(saw2 , movie4 , today5 )
2n−1
possibilities

score(saw2 , NULL, John1 ) + score(saw2 , NULL, that6 )
score(saw2 , NULL, a3 ) + score(saw2 , a3 , he7 )

Thought Experiment: Individual Decoding

*0

John1

saw2

a3

movie4

today5

that6

he7

liked8

score(saw2 , NULL, John1 ) + score(saw2 , NULL, movie4 )
+score(saw2 , movie4 , today5 )
2n−1
possibilities

score(saw2 , NULL, John1 ) + score(saw2 , NULL, that6 )
score(saw2 , NULL, a3 ) + score(saw2 , a3 , he7 )

Under Sibling Model, can solve for each word with Viterbi decoding.

Thought Experiment Continued

*0

John1

saw2

a3

movie4

today5

that6

he7

Idea: Do individual decoding for each head word using dynamic
programming.
If we’re lucky, we’ll end up with a valid ﬁnal tree.

liked8

Thought Experiment Continued

*0

John1

saw2

a3

movie4

today5

that6

he7

Idea: Do individual decoding for each head word using dynamic
programming.
If we’re lucky, we’ll end up with a valid ﬁnal tree.

liked8

Thought Experiment Continued

*0

John1

saw2

a3

movie4

today5

that6

he7

Idea: Do individual decoding for each head word using dynamic
programming.
If we’re lucky, we’ll end up with a valid ﬁnal tree.

liked8

Thought Experiment Continued

*0

John1

saw2

a3

movie4

today5

that6

he7

Idea: Do individual decoding for each head word using dynamic
programming.
If we’re lucky, we’ll end up with a valid ﬁnal tree.

liked8

Thought Experiment Continued

*0

John1

saw2

a3

movie4

today5

that6

he7

Idea: Do individual decoding for each head word using dynamic
programming.
If we’re lucky, we’ll end up with a valid ﬁnal tree.

liked8

Thought Experiment Continued

*0

John1

saw2

a3

movie4

today5

that6

he7

Idea: Do individual decoding for each head word using dynamic
programming.
If we’re lucky, we’ll end up with a valid ﬁnal tree.

liked8

Thought Experiment Continued

*0

John1

saw2

a3

movie4

today5

that6

he7

Idea: Do individual decoding for each head word using dynamic
programming.
If we’re lucky, we’ll end up with a valid ﬁnal tree.

liked8

Thought Experiment Continued

*0

John1

saw2

a3

movie4

today5

that6

he7

Idea: Do individual decoding for each head word using dynamic
programming.
If we’re lucky, we’ll end up with a valid ﬁnal tree.

liked8

Thought Experiment Continued

*0

John1

saw2

a3

movie4

today5

that6

he7

Idea: Do individual decoding for each head word using dynamic
programming.
If we’re lucky, we’ll end up with a valid ﬁnal tree.
But we might violate some constraints.

liked8

Dual Decomposition Idea

No
Constraints

Minimum
Spanning Tree

ArcFactored

Sibling
Model

Tree
Constraints

Individual
Decoding

Dual Decomposition Idea

No
Constraints

Minimum
Spanning Tree

ArcFactored

Sibling
Model

Tree
Constraints

Individual
Decoding

Dual
Decomposition

Dual Decomposition Structure

Goal y ∗ = arg max f (y )
y ∈Y

Dual Decomposition Structure

Goal y ∗ = arg max f (y )
y ∈Y

Rewrite as

argmax
z∈

f (z) + g (y )

Z, y ∈ Y

such that z = y

Dual Decomposition Structure

Goal y ∗ = arg max f (y )
y ∈Y

Rewrite as

argmax
z∈

f (z) + g (y )

Z, y ∈ Y

All Possible

such that z = y

Dual Decomposition Structure

Goal y ∗ = arg max f (y )
y ∈Y

Rewrite as

argmax
z∈

f (z) + g (y )

Z, y ∈ Y

All Possible

Valid Trees

such that z = y

Dual Decomposition Structure

Goal y ∗ = arg max f (y )
y ∈Y

Sibling

Rewrite as

argmax
z∈

f (z) + g (y )

Z, y ∈ Y

All Possible

Valid Trees

such that z = y

Dual Decomposition Structure

Goal y ∗ = arg max f (y )
y ∈Y

Sibling

Rewrite as

argmax
z∈

f (z) + g (y )

Z, y ∈ Y

All Possible

Arc-Factored

Valid Trees

such that z = y

Dual Decomposition Structure

Goal y ∗ = arg max f (y )
y ∈Y

Sibling

Rewrite as

argmax
z∈

f (z) + g (y )

Z, y ∈ Y

All Possible

Arc-Factored

Valid Trees

such that z = y
Constraint

Algorithm Sketch

Set penalty weights equal to 0 for all edges.
For k = 1 to K

Algorithm Sketch

Set penalty weights equal to 0 for all edges.
For k = 1 to K
z (k) ← Decode (f (z) + penalty) by Individual Decoding

Algorithm Sketch

Set penalty weights equal to 0 for all edges.
For k = 1 to K
z (k) ← Decode (f (z) + penalty) by Individual Decoding
y (k) ← Decode (g (y ) − penalty) by Minimum Spanning Tree

Algorithm Sketch

Set penalty weights equal to 0 for all edges.
For k = 1 to K
z (k) ← Decode (f (z) + penalty) by Individual Decoding
y (k) ← Decode (g (y ) − penalty) by Minimum Spanning Tree
If y (k) (i, j) = z (k) (i, j) for all i, j Return (y (k) , z (k) )

Algorithm Sketch

Set penalty weights equal to 0 for all edges.
For k = 1 to K
z (k) ← Decode (f (z) + penalty) by Individual Decoding
y (k) ← Decode (g (y ) − penalty) by Minimum Spanning Tree
If y (k) (i, j) = z (k) (i, j) for all i, j Return (y (k) , z (k) )
Else Update penalty weights based on y (k) (i, j) − z (k) (i, j)

Individual Decoding

Penalties
u(i, j) = 0 for all i,j

*0

John1

saw2

a3

movie4

today5

z ∗ = arg max(f (z) +
z∈Z

that6

he7

liked8

u(i, j)z(i, j))
i,j

Minimum Spanning Tree

*0

John1

saw2

a3

movie4

y ∗ = arg max(g (y ) −
y ∈Y

today5

that6

he7

liked8

u(i, j)y (i, j))
i,j

Key
f (z)
Z
y (i, j) = 1

⇐
⇐
if

Sibling Model
No Constraints
y contains dependency i, j

g (y )
Y

⇐
⇐

Arc-Factored Model
Tree Constraints

Individual Decoding

Penalties
u(i, j) = 0 for all i,j

*0

John1

saw2

a3

movie4

today5

z ∗ = arg max(f (z) +
z∈Z

that6

he7

liked8

u(i, j)z(i, j))
i,j

Minimum Spanning Tree

*0

John1

saw2

a3

movie4

y ∗ = arg max(g (y ) −
y ∈Y

today5

that6

he7

liked8

u(i, j)y (i, j))
i,j

Key
f (z)
Z
y (i, j) = 1

⇐
⇐
if

Sibling Model
No Constraints
y contains dependency i, j

g (y )
Y

⇐
⇐

Arc-Factored Model
Tree Constraints

Individual Decoding

Penalties
u(i, j) = 0 for all i,j

*0

John1

saw2

a3

movie4

today5

z ∗ = arg max(f (z) +
z∈Z

that6

he7

liked8

u(i, j)z(i, j))
i,j

Minimum Spanning Tree

*0

John1

saw2

a3

movie4

y ∗ = arg max(g (y ) −
y ∈Y

today5

that6

he7

liked8

u(i, j)y (i, j))
i,j

Key
f (z)
Z
y (i, j) = 1

⇐
⇐
if

Sibling Model
No Constraints
y contains dependency i, j

g (y )
Y

⇐
⇐

Arc-Factored Model
Tree Constraints

Individual Decoding

Penalties
u(i, j) = 0 for all i,j

*0

John1

saw2

a3

movie4

today5

z ∗ = arg max(f (z) +
z∈Z

that6

he7

liked8

u(i, j)z(i, j))
i,j

Minimum Spanning Tree

*0

John1

saw2

a3

movie4

y ∗ = arg max(g (y ) −
y ∈Y

today5

that6

he7

liked8

u(i, j)y (i, j))
i,j

Key
f (z)
Z
y (i, j) = 1

⇐
⇐
if

Sibling Model
No Constraints
y contains dependency i, j

g (y )
Y

⇐
⇐

Arc-Factored Model
Tree Constraints

Individual Decoding

Penalties
u(i, j) = 0 for all i,j
Iteration 1
u(8, 1)

*0

John1

saw2

a3

movie4

today5

z ∗ = arg max(f (z) +
z∈Z

that6

liked8

u(i, j)z(i, j))

-1

u(2, 6)

1

u(8, 7)

he7

-1

u(4, 6)

1

i,j

Minimum Spanning Tree

*0

John1

saw2

a3

movie4

y ∗ = arg max(g (y ) −
y ∈Y

today5

that6

he7

liked8

u(i, j)y (i, j))
i,j

Key
f (z)
Z
y (i, j) = 1

⇐
⇐
if

Sibling Model
No Constraints
y contains dependency i, j

g (y )
Y

⇐
⇐

Arc-Factored Model
Tree Constraints

Individual Decoding

Penalties
u(i, j) = 0 for all i,j
Iteration 1
u(8, 1)

*0

John1

saw2

a3

movie4

today5

z ∗ = arg max(f (z) +
z∈Z

that6

liked8

u(i, j)z(i, j))

-1

u(2, 6)

1

u(8, 7)

he7

-1

u(4, 6)

1

i,j

Minimum Spanning Tree

*0

John1

saw2

a3

movie4

y ∗ = arg max(g (y ) −
y ∈Y

today5

that6

he7

liked8

u(i, j)y (i, j))
i,j

Key
f (z)
Z
y (i, j) = 1

⇐
⇐
if

Sibling Model
No Constraints
y contains dependency i, j

g (y )
Y

⇐
⇐

Arc-Factored Model
Tree Constraints

Individual Decoding

Penalties
u(i, j) = 0 for all i,j
Iteration 1
u(8, 1)

*0

John1

saw2

a3

movie4

today5

z ∗ = arg max(f (z) +
z∈Z

that6

liked8

u(i, j)z(i, j))

-1

u(2, 6)

1

u(8, 7)

he7

-1

u(4, 6)

1

i,j

Minimum Spanning Tree

*0

John1

saw2

a3

movie4

y ∗ = arg max(g (y ) −
y ∈Y

today5

that6

he7

liked8

u(i, j)y (i, j))
i,j

Key
f (z)
Z
y (i, j) = 1

⇐
⇐
if

Sibling Model
No Constraints
y contains dependency i, j

g (y )
Y

⇐
⇐

Arc-Factored Model
Tree Constraints

Individual Decoding

Penalties
u(i, j) = 0 for all i,j
Iteration 1
u(8, 1)

a3

movie4

z ∗ = arg max(f (z) +
z∈Z

that6

he7

liked8

u(i, j)z(i, j))
i,j

Minimum Spanning Tree

*0

John1

saw2

a3

movie4

y ∗ = arg max(g (y ) −
y ∈Y

today5

that6

he7

1
1

Iteration 2
u(8, 1)

today5

u(8, 7)

-1
-2
2

u(8, 7)

saw2

-1

u(2, 6)

John1

u(2, 6)

u(4, 6)

*0

-1

u(4, 6)

1

liked8

u(i, j)y (i, j))
i,j

Key
f (z)
Z
y (i, j) = 1

⇐
⇐
if

Sibling Model
No Constraints
y contains dependency i, j

g (y )
Y

⇐
⇐

Arc-Factored Model
Tree Constraints

Individual Decoding

Penalties
u(i, j) = 0 for all i,j
Iteration 1
u(8, 1)

*0

John1

saw2

a3

movie4

today5

z ∗ = arg max(f (z) +
z∈Z

that6

liked8

u(i, j)z(i, j))
i,j

-1

u(2, 6)

1

u(8, 7)

he7

-1

u(4, 6)

1

Iteration 2
u(8, 1)

saw2

a3

movie4

y ∗ = arg max(g (y ) −
y ∈Y

today5

that6

he7

2

u(8, 7)
John1

-2

u(2, 6)
*0

-1

u(4, 6)

Minimum Spanning Tree

1

liked8

u(i, j)y (i, j))
i,j

Key
f (z)
Z
y (i, j) = 1

⇐
⇐
if

Sibling Model
No Constraints
y contains dependency i, j

g (y )
Y

⇐
⇐

Arc-Factored Model
Tree Constraints

Individual Decoding

Penalties
u(i, j) = 0 for all i,j
Iteration 1
u(8, 1)

*0

John1

saw2

a3

movie4

today5

z ∗ = arg max(f (z) +
z∈Z

that6

liked8

u(i, j)z(i, j))
i,j

-1

u(2, 6)

1

u(8, 7)

he7

-1

u(4, 6)

1

Iteration 2
u(8, 1)

saw2

a3

movie4

y ∗ = arg max(g (y ) −
y ∈Y

today5

that6

he7

2

u(8, 7)
John1

-2

u(2, 6)
*0

-1

u(4, 6)

Minimum Spanning Tree

1

liked8

u(i, j)y (i, j))
i,j

Key
f (z)
Z
y (i, j) = 1

⇐
⇐
if

Sibling Model
No Constraints
y contains dependency i, j

g (y )
Y

⇐
⇐

Arc-Factored Model
Tree Constraints

Individual Decoding

Penalties
u(i, j) = 0 for all i,j
Iteration 1
u(8, 1)

*0

John1

saw2

a3

movie4

today5

z ∗ = arg max(f (z) +
z∈Z

that6

liked8

u(i, j)z(i, j))
i,j

-1

u(2, 6)

1

u(8, 7)

he7

-1

u(4, 6)

1

Iteration 2
u(8, 1)

saw2

a3

movie4

y ∗ = arg max(g (y ) −
y ∈Y

today5

that6

he7

2

u(8, 7)
John1

-2

u(2, 6)
*0

-1

u(4, 6)

Minimum Spanning Tree

1

liked8

u(i, j)y (i, j))
i,j

Key
f (z)
Z
y (i, j) = 1

⇐
⇐
if

Sibling Model
No Constraints
y contains dependency i, j

g (y )
Y

⇐
⇐

Arc-Factored Model
Tree Constraints

Individual Decoding

Penalties
u(i, j) = 0 for all i,j
Iteration 1
u(8, 1)

*0

John1

saw2

a3

movie4

today5

z ∗ = arg max(f (z) +
z∈Z

that6

liked8

u(i, j)z(i, j))
i,j

-1

u(2, 6)

1

u(8, 7)

he7

-1

u(4, 6)

1

Iteration 2
u(8, 1)

saw2

a3

movie4

y ∗ = arg max(g (y ) −
y ∈Y

today5

that6

he7

2

u(8, 7)
John1

-2

u(2, 6)
*0

-1

u(4, 6)

Minimum Spanning Tree

1

liked8

Converged

u(i, j)y (i, j))

y ∗ = arg max f (y ) + g (y )
y ∈Y

i,j

Key
f (z)
Z
y (i, j) = 1

⇐
⇐
if

Sibling Model
No Constraints
y contains dependency i, j

g (y )
Y

⇐
⇐

Arc-Factored Model
Tree Constraints

Guarantees

Theorem
If at any iteration y (k) = z (k) , then (y (k) , z (k) ) is the global
optimum.
In experiments, we ﬁnd the global optimum on 98% of examples.

Guarantees

Theorem
If at any iteration y (k) = z (k) , then (y (k) , z (k) ) is the global
optimum.
In experiments, we ﬁnd the global optimum on 98% of examples.
If we do not converge to a match, we can still return an
approximate solution (more in the paper).

Extensions
Grandparent Models

*0

John1

saw2

a3

movie4

today5

that6

he7

liked8

f (y ) =...+ score(gp =∗0 , head = saw2 , prev =movie4 , mod =today5 )

Head Automata (Eisner, 2000)
Generalization of Sibling models
Allow arbitrary automata as local scoring function.

Experiments
Properties:
Exactness
Parsing Speed
Parsing Accuracy
Comparison to Individual Decoding
Comparison to LP/ILP
Training:
Averaged Perceptron (more details in paper)
Experiments on:
CoNLL Datasets
English Penn Treebank
Czech Dependency Treebank

How often do we exactly solve the problem?
100

98

96

94

92

90

r

Tu

e

Sw

o

Sl

r

Po

ut

D

g

an

D

En

ze

C

Percentage of examples where the dual decomposition ﬁnds
an exact solution.

Parsing Speed

50

25

40

20

30

15

20

10

10

5

0

0

Number of sentences parsed per second
Comparable to dynamic programming for projective parsing

r

e

Grandparent model

Tu

Sw

r

o

Sl

ut

Po

D

g

an

D

En

ze

C

e

r

Tu

Sw

r

o

Sl

ut

Po

g

an

D

D

ze

En

C

Sibling model

Accuracy
Dan
Dut
Por
Slo
Swe
Tur
Eng
Cze

Arc-Factored
89.7
82.3
90.7
82.4
88.9
75.7
90.1
84.4

Prev Best
91.5
85.6
92.1
85.6
90.6
76.4
—
—

Grandparent
91.8
85.8
93.0
86.2
91.4
77.6
92.5
87.3

Prev Best - Best reported results for CoNLL-X data set, includes
Approximate search (McDonald and Pereira, 2006)
Loop belief propagation (Smith and Eisner, 2008)
(Integer) Linear Programming (Martins et al., 2009)

Comparison to Subproblems
93

Individual
MST
Dual

92

91

90

89

88

g
En

F1 for dependency accuracy

Comparison to LP/ILP
Martins et al.(2009): Proposes two representations of
non-projective dependency parsing as a linear programming
relaxation as well as an exact ILP.
LP (1)
LP (2)
ILP
Use an LP/ILP Solver for decoding
We compare:
Accuracy
Exactness
Speed
Both LP and dual decomposition methods use the same model,
features, and weights w .

Comparison to LP/ILP: Accuracy
100

LP(1)
LP(2)
ILP
Dual

95

90

85

80

Dependency Accuracy

All decoding methods have comparable accuracy

Comparison to LP/ILP: Exactness and Speed
100

LP(1)
LP(2)
ILP
Dual

14
12

LP(1)
LP(2)
ILP
Dual

95
10
8

90

6
4

85

2
80

0

Percentage with exact solution

Sentences per second

Syntactic translation decoding
setup: assume a trained model for syntactic machine translation
problem: ﬁnd best derivation that maximizes the score of this
model
diﬃculty:
•

need to incorporate language model in decoding

•

empirically, relaxation is often not tight, so dual
decomposition does not always converge

strategy:
•

use a diﬀerent relaxation to handle language model

•

incrementally add constraints to ﬁnd exact solution

Syntactic Translation
Problem:
Decoding synchronous grammar for machine translation
Example:
<s> abarks le dug </s>
<s> the dog barks loudly </s>
Goal:
y ∗ = arg max f (y )
y

where y is a parse derivation in a synchronous grammar

Hiero Example
Consider the input sentence
<s> abarks le dug </s>
And the synchronous grammar
S → <s> X </s>, <s> X </s>
X → abarks X, X barks loudly
X → abarks X, barks X
X → abarks X, barks X loudly
X → le dug, the dog
X → le dug, a cat

Hiero Example
Apply synchronous rules to map this sentence
S
<s>

S

X
abarks

</s> <s>

X

X

X

le dug

barks loudly

the dog

Many possible mappings:
<s>
<s>
<s>
<s>
<s>
<s>

the dog barks loudly </s>
a cat barks loudly </s>
barks the dog </s>
barks a cat </s>
barks the dog loudly </s>
barks a cat loudly </s>

</s>

Translation Forest
Rule
1→
4→
4→
4→
5→
5→

<s> 4 </s>
5 barks loudly
barks 5
barks 5 loudly
the dog
a cat

Score
-1
2
0.5
3
-4
2.5

Example: a derivation in the translation forest
1
4

<s>
5
a

barks
cat

</s>
loudly

Scoring function
Score : sum of hypergraph derivation and language model
1
4

<s>
5
a

barks
cat

f (y ) = score(5 → a cat)

</s>
loudly

Scoring function
Score : sum of hypergraph derivation and language model
1
4

<s>
5
a

barks

</s>
loudly

cat

f (y ) = score(5 → a cat) + score(4 → 5 barks loudly)

Scoring function
Score : sum of hypergraph derivation and language model
1
4

<s>
5
a

barks

</s>
loudly

cat

f (y ) = score(5 → a cat) + score(4 → 5 barks loudly) + . . .
+score(<s>, the)

Scoring function
Score : sum of hypergraph derivation and language model
1
4

<s>
5
a

barks

</s>
loudly

cat

f (y ) = score(5 → a cat) + score(4 → 5 barks loudly) + . . .
+score(<s>, a) + score(a, cat)

Exact Dynamic Programming
To maximize combined model, need to ensure that bigrams are
consistent with parse tree.
1
4

<s>
5
a

barks
cat

</s>
loudly

Exact Dynamic Programming
To maximize combined model, need to ensure that bigrams are
consistent with parse tree.
<s>

1

</s>

<s>

<s>
<s>
<s>

5

cat

a

a

4

cat

barks barks

loudly

loudly

</s>

cat

Original Rules
5 → the dog
5 → a cat

New Rules
<s> 5cat →
barks 5cat →
<s> 5cat →
barks 5cat →

loudly

<s> thethe the dogdog
barks thethe the dogdog
<s> aa a catcat
barks aa a catcat

Lagrangian Relaxation Algorithm for
Syntactic Translation

Outline:
•

Algorithm for simpliﬁed version of translation

•

Full algorithm with certiﬁcate of exactness

•

Experimental results

Thought experiment: Greedy language model
Choose best bigram for a given word
barks
cat

<s>
dog

•

score(<s>, barks)

Thought experiment: Greedy language model
Choose best bigram for a given word
barks
cat

<s>
dog

•

score(<s>, barks)

•

score(dog, barks)

Thought experiment: Greedy language model
Choose best bigram for a given word
barks
cat

<s>
dog

•

score(<s>, barks)

•

score(dog, barks)

•

score(cat, barks)

Thought experiment: Greedy language model
Choose best bigram for a given word
barks
cat

<s>
dog

•

score(<s>, barks)

•

score(dog, barks)

•

score(cat, barks)

Can compute with a simple maximization
arg

max

w : w ,barks ∈B

score(w , barks)

Thought experiment: Full decoding
Step 1. Greedily choose best bigram for each word
</s>
barks

barks

loudly

the

dog

a

cat

Thought experiment: Full decoding
Step 1. Greedily choose best bigram for each word
</s>

barks

barks

dog

loudly

the

dog

a

cat

Thought experiment: Full decoding
Step 1. Greedily choose best bigram for each word
</s>

barks

loudly

barks

dog

barks

the

dog

a

cat

Thought experiment: Full decoding
Step 1. Greedily choose best bigram for each word
</s>

barks

loudly

the

barks

dog

barks

<s>

dog

a

cat

Thought experiment: Full decoding
Step 1. Greedily choose best bigram for each word
</s>

barks

loudly

the

dog

barks

dog

barks

<s>

the

a

cat

Thought experiment: Full decoding
Step 1. Greedily choose best bigram for each word
</s>

barks

loudly

the

dog

a

barks

dog

barks

<s>

the

<s>

cat

Thought experiment: Full decoding
Step 1. Greedily choose best bigram for each word
</s>

barks

loudly

the

dog

a

cat

barks

dog

barks

<s>

the

<s>

a

Thought experiment: Full decoding
Step 1. Greedily choose best bigram for each word
</s>

barks

loudly

the

dog

a

cat

barks

dog

barks

<s>

the

<s>

a

Step 2. Find the best derivation with ﬁxed bigrams

Thought experiment: Full decoding
Step 1. Greedily choose best bigram for each word
</s>

barks

loudly

the

dog

a

cat

barks

dog

barks

<s>

the

<s>

a

Step 2. Find the best derivation with ﬁxed bigrams
1
4

<s>
5

</s>

barks

a

cat

<s>

a

loudly

dog

barks

barks

Thought Experiment Problem
May produce invalid parse and bigram relationship
1
4

<s>
5

</s>

barks

a

cat

<s>

loudly

dog

barks

barks

a

Greedy bigram selection may conﬂict with the parse derivation

Thought Experiment Problem
May produce invalid parse and bigram relationship
1
4

<s>
5

</s>

barks

a

cat

<s>

loudly

dog

barks

barks

a

Greedy bigram selection may conﬂict with the parse derivation

Formal objective
Notation: y (w , v ) = 1 if the bigram w , v ∈ B is in y
Goal:
arg max f (y )
y ∈Y

such that for all words nodes yv

v
(1)

Formal objective
Notation: y (w , v ) = 1 if the bigram w , v ∈ B is in y
Goal:
arg max f (y )
y ∈Y

such that for all words nodes yv
w

v

yv

v

=

y (w , v )
w : w ,v ∈B

(1)

Formal objective
Notation: y (w , v ) = 1 if the bigram w , v ∈ B is in y
Goal:
arg max f (y )
y ∈Y

such that for all words nodes yv
w

v

yv

v

=

y (w , v )

(1)

y (v , w )

(2)

w : w ,v ∈B

yv

=
w : v ,w ∈B

Formal objective
Notation: y (w , v ) = 1 if the bigram w , v ∈ B is in y
Goal:
arg max f (y )
y ∈Y

such that for all words nodes yv
w

v

yv

v

=

y (w , v )

(1)

y (v , w )

(2)

w : w ,v ∈B

v

w

yv

=
w : v ,w ∈B

Formal objective
Notation: y (w , v ) = 1 if the bigram w , v ∈ B is in y
Goal:
arg max f (y )
y ∈Y

such that for all words nodes yv
w

v

yv

v

=

y (w , v )

(1)

y (v , w )

(2)

w : w ,v ∈B

v

w

yv

=
w : v ,w ∈B

Lagrangian: Relax constraint (2), leave constraint (1)

L(u, y ) = max f (y ) +
y ∈Y

w ,v

u(v ) yv −

w : v ,w ∈B



y (v , w )

For a given u, L(u, y ) can be solved by our greedy LM algorithm

Algorithm
Set u

(1)

(v ) = 0 for all v ∈ VL

For k = 1 to K
y (k) ← arg max L(k) (u, y )
y ∈Y

(k)

If yv

y (k) (v , w ) for all v Return (y (k) )

=
w : v ,w ∈B

Else



(k)

u (k+1) (v ) ← u (k) (v ) − αk yv

−

w : v ,w ∈B



y (k) (v , w )

Thought experiment: Greedy with penalties
Choose best bigram with penalty for a given word
barks
cat

<s>
dog

•

score(<s>, barks) − u(<s>) + u(barks)

Thought experiment: Greedy with penalties
Choose best bigram with penalty for a given word
barks
cat

<s>
dog

•

score(<s>, barks) − u(<s>) + u(barks)

•

score(cat, barks) − u(cat) + u(barks)

Thought experiment: Greedy with penalties
Choose best bigram with penalty for a given word
barks
cat

<s>
dog

•

score(<s>, barks) − u(<s>) + u(barks)

•

score(cat, barks) − u(cat) + u(barks)

•

score(dog, barks) − u(dog) + u(barks)

Thought experiment: Greedy with penalties
Choose best bigram with penalty for a given word
barks
cat

<s>
dog

•

score(<s>, barks) − u(<s>) + u(barks)

•

score(cat, barks) − u(cat) + u(barks)

•

score(dog, barks) − u(dog) + u(barks)

Can still compute with a simple maximization over
arg

max

w : w ,barks ∈B

score(w , barks) − u(w ) + u(barks)

Penalties
v
u(v)

Algorithm example
</s>
0

Greedy decoding

barks
0

loudly
0

the
0

dog
0

a
0

cat
0

Penalties
v
u(v)

Algorithm example
</s>
0

barks
0

loudly
0

the
0

dog
0

a
0

cat
0

Greedy decoding
</s>

barks

loudly

the

dog

a

cat

barks

dog

barks

<s>

the

<s>

a

Penalties
v
u(v)

Algorithm example
</s>
0

barks
0

loudly
0

the
0

dog
0

a
0

cat
0

Greedy decoding
</s>

barks

loudly

the

dog

a

cat

barks

dog

barks

<s>

the

<s>

a

1
4

<s>
5

</s>

barks

a

cat

<s>

a

loudly

dog

barks

barks

Penalties
v
u(v)

Algorithm example
</s>
0

barks
0

loudly
0

the
0

dog
0

a
0

cat
0

Greedy decoding
</s>

barks

loudly

the

dog

a

cat

barks

dog

barks

<s>

the

<s>

a

1
4

<s>
5

</s>

barks

a

cat

<s>

a

loudly

dog

barks

barks

Penalties
v
u(v)

Algorithm example
</s>
0

barks
-1

loudly
1

the
0

dog
-1

a
0

cat
1

Greedy decoding
</s>

barks

loudly

the

dog

a

cat

barks

dog

barks

<s>

the

<s>

a

1
4

<s>
5

</s>

barks

a

cat

<s>

a

loudly

dog

barks

barks

Penalties
v
u(v)

Algorithm example
</s>
0

Greedy decoding

barks
-1

loudly
1

the
0

dog
-1

a
0

cat
1

Penalties
v
u(v)

Algorithm example
</s>
0

barks
-1

loudly
1

the
0

dog
-1

a
0

cat
1

Greedy decoding
</s>

barks

loudly

the

dog

a

cat

loudly

cat

barks

<s>

the

<s>

a

Penalties
v
u(v)

Algorithm example
</s>
0

barks
-1

loudly
1

the
0

dog
-1

a
0

cat
1

Greedy decoding
</s>

barks

loudly

the

dog

a

cat

loudly

cat

barks

<s>

the

<s>

a

1
4

<s>
5

</s>

barks

a

cat

<s>

a

loudly

dog

barks

loudly

Penalties
v
u(v)

Algorithm example
</s>
0

barks
-1

loudly
1

the
0

dog
-1

a
0

cat
1

Greedy decoding
</s>

barks

loudly

the

dog

a

cat

loudly

cat

barks

<s>

the

<s>

a

1
4

<s>
5

</s>

barks

the

dog

<s>

the

loudly

cat

barks

loudly

Penalties
v
u(v)

Algorithm example
</s>
0

barks
-1

loudly
1

the
0

dog
-0.5

a
0

cat
0.5

Greedy decoding
</s>

barks

loudly

the

dog

a

cat

loudly

cat

barks

<s>

the

<s>

a

1
4

<s>
5

</s>

barks

the

dog

<s>

the

loudly

cat

barks

loudly

Penalties
v
u(v)

Algorithm example
</s>
0

Greedy decoding

barks
-1

loudly
1

the
0

dog
-0.5

a
0

cat
0.5

Penalties
v
u(v)

Algorithm example
</s>
0

barks
-1

loudly
1

the
0

dog
-0.5

a
0

cat
0.5

Greedy decoding
</s>

barks

loudly

the

dog

a

cat

loudly

dog

barks

<s>

the

<s>

a

Penalties
v
u(v)

Algorithm example
</s>
0

barks
-1

loudly
1

the
0

dog
-0.5

a
0

cat
0.5

Greedy decoding
</s>

barks

loudly

the

dog

a

cat

loudly

dog

barks

<s>

the

<s>

a

1
4

<s>
5

</s>

barks

the

dog

<s>

the

loudly

dog

barks

loudly

Constraint Issue
Constraints do not capture all possible reorderings
Example: Add rule 5 → cat a to forest. New derivation

Constraint Issue
Constraints do not capture all possible reorderings
Example: Add rule 5 → cat a to forest. New derivation
1
4

<s>
5

</s>

barks

cat

a

a

loudly

cat

loudly

barks

<s>

Satisﬁes both constraints (1) and (2), but is not self-consistent.

New Constraints: Paths
1
4

<s>
5
a
< a ↓>

barks

</s>
loudly

cat

Fix: In addition to
bigrams, consider paths
between terminal nodes
Example: Path marker
5 ↓, 10 ↓ implies that
between two word nodes,
we move down from node
5 to node 10

New Constraints: Paths
1
4

<s>
5
a
< a ↓>
< 5 ↓, a ↓>

barks

</s>
loudly

cat

Fix: In addition to
bigrams, consider paths
between terminal nodes
Example: Path marker
5 ↓, 10 ↓ implies that
between two word nodes,
we move down from node
5 to node 10

New Constraints: Paths
1
4

<s>
5
a
< a ↓>
< 5 ↓, a ↓>
< 4 ↓, 5 ↓>

barks

</s>
loudly

cat

Fix: In addition to
bigrams, consider paths
between terminal nodes
Example: Path marker
5 ↓, 10 ↓ implies that
between two word nodes,
we move down from node
5 to node 10

New Constraints: Paths
1
4

<s>
5
a
< a ↓>
< 5 ↓, a ↓>
< 4 ↓, 5 ↓>
< <s> ↑, 4 ↓>

barks

</s>
loudly

cat

Fix: In addition to
bigrams, consider paths
between terminal nodes
Example: Path marker
5 ↓, 10 ↓ implies that
between two word nodes,
we move down from node
5 to node 10

New Constraints: Paths
1
4

<s>
5
a
< a ↓>
< 5 ↓, a ↓>
< 4 ↓, 5 ↓>
< <s> ↑, 4 ↓>
< <s> ↑>

barks

</s>
loudly

cat

Fix: In addition to
bigrams, consider paths
between terminal nodes
Example: Path marker
5 ↓, 10 ↓ implies that
between two word nodes,
we move down from node
5 to node 10

Greedy Language Model with Paths
Step 1. Greedily choose best path each word
</s>
< </s> ↓>
< 4 ↑, </s> ↓>
< loudly ↑, 4 ↓>
< loudly ↑>

barks

loudly

the

dog

a

cat

Greedy Language Model with Paths
Step 1. Greedily choose best path each word
</s>

barks

< </s> ↓>

< barks ↓>

< 4 ↑, </s> ↓>

< 5 ↑, barks ↓>

< loudly ↑, 4 ↓>

< cat ↑, 5 ↑>

< loudly ↑>

< cat ↑>

loudly

the

dog

a

cat

Greedy Language Model with Paths
Step 1. Greedily choose best path each word
</s>

barks

loudly

< </s> ↓>

< barks ↓>

< loudly ↓>

< 4 ↑, </s> ↓>

< 5 ↑, barks ↓> < loudly ↓, barks ↑>

< loudly ↑, 4 ↓>

< cat ↑, 5 ↑>

< loudly ↑>

< cat ↑>

< barks ↑>

the

dog

a

cat

Greedy Language Model with Paths
Step 1. Greedily choose best path each word
</s>

barks

loudly

the

< </s> ↓>

< barks ↓>

< loudly ↓>

< the ↓>

< 4 ↑, </s> ↓>

< 5 ↑, barks ↓> < loudly ↓, barks ↑> < 5 ↓, the ↓>

< loudly ↑, 4 ↓>

< cat ↑, 5 ↑>

< loudly ↑>

< cat ↑>

< barks ↑>

< 4 ↓, 5 ↓>
< <s> ↑, 4 ↓>
< <s> ↑>

dog

a

cat

Greedy Language Model with Paths
Step 1. Greedily choose best path each word
</s>

barks

loudly

the

dog

< </s> ↓>

< barks ↓>

< loudly ↓>

< the ↓>

< dog ↓>

< 4 ↑, </s> ↓>

< 5 ↑, barks ↓> < loudly ↓, barks ↑> < 5 ↓, the ↓>

< loudly ↑, 4 ↓>

< cat ↑, 5 ↑>

< loudly ↑>

< cat ↑>

< barks ↑>

< 4 ↓, 5 ↓>
< <s> ↑, 4 ↓>
< <s> ↑>

< the ↑, dog ↓>
< the ↑>

a

cat

Greedy Language Model with Paths
Step 1. Greedily choose best path each word
</s>

barks

loudly

the

dog

a

< </s> ↓>

< barks ↓>

< loudly ↓>

< the ↓>

< dog ↓>

< a ↓>

< the ↑, dog ↓>

< 5 ↓, a ↓>

< the ↑>

< 4 ↓, 5 ↓>

< 4 ↑, </s> ↓>

< 5 ↑, barks ↓> < loudly ↓, barks ↑> < 5 ↓, the ↓>

< loudly ↑, 4 ↓>

< cat ↑, 5 ↑>

< loudly ↑>

< cat ↑>

< barks ↑>

< 4 ↓, 5 ↓>
< <s> ↑, 4 ↓>

< <s> ↑, 4 ↓>

< <s> ↑>

< <s> ↑>

cat

Greedy Language Model with Paths
Step 1. Greedily choose best path each word
</s>

barks

loudly

the

dog

a

cat

< </s> ↓>

< barks ↓>

< loudly ↓>

< the ↓>

< dog ↓>

< a ↓>

< cat ↓>

< the ↑, dog ↓>

< 5 ↓, a ↓>

< a ↑, cat ↓>

< the ↑>

< 4 ↓, 5 ↓>

< a ↑>

< 4 ↑, </s> ↓>

< 5 ↑, barks ↓> < loudly ↓, barks ↑> < 5 ↓, the ↓>

< loudly ↑, 4 ↓>

< cat ↑, 5 ↑>

< loudly ↑>

< cat ↑>

< barks ↑>

< 4 ↓, 5 ↓>
< <s> ↑, 4 ↓>

< <s> ↑, 4 ↓>

< <s> ↑>

< <s> ↑>

Greedy Language Model with Paths (continued)
Step 2. Find the best derivation over these elements

Greedy Language Model with Paths (continued)
Step 2. Find the best derivation over these elements
1
4

<s>

barks

5

loudly

< </s> ↓>

< barks ↓>

< loudly ↓>

< 4 ↑, </s> ↓>

a

cat

< a ↓>

< cat ↓>

< 5 ↓, a ↓>

< a ↑, cat ↓>

< cat ↑, 5 ↑>

< 4 ↓, 5 ↓>

< a ↑>

< cat ↑>

< <s> ↑, 4 ↓>
< <s> ↑>

</s>

< 5 ↑, barks ↓> < loudly ↓, barks ↑> < loudly ↑, 4 ↓>
< barks ↑>

< loudly ↑>

Eﬃciently Calculating Best Paths
There are too many paths to compute argmax directly, but we can
compactly represent all paths as a graph

< 2 ↑>

< 9 ↑>

< 2 ↑, 4 ↓>

< 4 ↓, 5 ↓>

< 6 ↑, 5 ↓>

< 5 ↓, 10 ↓>

< 5 ↓, 8 ↓>

< 10 ↓>

< 9 ↑, 5 ↑>

< 6 ↑>

< 4 ↓, 6 ↓>

< 6 ↑, 7 ↓>

< 11 ↑>

< 11 ↑, 5 ↑>

< 8 ↓>

< 6 ↓>

< 5 ↑, 6 ↓>

< 5 ↑, 7 ↓>

< 3 ↑>

< 7 ↑>

< 5 ↑, 4 ↑>

< 7 ↓>

Graph is linear in the size of the grammar
•

Green nodes represent leaving a word

•

Red nodes represent entering a word

•

Black nodes are intermediate paths

< 7 ↑, 4 ↑>

< 4 ↑, 3 ↓>

< 3 ↓>

< 8 ↑>

< 10 ↑>

< 3 ↑, 1 ↑>

< 8 ↑, 9 ↓>

< 10 ↑, 11 ↓>

< 9 ↓>

< 11 ↓>

Best Paths
< 2 ↑>

< 9 ↑>

< 2 ↑, 4 ↓>

< 4 ↓, 5 ↓>

< 6 ↑, 5 ↓>

< 5 ↓, 10 ↓>

< 5 ↓, 8 ↓>

< 10 ↓>

< 8 ↓>

< 6 ↑>

< 4 ↓, 6 ↓>

< 6 ↑, 7 ↓>

< 6 ↓>

< 11 ↑>

Goal: Find the best
< 9 ↑, 5 ↑>
< 11 ↑, 5 ↑>
path between all
word nodes (green
and red)
< 5 ↑, 6 ↓>

< 5 ↑, 7 ↓>

Method: Run
all-pairs shortest
path to ﬁnd best
< 7 ↓>
paths

< 5 ↑, 4 ↑

Full Algorithm

Algorithm is very similar to simple bigram case. Penalty weights are
associated with nodes in the graph instead of just bigram words
Theorem
If at any iteration the greedy paths agree with the derivation,
then (y (k) ) is the global optimum.
But what if it does not ﬁnd the global optimum?

Convergence
The algorithm is not guaranteed to converge
May get stuck between solutions.
1
4

<s>
5

</s>

barks

a

cat

<s>

a

loudly

dog

barks

loudly

Convergence
The algorithm is not guaranteed to converge
May get stuck between solutions.
1
4

<s>
5

</s>

barks

a

cat

<s>

a

loudly

dog

barks

loudly

Convergence
The algorithm is not guaranteed to converge
May get stuck between solutions.
1
4

<s>
5

</s>

barks

a

cat

<s>

a

loudly

dog

barks

loudly

Convergence
The algorithm is not guaranteed to converge
May get stuck between solutions.
1
4

<s>
5

</s>

barks

the

dog

<s>

the

loudly

cat

barks

loudly

Convergence
The algorithm is not guaranteed to converge
May get stuck between solutions.
1
4

<s>
5

</s>

barks

a

cat

<s>

a

loudly

dog

barks

loudly

Convergence
The algorithm is not guaranteed to converge
May get stuck between solutions.
1
4

<s>
5

</s>

barks

a

cat

<s>

a

loudly

dog

barks

loudly

Convergence
The algorithm is not guaranteed to converge
May get stuck between solutions.
1
4

<s>
5

</s>

barks

a

cat

<s>

a

loudly

dog

barks

loudly

Convergence
The algorithm is not guaranteed to converge
May get stuck between solutions.
1
4

<s>
5

</s>

barks

the

dog

<s>

the

loudly

cat

barks

loudly

Convergence
The algorithm is not guaranteed to converge
May get stuck between solutions.
1
4

<s>
5

</s>

barks

the

dog

<s>

loudly

cat

loudly

barks

the

Can ﬁx this by incrementally adding constraints to the problem

Tightening
Main idea: Keep partition sets (A and B). The parser treats all
words in a partition as the same word.
•

Initially place all words in the same partition.

•

If the algorithm gets stuck, separate words that conﬂict

•

Run the exact algorithm but only distinguish between
partitions (much faster than running full exact algorithm)

Example:
1
4

<s>
5

</s>

barks

a

cat

<s>

a

loudly

dog

barks

loudly

Partitions
A = {2,6,7,8,9,10,11}
B = {}

Tightening
Main idea: Keep partition sets (A and B). The parser treats all
words in a partition as the same word.
•

Initially place all words in the same partition.

•

If the algorithm gets stuck, separate words that conﬂict

•

Run the exact algorithm but only distinguish between
partitions (much faster than running full exact algorithm)

Example:
1
4

<s>
5

</s>

barks

a

cat

<s>

a

loudly

dog

barks

loudly

Partitions
A = {2,6,7,8,9,10,11}
B = {}

Tightening
Main idea: Keep partition sets (A and B). The parser treats all
words in a partition as the same word.
•

Initially place all words in the same partition.

•

If the algorithm gets stuck, separate words that conﬂict

•

Run the exact algorithm but only distinguish between
partitions (much faster than running full exact algorithm)

Example:
1
4

<s>
5

</s>

barks

a

cat

<s>

a

loudly

dog

barks

loudly

Partitions
A = {2,6,7,8,9,10,11}
B = {}

Tightening
Main idea: Keep partition sets (A and B). The parser treats all
words in a partition as the same word.
•

Initially place all words in the same partition.

•

If the algorithm gets stuck, separate words that conﬂict

•

Run the exact algorithm but only distinguish between
partitions (much faster than running full exact algorithm)

Example:
1
4

<s>
5

</s>

barks

the

dog

<s>

the

loudly

cat

barks

loudly

Partitions
A = {2,6,7,8,9,10,11}
B = {}

Tightening
Main idea: Keep partition sets (A and B). The parser treats all
words in a partition as the same word.
•

Initially place all words in the same partition.

•

If the algorithm gets stuck, separate words that conﬂict

•

Run the exact algorithm but only distinguish between
partitions (much faster than running full exact algorithm)

Example:
1
4

<s>
5

</s>

barks

the

dog

<s>

the

loudly

cat

barks

loudly

Partitions
A = {2,6,7,8,9,10}
B = {11}

Tightening
Main idea: Keep partition sets (A and B). The parser treats all
words in a partition as the same word.
•

Initially place all words in the same partition.

•

If the algorithm gets stuck, separate words that conﬂict

•

Run the exact algorithm but only distinguish between
partitions (much faster than running full exact algorithm)

Example:
1
4

<s>
5

</s>

barks

the

dog

<s>

the

loudly

cat

barks

loudly

Partitions
A = {2,6,7,8,9,10}
B = {11}

Tightening
Main idea: Keep partition sets (A and B). The parser treats all
words in a partition as the same word.
•

Initially place all words in the same partition.

•

If the algorithm gets stuck, separate words that conﬂict

•

Run the exact algorithm but only distinguish between
partitions (much faster than running full exact algorithm)

Example:
A
A

<s>
A
A

the
<s>

5

B

A

dog
the

1

A
4

A

B barks
dog

A </s>
A loudly
barks

loudly

Partitions
A = {2,6,7,8,9,10}
B = {11}

Experiments
Properties:
•

Exactness

•

Translation Speed

•

Comparison to Cube Pruning

Model:
•

Tree-to-String translation model (Huang and Mi, 2010)

•

Trained with MERT

Experiments:
•

NIST MT Evaluation Set (2008)

Exactness
Percent Exact
100

90

LR
ILP
DP
LP

80

70

60

50

LR
ILP
DP
LP

Lagrangian Relaxation
Integer Linear Programming
Exact Dynanic Programming
Linear Programming

Median Speed
Sentences Per Second
1.4
1.2

LR
ILP
DP
LP

1
0.8
0.6
0.4
0.2
0

LR
ILP
DP
LP

Lagrangian Relaxation
Integer Linear Programming
Exact Dynanic Programming
Linear Programming

Comparison to Cube Pruning: Exactness
Percent Exact
100

90

LR
Cube(50)
Cube(500)

80

70

60

50

40

LR
Cube(50)
Cube(500)

Lagrangian Relaxation
Cube Pruning (Beam=50)
Cube Pruning (Beam=500)

Comparison to Cube Pruning: Median Speed
Sentences Per Second
20

LR
Cube(50)
Cube(500)

15

10

5

0

LR
Cube(50)
Cube(500)

Lagrangian Relaxation
Cube Pruning (Beam=50)
Cube Pruning (Beam=500)

The Phrase-Based Decoding Problem
We have a source-language sentence x1 , x2 , . . . , xN
(xi is the i’th word in the sentence)
A phrase p is a tuple (s, t, e) signifying that words xs . . . xt
have a target-language translation as e
E.g., p = (2, 5, the dog) speciﬁes that words x2 . . . x5 have a
translation as the dog
Output from a phrase-based model is a derivation
y = p1 p2 . . . pL
where pj for j = 1 . . . L are phrases. A derivation deﬁnes a
translation e(y) formed by concatenating the strings
e(p1 )e(p2 ) . . . e(pL )

Scoring Derivations
Each phrase p has a score g(p).
For two consecutive phrases pk = (s, t, e) and
pk+1 = (s , t , e ), the distortion distance is
δ(t, s ) = |t + 1 − s |
The score for a derivation is
L

f (y) = h(e(y)) +

L−1

g(pk ) +
k=1

k=1

η × δ(t(pk ), s(pk+1 ))

where η ∈ R is the distortion penalty, and h(e(y)) is the
language model score

The Decoding Problem
Y is the set of all valid derivations
For a derivation y, y(i) is the number of times word i is
translated
A derivation y = p1 , p2 , . . . , pL is valid if:
y(i) = 1 for i = 1 . . . N
For each pair of consecutive phrases pk , pk+1 for
k = 1 . . . L − 1, we have δ(t(pk ), s(pk+1 )) ≤ d, where d is the
distortion limit.

Decoding problem is to ﬁnd
arg max f (y)
y∈Y

Exact Dynamic Programming
We can ﬁnd
arg max f (y)
y∈Y

using dynamic programming
But, the runtime (and number of states) is exponential in N .
Dynamic programming states are of the form
(w1 , w2 , b, r)
where
w1 , w2 are last two words of a hypothesis
b is a bit-string of length N , recording which words have been
translated (2N possibilities)
r is the end-point of the last phrase in the hypothesis

A Lagrangian Relaxation Algorithm

Deﬁne Y to be the set of derivations such that:
N
i=1 y(i)

=N
For each pair of consecutive phrases pk , pk+1 for
k = 1 . . . L − 1, we have δ(t(pk ), s(pk+1 )) ≤ d, where d is the
distortion limit.

Notes:
We have dropped the y(i) = 1 constraints.
We have Y ⊂ Y

Dynamic Programming over Y
We can ﬁnd
arg max f (y)
y∈Y

eﬃciently, using dynamic programming
Dynamic programming states are of the form
(w1 , w2 , n, r)
where
w1 , w2 are last two words of a hypothesis
n is the length of the partial hypothesis
r is the end-point of the last phrase in the hypothesis

A Lagrangian Relaxation Algorithm (continued)
The original decoding problem is
arg max f (y)
y∈Y

We can rewrite this as
arg max f (y) such that ∀i, y(i) = 1
y∈Y

We deal with the y(i) = 1 constraints using Lagrangian
relaxation

A Lagrangian Relaxation Algorithm (continued)
The Lagrangian is
L(u, y) = f (y) +
i

u(i)(y(i) − 1)

The dual objective is then
L(u) = max L(u, y).
y∈Y

and the dual problem is to solve
min L(u).
u

The Algorithm
Initialization: u0 (i) ← 0 for i = 1 . . . N
for t = 1 . . . T
y t = argmaxy∈Y L(ut−1 , y)
if y t (i) = 1 for i = 1 . . . N
return y t
else
for i = 1 . . . N
ut (i) = ut−1 (i) − αt y t (i) − 1
Figure: The decoding algorithm. αt > 0 is the step size at the t’th
iteration.

An Example Run of the Algorithm

¨
¨
¨
Input German: dadurch konnen die qualitat und die regelmaßige postzustellung auch weiterhin sichergestellt werden .
t

L(ut−1 )

1

-10.0988

y t (i)
0022330020001

2

-11.1597

0010001004151

3

-12.3742

3312200010001

4

-11.8623

0100011330301

5

-13.9916

0011324000101

6

-15.6558

1112020111111

7

-16.1022

1111111111111

derivation y t
˛
˛
˛
˛
˛
˛
˛
˛
˛
˛9, 9 ˛13, 13˛
˛9, 9 ˛6, 6 ˛5, 5 ˛3, 3 ˛
˛
4, 6
3, 6
˛
˛
˛
˛
˛
˛
˛
˛
˛
˛the quality and ˛also ˛ the ˛ and ˛ the ˛quality and ˛also ˛ . ˛
˛
˛
˛
˛
˛
˛
˛
˛
˛
˛
˛
˛
˛
˛
˛
˛
˛
˛
˛
˛
˛
˛
˛
˛
11, 13
˛
˛3, 3 ˛ 7, 7 ˛12, 12 ˛ 10, 10 ˛12, 12 ˛ 10, 10 ˛12, 12 ˛ 10, 10 ˛12, 12 ˛ 10, 10 ˛
˛ the ˛regular ˛ will ˛continue to ˛ be ˛continue to ˛ be ˛continue to ˛ be ˛continue to ˛be guaranteed .˛
˛
˛
˛
˛
˛
˛
˛
˛
˛
˛
˛9, 9 ˛13, 13˛
˛
˛5, 5 ˛2, 2 ˛ 1, 1 ˛ 4, 4 ˛
˛
3, 5
1, 2
1, 2
˛
˛
˛
˛
˛
˛
˛
˛
˛
˛
˛in that way , ˛ and ˛ can ˛thus ˛quality ˛in that way , ˛the quality and ˛also ˛ . ˛
˛
˛
˛
˛
˛
˛
˛
˛
˛
˛
˛
˛
˛
˛9, 9 ˛11, 11 ˛13, 13˛
˛9, 9 ˛11, 11 ˛
˛9, 9 ˛11, 11 ˛
˛
˛2, 2 ˛ 6, 7
8, 8
8, 8
8, 8
˛
˛
˛
˛
˛
˛
˛
˛
˛
˛
˛
˛
˛
˛ can ˛the regular ˛distribution should ˛also ˛ensure ˛distribution should ˛also ˛ensure ˛distribution should ˛also ˛ensure ˛ . ˛
˛
˛
˛
˛
˛
˛
˛
˛
˛
˛
˛
˛
˛
˛
˛
˛
˛
˛
˛
˛
˛
˛
˛
˛
5, 7
˛ 11, 11 ˛13, 13˛
˛3, 3 ˛ 7, 7 ˛5, 5 ˛ 7, 7 ˛5, 5 ˛ 7, 7 ˛6, 6 ˛ 4, 4 ˛
˛ the ˛regular ˛ and ˛regular ˛ and ˛regular ˛ the ˛quality ˛and the regular ˛ensured ˛ . ˛
˛
˛
˛
˛
˛
˛
˛
˛
˛
˛
˛ 9, 10
˛
˛6, 6 ˛
˛6, 6 ˛ 4, 4
˛
˛
11, 13
8, 8
3, 4
1, 2
˛
˛
˛
˛
˛
˛
˛
˛
˛
˛in that way , ˛the quality of ˛ the ˛quality of ˛ the ˛distribution should ˛continue to ˛be guaranteed .˛
˛
˛
˛
˛
˛
˛
˛
˛
˛
˛
˛ 9, 10
˛
˛
˛ 3, 4
11, 13
8, 8
5, 7
1, 2
˛
˛
˛
˛
˛
˛
˛
˛in that way , ˛the quality ˛and the regular ˛distribution should ˛continue to ˛be guaranteed .˛

Figure 2: An example run of the algorithm in Figure 1. For each value of t we show the dual value L(ut−1 ), the
derivation y t , and the number of times each word is translated, y t (i) for i = 1 . . . N . For each phrase in a derivation
we show the English string e, together with the span (s, t): for example, the ﬁrst phrase in the ﬁrst derivation has
English string the quality and, and span (3, 6). At iteration 7 we have y t (i) = 1 for i = 1 . . . N , and the translation is

Tightening the Relaxation
In some cases, the relaxation is not tight, and the algorithm
will not converge to y(i) = 1 for i = 1 . . . N
Our solution: incrementally add hard constraints until the
relaxation is tight
Deﬁnition: for any set C ⊆ {1, 2, . . . , N },
YC = {y : y ∈ Y , and ∀i ∈ C, y(i) = 1}
We can ﬁnd
arg max f (y)
y∈YC

using dynamic programming, with a 2|C| increase in the
number of states
Goal: ﬁnd a small set C such that Lagrangian relaxation with
YC returns an exact solution

An Example Run of the Algorithm

Input German: es bleibt jedoch dabei , dass kolumbien ein land ist , das aufmerksam beobachtet werden muss .
t L(ut−1 )
1
2

32
33
34
35
36
37
38
39
40
41

t

count(6) = 10; count(10) = 10; count(i) = 0 for all other i
adding constraints: 6 10
˛
˛
˛
˛
˛
˛
˛
˛
˛
˛ 7, 7
˛6, 6 ˛
˛16, 16 ˛
˛17, 17˛
1, 5
8, 12
13, 15
˛
˛
˛
˛
˛
˛
˛
˛
-17.229 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ˛
nonetheless , ˛colombia ˛ that ˛a country that ˛ must ˛be closely monitored ˛ . ˛
00000•000•0000000

42

t

y (i)
˛
˛
˛ derivation y ˛
˛
˛
˛
˛
˛
˛
˛
˛
˛17, 17˛
˛6, 6 ˛10, 10 ˛8, 8 ˛ 9, 12
˛6, 6 ˛10, 10 ˛ 8, 9
˛5, 6 ˛10, 10 ˛ 8, 9
˛
˛
˛
˛
˛
˛
˛
˛
˛
˛
˛
-11.8658 0 0 0 0 1 3 0 3 3 4 1 1 0 0 0 0 1 ˛
˛ a ˛country that ˛ . ˛
˛a country ˛ that ˛ is
˛a country ˛ that ˛ is
˛ that ˛ is
˛
˛
˛
˛
˛
˛
˛
˛
˛
˛
˛
˛
˛
˛
˛17, 17˛
˛11, 11 ˛16, 16 ˛
˛5, 5 ˛ 7, 7
˛1, 1 ˛
˛5, 5 ˛ 3, 3
˛ 3, 3
˛1, 1 ˛
13, 15
2, 3
2, 3
˛
˛
˛
˛
˛
˛
˛
˛
˛
˛
˛
˛
˛
-5.46647 2 2 4 0 2 0 1 0 0 0 1 0 1 1 1 1 1 ˛
˛ must ˛be closely monitored ˛ . ˛
˛however , ˛ it ˛is , however ˛ , ˛however , ˛ it ˛is , however ˛ , ˛colombia ˛ ,
.
.
.
˛
˛
˛
˛
˛
˛
˛
˛
˛
˛17, 17˛
˛
˛ 7, 7
˛10, 10 ˛8, 8 ˛ 9, 12
˛16, 16 ˛
13, 15
1, 5
˛
˛
˛
˛
˛
˛
˛
˛
˛
-17.0203 1 1 1 1 1 0 1 1 1 2 1 1 1 1 1 1 1 ˛
˛ a ˛country that ˛ must ˛be closely monitored ˛ . ˛
nonetheless , ˛colombia ˛ is
˛
˛
˛
˛
˛
˛
˛
˛
˛
˛
˛17, 17˛
˛ 11, 12 ˛16, 16 ˛
˛6, 6 ˛ 7, 7
˛
˛6, 6 ˛ 8, 9
13, 15
1, 5
˛
˛
˛
˛
˛
˛
˛
˛
˛
˛
-17.1727 1 1 1 1 1 2 1 1 1 0 1 1 1 1 1 1 1 ˛
nonetheless , ˛ that ˛a country ˛ that ˛colombia ˛, which ˛ must ˛be closely monitored ˛ . ˛
˛
˛
˛
˛
˛
˛
˛
˛
˛
˛
˛
˛
˛
˛
˛
˛
˛
˛
13, 15
1, 5
˛17, 17˛
˛16, 16 ˛
˛ 7, 7
˛10, 10 ˛8, 8 ˛ 9, 12
-17.0203 1 1 1 1 1 0 1 1 1 2 1 1 1 1 1 1 1 ˛
˛ a ˛country that ˛ must ˛be closely monitored ˛ . ˛
˛nonetheless , ˛colombia ˛ is
˛
˛
˛
˛
˛
˛
˛
˛
˛
˛17, 17˛
˛16, 16 ˛
˛ 7, 7
˛10, 10 ˛8, 8 ˛ 9, 12
˛
13, 15
1, 5
˛
˛
˛
˛
˛
˛
˛
˛
˛
-17.1631 1 1 1 1 1 0 1 1 1 2 1 1 1 1 1 1 1 ˛
˛ a ˛country that ˛ must ˛be closely monitored ˛ . ˛
nonetheless , ˛colombia ˛ is
˛
˛
˛
˛
˛
˛
˛
˛
˛
˛
˛17, 17˛
˛ 11, 12 ˛16, 16 ˛
˛6, 6 ˛ 7, 7
˛
˛6, 6 ˛ 8, 9
13, 15
1, 5
˛
˛
˛
˛
˛
˛
˛
˛
˛
˛
-17.0408 1 1 1 1 1 2 1 1 1 0 1 1 1 1 1 1 1 ˛
nonetheless , ˛ that ˛a country ˛ that ˛colombia ˛, which ˛ must ˛be closely monitored ˛ . ˛
˛
˛
˛
˛
˛
˛
˛
˛
˛
˛17, 17˛
˛16, 16 ˛
˛ 7, 7
˛10, 10 ˛8, 8 ˛ 9, 12
˛
13, 15
1, 5
˛
˛
˛
˛
˛
˛
˛
˛
-17.1727 1 1 1 1 1 0 1 1 1 2 1 1 1 1 1 1 1 ˛
˛ a ˛country that ˛ must ˛be closely monitored ˛ . ˛
˛nonetheless , ˛colombia ˛ is
˛
˛
˛
˛
˛
˛
˛
˛
˛
˛
˛17, 17˛
˛ 11, 12 ˛16, 16 ˛
˛6, 6 ˛ 7, 7
˛6, 6 ˛ 8, 9
˛
13, 15
1, 5
˛
˛
˛
˛
˛
˛
˛
˛
˛
˛
-17.0408 1 1 1 1 1 2 1 1 1 0 1 1 1 1 1 1 1 ˛
nonetheless , ˛ that ˛a country ˛ that ˛colombia ˛, which ˛ must ˛be closely monitored ˛ . ˛
˛
˛
˛
˛
˛
˛
˛
˛
˛
˛
˛17, 17˛
˛ 11, 12 ˛16, 16 ˛
˛6, 6 ˛ 7, 7
˛6, 6 ˛ 8, 9
˛
13, 15
1, 5
˛
˛
˛
˛
˛
˛
˛
˛
˛
˛
-17.1658 1 1 1 1 1 2 1 1 1 0 1 1 1 1 1 1 1 ˛
nonetheless , ˛ that ˛a country ˛ that ˛colombia ˛, which ˛ must ˛be closely monitored ˛ . ˛
˛
˛
˛
˛
˛
˛
˛
˛
˛
˛
˛ 7, 7
˛10, 10 ˛8, 8 ˛ 9, 12
˛16, 16 ˛
˛17, 17˛
1, 5
13, 15
˛
˛
˛
˛
˛
˛
˛
˛
-17.056 1 1 1 1 1 0 1 1 1 2 1 1 1 1 1 1 1 ˛
˛nonetheless , ˛colombia ˛ is
˛ a ˛country that ˛ must ˛be closely monitored ˛ . ˛
˛
˛
˛
˛
˛
˛
˛
˛
˛
˛
˛
˛6, 6 ˛ 8, 9
˛6, 6 ˛ 7, 7
˛ 11, 12 ˛16, 16 ˛
˛17, 17˛
1, 5
13, 15
˛
˛
˛
˛
˛
˛
˛
˛
˛
˛
-17.1732 1 1 1 1 1 2 1 1 1 0 1 1 1 1 1 1 1 ˛
nonetheless , ˛ that ˛a country ˛ that ˛colombia ˛, which ˛ must ˛be closely monitored ˛ . ˛

The Algorithm with Constraint Generation
Optimize(C, u)
while (dual value still improving)
y ∗ = argmaxy∈YC L(u, y)
if y ∗ (i) = 1 for i = 1 . . . N return y ∗
else for i = 1 . . . N
u(i) = u(i) − α (y ∗ (i) − 1)
count(i) = 0 for i = 1 . . . N
for k = 1 . . . K
y ∗ = argmaxy∈YC L(u, y)
if y ∗ (i) = 1 for i = 1 . . . N return y ∗
else for i = 1 . . . N
u(i) = u(i) − α (y ∗ (i) − 1)
count(i) = count(i) + [[y ∗ (i) = 1]]
Let C = set of G i’s that have the largest value for
count(i) and that are not in C
return Optimize(C ∪ C , u)

Number of Constraints Required

# cons.
0-0
1-3
4-6
7-9
x

1-10 words
11-20 words
21-30 words
31-40 words 41-50 words
All sentences
183 (98.9 %) 511 (91.6 %) 438 (77.4 %) 222 (64.0 %) 82 ( 48.8 %) 1,436 (78.7 %) 78.7 %
2 ( 1.1 %) 45 ( 8.1 %) 94 (16.6 %) 87 (25.1 %) 50 ( 29.8 %)
278 (15.2 %) 94.0 %
0 ( 0.0 %)
2 ( 0.4 %) 27 ( 4.8 %) 24 ( 6.9 %) 19 ( 11.3 %)
72 ( 3.9 %) 97.9 %
0 ( 0.0 %)
0 ( 0.0 %)
7 ( 1.2 %) 13 ( 3.7 %) 12 ( 7.1 %)
32 ( 1.8 %) 99.7 %
0 ( 0.0 %)
0 ( 0.0 %)
0 ( 0.0 %)
1 ( 0.3 %) 5 ( 3.0 %)
6 ( 0.3 %) 100.0 %

Table 2: Table showing the number of constraints added before convergence of the algorithm in Figure 3, broken down by sentence
length. Note that a maximum of 3 constraints are added at each recursive call, but that fewer than 3 constraints are added in cases
where fewer than 3 constraints have count(i) > 0. x indicates the sentences that fail to converge after 250 iterations. 78.7% of the
examples converge without adding any constraints.
# cons.
0-0
1-3
4-6
7-9

1-10 words
A*
w/o
0.8
0.8
2.4
2.9
0.0
0.0
0.0
0.0

11-20 words
A*
w/o
9.7
10.7
23.2
28.0
28.2
38.8
0.0
0.0

21-30 words
A* w/o
47.0 53.7
80.9 102.3
111.7 163.7
166.1 500.4

31-40 words
A*
w/o
153.6 178.6
277.4 360.8
309.5 575.2
361.0 1,467.6

41-50 words
A*
w/o
402.6 492.4
686.0 877.7
1,552.8 1,709.2
1,167.2 3,222.4

All sentences
A*
w/o
64.6
76.1
241.3 309.7
555.6 699.5
620.7 1,914.1

Time Required
# cons.
0-0
1-3
4-6
7-9
x

1-10 words
11-20 words
21-30 words
31-40 words 41-50 words
All sentences
183 (98.9 %) 511 (91.6 %) 438 (77.4 %) 222 (64.0 %) 82 ( 48.8 %) 1,436 (78.7 %) 78.7 %
2 ( 1.1 %) 45 ( 8.1 %) 94 (16.6 %) 87 (25.1 %) 50 ( 29.8 %)
278 (15.2 %) 94.0 %
0 ( 0.0 %)
2 ( 0.4 %) 27 ( 4.8 %) 24 ( 6.9 %) 19 ( 11.3 %)
72 ( 3.9 %) 97.9 %
0 ( 0.0 %)
0 ( 0.0 %)
7 ( 1.2 %) 13 ( 3.7 %) 12 ( 7.1 %)
32 ( 1.8 %) 99.7 %
0 ( 0.0 %)
0 ( 0.0 %)
0 ( 0.0 %)
1 ( 0.3 %) 5 ( 3.0 %)
6 ( 0.3 %) 100.0 %

Table 2: Table showing the number of constraints added before convergence of the algorithm in Figure 3, broken down by sentence
length. Note that a maximum of 3 constraints are added at each recursive call, but that fewer than 3 constraints are added in cases
where fewer than 3 constraints have count(i) > 0. x indicates the sentences that fail to converge after 250 iterations. 78.7% of the
examples converge without adding any constraints.
1-10 words
A*
w/o
0-0
0.8
0.8
1-3
2.4
2.9
4-6
0.0
0.0
7-9
0.0
0.0
mean
0.8
0.9
median 0.7
0.7
# cons.

11-20 words
A*
w/o
9.7
10.7
23.2
28.0
28.2
38.8
0.0
0.0
10.9
12.3
8.9
9.9

21-30 words
A* w/o
47.0 53.7
80.9 102.3
111.7 163.7
166.1 500.4
57.2 72.6
48.3 54.6

31-40 words
A*
w/o
153.6 178.6
277.4 360.8
309.5 575.2
361.0 1,467.6
203.4 299.2
169.7 202.6

41-50 words
A*
w/o
402.6 492.4
686.0 877.7
1,552.8 1,709.2
1,167.2 3,222.4
679.9 953.4
484.0 606.5

All sentences
A*
w/o
64.6
76.1
241.3 309.7
555.6 699.5
620.7 1,914.1
120.9 168.9
35.2
40.0

Table 3: The average time (in seconds) for decoding using the algorithm in Figure 3, with and without A* algorithm, broken down
by sentence length and the number of constraints that are added. A* indicates speeding up using A* search; w/o denotes without
using A*.

a general purpose integer linear programming (ILP)
solver, which solves the problem exactly.
The experiments focus on translation from German to English, using the Europarl data (Koehn,

ing times increase as the length of sentences, and
the number of constraints required, increase. The
average run time across all sentences is 120.9 seconds. Table 3 also shows the run time of the method
without the A* algorithm for decoding. The A* al-

Comparison to LP/ILP Decoding

method
length
1-10
Y
11-15
16-20
Y
1-10
11-15
set

ILP
mean median
275.2 132.9
2,707.8 1,138.5
20,583.1 3,692.6
257.2 157.7
N/A
N/A

LP
mean median
10.9
4.4
177.4
66.1
1,374.6 637.0
18.4
8.9
476.8 161.1

% frac.
12.4 %
40.8 %
59.7 %
1.1 %
3.0 %

Table 4: Average and median time of the LP/ILP solver (in

seconds). % frac. indicates how often the LP gives a fractional
answer. Y indicates the dynamic program using set Y as deﬁned in Section 4.1, and Y indicates the dynamic program using states (w1 , w2 , n, r). The statistics for ILP for length 16-20
is based on 50 sentences.

B

Tab

MO
ber/
tran

Number of Iterations Required

# iter.
0-7
8-15
16-30
31-60
61-120
121-250
x

1-10 words
11-20 words
21-30 words
31-40 words 41-50 words
166 (89.7 %) 219 (39.2 %) 34 ( 6.0 %)
2 ( 0.6 %) 0 ( 0.0 %)
17 ( 9.2 %) 187 (33.5 %) 161 (28.4 %) 30 ( 8.6 %) 3 ( 1.8 %)
1 ( 0.5 %) 93 (16.7 %) 208 (36.7 %) 112 (32.3 %) 22 ( 13.1 %)
1 ( 0.5 %) 52 ( 9.3 %) 105 (18.6 %) 99 (28.5 %) 62 ( 36.9 %)
0 ( 0.0 %)
7 ( 1.3 %) 54 ( 9.5 %) 89 (25.6 %) 45 ( 26.8 %)
0 ( 0.0 %)
0 ( 0.0 %)
4 ( 0.7 %) 14 ( 4.0 %) 31 ( 18.5 %)
0 ( 0.0 %)
0 ( 0.0 %)
0 ( 0.0 %)
1 ( 0.3 %) 5 ( 3.0 %)

421
398
436
319
195
49
6

All sentences
(23.1 %) 23.1 %
(21.8 %) 44.9 %
(23.9 %) 68.8 %
(17.5 %) 86.3 %
(10.7 %) 97.0 %
( 2.7 %) 99.7 %
( 0.3 %) 100.0 %

Table 1: Table showing the number of iterations taken for the algorithm to converge. x indicates sentences that fail to
converge after 250 iterations. 97% of the examples converge within 120 iterations.

have or haven’t been translated in a hypothesis (partial derivation). Note that if C = {1 . . . N }, we have
YC = Y, and the dynamic program will correspond
to exhaustive dynamic programming.

the dual is still improving. In a second step, if a solution has not been found, the algorithm runs for K
more iterations, thereby choosing G additional constraints, then recursing.
If at any stage the algorithm ﬁnds a solution y ∗

Summary
presented dual decomposition as a method for decoding in NLP
formal guarantees
•

gives certiﬁcate or approximate solution

•

can improve approximate solutions by tightening relaxation

eﬃcient algorithms
•

uses fast combinatorial algorithms

•

can improve speed with lazy decoding

widely applicable
•

demonstrated algorithms for a wide range of NLP tasks
(parsing, tagging, alignment, mt decoding)

References I
Y. Chang and M. Collins. Exact Decoding of Phrase-based
Translation Models through Lagrangian Relaxation. In To
appear proc. of EMNLP, 2011.
J. DeNero and K. Macherey. Model-Based Aligner Combination
Using Dual Decomposition. In Proc. ACL, 2011.
Michael Held and Richard M. Karp. The traveling-salesman
problem and minimum spanning trees: Part ii. Mathematical
Programming, 1:6–25, 1971. ISSN 0025-5610. URL
http://dx.doi.org/10.1007/BF01584070.
10.1007/BF01584070.
D. Klein and C.D. Manning. Factored A* Search for Models over
Sequences and Trees. In Proc IJCAI, volume 18, pages
1246–1251. Citeseer, 2003.
N. Komodakis, N. Paragios, and G. Tziritas. Mrf energy
minimization and beyond via dual decomposition. IEEE
Transactions on Pattern Analysis and Machine Intelligence,
2010. ISSN 0162-8828.

References II

Terry Koo, Alexander M. Rush, Michael Collins, Tommi Jaakkola,
and David Sontag. Dual decomposition for parsing with
non-projective head automata. In EMNLP, 2010. URL
http://www.aclweb.org/anthology/D10-1125.
B.H. Korte and J. Vygen. Combinatorial Optimization: Theory and
Algorithms. Springer Verlag, 2008.
C. Lemar´chal. Lagrangian Relaxation. In Computational
e
Combinatorial Optimization, Optimal or Provably Near-Optimal
Solutions [based on a Spring School], pages 112–156, London,
UK, 2001. Springer-Verlag. ISBN 3-540-42877-1.
Angelia Nedi´ and Asuman Ozdaglar. Approximate primal
c
solutions and rate analysis for dual subgradient methods. SIAM
Journal on Optimization, 19(4):1757–1780, 2009.
Christopher Raphael. Coarse-to-ﬁne dynamic programming. IEEE
Transactions on Pattern Analysis and Machine Intelligence, 23:
1379–1390, 2001.

References III

A.M. Rush and M. Collins. Exact Decoding of Syntactic
Translation Models through Lagrangian Relaxation. In Proc.
ACL, 2011.
A.M. Rush, D. Sontag, M. Collins, and T. Jaakkola. On Dual
Decomposition and Linear Programming Relaxations for Natural
Language Processing. In Proc. EMNLP, 2010.
Hanif D. Sherali and Warren P. Adams. A hierarchy of relaxations
and convex hull characterizations for mixed-integer zero–one
programming problems. Discrete Applied Mathematics, 52(1):83
– 106, 1994.
D.A. Smith and J. Eisner. Dependency Parsing by Belief
Propagation. In Proc. EMNLP, pages 145–156, 2008. URL
http://www.aclweb.org/anthology/D08-1016.
D. Sontag, T. Meltzer, A. Globerson, T. Jaakkola, and Y. Weiss.
Tightening LP relaxations for MAP using message passing. In
Proc. UAI, 2008.

