Spectral Learning of Reﬁnement HMMs
Karl Stratos1

Alexander M. Rush2

Shay B. Cohen1

Michael Collins1

1

Department of Computer Science, Columbia University, New-York, NY 10027, USA
2
MIT CSAIL, Cambridge, MA, 02139, USA
{stratos,scohen,mcollins}@cs.columbia.edu, srush@csail.mit.edu
Abstract
We derive a spectral algorithm for learning the parameters of a reﬁnement HMM.
This method is simple, efﬁcient, and can
be applied to a wide range of supervised
sequence labeling tasks. Like other spectral methods, it avoids the problem of local optima and provides a consistent estimate of the parameters. Our experiments
on a phoneme recognition task show that
when equipped with informative feature
functions, it performs signiﬁcantly better
than a supervised HMM and competitively
with EM.

1

Introduction

Consider the task of supervised sequence labeling. We are given a training set where the j’th
training example consists of a sequence of ob(j)
(j)
servations x1 ...xN paired with a sequence of
(j)
(j)
labels a1 ...aN and asked to predict the correct labels on a test set of observations. A
common approach is to learn a joint distribution over sequences p(a1 . . . aN , x1 . . . xN ) as a
hidden Markov model (HMM). The downside of
HMMs is that they assume each label ai is independent of labels before the previous label ai−1 .
This independence assumption can be limiting,
particularly when the label space is small. To relax this assumption we can reﬁne each label ai
with a hidden state hi , which is not observed in
the training data, and model the joint distribution p(a1 . . . aN , x1 . . . xN , h1 . . . hN ). This reﬁnement HMM (R-HMM), illustrated in ﬁgure 1,
is able to propagate information forward through
the hidden state as well as the label.
Unfortunately, estimating the parameters of an
R-HMM is complicated by the unobserved hidden variables. A standard approach is to use the
expectation-maximization (EM) algorithm which

a1

a2

aN

a1 , h1

a2 , h2

aN , hN

h1

h2

hN

x1

x2

xN

x1

x2

xN

(a)

(b)

Figure 1: (a) An R-HMM chain. (b) An equivalent
representation where labels and hidden states are
intertwined.

has no guarantee of ﬁnding the global optimum of
its objective function. The problem of local optima prevents EM from yielding statistically consistent parameter estimates: even with very large
amounts of data, EM is not guaranteed to estimate
parameters which are close to the “correct” model
parameters.
In this paper, we derive a spectral algorithm for
learning the parameters of R-HMMs. Unlike EM,
this technique is guaranteed to ﬁnd the true parameters of the underlying model under mild conditions on the singular values of the model. The algorithm we derive is simple and efﬁcient, relying
on singular value decomposition followed by standard matrix operations.
We also describe the connection of R-HMMs
to L-PCFGs. Cohen et al. (2012) present a spectral algorithm for L-PCFG estimation, but the
na¨ve transformation of the L-PCFG model and
ı
its spectral algorithm to R-HMMs is awkward and
opaque. We therefore work through the non-trivial
derivation the spectral algorithm for R-HMMs.
We note that much of the prior work on spectral algorithms for discrete structures in NLP has
shown limited experimental success for this family of algorithms (see, for example, Luque et al.,
2012). Our experiments demonstrate empirical

success for the R-HMM spectral algorithm. The
spectral algorithm performs competitively with
EM on a phoneme recognition task, and is more
stable with respect to the number of hidden states.
Cohen et al. (2013) present experiments with a
parsing algorithm and also demonstrate it is competitive with EM. Our set of experiments comes as
an additional piece of evidence that spectral algorithms can function as a viable, efﬁcient and more
principled alternative to the EM algorithm.

2

Related Work

Recently, there has been a surge of interest in spectral methods for learning HMMs (Hsu et al., 2012;
Foster et al., 2012; Jaeger, 2000; Siddiqi et al.,
2010; Song et al., 2010). Like these previous
works, our method produces consistent parameter
estimates; however, we estimate parameters for a
supervised learning task. Balle et al. (2011) also
consider a supervised problem, but our model is
quite different since we estimate a joint distribution p(a1 . . . aN , x1 . . . xN , h1 . . . hN ) as opposed
to a conditional distribution and use feature functions over both the labels and observations of the
training data. These feature functions also go beyond those previously employed in other spectral
work (Siddiqi et al., 2010; Song et al., 2010). Experiments show that features of this type are crucial for performance.
Spectral learning has been applied to related
models beyond HMMs including: head automata
for dependency parsing (Luque et al., 2012),
tree-structured directed Bayes nets (Parikh et al.,
2011), ﬁnite-state transducers (Balle et al., 2011),
and mixture models (Anandkumar et al., 2012a;
Anandkumar et al., 2012b).
Of special interest is Cohen et al. (2012), who
describe a derivation for a spectral algorithm for
L-PCFGs. This derivation is the main driving
force behind the derivation of our R-HMM spectral algorithm. For work on L-PCFGs estimated
with EM, see Petrov et al. (2006), Matsuzaki et al.
(2005), and Pereira and Schabes (1992). Petrov
et al. (2007) proposes a split-merge EM procedure
for phoneme recognition analogous to that used in
latent-variable parsing.

3

The R-HMM Model

We decribe in this section the notation used
throughout the paper and the formal details of RHMMs.

3.1

Notation

We distinguish row vectors from column vectors
when such distinction is necessary. We use a
superscript
to denote the transpose operation.
We write [n] to denote the set {1, 2, . . . , n} for
any integer n ≥ 1. For any vector v ∈ Rm ,
diag(v) ∈ Rm×m is a diagonal matrix with entries v1 . . . vm . For any statement S, we use [[S]]
to refer to the indicator function that returns 1 if S
is true and 0 otherwise. For a random variable X,
we use E[X] to denote its expected value.
A tensor C ∈ Rm×m×m is a set of m3 values Ci,j,k for i, j, k ∈ [m]. Given a vector v ∈
Rm , we deﬁne C(v) to be the m × m matrix
with [C(v)]i,j = k∈[m] Ci,j,k vk . Given vectors
x, y, z ∈ Rm , C = xy z is an m×m×m tensor
with [C]i,j,k = xi yj zk .
3.2

Deﬁnition of an R-HMM

An R-HMM is a 7-tuple l, m, n, π, o, t, f for integers l, m, n ≥ 1 and functions π, o, t, f where
• [l] is a set of labels.
• [m] is a set of hidden states.
• [n] is a set of observations.
• π(a, h) is the probability of generating a ∈
[l] and h ∈ [m] in the ﬁrst position in the
labeled sequence.
• o(x|a, h) is the probability of generating x ∈
[n], given a ∈ [l] and h ∈ [m].
• t(b, h |a, h) is the probability of generating
b ∈ [l] and h ∈ [m], given a ∈ [l] and
h ∈ [m].
• f (∗|a, h) is the probability of generating the
stop symbol ∗, given a ∈ [l] and h ∈ [m].
See ﬁgure 1(b) for an illustration. At any time step
of a sequence, a label a is associated with a hidden
state h. By convention, the end of an R-HMM
sequence is signaled by the symbol ∗.
For the subsequent illustration, let N be the
length of the sequence we consider. A full sequence consists of labels a1 . . . aN , observations
x1 . . . xN , and hidden states h1 . . . hN . The model
assumes
p(a1 . . . aN , x1 . . . xN , h1 . . . hN ) = π(a1 , h1 )×
N −1

N

o(xi |ai , hi ) ×
i=1

t(ai+1 , hi+1 |ai , hi ) × f (∗|aN , hN )
i=1

Input: a sequence of observations x1 . . . xN ; operators
C b|a , C ∗|a , c1 , ca
a x
Output: µ(a, i) for all a ∈ [l] and i ∈ [N ]

for all labels a ∈ [l] and positions i ∈ [N ]. Then
the most likely label at each position i is given by
a∗ = arg max µ(a, i)
i

[Forward case]

a∈[l]

1
• αa ← c1 for all a ∈ [l].
a

• For i = 1 . . . N − 1
i
C b|a (cai ) × αa for all b ∈ [l]
x

i+1
αb ←
a∈[l]

• Tensors C b|a ∈ Rm×m×m for a, b ∈ [l]

[Backward case]

• Tensors C ∗|a ∈ R1×m×m for a ∈ [l]

N
• βa +1 ← C ∗|a (caN ) for all a ∈ [l]
x

• Column vectors c1 ∈ Rm for a ∈ [l]
a

• For i = N . . . 1
i
βa

i+1
βb

←

The marginals can be computed using a tensor
variant of the forward-backward algorithm, shown
in ﬁgure 2. The algorithm takes additional quantities C b|a , C ∗|a , c1 , ca called the operators:
a x

×C

b|a

(cai )
x

for all a ∈ [l]

b∈[l]

[Marginals]
i
i
• µ(a, i) ← βa × αa for all a ∈ [l], i ∈ [N ]

• Row vectors ca ∈ Rm for a ∈ [l] and x ∈ [n]
x
The following proposition states that these operators can be deﬁned in terms of the R-HMM parameters to guarantee the correctness of the algorithm.
Proposition 4.1. Given an R-HMM with parameters π a , oa , T b|a , f a , for any vector v ∈ Rm
x
deﬁne the operators:
C b|a (v) = T b|a diag(v)

A skeletal sequence consists of labels a1 . . . aN
and observations x1 . . . xN without hidden states.
Under the model, it has probability
p(a1 . . . aN , x1 . . . xN )
=

p(a1 . . . aN , x1 . . . xN , h1 . . . hN )
h1 ...hN

An equivalent deﬁnition of an R-HMM is
given by organizing the parameters in matrix
form. Speciﬁcally, an R-HMM has parameters
π a , oa , T b|a , f a where π a ∈ Rm is a column
x
vector, oa is a row vector, T b|a ∈ Rm×m is a max
trix, and f a ∈ Rm is a row vector, deﬁned for all
a, b ∈ [l] and x ∈ [n]. Their entries are set to
• [π a ]h = π(a, h) for h ∈ [m]

• [oa ]h = o(x|a, h) for h ∈ [m]
x
• [T b|a ]h ,h = t(b, h |a, h) for h, h ∈ [m]
• [f a ]h = f (∗|a, h) for h ∈ [m]

4

The Forward-Backward Algorithm

Given an observation sequence x1 . . . xN , we want
to infer the associated sequence of labels under
an R-HMM. This can be done by computing the
marginals of x1 . . . xN
µ(a, i) =

p(a1 . . . aN , x1 . . . xN )
a1 ...aN : ai =a

c1 = π a
a

C ∗|a (v) = f a diag(v)

Figure 2: The forward-backward algorithm

ca = oa
x
x

Then the algorithm in ﬁgure 2 correctly computes
marginals µ(a, i) under the R-HMM.
The proof is an algebraic veriﬁcation and deferred
to the appendix. Note that the running time of the
algorithm as written is O(l2 m3 N ).1
Proposition 4.1 can be generalized to the following theorem. This theorem implies that the operators can be linearly transformed by some invertible matrices as long as the transformation leaves
the embedded R-HMM parameters intact. This
observation is central to the derivation of the spectral algorithm which estimates the linearly transformed operators but not the actual R-HMM parameters.
Theorem 4.1. Given an R-HMM with parameters
π a , oa , T b|a , f a , assume that for each a ∈ [l] we
x
have invertible m × m matrices Ga and H a . For
any vector v ∈ Rm deﬁne the operators:

C b|a (v) = Gb T b|a diag(vH a )(Ga )−1 c1 = Ga π a
a
C ∗|a (v) = f a diag(vH a )(Ga )−1

ca = oa (H a )−1
x
x

Then the algorithm in ﬁgure 2 correctly computes
marginals µ(a, i) under the R-HMM.
The proof is similar to that of Cohen et al. (2012).
We can reduce the complexity to O(l2 m2 N ) by precomputing the matrices C b|a (ca ) for all a, b ∈ [l] and x ∈ [n]
x
after parameter estimation.
1

5

F1

Spectral Estimation of R-HMMs

In this section, we derive a consistent estimator for
the operators C b|a , C ∗|a , c1 , ca in theorem 4.1
a x
through the use of singular-value decomposition
(SVD) followed by the method of moments.
Section 5.1 describes the decomposition of the
R-HMM model into random variables which are
used in the ﬁnal algorithm. Section 5.2 can be
skimmed through on the ﬁrst reading, especially
if the reader is familiar with other spectral algorithms. It includes a detailed account of the derivation of the R-HMM algorithm.
For a ﬁrst reading, note that an R-HMM sequence can be seen as a right-branching L-PCFG
tree. Thus, in principle, one can convert a sequence into a tree and run the inside-outside algorithm of Cohen et al. (2012) to learn the parameters of an R-HMM. However, projecting this transformation into the spectral algorithm for L-PCFGs
is cumbersome and unintuitive. This is analogous to the case of the Baum-Welch algorithm for
HMMs (Rabiner, 1989), which is a special case of
the inside-outside algorithm for PCFGs (Lari and
Young, 1990).
5.1

Random Variables

We ﬁrst introduce the random variables underlying the approach then describe the operators based on these random variables. From
p(a1 . . . aN , x1 . . . xN , h1 . . . hN ), we draw an RHMM sequence (a1 . . . aN , x1 . . . xN , h1 . . . hN )
and choose a time step i uniformly at random from
[N ]. The random variables are then deﬁned as
X = xi
A1 = ai and A2 = ai+1
(if i = N , A2 = ∗)
H1 = hi and H2 = hi+1
F1 = (ai . . . aN , xi . . . xN )
(future)
F2 = (ai+1 . . . aN , xi+1 . . . xN )
(skip-future)
P = (a1 . . . ai , x1 . . . xi−1 )
(past)
R = (ai , xi )
(present)
D = (a1 . . . aN , x1 . . . xi−1 , xi+1 . . . xN )
(destiny)
B = [[i = 1]]

Figure 3 shows the relationship between the random variables. They are deﬁned in such a way
that the future is independent of the past and the
present is independent of the destiny conditioning
on the current node’s label and hidden state.
Next, we require a set of feature functions over
the random variables.
• φ maps F1 , F2 to φ(F1 ), φ(F2 ) ∈ Rd1 .

F2

P
a1

ai−1

ai

ai+1

aN

x1

xi−1

xi

xi+1

xN

(a)
R

D
a1

ai−1

ai

ai+1

aN

x1

xi−1

xi

xi+1

xN

(b)
Figure 3: Given an R-HMM sequence, we deﬁne
random variables over observed quantities so that
conditioning on the current node, (a) the future F1
is independent of the past P and (b) the present R
is independent of the density D.
• ψ maps P to ψ(P ) ∈ Rd2 .
• ξ maps R to ξ(R) ∈ Rd3 .
• υ maps D to υ(D) ∈ Rd4 .
We will see that the feature functions should be
chosen to capture the inﬂuence of the hidden
states. For instance, they might track the next label, the previous observation, or important combinations of labels and observations.
Finally, we require projection matrices
Φa ∈ Rm×d1
Ξa ∈ Rm×d3

Ψa ∈ Rm×d2
Υa ∈ Rm×d4

deﬁned for all labels a ∈ [l]. These matrices
will project the feature vectors of φ, ψ, ξ, and υ
from (d1 , d2 , d3 , d4 )-dimensional spaces to an mdimensional space. We refer to this reduced dimensional representation by the following random
variables:
F 1 = ΦA1 φ(F1 )
F2 = Φ

φ(F2 )

(projected skip-future: if i = N , F 2 = 1)

A1

ψ(P )

(projected past)

P =Ψ
R=Ξ

(projected future)

A2

A1

ξ(R)

D = ΥA1 υ(D)

(projected present)
(projected destiny)

Note that they are all vectors in Rm .

5.2

Estimation of the Operators

Since F 1 , F 2 , P , R, and D do not involve hidden variables, the following quantities can be directly estimated from the training data of skeletal
sequences. For this reason, they are called observable blocks:
Σa = E[F 1 P |A1 = a]
Λ = E[R D |A1 = a]
da = E[[[X = x]]D |A1 = a]
x

∀a ∈ [l]
∀a, b ∈ [l]

C b|a (v) = Db|a (v)(Σa )−1

(1)

C ∗|a (v) = D∗|a (v)(Σa )−1

(2)

=

da (Λa )−1
x

= E[[[A1 = a]]F 1 |B = 1]

(3)
(4)

To derive this result, we use the following deﬁnition to help specify the conditions on the expectations of the feature functions.
Deﬁnition. For each a ∈ [l], deﬁne matrices
I a ∈ Rd1 ×m , J a ∈ Rd2 ×m , K a ∈ Rd3 ×m , W a ∈
Rd4 ×m by
a

[I ]k,h = E[[φ(F1 )]k |A1 = a, H1 = h]
[J a ]k,h = E[[ψ(P )]k |A1 = a, H1 = h]

[K a ]k,h = E[[ξ(R)]k |A1 = a, H1 = h]

[W a ]k,h = E[[υ(D)]k |A1 = a, H1 = h]
In addition, let Γa ∈ Rm×m be a diagonal matrix
with [Γa ]h,h = P (H1 = h|A1 = a).
We now state the conditions for the correctness of
Eq. (1-4). For each label a ∈ [l], we require that
Condition 6.1 I a , J a , K a , W a have rank m.

a
a
Ψa = [v1 . . . vm ]

a
a
Ξa = [l1 . . . lm ]

a
a
Υa = [r1 . . . rm ]

Then the following m × m matrices
Ga = Φa I a

∀a ∈ [l], x ∈ [n]

The main result of this paper is that under certain conditions, matrices Σa and Λa are invertible and the operators C b|a , C ∗|a , c1 , ca in thea x
orem 4.1 can be expressed in terms of these observable blocks.

ca
x
c1
a

Φa = [ua . . . ua ]
1
m

∀a ∈ [l]

a

Db|a = E[[[A2 = b]]F 2 P R |A1 = a]

a
a
Let ua . . . ua ∈ Rd1 and v1 . . . vm ∈ Rd2 be the
m
1
top m left and right singular vectors of Ωa . Sima
a
a
a
ilarly, let l1 . . . lm ∈ Rd3 and r1 . . . rm ∈ Rd4 be
the top m left and right singular vectors of Ψa .
Deﬁne projection matrices

G a = Ψa J a

H a = Ξa K a

Ha = Υa W a

are invertible.
The proof resembles that of lemma 2 of Hsu et al.
(2012). Finally, we state the main result that shows
C b|a , C ∗|a , c1 , ca in Eq. (1-4) using the projeca x
tions from proposition 5.1 satisfy theorem 4.1. A
sketch of the proof is deferred to the appendix.
Theorem 5.1. Assume conditions 6.1 and 6.2
hold. Let Φa , Ψa , Ξa , Υa be the projection matrices from proposition 5.1. Then the operators in
Eq. (1-4) satisfy theorem 4.1.
In summary, these results show that with the
proper selection of feature functions, we can construct projection matrices Φa , Ψa , Ξa , Υa to obtain operators C b|a , C ∗|a , c1 , ca which satisfy
a x
the conditions of theorem 4.1.

6

The Spectral Estimation Algorithm

In this section, we give an algorithm to estimate
the operators C b|a , C ∗|a , c1 , ca from samples of
a x
skeletal sequences. Suppose the training set consists of M skeletal sequences (a(j) , x(j) ) for j ∈
[M ]. Then M samples of the random variables can
be derived from this training set as follows
• At each j ∈ [M ], choose a position
ij uniformly at random from the positions
in (a(j) , x(j) ). Sample the random variables (X, A1 , A2 , F1 , F2 , P, R, D, B) using
the procedure deﬁned in section 5.1.

Condition 6.2 [Γa ]h,h > 0 for all h ∈ [m].

This process yields M samples

The conditions lead to the following proposition.

(x(j) , a1 , a2 , f1 , f2 , p(j) , r(j) , d(j) , b(j) ) for j ∈ [M ]

Proposition 5.1. Assume Condition 6.1 and 6.2
hold. For all a ∈ [l], deﬁne matrices
Ωa = E[φ(F1 )ψ(P ) |A1 = a] ∈ Rd1 ×d2
1

Ωa = E[ξ(R)υ(D) |A1 = a] ∈ Rd3 ×d4
2

(j)

(j)

(j)

(j)

Assuming (a(j) , x(j) ) are i.i.d. draws from
the PMF p(a1 . . . aN , x1 . . . xN ) over skeletal sequences under an R-HMM, the tuples obtained
through this process are i.i.d. draws from the joint
PMF over (X, A1 , A2 , F1 , F2 , P, R, D, B).

[Singular Value Decomposition]
• For each label a ∈ [l], compute empirical estimates of

accuracy

Input: samples of (X, A1 , A2 , F1 , F2 , P, R, D, B); feature
functions φ, ψ, ξ, and υ; number of hidden states m
ˆ
ˆ
Output: estimates C b|a , C ∗|a , c1 , ca of the operators
ˆa ˆx
used in algorithm 2

Ωa = E[φ(F1 )ψ(P ) |A1 = a]
1
Ωa = E[ξ(R)υ(D) |A1 = a]
2
and obtain their singular vectors via an SVD. Use
the top m singular vectors to construct projections
ˆ ˆ ˆ ˆ
Φa , Ψa , Ξa , Υa .
[Sample Projection]
• Project (d1 , d2 , d3 , d4 )-dimensional samples of
(φ(F1 ), φ(F2 ), ψ(P ), ξ(R), υ(D))
ˆ ˆ ˆ ˆ
with matrices
Φa , Ψa , Ξa , Υa
dimensional samples of

Σa = E[F 1 P |A1 = a]
Λa = E[R D |A1 = a]
Db|a = E[[[A2 = b]]F 2 P R |A1 = a]
da = E[[[X = x]]D |A1 = a]
x
and also c1 = E[[[A1 = a]]F 1 |B = 1]. Finally, set
ˆa
ˆ
ˆ
ˆ
C b|a (v) ← Db|a (v)(Σa )−1
ˆ
ˆ
ˆ
C ∗|a (v) ← D∗|a (v)(Σa )−1
a
a ˆ a −1
ˆ
cx ← dx (Λ )
ˆ

The algorithm in ﬁgure 4 shows how to derive
estimates of the observable representations from
these samples. It ﬁrst computes the projection
matrices Φa , Ψa , Ξa , Υa for each label a ∈ [l]
by computing empirical estimates of Ωa and Ωa
1
2
in proposition 5.1, calculating their singular vectors via an SVD, and setting the projections in
terms of these singular vectors. These projection
matrices are then used to project (d1 , d2 , d3 , d4 )-

5

10

15
20
hidden states (m)

25

30

(j)

(j)

φ(f1 ), φ(f2 ), ψ(p(j) ), ξ(r(j) ), υ(d(j) )
down to m-dimensional vectors
f (j) , f (j) , p(j) , r(j) , d(j)
1
2
for all j ∈ [M ]. It then computes correlation
between these vectors in this lower dimensional
space to estimate the observable blocks which are
used to obtain the operators as in Eq. (1-4). These
operators can be used in algorithm 2 to compute
marginals.
As in other spectral methods, this estimation algorithm is consistent, i.e., the marginals µ(a, i)
ˆ
computed with the estimated operators approach
the true marginal values given more data. For
details, see Cohen et al. (2012) and Foster et al.
(2012).

7
Figure 4: The spectral estimation algorithm

0

dimensional feature vectors

[Method of Moments]
• For each a, b ∈ [l] and x ∈ [n], compute empirical
ˆx
ˆ ˆ ˆ
estimates Σa , Λa , Db|a , da of the observable blocks

Spectral
EM

Figure 5: Accuracy of the spectral algorithm and
EM on TIMIT development data for varying numbers of hidden states m. For EM, the highest scoring iteration is shown.

to obtain m-

(F 1 , F 2 , P , R, D)

57.5
57.0
56.5
56.0
55.5
55.0
54.5
54.0

Experiments

We apply the spectral algorithm for learning
R-HMMs to the task of phoneme recognition.
The goal is to predict the correct sequence of
phonemes a1 . . . aN for a given a set of speech
frames x1 . . . xN . Phoneme recognition is often
modeled with a ﬁxed-structure HMM trained with
EM, which makes it a natural application for spectral training.
We train and test on the TIMIT corpus of spoken
language utterances (Garofolo and others, 1988).
The label set consists of l = 39 English phonemes
following a standard phoneme set (Lee and Hon,
1989). For training, we use the sx and si utterances of the TIMIT training section made up of

ai+1 × xi , ai+1 , xi , np(ai . . . aN )
(ai−1 , xi−1 ), ai−1 , xi−1 , pp(a1 . . . ai )
xi
ai−1 × xi−1 , ai−1 , xi−1 , pp(a1 . . . ai ),
pos(a1 . . . aN )

φ(F1 )
ψ(P )
ξ(R)
υ(D)
pp
...

iy

m

b
r

r

r

np

e
r

r

r

ow

...

Figure 6: The feature templates for phoneme
recognition. The simplest features look only at the
current label and observation. Other features indicate the previous phoneme type used before ai
(pp), the next phoneme type used after ai (np),
and the relative position (beginning, middle, or
end) of ai within the current phoneme (pos). The
ﬁgure gives a typical segment of the phoneme sequence a1 . . . aN
M = 3696 utterances. The parameter estimate is
smoothed using the method of Cohen et al. (2013).
Each utterance consists of a speech signal
aligned with phoneme labels. As preprocessing,
we divide the signal into a sequence of N overlapping frames, 25ms in length with a 10ms step
size. Each frame is converted to a feature representation using MFCC with its ﬁrst and second
derivatives for a total of 39 continuous features.
To discretize the problem, we apply vector quantization using euclidean k-means to map each frame
into n = 10000 observation classes. After preprocessing, we have 3696 skeletal sequence with
a1 . . . aN as the frame-aligned phoneme labels and
x1 . . . xN as the observation classes.
For testing, we use the core test portion of
TIMIT, consisting of 192 utterances, and for development we use 200 additional utterances. Accuracy is measured by the percentage of frames
labeled with the correct phoneme. During inference, we calculate marginals µ for each label at
each position i and choose the one with the highest
marginal probability, a∗ = arg maxa∈[l] µ(a, i).
i
The spectral method requires deﬁning feature
functions φ, ψ, ξ, and υ. We use binary-valued
feature vectors which we specify through features
templates, for instance the template ai × xi corresponds to binary values for each possible label and
output pair (ln binary dimensions).
Figure 6 gives the full set of templates. These
feature functions are specially for the phoneme
labeling task. We note that the HTK baseline
explicitly models the position within the current

Method
EM(4)
EM(24)
S PECTRAL (24), no np, pp, pos
S PECTRAL (24), no pos
S PECTRAL (24)

Accuracy
56.80
56.23
55.45
56.56
56.94

Figure 7: Feature ablation experiments on TIMIT
development data for the best spectral model (m =
24) with comparisons to the best EM model (m =
4) and EM with m = 24.
Method
U NIGRAM
HMM
EM(4)
S PECTRAL (24)
HTK

Accuracy
48.04
54.08
55.49
55.82
55.70

Figure 8: Performance of baselines and spectral
R-HMM on TIMIT test data. Number of hidden
states m optimized on development data (see ﬁgure 5). The improvement of the spectral method
over the EM baseline is signiﬁcant at the p ≤ 0.05
level (and very close to signiﬁcant at p ≤ 0.01,
with a precise value of p ≤ 0.0104).
phoneme as part of the HMM structure. The spectral method is able to encode similar information
naturally through the feature functions.
We implement several baseline for phoneme
recognition: U NIGRAM chooses the most likely
label, arg maxa∈[l] p(a|xi ), at each position;
HMM is a standard HMM trained with maximumlikelihood estimation; EM(m) is an R-HMM
with m hidden states estimated using EM; and
S PECTRAL (m) is an R-HMM with m hidden
states estimated with the spectral method described in this paper. We also compare to HTK,
a ﬁxed-structure HMM with three segments per
phoneme estimated using EM with the HTK
speech toolkit. See Young et al. (2006) for more
details on this method.
An important consideration for both EM and the
spectral method is the number of hidden states m
in the R-HMM. More states allow for greater label
reﬁnement, with the downside of possible overﬁtting and, in the case of EM, more local optima.
To determine the best number of hidden states, we
optimize both methods on the development set for
a range of m values between 1 to 32. For EM,

we run 200 training iterations on each value of m
and choose the iteration that scores best on the development set. As the spectral algorithm is noniterative, we only need to evaluate the development set once per m value. Figure 5 shows the
development accuracy of the two method as we
adjust the value of m. EM accuracy peaks at 4
hidden states and then starts degrading, whereas
the spectral method continues to improve until 24
hidden states.
Another important consideration for the spectral
method is the feature functions. The analysis suggests that the best feature functions are highly informative of the underlying hidden states. To test
this empirically we run spectral estimation with a
reduced set of features by ablating the templates
indicating adjacent phonemes and relative position. Figure 7 shows that removing these features
does have a signiﬁcant effect on development accuracy. Without either type of feature, development accuracy drops by 1.5%.
We can interpret the effect of the features in
a more principled manner. Informative features
yield greater singular values for the matrices Ωa
1
and Ωa , and these singular values directly affect
2
the sample complexity of the algorithm; see Cohen
et al. (2012) for details. In sum, good feature functions lead to well-conditioned Ωa and Ωa , which in
1
2
turn require fewer samples for convergence.
Figure 8 gives the ﬁnal performance for the
baselines and the spectral method on the TIMIT
test set. For EM and the spectral method, we
use best performing model from the development data, 4 hidden states for EM and 24 for
the spectral method. The experiments show that
R-HMM models score signiﬁcantly better than a
standard HMM and comparatively to the ﬁxedstructure HMM. In training the R-HMM models,
the spectral method performs competitively with
EM while avoiding the problems of local optima.

8

Conclusion

This paper derives a spectral algorithm for the
task of supervised sequence labeling using an RHMM. Unlike EM, the spectral method is guaranteed to provide a consistent estimate of the parameters of the model. In addition, the algorithm
is simple to implement, requiring only an SVD
of the observed counts and other standard matrix operations. We show empirically that when
equipped with informative feature functions, the

spectral method performs competitively with EM
on the task of phoneme recognition.

Appendix
Proof of proposition 4.1. At any time step i ∈ [N ] in the algorithm in ﬁgure 2, for all label a ∈ [l] we have a column
i
i
vector αa ∈ Rm and a row vector βa ∈ Rm . The value of
these vectors at each index h ∈ [m] can be veriﬁed as
i
[αa ]h =

p(a1 . . . ai , x1 . . . xi−1 , h1 . . . hi )
a1 ...ai ,h1 ...hi :
ai =a,hi =h

i
[βa ]h =

p(ai+1 . . . aN , xi . . . xN , hi+1 . . . hN |ai , hi )
ai ...aN ,hi ...hN :
ai =a,hi =h
i i
Thus βa αa is a scalar equal to

p(a1 . . . aN , x1 . . . xN , h1 . . . hN )
a1 ...aN ,h1 ...hN :
ai =a

which is the value of the marginal µ(a, i).
Proof of theorem 5.1. It can be veriﬁed that c1 = Ga π a . For
a
the others, under the conditional independence illustrated in
ﬁgure 3 we can decompose the observable blocks in terms of
the R-HMM parameters and invertible matrices
Σa = Ga Γa (G a )

Λa = H a Γa (Ha )

Db|a (v) = Gb T b|a diag(vH a )Γa (G a )
D∗|a (v) = f a diag(vH a )Γa (G a )

da = oa Γa (Ha )
x
x

using techniques similar to those sketched in Cohen et al.
(2012). By proposition 5.1, Σa and Λa are invertible, and
these observable blocks yield the operators that satisfy theorem 4.1 when placed in Eq. (1-3).

References
A. Anandkumar, D. P. Foster, D. Hsu, S.M. Kakade, and Y.K.
Liu. 2012a. Two svds sufﬁce: Spectral decompositions
for probabilistic topic modeling and latent dirichlet allocation. Arxiv preprint arXiv:1204.6703.
A. Anandkumar, D. Hsu, and S.M. Kakade. 2012b. A
method of moments for mixture models and hidden
markov models. Arxiv preprint arXiv:1203.0683.
B. Balle, A. Quattoni, and X. Carreras. 2011. A spectral
learning algorithm for ﬁnite state transducers. Machine
Learning and Knowledge Discovery in Databases, pages
156–171.
S. B. Cohen, K. Stratos, M. Collins, D. P. Foster, and L. Ungar. 2012. Spectral learning of latent-variable PCFGs. In
Proceedings of the 50th Annual Meeting of the Association
for Computational Linguistics. Association for Computational Linguistics.
S. B. Cohen, K. Stratos, M. Collins, D. P. Foster, and L. Ungar. 2013. Experiments with spectral learning of latentvariable pcfgs. In Proceedings of the 2013 Conference of
the North American Chapter of the Association for Computational Linguistics: Human Language Technologies.

D. P. Foster, J. Rodu, and L.H. Ungar. 2012. Spectral dimensionality reduction for hmms. Arxiv preprint
arXiv:1203.6130.
J. S. Garofolo et al. 1988. Getting started with the darpa
timit cd-rom: An acoustic phonetic continuous speech
database. National Institute of Standards and Technology
(NIST), Gaithersburgh, MD, 107.
D. Hsu, S.M. Kakade, and T. Zhang. 2012. A spectral algorithm for learning hidden markov models. Journal of
Computer and System Sciences.
H. Jaeger. 2000. Observable operator models for discrete
stochastic time series. Neural Computation, 12(6):1371–
1398.
K. Lari and S. J. Young. 1990. The estimation of stochastic
context-free grammars using the inside-outside algorithm.
Computer speech & language, 4(1):35–56.
K.F. Lee and H.W. Hon. 1989. Speaker-independent phone
recognition using hidden markov models. Acoustics,
Speech and Signal Processing, IEEE Transactions on,
37(11):1641–1648.
F. M. Luque, A. Quattoni, B. Balle, and X. Carreras. 2012.
Spectral learning for non-deterministic dependency parsing. In EACL, pages 409–419.
T. Matsuzaki, Y. Miyao, and J. Tsujii. 2005. Probabilistic cfg
with latent annotations. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 75–82. Association for Computational Linguistics.
A. Parikh, L. Song, and E.P. Xing. 2011. A spectral algorithm for latent tree graphical models. In Proceedings of
the 28th International Conference on Machine Learning.
F. Pereira and Y. Schabes. 1992. Inside-outside reestimation from partially bracketed corpora. In Proceedings
of the 30th annual meeting on Association for Computational Linguistics, pages 128–135. Association for Computational Linguistics.
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006. Learning accurate, compact, and interpretable tree annotation.
In Proceedings of the 21st International Conference on
Computational Linguistics and the 44th annual meeting
of the Association for Computational Linguistics, pages
433–440. Association for Computational Linguistics.
Slav Petrov, Adam Pauls, and Dan Klein. 2007. Learning structured models for phone recognition. In Proc. of
EMNLP-CoNLL.
L. R. Rabiner. 1989. A tutorial on hidden markov models
and selected applications in speech recognition. Proceedings of the IEEE, 77(2):257–286.
S. Siddiqi, B. Boots, and G. J. Gordon. 2010. Reducedrank hidden Markov models. In Proceedings of the Thirteenth International Conference on Artiﬁcial Intelligence
and Statistics (AISTATS-2010).
L. Song, B. Boots, S. Siddiqi, G. Gordon, and A. Smola.
2010. Hilbert space embeddings of hidden markov models. In Proceedings of the 27th International Conference
on Machine Learning. Citeseer.
S. Young, G. Evermann, M. Gales, T. Hain, D. Kershaw,
XA Liu, G. Moore, J. Odell, D. Ollason, D. Povey, et al.
2006. The htk book (for htk version 3.4).

