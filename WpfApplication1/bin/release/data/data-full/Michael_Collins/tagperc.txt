Discriminative Training Methods for Hidden Markov Models:
Theory and Experiments with Perceptron Algorithms
Michael Collins
AT&T Labs-Research, Florham Park, New Jersey.
mcollins@research.att.com
Abstract
We describe new algorithms for training tagging models, as an alternative
to maximum-entropy models or conditional random ﬁelds (CRFs). The algorithms rely on Viterbi decoding of
training examples, combined with simple additive updates. We describe theory justifying the algorithms through
a modiﬁcation of the proof of convergence of the perceptron algorithm for
classiﬁcation problems. We give experimental results on part-of-speech tagging and base noun phrase chunking, in
both cases showing improvements over
results for a maximum-entropy tagger.

1

Introduction

Maximum-entropy (ME) models are justiﬁably
a very popular choice for tagging problems in
Natural Language Processing: for example see
(Ratnaparkhi 96) for their use on part-of-speech
tagging, and (McCallum et al. 2000) for their
use on a FAQ segmentation task. ME models
have the advantage of being quite ﬂexible in the
features that can be incorporated in the model.
However, recent theoretical and experimental results in (Laﬀerty et al. 2001) have highlighted
problems with the parameter estimation method
for ME models. In response to these problems,
they describe alternative parameter estimation
methods based on Conditional Markov Random
Fields (CRFs). (Laﬀerty et al. 2001) give experimental results suggesting that CRFs can perform signiﬁcantly better than ME models.
In this paper we describe parameter estimation algorithms which are natural alternatives to
CRFs. The algorithms are based on the perceptron algorithm (Rosenblatt 58), and the voted
or averaged versions of the perceptron described

in (Freund & Schapire 99). These algorithms
have been shown by (Freund & Schapire 99) to
be competitive with modern learning algorithms
such as support vector machines; however, they
have previously been applied mainly to classiﬁcation tasks, and it is not entirely clear how the
algorithms can be carried across to NLP tasks
such as tagging or parsing.
This paper describes variants of the perceptron algorithm for tagging problems. The algorithms rely on Viterbi decoding of training
examples, combined with simple additive updates. We describe theory justifying the algorithm through a modiﬁcation of the proof of convergence of the perceptron algorithm for classiﬁcation problems. We give experimental results
on part-of-speech tagging and base noun phrase
chunking, in both cases showing improvements
over results for a maximum-entropy tagger (a
11.9% relative reduction in error for POS tagging, a 5.1% relative reduction in error for NP
chunking). Although we concentrate on tagging
problems in this paper, the theoretical framework and algorithm described in section 3 of
this paper should be applicable to a wide variety of models where Viterbi-style algorithms
can be used for decoding: examples are Probabilistic Context-Free Grammars, or ME models
for parsing. See (Collins and Duﬀy 2001; Collins
and Duﬀy 2002; Collins 2002) for other applications of the voted perceptron to NLP problems.1

2

Parameter Estimation

2.1

HMM Taggers

In this section, as a motivating example, we describe a special case of the algorithm in this
paper: the algorithm applied to a trigram tagger. In a trigram HMM tagger, each trigram
1

The theorems in section 3, and the proofs in section 5, apply directly to the work in these other papers.

of tags and each tag/word pair have associated
parameters. We write the parameter associated
with a trigram x, y, z as αx,y,z , and the parameter associated with a tag/word pair (t, w) as
αt,w . A common approach is to take the parameters to be estimates of conditional probabilities:
αx,y,z = log P (z | x, y), αt,w = log P (w | t).
For convenience we will use w[1:n] as shorthand for a sequence of words [w1 , w2 . . . wn ],
and t[1:n] as shorthand for a taq sequence
[t1 , t2 . . . tn ]. In a trigram tagger the score for
a tagged sequence t[1:n] paired with a word sequence w[1:n] is2 n αti−2 ,ti−1 ,ti + n αti ,wi .
i=1
i=1
When the parameters are conditional probabilities as above this “score” is an estimate of the
log of the joint probability P (w[1:n] , t[1:n] ). The
Viterbi algorithm can be used to ﬁnd the highest
scoring tagged sequence under this score.
As an alternative to maximum–likelihood parameter estimates, this paper will propose the
following estimation algorithm. Say the training set consists of n tagged sentences, the i’th
sentence being of length ni . We will write these
i
examples as (w[1:ni ] , ti i ] ) for i = 1 . . . n. Then
[1:n
the training algorithm is as follows:
• Choose a parameter T deﬁning the number
of iterations over the training set.3
• Initially set all parameters αx,y,z and αt,w
to be zero.
• For t = 1 . . . T, i = 1 . . . n: Use the Viterbi
algorithm to ﬁnd the best tagged sequence for
i
sentence w[1:ni ] under the current parameter
settings: we call this tagged sequence z[1:ni ] .
For every tag trigram x, y, z seen c1 times in
ti i ] and c2 times in z[1:ni ] where c1 = c2 set
[1:n
αx,y,z = αx,y,z + c1 − c2 . For every tag/word
i
pair t, w seen c1 times in (w[1:ni ] , ti i ] ) and
[1:n
i
c2 times in (w[1:ni ] , z[1:ni ] ) where c1 = c2 set
αt,w = αt,w + c1 − c2 .
As an example, say the i’th tagged sentence
i
(w[1:ni ] , ti i ] ) in training data is
[1:n

the/D man/N saw/N the/D dog/N
Then the parameter update will add 1 to the
parameters αD,N,V , αN,V,D , αV,D,N , αV,saw and
subtract 1 from the parameters αD,N,N , αN,N,D ,
αN,D,N , αN,saw . Intuitively this has the effect of increasing the parameter values for features which were “missing” from the proposed
sequence z[1:ni ] , and downweighting parameter
values for “incorrect” features in the sequence
z[1:ni ] . Note that if z[1:ni ] = ti i ] — i.e., the
[1:n
proposed tag sequence is correct — no changes
are made to the parameter values.
2.2

Local and Global Feature Vectors

We now describe how to generalize the algorithm
to more general representations of tagged sequences. In this section we describe the featurevector representations which are commonly used
in maximum-entropy models for tagging, and
which are also used in this paper.
In maximum-entropy taggers (Ratnaparkhi
96; McCallum et al. 2000), the tagging problem is decomposed into sequence of decisions in
tagging the problem in left-to-right fashion. At
each point there is a “history” – the context in
which a tagging decision is made – and the task
is to predict the tag given the history. Formally,
a history is a 4-tuple t−1 , t−2 , w[1:n] , i where
t−1 , t−2 are the previous two tags, w[1:n] is an array specifying the n words in the input sentence,
and i is the index of the word being tagged. We
use H to denote the set of all possible histories.
Maximum-entropy models represent the tagging task through a feature-vector representation
of history-tag pairs. A feature vector representation φ : H × T → Rd is a function φ that maps a
history–tag pair to a d-dimensional feature vector. Each component φs (h, t) for s = 1 . . . d
could be an arbitrary function of (h, t). It is
common (e.g., see (Ratnaparkhi 96)) for each
feature φs to be an indicator function. For example, one such feature might be

the/D man/N saw/V the/D dog/N
and under the current parameter settings the
i
highest scoring tag sequence (w[1:ni ] , z[1:ni ] ) is
2
3

We take t−1 and t−2 to be special NULL tag symbols.
T is usually chosen by tuning on a development set.

φ1000 (h, t) =


 1 if current word wi is the


and t = DT


 0 otherwise

Similar features might be deﬁned for every
word/tag pair seen in training data. Another

feature type might track trigrams of tags, for example φ1001 (h, t) = 1 if t−2 , t−1 , t = D, N, V
and 0 otherwise. Similar features would be deﬁned for all trigrams of tags seen in training. A
real advantage of these models comes from the
freedom in deﬁning these features: for example,
(Ratnaparkhi 96; McCallum et al. 2000) both
describe feature sets which would be diﬃcult to
incorporate in a generative model.
In addition to feature vector representations
of history/tag pairs, we will ﬁnd it convenient
to deﬁne feature vectors of (w[1:n] , t[1:n] ) pairs
where w[1:n] is a sequence of n words, and t[1:n]
is an entire tag sequence. We use Φ to denote a function from (w[1:n] , t[1:n] ) pairs to ddimensional feature vectors. We will often refer
to Φ as a “global” representation, in contrast
to φ as a “local” representation. The particular
global representations considered in this paper
are simple functions of local representations:

where hi = ti−1 , ti−2 , w[1:n] , i . Given parameter values α, and an input sentence w[1:n] , the
¯
highest probability tagged sequence under the
formula in Eq. 2 can be found eﬃciently using
the Viterbi algorithm.
The parameter vector α is estimated from a
¯
training set of sentence/tagged-sequence pairs.
Maximum-likelihood parameter values can be
estimated using Generalized Iterative Scaling
(Ratnaparkhi 96), or gradient descent methods.
In some cases it may be preferable to apply a
bayesian approach which includes a prior over
parameter values.
2.4

A New Estimation Method

We now describe an alternative method for estimating parameters of the model. Given a sequence of words w[1:n] and a sequence of part of
speech tags, t[1:n] , we will take the “score” of a
tagged sequence to be

n

φs (hi , ti )

Φs (w[1:n] , t[1:n] ) =

(1)

i=1

Maximum-Entropy Taggers

In maximum-entropy taggers the feature vectors
φ together with a parameter vector α ∈ Rd are
¯
used to deﬁne a conditional probability distribution over tags given a history as
P (t | h, α) =
¯

e

s

αs φs (h,t)

Z(h, α)
¯

where Z(h, α) = l∈T e s αs φs (h,l) . The log of
¯
this probability has the form log p(t | h, α) =
¯
d
¯
s=1 αs φs (h, t) − log Z(h, α), and hence the log
probability for a (w[1:n] , t[1:n] ) pair will be
d

αs φs (hi , ti ) −
i s=1

log Z(hi , α)
¯
i

d

(2)

d

αs φs (hi , ti ) =
i=1 s=1

where hi = ti−1 , ti−2 , w[1:n] , i . Each global
feature Φs (w[1:n] , t[1:n] ) is simply the value for
the local representation φs summed over all history/tag pairs in (w[1:n] , t[1:n] ). If the local features are indicator functions, then the global features will typically be “counts”. For example,
with φ1000 deﬁned as above, Φ1000 (w[1:n] , t[1:n] )
is the number of times the is seen tagged as DT
in the pair of sequences (w[1:n] , t[1:n] ).
2.3

n

αs Φs (w[1:n] , t[1:n] ) .
s=1

where hi is again ti−1 , ti−2 , w[1:n] , i . Note that
this is almost identical to Eq. 2, but without the
local normalization terms log Z(hi , α). Under
¯
this method for assigning scores to tagged sequences, the highest scoring sequence of tags for
an input sentence can be found using the Viterbi
algorithm. (We can use an almost identical decoding algorithm to that for maximum-entropy
taggers, the diﬀerence being that local normalization terms do not need to be calculated.)
We then propose the training algorithm in ﬁgure 1. The algorithm takes T passes over the
training sample. All parameters are initially set
to be zero. Each sentence in turn is decoded using the current parameter settings. If the highest scoring sequence under the current model is
not correct, the parameters αs are updated in a
simple additive fashion.
Note that if the local features φs are indicator functions, then the global features Φs will be
counts. In this case the update will add cs − ds
to each parameter αs , where cs is the number
of times the s’th feature occurred in the correct tag sequence, and ds is the number of times

Inputs:
A training set of tagged sentences,
i
(w[1:ni ] , ti i ] ) for i = 1 . . . n.
A parameter T
[1:n
specifying number of iterations over the training set. A
“local representation” φ which is a function that maps
history/tag pairs to d-dimensional feature vectors. The
global representation Φ is deﬁned through φ as in Eq. 1.
Initialization: Set parameter vector α = 0.
¯
Algorithm:
For t = 1 . . . T, i = 1 . . . n
• Use the Viterbi algorithm to ﬁnd the output of the
model on the i’th training sentence with the current parameter settings, i.e.,
z[1:ni ] = arg maxu[1:n

i]

∈T ni

s

i
αs Φs (w[1:ni ] , u[1:ni ] )

where T ni is the set of all tag sequences of length ni .
• If z[1:ni ] = ti i ] then update the parameters
[1:n
i
i
αs = αs + Φs (w[1:ni ] , ti i ] ) − Φs (w[1:ni ] , z[1:ni ] )
[1:n

Inputs: Training examples (xi , yi )
Initialization: Set α = 0
¯
Algorithm:
For t = 1 . . . T , i = 1 . . . n
Calculate zi = arg maxz∈GEN(xi ) Φ(xi , z) · α
¯
If(zi = yi ) then α = α + Φ(xi , yi ) − Φ(xi , zi )
¯
¯
Output: Parameters α
¯

Figure 2: A variant of the perceptron algorithm.
carried over to problems such as tagging.
The task is to learn a mapping from inputs
x ∈ X to outputs y ∈ Y. For example, X might
be a set of sentences, with Y being a set of possible tag sequences. We assume:
• Training examples (xi , yi ) for i = 1 . . . n.

Output: Parameter vector α.
¯

Figure 1: The training algorithm for tagging.
it occurs in highest scoring sequence under the
current model. For example, if the features φs
are indicator functions tracking all trigrams and
word/tag pairs, then the training algorithm is
identical to that given in section 2.1.
2.5

Averaging Parameters

There is a simple reﬁnement to the algorithm
in ﬁgure 1, called the “averaged parameters”
t,i
method. Deﬁne αs to be the value for the s’th
parameter after the i’th training example has
been processed in pass t over the training data.
Then the “averaged parameters” are deﬁned as
t,i
γs = t=1...T,i=1...n αs /nT for all s = 1 . . . d.
It is simple to modify the algorithm to store
this additional set of parameters. Experiments
in section 4 show that the averaged parameters
perform signiﬁcantly better than the ﬁnal paT,n
rameters αs . The theory in the next section
gives justiﬁcation for the averaging method.

3

Theory Justifying the Algorithm

In this section we give a general algorithm for
problems such as tagging and parsing, and give
theorems justifying the algorithm. We also show
how the tagging algorithm in ﬁgure 1 is a special case of this algorithm. Convergence theorems for the perceptron applied to classiﬁcation
problems appear in (Freund & Schapire 99) –
the results in this section, and the proofs in section 5, show how the classiﬁcation results can be

• A function GEN which enumerates a set of
candidates GEN(x) for an input x.
• A representation Φ mapping each (x, y) ∈
X × Y to a feature vector Φ(x, y) ∈ Rd .
• A parameter vector α ∈ Rd .
¯
The components GEN, Φ and α deﬁne a map¯
ping from an input x to an output F (x) through
F (x) = arg

max

y∈GEN(x)

Φ(x, y) · α
¯

where Φ(x, y) · α is the inner product
¯
s αs Φs (x, y). The learning task is to set the
parameter values α using the training examples
¯
as evidence.
The tagging problem in section 2 can be
mapped to this setting as follows:
• The training examples are sentence/taggedi
sequence pairs: xi = w[1:ni ] and yi = ti i ]
[1:n
for i = 1 . . . n.
• Given a set of possible tags T , we deﬁne
GEN(w[1:n] ) = T n , i.e., the function GEN
maps an input sentence w[1:n] to the set of
all tag sequences of length n.
• The
representation
Φ(x, y)
=
Φ(w[1:n] , t[1:n] ) is deﬁned through local
feature vectors φ(h, t) where (h, t) is a
history/tag pair. (See Eq. 1.)
Figure 2 shows an algorithm for setting the
weights α. It can be veriﬁed that the training
¯

algorithm for taggers in ﬁgure 1 is a special case
of this algorithm, if we deﬁne (xi , yi ), GEN and
Φ as just described.
We will now give a ﬁrst theorem regarding
the convergence of this algorithm. This theorem
therefore also describes conditions under which
the algorithm in ﬁgure 1 converges. First, we
need the following deﬁnition:
Deﬁnition 1 Let GEN(xi ) = GEN(xi ) − {yi }. In
other words GEN(xi ) is the set of incorrect candidates
for an example xi . We will say that a training sequence
(xi , yi ) for i = 1 . . . n is separable with margin δ > 0
if there exists some vector U with ||U|| = 1 such that
∀i, ∀z ∈ GEN(xi ),

U · Φ(xi , yi ) − U · Φ(xi , z) ≥ δ (3)

(||U|| is the 2-norm of U, i.e., ||U|| =

s

U2 .)
s

We can then state the following theorem (see
section 5 for a proof):
Theorem 1 For any training sequence (xi , yi ) which is
separable with margin δ, then for the perceptron algorithm
in ﬁgure 2
R2
Number of mistakes ≤ 2
δ
where R is a constant such that ∀i, ∀z
∈
GEN(xi ) ||Φ(xi , yi ) − Φ(xi , z)|| ≤ R.

This theorem implies that if there is a parameter vector U which makes zero errors on the
training set, then after a ﬁnite number of iterations the training algorithm will have converged
to parameter values with zero training error. A
crucial point is that the number of mistakes is independent of the number of candidates for each
example (i.e. the size of GEN(xi ) for each i),
depending only on the separation of the training
data, where separation is deﬁned above. This
is important because in many NLP problems
GEN(x) can be exponential in the size of the
inputs. All of the convergence and generalization results in this paper depend on notions of
separability rather than the size of GEN.
Two questions come to mind. First, are there
guarantees for the algorithm if the training data
is not separable? Second, performance on a
training sample is all very well, but what does
this guarantee about how well the algorithm
generalizes to newly drawn test examples? (Freund & Schapire 99) discuss how the theory can
be extended to deal with both of these questions.
The next sections describe how these results can
be applied to the algorithms in this paper.

3.1

Theory for inseparable data

In this section we give bounds which apply when
the data is not separable. First, we need the
following deﬁnition:
Deﬁnition 2 Given a sequence (xi , yi ), for a U, δ pair
deﬁne mi = U · Φ(xi , yi ) − maxz∈GEN(xi ) U · Φ(xi , z) and
i

= max{0, δ − mi }. Finally, deﬁne DU,δ =

n
2
.
i=1 i

The value DU,δ is a measure of how close U
is to separating the training data with margin δ.
DU,δ is 0 if the vector U separates the data with
at least margin δ. If U separates almost all of
the examples with margin δ, but a few examples
are incorrectly tagged or have margin less than
δ, then DU,δ will take a relatively small value.
The following theorem then applies (see section 5 for a proof):
Theorem 2 For any training sequence (xi , yi ), for the
ﬁrst pass over the training set of the perceptron algorithm
in ﬁgure 2,
Number of mistakes ≤ min
U,δ

(R + DU,δ )2
δ2

where R is a constant such that ∀i, ∀z
∈
GEN(xi )
||Φ(xi , yi ) − Φ(xi , z)|| ≤ R, and the
min is taken over δ > 0, ||U|| = 1.

This theorem implies that if the training data
is “close” to being separable with margin δ –
i.e., there exists some U such that DU,δ is relatively small – then the algorithm will again make
a small number of mistakes. Thus theorem 2
shows that the perceptron algorithm can be robust to some training data examples being difﬁcult or impossible to tag correctly.
3.2

Generalization results

Theorems 1 and 2 give results bounding the
number of errors on training samples, but the
question we are really interested in concerns
guarantees of how well the method generalizes
to new test examples. Fortunately, there are
several theoretical results suggesting that if the
perceptron algorithm makes a relatively small
number of mistakes on a training sample then it
is likely to generalize well to new examples. This
section describes some of these results, which
originally appeared in (Freund & Schapire 99),
and are derived directly from results in (Helmbold and Warmuth 95).

First we deﬁne a modiﬁcation of the perceptron algorithm, the voted perceptron. We can
consider the ﬁrst pass of the perceptron algorithm to build a sequence of parameter settings α1,i for i = 1 . . . n. For a given test ex¯
ample x, each of these will deﬁne an output
vi = arg maxz∈GEN(x) α1,i · Φ(x, z). The voted
¯
perceptron takes the most frequently occurring
output in the set {v1 . . . vn }. Thus the voted
perceptron is a method where each of the parameter settings α1,i for i = 1 . . . n get a sin¯
gle vote for the output, and the majority wins.
The averaged algorithm in section 2.5 can be
considered to be an approximation of the voted
method, with the advantage that a single decoding with the averaged parameters can be performed, rather than n decodings with each of
the n parameter settings.
In analyzing the voted perceptron the one assumption we will make is that there is some
unknown distribution P (x, y) over the set X ×
Y, and that both training and test examples
are drawn independently, identically distributed
(i.i.d.) from this distribution. Corollary 1 of
(Freund & Schapire 99) then states:
Theorem 3 (Freund & Schapire 99) Assume all examples are generated i.i.d.
at random.
Let
(x1 , y1 ) . . . (xn , yn ) be a sequence of training examples
and let (xn+1 , yn+1 ) be a test example. Then the probability (over the choice of all n + 1 examples) that the
voted-perceptron algorithm does not predict yn+1 on input xn+1 is at most
(R + DU,δ )2
2
En+1 min
U,δ
n+1
δ2
where En+1 [] is an expected value taken over n + 1 examples, R and DU,δ are as deﬁned above, and the min is
taken over δ > 0, ||U|| = 1.

4
4.1

Experiments
Data Sets

We ran experiments on two data sets: part-ofspeech tagging on the Penn Wall Street Journal
treebank (Marcus et al. 93), and base nounphrase recognition on the data sets originally introduced by (Ramshaw and Marcus 95). In each
case we had a training, development and test set.
For part-of-speech tagging the training set was
sections 0–18 of the treebank, the development
set was sections 19–21 and the ﬁnal test set was
sections 22-24. In NP chunking the training set

Current word
Previous word
Word two back
Next word
Word two ahead
Bigram features

Current tag
Previous tag
Tag two back
Next tag
Tag two ahead
Bigram tag features

Trigram tag features

wi
wi−1
wi−2
wi+1
wi+2
wi−2 , wi−1
wi−1 , wi
wi , wi+1
wi+1 , wi+2
pi
pi−1
pi−2
pi+1
pi+2
pi−2 , pi−1
pi−1 , pi
pi , pi+1
pi+1 , pi+2
pi−2 , pi−1 , pi
pi−1 , pi , pi+1
pi , pi+1 , pi+2

&
&
&
&
&
&
&
&
&
&
&
&
&
&
&
&
&
&
&
&
&

ti
ti
ti
ti
ti
ti
ti
ti
ti
ti
ti
ti
ti
ti
ti
ti
ti
ti
ti
ti
ti

Figure 3: Feature templates used in the NP chunking
experiments. wi is the current word, and w1 . . . wn is the
entire sentence. pi is POS tag for the current word, and
p1 . . . pn is the POS sequence for the sentence. ti is the
chunking tag assigned to the i’th word.

was taken from section 15–18, the development
set was section 21, and the test set was section
20. For POS tagging we report the percentage
of correct tags on a test set. For chunking we
report F-measure in recovering bracketings corresponding to base NP chunks.
4.2

Features

For POS tagging we used identical features to
those of (Ratnaparkhi 96), the only diﬀerence
being that we did not make the rare word distinction in table 1 of (Ratnaparkhi 96) (i.e.,
spelling features were included for all words in
training data, and the word itself was used as a
feature regardless of whether the word was rare).
The feature set takes into account the previous
tag and previous pairs of tags in the history, as
well as the word being tagged, spelling features
of the words being tagged, and various features
of the words surrounding the word being tagged.
In the chunking experiments the input “sentences” included words as well as parts-of-speech
for those words from the tagger in (Brill 95). Table 3 shows the features used in the experiments.
The chunking problem is represented as a threetag task, where the tags are B, I, O for words
beginning a chunk, continuing a chunk, and being outside a chunk respectively. All chunks begin with a B symbol, regardless of whether the
previous word is tagged O or I.

NP Chunking Results
Method
F-Measure
Perc, avg, cc=0
93.53
Perc, noavg, cc=0 93.04
Perc, avg, cc=5
93.33
Perc, noavg, cc=5 91.88
ME, cc=0
92.34
ME, cc=5
92.65

Numits
13
35
9
39
900
200

POS Tagging Results
Method
Perc, avg, cc=0
Perc, noavg, cc=0
Perc, avg, cc=5
Perc, noavg, cc=5
ME, cc=0
ME, cc=5

Error rate/%
2.93
3.68
3.03
4.04
3.4
3.28

Numits
10
20
6
17
100
200

Figure 4: Results for various methods on the part-ofspeech tagging and chunking tasks on development data.
All scores are error percentages. Numits is the number
of training iterations at which the best score is achieved.
Perc is the perceptron algorithm, ME is the maximum
entropy method. Avg/noavg is the perceptron with or
without averaged parameter vectors. cc=5 means only
features occurring 5 times or more in training are included, cc=0 means all features in training are included.

4.3

Results

We applied both maximum-entropy models and
the perceptron algorithm to the two tagging
problems. We tested several variants for each
algorithm on the development set, to gain some
understanding of how the algorithms’ performance varied with various parameter settings,
and to allow optimization of free parameters so
that the comparison on the ﬁnal test set is a fair
one. For both methods, we tried the algorithms
with feature count cut-oﬀs set at 0 and 5 (i.e.,
we ran experiments with all features in training
data included, or with all features occurring 5
times or more included – (Ratnaparkhi 96) uses
a count cut-oﬀ of 5). In the perceptron algorithm, the number of iterations T over the training set was varied, and the method was tested
with both averaged and unaveraged parameter
T,n
T,n
vectors (i.e., with αs and γs , as deﬁned in
section 2.5, for a variety of values for T ). In
the maximum entropy model the number of iterations of training using Generalized Iterative
Scaling was varied.
Figure 4 shows results on development data
on the two tasks. The trends are fairly clear:
averaging improves results signiﬁcantly for the

perceptron method, as does including all features rather than imposing a count cut-oﬀ of 5.
In contrast, the ME models’ performance suﬀers
when all features are included. The best perceptron conﬁguration gives improvements over the
maximum-entropy models in both cases: an improvement in F-measure from 92.65% to 93.53%
in chunking, and a reduction from 3.28% to
2.93% error rate in POS tagging. In looking
at the results for diﬀerent numbers of iterations
on development data we found that averaging
not only improves the best result, but also gives
much greater stability of the tagger (the nonaveraged variant has much greater variance in
its scores).
As a ﬁnal test, the perceptron and ME taggers were applied to the test sets, with the optimal parameter settings on development data.
On POS tagging the perceptron algorithm gave
2.89% error compared to 3.28% error for the
maximum-entropy model (a 11.9% relative reduction in error). In NP chunking the perceptron algorithm achieves an F-measure of 93.63%,
in contrast to an F-measure of 93.29% for the
ME model (a 5.1% relative reduction in error).

5

Proofs of the Theorems

This section gives proofs of theorems 1 and 2.
The proofs are adapted from proofs for the classiﬁcation case in (Freund & Schapire 99).
Proof of Theorem 1: Let αk be the weights
¯
before the k’th mistake is made. It follows that
α1 = 0. Suppose the k’th mistake is made at
¯
the i’th example. Take z to the output proposed
at this example, z = argmaxy∈GEN(xi ) Φ(xi , y) ·
αk . It follows from the algorithm updates that
¯
αk+1 = αk + Φ(xi , yi ) − Φ(xi , z). We take inner
¯
¯
products of both sides with the vector U:
U · αk+1 = U · αk + U · Φ(xi , yi ) − U · Φ(xi , z)
¯
¯
≥ U · αk + δ
¯

where the inequality follows because of the property of U assumed in Eq. 3. Because α1 = 0,
¯
and therefore U · α1 = 0, it follows by induc¯
tion on k that for all k, U · αk+1 ≥ kδ. Be¯
k+1 ≤ ||U|| ||¯ k+1 ||, it follows that
cause U · α
¯
α
||¯ k+1 || ≥ kδ.
α
We also derive an upper bound for ||¯ k+1 ||2 :
α
||αk+1 ||2
¯

=

||αk ||2 + ||Φ(xi , yi ) − Φ(xi , z)||2
¯
+2αk · (Φ(xi , yi ) − Φ(xi , z))
¯

≤

||αk ||2 + R2
¯

where the inequality follows because
||Φ(xi , yi ) − Φ(xi , z)||2 ≤ R2 by assumption, and αk · (Φ(xi , yi ) − Φ(xi , z)) ≤ 0 because
¯
z is the highest scoring candidate for xi under
the parameters αk . It follows by induction that
¯
||¯ k+1 ||2 ≤ kR2 .
α
Combining the bounds ||¯ k+1 || ≥ kδ and
α
||¯ k+1 ||2 ≤ kR2 gives the result for all k that
α
k 2 δ 2 ≤ ||¯ k+1 ||2 ≤ kR2
α

⇒

k ≤ R2 /δ 2

Proof of Theorem 2: We transform the representation Φ(x, y) ∈ Rd to a new representation
¯
Φ(x, y) ∈ Rd+n as follows. For i = 1 . . . d de¯
ﬁne Φi (x, y) = Φi (x, y). For j = 1 . . . n deﬁne
¯
Φd+j (x, y) = ∆ if (x, y) = (xj , yj ), 0 otherwise,
where ∆ is a parameter which is greater than 0.
Similary, say we are given a U, δ pair, and corresponding values for i as deﬁned above. We
¯
deﬁne a modiﬁed parameter vector U ∈ Rd+n
¯ i = Ui for i = 1 . . . d and Ud+j = j /∆
¯
with U
for j = 1 . . . n. Under these deﬁnitions it can be
veriﬁed that
¯ ¯
¯ ¯
∀i, ∀z ∈ GEN(xi ), U · Φ(xi , yi ) − U · Φ(xi , z) ≥ δ
¯ i , yi ) − Φ(xi , z)||2 ≤ R2 + ∆2
¯
∀i, ∀z ∈ GEN(xi ), ||Φ(x
2
¯
||U||2 = ||U||2 + i 2 /∆2 = 1 + DU,δ /∆2
i

¯ ¯
It can be seen that the vector U/||U|| separates
2
the data with margin δ/ 1 + DU,δ /∆2 . By theorem 1, this means that the ﬁrst pass of the per¯
ceptron algorithm with representation Φ makes
D2

U,δ
at most kmax (∆) = δ12 (R2 + ∆2 )(1 + ∆2 ) mistakes. But the ﬁrst pass of the original algorithm with representation Φ is identical to the
ﬁrst pass of the algorithm with representation
¯
Φ, because the parameter weights for the addi¯
tional features Φd+j for j = 1 . . . n each aﬀect a
single example of training data, and do not aﬀect
the classiﬁcation of test data examples. Thus
the original perceptron algorithm also makes at
most kmax (∆) mistakes on its ﬁrst pass over the
training data. Finally, we can minimize kmax (∆)
with respect to ∆, giving ∆ = RDU,δ , and
2
kmax ( RDU,δ ) = (R2 + DU,δ )/δ 2 , implying the
bound in the theorem.

6

Conclusions

We have described new algorithms for tagging,
whose performance guarantees depend on a notion of “separability” of training data examples. The generic algorithm in ﬁgure 2, and

the theorems describing its convergence properties, could be applied to several other models
in the NLP literature. For example, a weighted
context-free grammar can also be conceptualized as a way of deﬁning GEN, Φ and α, so the
¯
weights for generative models such as PCFGs
could be trained using this method.
Acknowledgements
Thanks to Nigel Duﬀy, Rob Schapire and Yoram
Singer for many useful discussions regarding
the algorithms in this paper, and to Fernando
Pereira for pointers to the NP chunking data
set, and for suggestions regarding the features
used in the experiments.

References
Brill, E. (1995). Transformation-Based Error-Driven
Learning and Natural Language Processing: A Case
Study in Part of Speech Tagging. Computational Linguistics.
Collins, M., and Duﬀy, N. (2001). Convolution Kernels
for Natural Language. In Proceedings of Neural Information Processing Systems (NIPS 14).
Collins, M., and Duﬀy, N. (2002). New Ranking Algorithms for Parsing and Tagging: Kernels over Discrete
Structures, and the Voted Perceptron. In Proceedings
of ACL 2002.
Collins, M. (2002). Ranking Algorithms for Named–
Entity Extraction: Boosting and the Voted Perceptron. In Proceedings of ACL 2002.
Freund, Y. & Schapire, R. (1999). Large Margin Classiﬁcation using the Perceptron Algorithm. In Machine
Learning, 37(3):277–296.
Helmbold, D., and Warmuth, M. On weak learning. Journal of Computer and System Sciences, 50(3):551-573,
June 1995.
Laﬀerty, J., McCallum, A., and Pereira, F. (2001). Conditional random ﬁelds: Probabilistic models for segmenting and labeling sequence data. In Proceedings of
ICML 2001.
McCallum, A., Freitag, D., and Pereira, F. (2000) Maximum entropy markov models for information extraction and segmentation. In Proceedings of ICML 2000.
Marcus, M., Santorini, B., & Marcinkiewicz, M. (1993).
Building a large annotated corpus of english: The
Penn treebank. Computational Linguistics, 19.
Ramshaw, L., and Marcus, M. P. (1995). Text Chunking
Using Transformation-Based Learning. In Proceedings
of the Third ACL Workshop on Very Large Corpora,
Association for Computational Linguistics, 1995.
Ratnaparkhi, A. (1996). A maximum entropy part-ofspeech tagger. In Proceedings of the empirical methods
in natural language processing conference.
Rosenblatt, F. 1958. The Perceptron: A Probabilistic
Model for Information Storage and Organization in the
Brain. Psychological Review, 65, 386–408. (Reprinted
in Neurocomputing (MIT Press, 1998).)

