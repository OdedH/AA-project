Constraint Driven Transliteration Discovery 1
DAN G OLDWASSER , M ING -W EI C HANG , Y UANCHENG T U & DAN ROTH
University of Illinois at Urbana-Champaign
Abstract
This paper introduces a novel constraint-driven learning framework for
identifying named-entity (NE) transliterations. Traditional approaches to
the problem of discovering transliterations depend heavily on correctly
segmenting the target and the transliteration candidate and on and aligning these segments.
In this work we propose to formulate the process of aligning segments as
a constrained optimization problem. We consider the aligned segments as
a latent feature representation and show how to infer an optimal latent representation and how to use it in order to learn an improved discriminative
transliteration classiﬁer. Our algorithm is an EM-like iterative algorithm
that alternates between an optimization step for the latent representation
and a learning step for the classiﬁer’s parameters.
We apply this method both in supervised and unsupervised settings, and
show that our model can signiﬁcantly outperform previous methods trained
using considerably more resources.

1 Introduction
Named entity (NE) transliteration is the process of transcribing a NE from a
source language to some target language while preserving its pronunciation in
the original language. Automatic NE transliteration is an important component in many cross-language applications, such as Cross-Lingual Information
Retrieval (CLIR) and Machine Translation (MT) [Hermjakob et al. 08; Klementiev & Roth 06a; Meng et al. 01; Knight & Graehl 98].
It might initially seem that transliteration is an easy task, requiring only ﬁnding a phonetic mapping between character sets. However, simply matching every
source language character to its target language counterpart is not likely to work
well as in practice this mapping depends on the context the characters appear
in and on transliteration conventions which may change across domains. As a
result, current approaches employ machine learning methods.
Recently, several methods focus on NE transliteration discovery, a framework for discovering occurrences of NE in a bilingual corpora. In these settings
1

This paper extends and uniﬁes our previous works [Goldwasser & Roth 08b] and [Chang et
al. ].

2 DAN GOLDWASSER, MING-WEI CHANG, YUANCHENG TU & DAN ROTH
a classiﬁer is trained to determine if a given pair of words constitute a transliteration pair. The success of these methods depends heavily on correctly segmenting the input words and matching the segments across the two words. Recent
discriminative transliteration methods avoid this difﬁcult step and encode the
possible alignments as features, and let a discriminative training algorithm assign weights appropriately. Although the relevancy of pairwise features is context sensitive and there are contextual constraints among them, the underlying
assumption behind these methods is that a discriminative approach will be sufﬁcient to account for those by weighing features appropriately using sufﬁcient
training data. This has been shown to be difﬁcult for language pairs which are
very different, such as English and Hebrew [Goldwasser & Roth 08a].
In this work we combine an explicit alignment process in a discriminative
training framework, and directly consider the dependency between correctly
aligning the candidate words characters and correct transliteration classiﬁcation
decisions. Our model learns how to correctly align the two words and uses that
alignment to learn a better classiﬁcation model by using the aligned substrings as
the feature representation of the word pair. We formulate the alignment process
as a constrained optimization process that, given the model parameters (i.e, the
local mapping weights), ﬁnds the best global alignment between the two words.
The ﬂexibility of the model allows us to incorporate prior knowledge about the
two languages directly as constraints. After features are extracted, we use a
discriminative learning algorithm to update the model, and use the new weight
vector to determine the objective function for the optimization based feature extraction.
We apply this method in both supervised and unsupervised settings and consider several different alignment models. We bootstrap the unsupervised model
with local information only, corresponding only to a partial mapping between
the two character sets, and learn from unlabeled data the complete mapping and
the relevant context needed to disambiguate the different possible alignment (or
feature activation) decisions.
We tested our approach on three very different languages - Russian, a Slavic
language, Hebrew, a Semitic language, and Chinese, a Sino-Tibetan language.
We show that using our approach we can train a robust transliteration model and
outperform existing discriminative method using less resources. Interestingly,
when working in an unsupervised setting, we show that using a simple resource
- a Romanization table, is enough to bootstrap the model, and outperform supervised methods.
The rest of the paper is organized as follows. Sec. 2 brieﬂy examines related
work. Sec. 3 explains our model and Sec. 3.3 provides a linguistic intuition for it.
Sec. 4 describes our experiments and evaluates our results, and Sec. 5 concludes.

CONSTRAINT DRIVEN TRANSLITERATION DISCOVERY

3

Fig. 1: Left: The space of all possible features that can be generated given a word pair,
and the pruned features representation generated by the inference process. Right:
Bipartite graph representation of character unigram alignment corresponding to the
generated features.

2 Related Works
Transliteration methods typically fall into two categories: generative approaches
[Li et al. 04; Jung et al. 00; Knight & Graehl 98] that produce the target transliteration given a source language NE, and discriminative approaches [Goldwasser
& Roth 08b; Bergsma & Kondrak 07; Sproat et al. 06; Klementiev & Roth 06a],
that identify the correct transliteration of a word in the source language given
several candidates in the target language. Discriminative approaches, when used
for discovering NE in a bilingual corpora avoid the Out-Of-Vocabulary problem
by choosing the transliteration candidates from the corpora. These methods typically make very little assumptions about the source and target languages and
require considerably less data to converge. Training the transliteration model is
typically done under supervised settings [Bergsma & Kondrak 07; Goldwasser
& Roth 08b], or weakly supervised settings with additional temporal information [Sproat et al. 06; Klementiev & Roth 06a].
Incorporating knowledge encoded as constraints into learning problems has
attracted a lot of attention in the NLP community recently, both in supervised settings [Roth & Yih 04; Riedel & Clarke 06] and unsupervised settings [Haghighi
& Klein 06; Chang et al. 07] where constraints are used to bootstrap the model.
[Chang et al. 07] describes an unsupervised training of a Constrained Conditional Model (CCM), a general framework for combining statistical models with
declarative constraints. We extend this work to include constraints over possible
assignments to latent variables which, in turn, deﬁne the underlying representation for the learning problem.
In the transliteration community there are several works [Bergsma & Kondrak 07; Goldwasser & Roth 08b; Chang et al. ] that show how the feature
representation of a word pair can be restricted to facilitate learning a string sim-

4 DAN GOLDWASSER, MING-WEI CHANG, YUANCHENG TU & DAN ROTH
ilarity model. We follow the approach discussed in [Goldwasser & Roth 08b],
which considers the feature representation as a structured prediction problem
and ﬁnds the set of optimal assignments (or feature activations), under a set of
legitimacy constraints. This approach stresses the importance of interaction between learning and inference, as the model iteratively uses inference to improve
the sample representation for the learning problem and uses the learned model
to improve the accuracy of the inference process. We adapt this approach to unsupervised settings using self-training, where iterating over the data provides a
better classiﬁcation function to label the data for the next training iteration.
3 Constraint Driven Transliteration Model
In this section we present our Constraint Driven Transliteration framework. We
run an EM like iterative procedure that alternates between an inference step and
a learning step. Inference serves to align the word pair and extract features accordingly; this feature representation is used by the learning algorithm which,
in turn, learns the new model parameters thus providing the inference procedure
with a better objective function. This process is described in Algorithm 1.
The model presented in this section can be applied in both supervised settings
where annotated examples in the form of correct transliteration pairs are available and in unsupervised settings, where this supervision is self generated by the
algorithm. In the latter case the initial objective function for the inference process is seeded with a Romanization table - a partial mapping between the source
and target character sets. In the rest of this section we describe our framework
in detail and explain the differences between the supervised and unsupervised
instantiations of the framework.
Transliteration Model Our model works in a Discovery setting, where given
a source language NE, the model ﬁnds its target language counterpart in a document. We use a linear transliteration model, mapping a source language NE and
a target language candidate word into a Real number - the candidate pair translit0
k
eration score. Given a source word NE, vs , and a list of target words vt . . . vt ,
each candidate target word is paired with the source word NE. These pairs are
ranked according to their transliteration score and the model outputs the pair with
the highest score.
Features in our model are character n-gram pairs (ss , st ), where ss is a source
word character n-gram and st is a target word character n-gram. In our experiments we used unigram and bigram pairs. The feature representation of a word
pair vs , vt is denoted by F (vs , vt ).
Each feature (ss , st ) is assigned a weight W (ss , st ) ∈ R, used for deciding
the score assigned to that representation. The weight vector is learned using a
linear learning algorithm.

CONSTRAINT DRIVEN TRANSLITERATION DISCOVERY

5

In the rest of this section we describe how to obtain F (·) and how to initialize
and train W (·).
Initialization The weight vector W is initialized differently when working in
supervised or unsupervised settings. When training data is available, it is used
directly to initialize the model’s parameters:
W (ss , st ) =

#(ss , st ) #(ss , st )
×
,
#(ss )
#(st )

where #(ss , st ) is the number of occurrences of that feature in the positive sample set. We use a simple feature extraction technique at this initial stage - features
are extracted by considering all possible alignments between the source and target word characters and character bigrams. #(sL ), L = s, t is the number of
occurrences of an individual substring, sL , in any of the features extracted from
positive samples in the training set.
In the unsupervised case the model is bootstrapped using a romanization table
T .This table contains a partial mapping between the source and target character
sets, typically mapping each character to its predominant counterpart. We use
this table directly by assigning a uniform zero weight to character level mappings
appearing in the table, and a (-1) penalty otherwise.
W (ss , st ) =

0
: (ss , st ) ∈ T
−1 : (ss , st ) ∈ T
/

Inference based Feature Extraction Given a word pair (vs , vt ), a feature extraction process is used to determine the feature representation of the pair. Unlike traditional feature extraction approaches, our feature representation function
does not produce a ﬁxed feature representation. The feature extraction process
is formalized as a constrained optimization problem that captures the interdependencies between the features used to represent the sample, and encodes these
dependencies as constraints restricting the space of possible feature activation
combinations. That is, obtaining F (vs , vt ) requires solving an optimization problem, the technical details are described in Sec. 3.1. The constraints we use are
described in Sec. 3.2.
Prediction For training to take place each feature representation should be associated with a label. In the supervised case labels are available, in the unsupervised case the model’s predictions are converted into labels. The model ranks
the different candidates for every source NE according to the similarity score
associated with their chosen representation. Each source NE paired with its top

6 DAN GOLDWASSER, MING-WEI CHANG, YUANCHENG TU & DAN ROTH
ranked transliteration is labeled as a positive example, we leave the other top k
ranking pairs unlabeled, and the rest of the samples are considered as negative
samples.
Training The labeled data can now be used directly to train the model and replace the initial weights with weights which are discriminatively learned. This
process is repeated several times until the model converges. Over the different
training iterations we expect the model to generate a better representation (and
a better classiﬁcation in the unsupervised case), thus allowing the model to improve over multiple training iterations.
Input: Constraints C, Transliteration data: D={(Vs ,Vt )}
Initialization: Assign weights to table W : (Ss , St ) → R
while not converged do
Inference: Generate a feature representation D∗
D∗ ← (vs ,vt )∈D F (vs , vt ). Use C and W to generate F (vs , vt )
Prediction: Associate a label with every instance representation F (vs , vt )
Training: Train the new transliteration model
W ← train(D∗ )
end while
Algorithm 1: Constraint Driven Transliteration Framework.
In the rest of this section we explain this process in detail. We deﬁne the
feature extraction inference process in Sec. 3.1, the constraints used in Sec. 3.2,
the linguistic intuition for our model is described in Sec. 3.3 and the inference
algorithm in Sec. 3.4.
3.1

Finding Feature Representation as Constrained Optimization

Deciding if a target word is a transliteration of a source word is a binary classiﬁcation problem. However, this classiﬁcation problem is deﬁned over an unknown
(or hidden) structure. Successfully recovering this structure has high impact on
successful classiﬁcation. We use the formulation of Constrained Conditional
Models (CCMs) [Roth & Yih 04; Roth & Yih 07; Chang et al. 08] to uncover
this structure - feature activation decisions are deﬁned as a set of latent variables
and the dependencies between feature activations are captured using constraints
over assignments to these variables.
Initial Feature Representation Given a word pair, the set of all possible features consists of all possible character bigram and unigram mappings from the

CONSTRAINT DRIVEN TRANSLITERATION DISCOVERY

7

source word to the target word. Character omission is modeled by mapping the
character to the blank character (denoted as ’ ’). This representation is depicted
in Figure 1. This process is formally deﬁned as an operator mapping a transliteration candidate pair to a set of binary variables, denoted as All-F eatures (AF ).
AF = {(ss , st )|ss ∈ vs ∪ { }, st ∈ vt ∪ { }}
Representation Decision The initial sample representation (AF ) is obtained
by coupling substrings from the two terms without considering the dependencies between the possible combinations. To facilitate learning, this representation should be pruned to consider only feature activations corresponding to legal
alignments of the two words n-grams. This is done by selecting a subset F ⊂ AF
of the possible features, containing a character unigram and bigram alignment of
the two words. Figure 1 provides an example of the features generated given a
word pair.
The feature extraction process is formulated as a linear optimization problem
over a set of binary variables, encoding feature activations in AF . The objective
function maximized is a linear function over the variables in AF , each with
its weight as a coefﬁcient, as in the left part of Equation 1 below. We seek to
maximize this linear sum subject to a set of constraints. These represent the
dependencies between selections and prior knowledge about possible legitimate
character mappings and correspond to the right side of Equation 1. The score of
the representation F (vs , vt ) can be written as follows:
score(F (vs , vt )) = W · F (vs , vt ) −

ρci (F (vs , vt )

(1)

ci ∈C

In our settings only hard constraints are used and therefore the penalty (ρ)
for violating any of the constraints is set to ∞. The speciﬁc constraints used
are discussed in Sec. 3.2. The result of the optimization process is a set F of
active features, deﬁned in Equation 2. The result of this process is described in
Figure 1.
F ∗ (vs , vt ) = arg maxF ⊂AF (vs ,vt ) score(F ).
(2)
Transliteration Decision The ranking process done by our model can now
be naturally deﬁned. Given a source word vs , and a set of candidates target
n
0
words vt , . . . , vt , ﬁnd the candidate whose optimal representation maximizes
Equation 1. This process is deﬁned in Equation 3.
∗
i
vt = arg max score(F (vs , vt )).
i
vt

(3)

8 DAN GOLDWASSER, MING-WEI CHANG, YUANCHENG TU & DAN ROTH
3.2

Incorporating Mapping Constraints

We consider two types of constraints: general constraints that apply to all languages and language speciﬁc constraints. General constraints encode global
restrictions, capturing the dependencies between different mapping decisions.
Language speciﬁc constraints typically impose a local restriction such as forcing some of the possible character mapping decisions. The linguistic intuition
behind these constraints is discussed in Section 3.3.
General constraints: To facilitate readability we denote the feature activations
as a Boolean variables, where aij denotes a unigram mapping feature activation
- where i denotes the i-th source word character, j the j-th target word character.
Similarly, aij,lm denotes a bigram feature activation, mapping the the i-th and l-th
source word characters to the j-th and m-th target word characters respectively.
• Coverage - Every character unigram (or bigram) must be mapped only to
a single character unigram (or bigram), or to the blank character. For the
unigram case this can be formally written as:
j aij ≤ 1 and
i aij ≤ 1.
• No Crossing - Every character mapping, except mapping to blank character, should preserve the order of appearance in the source and target words,
or formally for the unigram case ∀i, j (aij = 1) ⇒ (∀l < i, ∀k > j, alk = 0).
And
∀i, j (aij = 1) ⇒ (∀l > i, ∀k < j, alk = 0).
• Unigram and Bigram alignment consistency - every bigram and unigram
feature decision with overlapping indices should be consistent with each
other.
∀i, j, l, m s.t. (l = i + 1 ∧ m = j + 1 ), (aij,lm ↔ (aij ∧ alm ))
Language speciﬁc constraints
• Restricted Mapping: These constraints restrict the possible local mappings
between source and target language characters. We maintain a list of possible mappings cs → Θcs , where Θcs ⊆ Ct and ct → Θct , where Θct ⊆ Cs .
Any feature (cs , ct ) such that cs ∈ Θct or ct ∈ Θcs is penalized in our
/
/
model.
• Length restriction: An additional constraint restricts the size difference
between the two words. We formulate this as follows: ∀vs ∈ Vs , ∀vt ∈ Vt ,
if γ|vt | > |vs | and γ|vs | > |vt |, score(F (vs , vt )) = −∞. Although γ can
take different values for different languages, we simply set γ to 2 in this
paper.

CONSTRAINT DRIVEN TRANSLITERATION DISCOVERY

9

In addition to biasing the model to choose the right candidate, the constraints
also provide a computational advantage: a given a word pair is eliminated from
consideration when the length restriction is not satisﬁed or there is no way to
satisfy the restricted mapping constraints.
3.3

Encoding Language Speciﬁc Knowledge as Constraints

Language speciﬁc constraints indicate phonetic mapping tendency between source
and target languages. For example, certain n-gram phonemic mappings, such as
r → l from English to Chinese, are language speciﬁc and can be captured by
language speciﬁc sound change patterns.
These patterns have been used by other systems as features or pseudofeatures [Yoon et al. 07]. However, in our system these language speciﬁc rule-ofthumbs are systematically used as constraints to exclude impossible alignments
and therefore generate better features for learning. We used 20 language speciﬁc
constraints for English-Chinese pairings, 24 constraints for English-Hebrew and
17 for English-Russian.
3.4

Efﬁcient Inference

The optimization problem deﬁned in Equation 2 is formulated as an Integer Linear Program (ILP). However, given the structure of the problem it is possible
to develop an efﬁcient dynamic programming algorithm for it, based on the algorithm for ﬁnding the minimal edit distance of two strings. The complexity
of ﬁnding the optimal set of features is only quadratic in the size of the input
pair, a clear improvement over the ILP exponential time algorithm. The algorithm minimizes the weighted edit distance between the strings, and produces a
character alignment that satisﬁes the general constraints (Sec. 3.2). Our modiﬁcations are only concerned with incorporating the language-speciﬁc constraints
into the algorithm and ensuring the consistency between unigram and bigram
level features. The ﬁrst can be done simply by assigning a negative inﬁnity score
to any alignment decision not satisfying these constraints. We modify the algorithm to consider at each stage the decision that minimizes the edit cost of both
unigram and bigram edit operations, thus ensuring that the resulting alignment
is the optimal one and that unigram level mapping decisions do not conﬂict with
bigram level mapping decisions.
4 Experiments and Analysis
We evaluated our method empirically in both supervised and unsupervised settings, observing both the overall performance in the classiﬁcation task and the
resources required for achieving this performance. We compared our method to

10 DAN GOLDWASSER, MING-WEI CHANG, YUANCHENG TU & DAN ROTH
previously published results and show that our model outperforms other models
signiﬁcantly using only a fraction of the resources needed to train previous models. To obtain a better understanding of the model we also describe an ablation
study, evaluating the individual contribution of each of the model’s elements.
We start by describing the experimental settings and datasets used. We then
proceed to describe and analyze the results.
4.1

Experimental Settings

In our experiments the system is evaluated on its ability to correctly identify the
correct transliteration for each source word. The test data consists of pairs of
words obtained by pairing every source word NE with all target words. We evaluated the system’s performance using two measures adopted in many transliteration works. The ﬁrst one is Mean Reciprocal Rank (MRR), used in [Tao et
al. 06; Sproat et al. 06], which is the average of the multiplicative inverse of
the rank of the correct answer. Formally, Let n be the number of source NEs.
Let GoldRank(i) be the rank the algorithm assigns to the correct transliteration.
Then, MRR is deﬁned as:
1
MRR =
n

n

i=1

1
.
goldRank(i)

Another measure is Accuracy (ACC) used in [Klementiev & Roth 06a; Goldwasser & Roth 08a], which is the percentage of the candidates the algorithm
ranks at the top, that are indeed the correct transliteration.
4.2

Datasets

We experimented with three different target languages Russian, Chinese, and
Hebrew. We used English as the source language in all these experiments.
The Russian data set1 , originally introduced in [Klementiev & Roth 06b], is
comprised of temporally aligned news articles. The dataset contains 727 single
word English NEs with a corresponding set of 50,648 potential Russian candidate words which include not only name entities, but also other words appearing
in the news articles.
The Chinese dataset is taken directly from an English-Chinese transliteration dictionary, derived from LDC Gigaword corpus2 . The entire dictionary
consists of 74,396 pairs of English-Chinese NEs, where Chinese NEs are written in Pinyin, a romanized spelling system of Chinese. In [Tao et al. 06] a
dataset which contains about 600 English NEs and 700 Chinese candidates is
1
2

The corpus is available http://L2R.cs.uiuc.edu/∼cogcomp.
http://www.ldc.upenn.edu

CONSTRAINT DRIVEN TRANSLITERATION DISCOVERY

11

Fig. 2: Comparison between our model (denoted UCD) and weakly supervised learning
methods [Klementiev & Roth 06b]. Note that one of the models proposed in [Klementiev &
Roth 06b] takes advantage of the temporal information. Our best model, the unsupervised
learning with all constraints, outperforms both models in [Klementiev & Roth 06b], even
though we do not use any temporal information.

used. Since the dataset is not publicly available, we created a dataset in a similar way. We randomly selected approximately 600 NE pairs and then added
100 candidates which do not correspond to any of the English NE previously
selected.
The Hebrew dataset, originally introduced in [Goldwasser & Roth 08a], consists of 550 English-Hebrew transliteration pairs extracted from Wikipedia. In
our experiments we used 250 of these NE as training data when working in supervised settings, and the other 300 were used as testing data for both the supervised
and unsupervised settings.
4.3

Unsupervised Settings

We start by reporting the results obtained by the unsupervised instantiation of our
model. We evaluate our model over three different language pairs- Russian, Chinese and Hebrew. Our implementation uses the Support Vector Machine (SVM)
learning algorithm with linear kernel as our underlying learning algorithm. We
used the package LIBLINEAR [Hsieh et al. 08] in our experiments.
Our full model uses both unigram and bigram features. However, the supervision signal, obtained from the Romanization table, is limited to unigram
features alone. To provide the unsupervised model with a better starting point,
the system was trained in two stages - ﬁrst, using only unigram features, initialized using the Romanization table, and once the model converged we added the
bigram features, initialized with a weight of 0. Experiments showed that this
training protocol resulted in a considerable improvement.

12 DAN GOLDWASSER, MING-WEI CHANG, YUANCHENG TU & DAN ROTH

Fig. 3: Comparison between supervised and unsupervised models, tested on
English-Hebrew NE pairs. We show the learning curve for the unsupervised version of our
model (denoted UCD), tested on the English-Hebrew dataset. We compare it to the supervised
model presented in [Goldwasser & Roth 08a] (denoted GR08a). Results show a signiﬁcant
improvement when using our model

We begin by comparing our model to previously published models tested
over the same data, in two different languages, Russian and Hebrew. The results of these experiments are reported using the evaluation measures used in the
original papers and are summarized in Table 1.
To evaluate our performance over the English-Russian dataset, we compare
our results to the model presented in [Klementiev & Roth 06b], a weakly supervised algorithm that uses both phonetic information and temporal information. The model is bootstrapped using a set of 20 labeled examples. In their
setting the candidates are ranked by combining two scores, one obtained using
the transliteration model and a second by comparing the relative occurrence frequency of terms over time in both languages. Due to computational tractability
reasons we slightly changed Algorithm 1 to use only a small subset of the possible negative examples, and use only unigram features. The results show a significant improvement for the English-Russian dataset when compared to a previous
semi-supervised system, which uses a stronger initial supervision signal. Figure 2 describes the learning curve of our method over the Russian dataset. We
compared our algorithm to two models described in [Klementiev & Roth 06b]
- one uses only phonetic similarity and the second also considers temporal cooccurrence similarity when ranking the transliteration candidates. Both models
converge after 50 iterations. When comparing our model to [Klementiev & Roth
06b], we found that even though our model ignores the temporal information
it achieves better results and converges after fewer iterations. Their results report a signiﬁcant improvement when using temporal information - improving an
ACC score of 41% without temporal information to 63% when using it. Since

CONSTRAINT DRIVEN TRANSLITERATION DISCOVERY

Language
Rus. (ACC)
Heb. (MRR)

Unsup. model
73
0.921

13

Prev. works
63 (41) (KR’06)
0.894 (Sup. model)

Table 1: Comparison to previously published results. Unsup. model denotes our model,
operating in an unsupervised setting. KR’06 is described in [Klementiev & Roth 06b] and Sup.
model denotes our model working in a supervised setting.
Settings
Roman. table
unig.
Roman. table +learn. unig.
+Gen Const.
unig.
+Gen Const. +learn. unig.
+All Const.
unig.
+All Const.
+learn. unig.
+All Const.
+learn. big. (i.)
+All Const.
+learn. big. (c.)

Chinese
0.019 (0.5)
0.020 (0.3)
0.746 (67.1)
0.867 (82.2)
0.801 (73.4)
0.889 (84.7)
0.871 (83.4)
0.902 (86.1)

Russian
0.034 (1.0)
0.048 (1.3)
0.809 (74.3)
0.906 (86.7)
0.849 (79.3)
0.931 (90.0)
0.903 (83.0)
0.943 (90.4)

Hebrew
0.046 (1.7)
0.028 (0.7)
0.533 (45.0)
0.834 (76.0)
0.743 (66.0)
0.899 (85.0)
0.884 (83.7)
0.921 (87.3)

Table 2: Results of an ablation study of the unsupervised method for three target languages
. Results for ACC are inside parentheses, and for MRR outside. When the learning algorithm is
used, the results after 20 rounds of constraint driven learning are reported. Note that using
linguistic constraints has a signiﬁcant impact in the English-Hebrew experiments. Our results
show that a small amount of constraints can go a long way, and better constraints lead to better
learning performance.

the temporal information is orthogonal to the transliteration model, our model
should similarly beneﬁt from incorporating the temporal information.
To evaluate our performance over the English-Hebrew dataset, we compare
our performance to the model presented in [Goldwasser & Roth 08a] a supervised discriminative model trained using 250 labeled examples. This model uses
the same feature extraction method as [Klementiev & Roth 06b], which does not
restrict the feature representation of the word pairs. The results show that a signiﬁcant improvement is obtained when using our model. Figure 3 describes the
learning curve of our model over the English-Hebrew dataset.
Unfortunately, we could not ﬁnd a published Chinese dataset. However, our
system achieved similar results to other systems, over a different dataset with
similar number of training examples. For example, [Sproat et al. 06] presents
a supervised system that achieves a MRR score of 0.89, when evaluated over a
dataset consisting of 400 English NE and 627 Chinese words. Our results for a
different dataset of similar size are reported in Table 2.

14 DAN GOLDWASSER, MING-WEI CHANG, YUANCHENG TU & DAN ROTH
4.4

Ablation Study

Our system combines several resources and exploits several different intuitions
about the transliteration domain. The resources used in our framework consist of a Romanization table and language speciﬁc transliteration constraints;
in addition our system encodes the dependency between feature activations as
general constraints, and it can make use of character unigrams features only, or
both character unigrams and bigrams features. To understand the impact of each
component we experimented with different combinations of these components,
resulting in different testing conﬁgurations. The results are presented in Table 2,
and explained below.
Romanization Table: We initialized the weight vector using a Romanization
table and did not use any constraints. To generate features we used a modiﬁed
version of our AF operator (see Sec. 3), which generates features by coupling
characters in close positions in the source and target words. This conﬁguration
is equivalent to the model used in [Klementiev & Roth 06b].
+General Constraints: This conﬁguration uses the Romanization table for initializing the weight vector and uses general transliteration constraints (see Sec. 3.2)
for feature extraction.
+All Constraints: This conﬁguration uses language speciﬁc constraints in addition to the general transliteration constraints to generate the feature representation. (see Sec. 3.3).
+Learning: Indicates that after initializing the weight vector, we update the
weight using Algorithm 1. In all of the experiments, we report the results after 20 training iterations.
Feature Representation: We evaluated our model using unigram and bigram
feature models. The Romanization table provides an initial model only for the
unigram features, bigram features weights are initially assigned a uniform 0
weight, and learned gradually. We considered three options- using just unigram
features, using bigram features in the initial model (denoted i. in table 2) or after
the unigram feature model converged (denoted c. in table 2)
Results Analysis The results are summarized in Table 2. Due to the size of
the Russian dataset, we used a subset consisting of 300 English NEs and their
matching Russian transliterations for the analysis presented here. After observing the results, we discovered the following regularities in our results for all three
languages.
Using the Romanization table directly without constraints results in very poor
performance, even after learning. This serves as an indication of the difﬁculty
of the transliteration problem and the difﬁculty earlier works faced when using
only Romanization tables. However, when used in conjunction with constraints,
results improve dramatically. For example, in the English-Chinese data set, we

CONSTRAINT DRIVEN TRANSLITERATION DISCOVERY

15

improve MRR from 0.02 to 0.746 and for the English-Russian data set we improve 0.03 to 0.8. Interestingly, the results for the English-Hebrew data set are
lower than for other languages - we achieve 0.53 MRR in this setting. We attribute the difference to the quality of the mapping in the Romanization table for
this language pair. Indeed, the weights learned after 20 training iterations improve the results to 0.83. This improvement is consistent across all languages,
after learning we are able to achieve a MRR score of 0.87 for the English-Chinese
data set and 0.91 for the English-Russian data set. These results show that Romanization table contains enough information to bootstrap the model when used
in conjunction with constraints.
Bootstrapping the weight vector using language speciﬁc constraints can further improve the results. They provide several advantages: a better starting point,
an improved learning rate and a better ﬁnal model. This is clear in all three languages, for example results for the Russian and Chinese bootstrapped models
improve by 5%, and by over 20% for Hebrew. After training the difference is
smaller: only 3% for the ﬁrst two and 6% for Hebrew.
Using bigram features increases the expressivity of the model, as it enables
the model to identify the context required to disambiguate character mapping decisions and captures phonetic patterns expressed using several characters. However using a more expressive model increases the difﬁculty of the learning problem. When working in an unsupervised setting, a Romanization table may not
provide a starting point that is strong enough to bootstrap the extended model.
Our experiments indeed show that performance degrades when the extended
model is bootstrapped using the Romanization table. However by allowing the
model to stabilize using only the unigram features we were able to provide the
unsupervised method with a better starting point, resulting in an improved overall
performance.
4.5

Supervised Settings

We also evaluated our system in a supervised setting over the English-Hebrew
data. We compare our model to a different discriminative system presented in
[Goldwasser & Roth 08a] evaluated over the same dataset. Both systems were
trained using 250 transliteration pairs, and trained using SNoW [Roth 98] implementation of the perceptron algorithm.
Our model converged after two iterations over the training data, and was then
applied to the testing data, consisting of 300 samples. The results summarized
in table 3 show a signiﬁcant improvement. Moreover, as can be observed in
Figure 4, our model can better use the training data provided - using as little
as 10 training examples the resulting model can outperform the baseline model
trained using 250 labeled examples. When provided with more data, results improve considerably, while the performance improvement of the baseline model

16 DAN GOLDWASSER, MING-WEI CHANG, YUANCHENG TU & DAN ROTH
Language
Heb. (MRR)

Sup. model
0.894

GR’08
0.51

Table 3: Applying our model in supervised settings, over the English-Hebrew data. Results
are compared to previously published system [Goldwasser & Roth 08a]. Both systems were
trained using 250 positive samples.

Fig. 4: Comparing our method (denoted as SCD in the graph) to [Goldwasser & Roth
08a] over the English-Hebrew data, using different training sets. Results show that using as
little as 10 labeled examples our method can outperform a system trained using 250 labeled
examples.

decreases as more training data is added. In Figure 5 we compare the supervised
and unsupervised versions of our framework over the English-Hebrew dataset.
Interestingly, the unsupervised system outperforms the supervised version of the
system. This can be explained by the fact that the unsupervised system uses the
testing data as training data, allowing it to better adapt to the speciﬁc classiﬁcation instances as it iterates over that data.
5 Conclusion
We introduce a constraint driven approach for named entity transliteration discovery. This approach identiﬁes the dependency between good representation
and successful classiﬁcation and iterates between the two stages. We describe
how to apply the model in both supervised and unsupervised settings, using
only a romanization table. In doing that we show that romanization tables are
a very useful resource for transliteration discovery if the proper constraints are
enforced. Even without using any labeled data, our model can outperform existing supervised models and weakly supervised models.

CONSTRAINT DRIVEN TRANSLITERATION DISCOVERY

17

Fig. 5: Comparison between supervised and unsupervised models, tested on
English-Hebrew NE pairs. We show the learning curves for Hebrew under two different
settings: unsupervised (denoted UCD) and supervised (denoted SCD). The model presented in
[Goldwasser & Roth 08a] (denoted GR08a) is also included here. Note that our unsupervised
model outperforms the supervised model, trained with 250 labeled examples. See the text for
more comparisons and details.

6 Acknowledgments
This work is partly supported by NSF grant SoD-HCER-0613885 and DARPA
funding under the Bootstrap Learning Program.
REFERENCES
[Bergsma & Kondrak 07] S. Bergsma and G. Kondrak. Alignment-based discriminative
string similarity. Proc. of the Annual Meeting of the Association of Computational
Linguistics (ACL), 656–663, Prague, Czech Republic, 2007. Association for Computational Linguistics.
[Chang et al. ] M. Chang, D. Goldwasser, D. Roth, and Y. Tu. Unsupervised constraint
driven learning for transliteration discovery. Proc. of the Annual Meeting of the
North American Association of Computational Linguistics (NAACL).
[Chang et al. 07] M. Chang, L. Ratinov, and D. Roth. Guiding semi-supervision with
constraint-driven learning. Proc. of the Annual Meeting of the Association of Computational Linguistics (ACL), 280–287, Prague, Czech Republic, 2007. Association
for Computational Linguistics.
[Chang et al. 08] M. Chang, L. Ratinov, N. Rizzolo, and D. Roth. Learning and inference
with constraints. Proc. of the National Conference on Artiﬁcial Intelligence (AAAI),
2008.
[Goldwasser & Roth 08a] D. Goldwasser and D. Roth.

Active sample selection for

18 DAN GOLDWASSER, MING-WEI CHANG, YUANCHENG TU & DAN ROTH
named entity transliteration. Proc. of the Annual Meeting of the Association of Computational Linguistics (ACL), 2008.
[Goldwasser & Roth 08b] D. Goldwasser and D. Roth. Transliteration as constrained
optimization. Proc. of the Conference on Empirical Methods for Natural Language
Processing (EMNLP), 353–362, 2008.
[Haghighi & Klein 06] A. Haghighi and D. Klein. Prototype-driven learning for sequence models. Proc. of the Annual Meeting of the North American Association
of Computational Linguistics (NAACL), 2006.
[Hermjakob et al. 08] U. Hermjakob, K. Knight, and H. Daum´ III. Name translation in
e
statistical machine translation - learning when to transliterate. Proc. of the Annual
Meeting of the Association of Computational Linguistics (ACL), 389–397, Columbus, Ohio, 2008.
[Hsieh et al. 08] Cho-Jui Hsieh, Kai-Wei Chang, Chih-Jen Lin, S. Sathiya Keerthi, and
S. Sundararajan. A dual coordinate descent method for large-scale linear svm.
ICML ’08: Proceedings of the 25th international conference on Machine learning,
408–415, New York, NY, USA, 2008. ACM.
[Jung et al. 00] S. Jung, S. Hong, and E. Paek. An english to korean transliteration model
of extended markov window. Proc. the International Conference on Computational
Linguistics (COLING), 383–389, 2000.
[Klementiev & Roth 06a] A. Klementiev and D. Roth. Named entity transliteration and
discovery from multilingual comparable corpora. Proc. of the Annual Meeting of the
North American Association of Computational Linguistics (NAACL), 82–88, 2006.
[Klementiev & Roth 06b] A. Klementiev and D. Roth. Weakly supervised named entity transliteration and discovery from multilingual comparable corpora. Proc.
of the Annual Meeting of the Association of Computational Linguistics (ACL),
USS,TL,ADAPT, 2006.
[Knight & Graehl 98] K. Knight and J. Graehl. Machine transliteration. Computational
Linguistics, 599–612, 1998.
[Li et al. 04] H. Li, M. Zhang, and J. Su. A joint source-channel model for machine
transliteration. Proc. of the Annual Meeting of the Association of Computational
Linguistics (ACL), 159–166, Barcelona, Spain, 2004.
[Meng et al. 01] H. Meng, W. Lo, B. Chen, and K. Tang. Generating phonetic cognates
to handle named entities in english-chinese cross-langauge spoken document retreival. Proceedings of the Automatic Speech Recognition and Understanding Workshop, 389–397, 2001.
[Riedel & Clarke 06] S. Riedel and J. Clarke. Incremental integer linear programming
for non-projective dependency parsing. Proc. of the Conference on Empirical Methods for Natural Language Processing (EMNLP), 129–137, Sydney, Australia, 2006.

CONSTRAINT DRIVEN TRANSLITERATION DISCOVERY

19

[Roth & Yih 04] D. Roth and W. Yih. A linear programming formulation for global inference in natural language tasks. 1–8. Association for Computational Linguistics,
2004.
[Roth & Yih 07] D. Roth and W. Yih. Global inference for entity and relation identiﬁcation via a linear programming formulation. In Lise Getoor and Ben Taskar (eds),
Introduction to Statistical Relational Learning. MIT Press, 2007.
[Roth 98] D. Roth. Learning to resolve natural language ambiguities: A uniﬁed approach. Proceedings of the National Conference on Artiﬁcial Intelligence (AAAI),
806–813, 1998.
[Sproat et al. 06] R. Sproat, T. Tao, and C. Zhai. Named entity transliteration with comparable corpora. Proc. of the Annual Meeting of the Association of Computational
Linguistics (ACL), 73–80, Sydney, Australia, 2006.
[Tao et al. 06] T. Tao, S. Yoon, A. Fister, R. Sproat, and C. Zhai. Unsupervised named
entitly transliteration using temporal and phonetic correlation. Proc. of the Conference on Empirical Methods for Natural Language Processing (EMNLP), 250–257,
2006.
[Yoon et al. 07] S. Yoon, K. Kim, and R. Sproat. Multilingual transliteration using feature based phonetic method. Proc. of the Annual Meeting of the Association of
Computational Linguistics (ACL), 112–119, Prague, Czech Republic, 2007.

