Unsupervised Discovery of Opposing Opinion Networks
From Forum Discussions
Yue Lu

Hongning Wang, ChengXiang Zhai, Dan Roth

Twitter Inc.
1355 Market St. Suit 900
San Francisco, CA 94103

Department of Computer Science
University of Illinois at Urbana-Champaign
Urbana, IL 61820

yuelu@twitter.com

{wang296, czhai, danr}@illinois.edu

ABSTRACT
With more and more people freely express opinions as well as actively interact with each other in discussion threads, online forums
are becoming a gold mine with rich information about people’s
opinions and social behaviors. In this paper, we study an interesting new problem of automatically discovering opposing opinion networks of users from forum discussions, which are subset of
users who are strongly against each other on some topic. Toward
this goal, we propose to use signals from both textual content (e.g.,
who says what) and social interactions (e.g., who talks to whom)
which are both abundant in online forums. We also design an optimization formulation to combine all the signals in an unsupervised
way. We created a data set by manually annotating forum data on
ﬁve controversial topics and our experimental results show that the
proposed optimization method outperforms several baselines and
existing approaches, demonstrating the power of combining both
text analysis and social network analysis in analyzing and generating the opposing opinion networks.

Categories and Subject Descriptors
H.3.m [Information Storage and Retrieval]: Miscellaneous; I.2.6
[Artiﬁcial Intelligence]: Learning

General Terms
Algorithms, Experimentation

Keywords
opinion analysis, social network analysis, optimization, online forums, linear programming

1. INTRODUCTION
Online forum is one of the early applications managing and promoting user generated content. Although being simple in its design – users carry out discussion in the form of message threads,
forums remain prevalent and popular even during the recent rise
of many sophisticated Web 2.0 applications. As users actively express their opinions and exchange their knowledge on all kinds of
topics/issues, e.g., technology, sports, religion, and politics, forums
are becoming a great source for opinion mining. However, the simple design of forums combined with rapidly accumulated data make
it challenging to make sense out of the forum discussions.

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
CIKM’12, October 29–November 2, 2012, Maui, HI, USA.
Copyright 2012 ACM 978-1-4503-1156-4/12/10 ...$10.00.

Supporting Group

Against Group

It's a form of
population
control.
Abortions
will never
be illegal

No criminal
punishment for a
woman who gets
an abortion.

Woman who
gets an
abortion
should get
life in prison.

Figure 1: Example Opposing Opinion Network for the Thread
on “Abortion”
In this paper, we study an interesting new problem of automatically discovering opposing opinion networks from forum discussions, which are deﬁned as latent user groups with strong opposing
opinions on different topics, i.e., a supporting group and an against
group. There is an example illustration in Figure 1 about opposing
user groups on the topic “Abortion”. We can see that such discovered opinion networks can serve as a concise and interesting summary of the topics and users in the forum discussions. They can
also provide a sense of “virtual community” for the online users, to
help them engage in the forum activities more easily. Once we have
identiﬁed the latent opposing opinion networks, they can enable a
number of interesting applications that add social components to
forums. For example, we can detect semantically similar topics
which involve similar groups of opposing users. We can also ﬁnd
users of similar minds who often agree with each other across different topics or “enemy” users who are often against each other
across different topics.
Discovering the opposing opinion networks is related to some
existing work on opinion mining, which we will review in detail in
Section 6. In short, our work is distinguished from existing work
because we exploit the unique characteristics of forum data in an
unsupervised way: combining signals from both textual content
(e.g. who said what) and social interactions (e.g. who talks to
whom). More speciﬁcally, from the textual content analysis perspective, we propose two kinds of analysis: (1) topic model analysis of aspect mentions in post text and (2) bootstrapping-based
classiﬁcation of agree and disagree relations between posts. From
the social network analysis perspective, we form two assumptions:
(1) user consistency across different posts in the same thread and
(2) user-user relation consistency in the same thread. Finally, to
consolidate all the signals together in a uniﬁed way, we design an

optimization formulation which can be formulated and solved as a
linear programming problem.
There is no existing public data set to evaluate opposing opinion networks, because we need both the textual content and the
social iterations among users. So, we created a new data set of ﬁve
controversial topics in forum discussion, and showed that the proposed optimization method outperforms several baselines and existing approaches, including a SentiWordNet method, a clustering
based method and a Max-Cut method .

2. PROBLEM FORMULATION
More formally, a forum F can be considered as a set of discussion threads, i.e. F = {T H1 , T H2 , · · · }, where each thread
T H = (T, P, M, R) ∈ F is a tuple of (1) a topic or issue T ; (2) a
sequence of posts P = {d1 , d2 , · · · , dn }, where each di is a post
with textual content; (3) a authorship matrix M , where Mji = 1
if post di was written by uj ; and (4) a partial reply-to relation between posts R ⊂ P × P , where Rij = 1 if di replies to dj .
Assuming that the issue T is given for each thread, which can be
obtained from the forum (e.g. “Gay Rights” sub-forum) or can be
retrieved using IR methods (e.g. using “Gay Rights” as keywords
to retrieve all relevant threads), we aim at automatically discovery
of opposing opinion network for each issue:
Deﬁnition (Opposing Opinion Network): is a multi-graph (U, E)
among forum users U = {u1 , u2 , · · · , um } and each edge
(ui , uj , t, at ) ∈ E carries an agreement weight at ∈ [−1, 1]
ij
ij
conditioned on an issue T . And U = U + ∪ U − ∪ U 0 where we
are only interested in the supporting/against group U + and U − .

It is not trivial to identify each user’s opinion for any given issue
directly, especially in an unsupervised way, because: (1) treating
each user as a single document would lose the rich information at
the local post-level (e.g. single post content, reply-to relation); (2)
forum users sometimes explicitly express their opinions toward an
issue using sentiment words, sometimes not; (3) sometimes users
interact and argue with each other, so that expressing their attitudes
in an implicit way. Thus, to be most effective in understanding
user’s opinions, we need to consider the rich information around
posts, including users direct opinions toward the issue and indirect
attitudes toward other users.
To this end, we propose to decompose the problem: identify each
user uj ’s opinion oj in a given thread by aggregating her opinions at the post level, v(di ) ∈ [−1, 1] (or vi for short), i.e, oj =
Then, given a threshold t ∈ [0, 1], we can get a supportvT Mj
i=1 Mji

ing user group and an against user group U + = {uj | ∑n
−

vT Mj
{uj | ∑n Mji
i=1

representative unsupervised approach is to use SentiWordNet1 to
score the text. Besides the sentiments, the choice of words can be
indicative of the opinions too. For example on the “Abortion” issue:
a user who posted about “child” and “life” is more likely from an
against group, while someone speaking about “rape” and “choice”
is more likely from a supporting group.
The Quotation Part (i.e., Reply-to): This is an optional part of a
post and is usually visualized using different font or color in online forums. In this kind of interaction format, the authors usually
directly express their attitude toward the quoted post/user in the
ﬁrst sentence of the reply. It is a very strong indicator the relation
between users, if we can automatically classify such sentence as
showing users agreement/disagreement, e.g., “totally!” is agreement or “it doesn’t make sense to me.” shows disagreement. As
a result, the quotation part provides us a valuable source to understand the interaction between users.

4.2

Analysis of Agree/Disagree Relations between Posts

We discuss, for each forum thread, how we infer relations between posts as agreement/disagreement, based on multiple signals.

3. METHOD OVERVIEW

vT Mj
∑n
.
i=1 Mji

Figure 2: Illustration of a Forum Post

>

< −t}. Now, the problem is reduced
t} and U =
to identifying opinions in each post, i.e., assigning an opinion score
in [−1, 1] to each post di as the degree of support or against toward
the given issue. This reduction allows us to naturally incorporate a
richer set of information when inferring user opinion in each post.

Using “Reply-to” and “User Relation Consistency” With no labeled training data, it is difﬁcult to classify a reply-to text as agreement or disagreement. Because it is impossible to list all the patterns beforehand, i.e., various ways to express users attitude. Thus,
we design a bootstrapping method to do classiﬁcation.
We ﬁrst extract all the ﬁrst sentence in quotation text, from the
whole forum of more than 1 million posts, and then label these sentences with only a handful of agreement/disagreement patterns (13
in total), such as “I agree” and “I disagree”. After that, we bootstrap other patterns with the help of “user relation consistency”:
suppose we observe one post from ui replies to a post from uj and
matches the initial “agree” pattern; then we assume that all other
reply-to sentences between ui and uj in the same thread discussion also follow the “agree” attitude. In this way, we can extract
all the “agree” sentences P agr and “disagree” sentences P dis from
the whole forum. Essentially, we rely on the users themselves to
obtain different ways of expressing agreement or disagreement.
Now, given a new reply-to sentence tij (indicating that post di
replies to post dj ), we can just compare the similarity Sim(tij , P agr )
v.s. Sim(tij , P dis ). Here, Sim(x, y) is the max cosine similarity
between a text and a set of text. We can now mark some of the
reply-to relations in R as agree Ragr ⊆ R or disagree Rdis ⊆ R,
using the following equations:
agr
Ri,j =

We will brieﬂy introduce what a forum post (illustrated in Figure2)
is consist of and follow up with more details in the next section.
The Statement Part: A mandatory part of the post, where forum
users express their opinions in their own words. Most of previous
opinion analysis work only focuses on this part. For example, a

Sim(tij , P agr ),

if

Sim(tij , P agr )
≥α
Sim(tij , P dis )

dis
Ri,j =

4. IDENTIFY OPINIONS IN POSTS
4.1 Analysis of Textual Content in Posts

Sim(tij , P dis ),

if

Sim(tij , P agr )
1
≤
Sim(tij , P dis )
α

Using “User Consistency”: Assuming that most users are logically reasonable, in one single thread, different posts from the same
user tend to express consistent opinion. More speciﬁcally, suppose
1

http://sentiwordnet.isti.cnr.it/

Topics
abortion
healthcare reform
illegal immigrants
iraq war
president obama

we know one post from ui show strong support for a given issue;
then all the other posts written by ui in this thread would be likely
to follow this support opinion. Following this assumption, we can
encode it as a matrix A ⊆ P × P from a given thread, indicating
agreement relation among posts written by the same author, where
Ai,j = 1 iff di and dj are posts from the same user, i.e.,
Ai,j = 1,

if user(di ) = user(dj )

Using “Framing”: It has been found that users with different sentiments/positions would focus on different aspects of the topic, which
is called “framing” [18]. For example, on the abortion issue, prochoice people would emphasize women’s rights and freedom while
pro-life people would focus on the crude process of abortion. Apparently, these two opposing user groups tend to share similar mentions of aspects within the group and different mentions between
groups. To capture this, we ﬁrst employ a topic modeling method
[8] to extract the hidden aspects of discussion, so that we get a
number of K aspect models p(w|θ) for each thread and an aspect
distribution p(θ|d) for each post in this thread. Then, given any two
posts from the same thread, if the two corresponding aspect distributions have high positive correlation, their opinions tend to agree;
otherwise, their opinions tend to disagree. Denoting corr(di , dj ) =
correlation(p(θ|di ), p(θ|dj )) as the Pearson correlation coefﬁcients, we can have another measure of post-post relations as agreement T agr ⊆ P × P or disagreement T dis ⊆ P × P , using the
following equations:
agr
Ti,j =
dis
Ti,j

=

corr(di , dj ),

minimize

n
∑

|vi − si |

i=1

Capturing Agreement: With three constructed matrices A, Ragr
and T agr to encode the signals indicating agreement relation between posts, we are giving a linear penalty if the two opinion scores
differ a lot, if we believe the two corresponding posts should agree
with each other:
n
n
∑ ∑
agr
agr
minimize
(Ri,j + Ti,j + Ai,j )|vi − vj |
i=1 j=i+1

Capturing Disagreement: We have constructed matrices Rdis
and T dis to encode the signals indicating disagreement relation between posts. To capture such disagreement, we ﬁrst separate the
representation of “sign” and “absolute value” in each opinion score
+
−
vi by introducing two non-negative variables vi , vi and a con+
−
straint vi = vi − vi . In order to ensure that no more than one
+
−
of vi , vi is positive (the other being zero), we also need to min+
−
imize (vi + vi ). In this way: vi being positive is equivalent to


n
n
∑ ∑

minimize

+
(|vi

−

dis
dis
[(Ri,j + Ti,j )×



−
vj |

i=1 j=i+1

+

−
|vi

−

+
vj |)]

}
n
∑
+
−
+µ
(vi + vi )
i=1

subject to ∀i ∈ {1, · · · , n}, vi =

+
vi

−

−
vi

+
−
and vi , vi ≥ 0.

Full Objective Function: Putting every term together, we have
the following objective function:
{
v

= argminv

µ

n
n
∑
∑
−
+
(vi + vi ) + λsenti
|vi − si |
i=1

4.3 Optimization Formulation

Capturing Sentiment Priors: Using the following term, we ensure that our opinion assignment does not deviate too much from
the sentiment tagging especially when the sentiment score is high.

# ReplyTo
27
29.2
24.6
26.4
24.8

+
−
+
vi = vi and vi = 0; vi being negative is equivalent to vi = 0
−
+
−
and vi = −vi ; vi being zero is equivalent to vi = vi = 0.
If there is an entry (i, j) active in Rdis or T dis , we want to make
the two corresponding opinion scores vi and vj have opposite sign
but similar absolute value, by the following terms and constraints:

if corr(di , dj ) < −β

We have introduced and analyzed different signals that can indicate the opinions in forum posts, but it is still not clear how we
can combine multiple signals. One way to combine these signals
is to use the agree/disagree information as distance measures between posts and then apply clustering-like methods, e.g. MaxCut
as in [11]. However, (1) the clustering or partition results cannot
tell which group is supporting and which is against; (2) a hard partition does not distinguish users with strong support/against opinions
from those with a balanced view. Instead, we propose an optimization formulation that tries to ﬁnd opinion assignment to each post
vi to capture the different signals introduced before.

# Posts/Thread
59.4
64.6
61.4
64.8
61.8

Table 1: Basic Statistics of Data Sets

if corr(di , dj ) > β

−corr(di , dj ),

# Posts/User
3.19
3.85
2.94
3.31
3.22

+

λagr

n
n
∑ ∑

i=1

agr
agr
(Ri,j + Ti,j + Ai,j )|vi − vj |

i=1 j=i+1

+

λdis

n
n
∑ ∑
i=1 j=i+1



+
−
−
+
dis
dis
(Ri,j + Ti,j )(|vi − vj | + |vi − vj |)


subject to
∀i ∈ {1, · · · , n},

−1 ≤ vi ≤ 1

∀i ∈ {1, · · · , n},

+
−
vi = vi − vi

∀i ∈ {1, · · · , n},

+
−
vi , vi ≥ 0

where λs and µ are the weights to trade off different components;
A, Ragr , Rdis , T agr and T dis matrices are obtained as described
in Section 4.2, while v, v+ and v− are the variables. With this
formulation, we can use standard techniques to transform it to a
Linear Programming problem and solve it efﬁciently.

5. EXPERIMENTS
5.1 Data sets and Human Annotation
We created our own data sets from an online military forum2 . We
crawled 43,483 threads of discussions, containing 1,343,427 posts,
from “Hot Topics & Current Events”. In order to make it easier
for the human judges to annotate, we further narrowed down to ﬁve
popular and controversial topics , and applied information retrieval
method to retrieve the top ﬁve most relevant threads for each topic.
The basic statistics are in Table 1.
We ﬁrst asked our colleagues to label each post as “Support”,
“Against” or “Not Sure” about the given topic. In total, we have
collected 230 posts with agreed labels, where 26% as “Support”,
41% as “Against” and 33% as “Not Sure”. In particular, the true
disagreement rate among the judgments (i.e., “Support” v.s. “Against”)
is low: only 12.31%, showing that the task is designed reasonably. Then, we further utilized the crowd sourcing service through
2

forums.military.com

Method
MaxCut
R+T +A
R
T
A

CrowdFlower3 to get more labels on all 1584 posts. For better
quality control, we further required each post to be labeled by at
least three annotators. The annotation results basically followed the
statistics of the ﬁrst round of annotation: 30% as “Support”, 43%
as “Against” and 26% as “Not Sure”. Also, according to CrowdFlower’s statistic, the agreement among the annotators is 0.7584.
We will use the CrowdFlower annotation4 as our ground truth.

5.2 Methods for Comparison
LP: Our proposed method, solved using PyGLPK toolkit5 . All experiments are performed when setting K the number of topics to
be 5, and the threshold α and β to 2.
UserClustering: We build similarity graph among users from cosine similarity of their bag-of-words representation and apply graph
partition based clustering of two groups of users (using CLUTO6 ).
SentiWordNet: We tag each post by taking the average the SentiWordNet opinion score of words, then each user’s opinion is the
averaged opinion score of all her posts. It represents an unsupervised sentiment analysis method which only relies on the text.
MaxCut: The method proposed in [11] is solving a similar problem. They ﬁrst classify each reply-to relation as agree/disagree/neutral
using a pattern dictionary; then user-user relation is deﬁned as a linear combination of their post-post relations (positive if disagree and
negative if agree); ﬁnally, a MaxCut algorithm is performed on this
user-user graph to generate the user groups partition. Since we do
not have their algorithm implementation or their pattern dictionary,
we use SentiWordNet and the same set of patterns from our method
as the ﬁrst step classiﬁer. Then we use a semi-deﬁnite non-convex
programming solver 7 to sovle the MaxCut problem.

5.3 Evaluation of Agree/Disagree Classiﬁcation
We ﬁrst evaluate the accuracy of the local classiﬁcation of relations between posts. We derive the ground truth of post-post relations from post level judgment, and ignore the cases when either
post is annotated as “Not Sure”. Note that, instead of aim at inferring the relations between every pair of posts, we only need a
subset of high-conﬁdence pair-wise relations, because we will only
use them to better predict the point-wise labels.
In Table 2, we compare the results8 from the ﬁrst step of MaxCut
and our methods of extracting matrices R (derived from reply-to),
T (derived from topic modeling), and A (derived from user consistency assumption). Top part of the table shows that our method
outperform the MaxCut method in all metrics. In particular, the
rule-based classiﬁer in MaxCut cannot handle different vocabulary
and possibly slangs in the forums, resulting in very low recall. Our
classiﬁer is learned by exploiting the forum data itself, including
both textual analysis and social network analysis.
To further understand each signal in our method, we evaluate
them individually in the bottom part of the table. (1) The A matrix
3

www.crowdflower.com
data and annotation available at http://sifaka.cs.uiuc.
edu/~yuelu2/forumdata/
5
http://tfinley.net/software/pyglpk/
6
http://glaros.dtc.umn.edu/gkhome/cluto/
cluto/overview
7
http://www.stanford.edu/~yyye/Col.html
8
Note that the absolute numbers of recall (thus also F1 measure)
in are generally very low, but this does not mean the algorithms are
performing poorly. Since we created some synthetic agree/disagree
pairs to test the classiﬁcation results, N point-wise labels result in
N ∗ N pair-wise ground truth of agree/disagree samples, which is
a large number for the denominator of recall calculation (most of
them we will not encounter in real data).
4

Precision
0.4732
0.6010
0.5582
0.5632
0.6791

Recall
0.0090
0.1942
0.0036
0.1134
0.0900

F1 Measure
0.0177
0.2936
0.0071
0.1888
0.1589

Table 2: Accuracy of Agree/Disagree Classiﬁcation
performs the best in precision, indicating that author consistency
assumption is most accurate in identifying post-post relation. (2)
The R matrix is sparse as indicated by the lowest recall, because it
relies on explicit reply-to relations, which is only a small subset as
in the forum data. In comparison, A matrix relies on multiple posts
written by the same user, which is often the case, thus providing
higher recall. The T matrix only depends on the post content, thus
applies to all posts, generating the highest recall. (3) Finally, the recall of R + T + A is almost the same as the sum of that of the three
matrices, suggesting that these three matrices provide complementary information. Thus, by combining them together, we can get the
most comprehensive information about post-post agree/disagree.
Now that we have shown that our methods discover more accurate relations between posts, in the next set of evaluation we will
further test the performance of using these post-post relations to
discover opposing opinion network.

5.4

Evaluation of Opposing Opinion Network

Since we only care about the strongly opposing users, we only
take as ground truth: users with at least two posts and aggregated
ground-truth opinion score |s| > 0.5. This results in 57 supporting
users and 78 against users for the selected 5 topics. Note that the
ground truth is only a subset of users that we have conﬁdent human
labels, so we can only evaluate the Accuracy on this subset. We
also evaluate Mean Squared Error (MSE).
In the top half of Table 3, we compare three baselines with our
Linear Programming (LP) method. We use a threshold t = 0 to
decide the supporting/against group for SentiWordNet and LP. We
can see that
• UserClustering is not performing well, which shows that treating
each user as a big document and ignoring the relations among
posts is not effective.
• MaxCut (the original implementation as in [11]) is not performing well either, suggesting that only considering agree/disagree
relations among users is not effective enough. In particular, the
support and against groups are very unbalanced, which is because
of the sensitivity of partition based methods.
• SentiWordNet (t=0) provides better accuracy than the previous
two baselines, suggesting that the sentiment score of the text the
users posted is a reasonable way to infer their opinion group.
• Our LP (t=0) method clearly outperforms all three baselines in
all measures. This is because we include both “who said what”
(through the sentiment term) and “who talks to whom” (through
the agree/disagree terms) in the objective function to understand
the user’s opinions in a more comprehensive way. Since we use
SentiWordNet as one term in the objective function, this shows
that the other terms capturing agreement/disagreement further
help adjusting the opinion scores more accurately.
We further conduct more analysis experiments in order to (1) allow more fair comparison with baselines, and (2) better understand
the performance gain of our LP method over existing baselines.
The results are summarized in the second half of Table 3.
• MaxCut+: We ﬁrst try to improve MaxCut by using our full set
of heuristics as input in addition to the sentiment heuristic in the

Method
UserClustering
MaxCut (original)
SentiWordNet (t=0)
LP (t=0)
MaxCut+
LP*
SentiWordNet*

Accuracy
(For)
0.6250
0.6071
0.6250
0.6429
0.0714
0.5893
0.6964

Accuracy
(Against)
0.4615
0.4487
0.5250
0.5513
0.8590
0.5513
0.4103

Accuracy
(For+Against)
0.5299
0.5149
0.5670
0.5896
0.5299
0.5672
0.5299

MSE

7.

0.4535
0.4528
0.4470
0.4103
0.4444
0.4149
0.4631

In this paper, we study an interesting new problem of discovering opposing opinion networks from forum discussions, which are
essentially a type of latent social networks based on user opinions:
the strongly opposing user groups. We propose a method to discover them in an unsupervised way using signals from both textual
content and social interactions. We also created a manually annotated forum data set on ﬁve controversial topics, and demonstrated
that the proposed optimization method outperforms several baselines and existing approaches.
Our work is the ﬁrst step in a novel text mining direction where
the focus is on analyzing latent user behavior and social structure
behind text. The opposing opinion network we studied is only
a simple form of general opinion networks, which may include
multiple user opinion groups. For example, we may further infer
more speciﬁc relations among users from the opinion network, e.g.
friends, enemies, followers, etc.

Table 3: Accuracy of User Opinion Prediction
original paper. However, since we introduce many agreement
heuristics which get translated into negative edge weights, MaxCut will produce even more unbalanced output, so as to maximize
the weight sum of cut edges. It is clear that MaxCut (with full
heuristics) shows large difference between the accuracy in two
groups. It suggests that the improvement gain of our LP method
comes from not only the useful heuristics but also the effective
optimization formulation.
• LP* and SentiWordNet*: In order to account for the possible
bias of unbalanced partition in MaxCut, we further evaluate SentiWordNet and LP using the same partition size as MaxCut. More
speciﬁcally, we ﬁrst rank the users by the opinion scores output
by SentiWordNet or LP, and then partition into user groups is
done so that the ratio of supporting users v.s. against users is the
same as that from MaxCut output. As can be seen in the table,
both methods still outperform MaxCut, showing that the output
opinion scores are not only more ﬂexible than a hard partition but
also more accurate.
In addition, we tried varying the weights in the objective function 4.3, and improved prediction accuracy over other methods are
consistent as long as we leave sufﬁcient weight on sentiment scores
λsenti (i.e., at least twice as much as λagr and λdis ) while maintaining a small µ, e.g., 0.01. We will leave automatic weight learning as future work.

6. RELATED WORK
Discovering opposing user networks is a new problem different
from existing work, but there are also important connections. First,
opinion analysis in text data has received extensive attention these
years [10, 4, 13, 9, 5, 12, 7], but these methods focus on predicting
a sentiment class/rating of the text individually, so we cannot take
the rich social interaction context into consideration. In the experiments, we included a baseline using SentiWordNet that represents
an unsupervised document level sentiment analysis work.
Related work (e.g. [17, 2, 16, 19, 3]) on identifying user opinions (or stands, views) is usually deﬁned as a classiﬁcation problem
of the users into predeﬁned groups. Instead, we are only interested
in a subset of users who are strongly against each other. Nevertheless, this line of related work is highly relevant. However, these
previous work either use the textual content without much consideration of social interactions among users (e.g. [6, 15, 14]) or use
the social link information without using much of the content information (e.g. [1, 11]). Instead, our work has shown that the combination of content analysis and social network analysis is most
powerful for inferring user opinions.
Some of the work (e.g., [16, 2]) have also explored the relations
among users. In comparison to those supervised methods, we study
the user opinions in the more difﬁcult unsupervised setting, and we
propose an effective optimization formulation to combine signals
from unsupervised content analysis and social network analysis.
Moreover, our unsupervised approach can be easily incorporated
into any supervised learning as a way to provide intelligent features.

8.

CONCLUSIONS

REFERENCES

[1] R. Agrawal, S. Rajagopalan, R. Srikant, and Y. Xu. Mining
newsgroups using networks arising from social behavior. In WWW
’03, pages 529–535. ACM.
[2] M. Bansal, C. Cardie, and L. Lee. The power of negative thinking:
Exploiting label disagreement in the min-cut classiﬁcation
framework. COLING’08: Companion volume: Posters, pages 13–16.
[3] C. Burfoot, S. Bird, and T. Baldwin. Collective classiﬁcation of
congressional ﬂoor-debate transcripts. In ACL’11: Human Language
Technologies, pages 1506–1515.
[4] H. Cui, V. Mittal, and M. Datar. Comparative experiments on
sentiment classiﬁcation for online product reviews. In Proceedings of
the National Conference on Artiﬁcial Intelligence, page 1265, 2006.
[5] K. Dave, S. Lawrence, and D. Pennock. Mining the peanut gallery:
opinion extraction and semantic classiﬁcation of product reviews. In
WWW’03, pages 519–528. ACM.
[6] M. Galley, K. McKeown, J. Hirschberg, and E. Shriberg. Identifying
agreement and disagreement in conversational speech: use of
bayesian networks to model pragmatic dependencies. In ACL ’04.
[7] A. Goldberg and X. Zhu. Seeing stars when there aren’t many stars:
graph-based semi-supervised learning for sentiment categorization.
In TextGraphs-1, pages 45–52. ACL, 2006.
[8] T. Hofmann. Probabilistic latent semantic indexing. In SIGIR’99.
[9] S. Kim and E. Hovy. Determining the sentiment of opinions. In
COLLING’04, pages 1367–1373. ACL, 2004.
[10] B. Liu. Sentiment analysis and subjectivity. Handbook of Natural
Language Processing Second Edition, pages 1–38, 2010.
[11] A. Murakami and R. Raymond. Support or oppose?: classifying
positions in online debates from reply activities and opinion
expressions. In COLING ’10: Posters, pages 869–875.
[12] B. Pang and L. Lee. Seeing stars: Exploiting class relationships for
sentiment categorization with respect to rating scales. In ACL’05,
pages 115–124. Association for Computational Linguistics.
[13] B. Pang, L. Lee, and S. Vaithyanathan. Thumbs up?: sentiment
classiﬁcation using machine learning techniques. In ACL’02, pages
79–86. Association for Computational Linguistics, 2002.
[14] S. Park, K. S. Lee, and J. Song. Contrasting opposing views of news
articles on contentious issues. In ACL’11, pages 340–349, 2011.
[15] S. Somasundaran and J. Wiebe. Recognizing stances in online
debates. In ACL/AFNLP’09, pages 226–234.
[16] S. Somasundaran and J. Wiebe. Recognizing stances in ideological
on-line debates. In CAAGET’10, NAACL-HLT’10, pages 116–124.
Association for Computational Linguistics, 2010.
[17] M. Thomas, B. Pang, and L. Lee. Get out the vote: Determining
support or opposition from Congressional ﬂoor-debate transcripts. In
Proceedings of EMNLP, pages 327–335, 2006.
[18] D. Tversky, Amos; Kahneman. The framing of decisions and the
psychology of choice. pages 453–458, 1981.
[19] A. Yessenalina, Y. Yue, and C. Cardie. Multi-level structured models
for document-level sentiment classiﬁcation. In EMNLP’10, pages
1046–1056. Association for Computational Linguistics.

