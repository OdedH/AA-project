A Game-Theoretic Approach to
Generating Spatial Descriptions
Dave Golland

Percy Liang

Dan Klein

Language is about Communication
Goal: refer to O1
O2
O1

O3

[Grice, 1975]
2

Language is about Communication
Goal: refer to O1
Strategy 1: speak the truth
(Maxim of Quality)

O2
O1

O3

[Grice, 1975]
2

Language is about Communication
Goal: refer to O1
Strategy 1: speak the truth
(Maxim of Quality)
on O3

O2
O1

O3

[Grice, 1975]
2

Language is about Communication
Goal: refer to O1
Strategy 1: speak the truth
(Maxim of Quality)
on O3

right of O2

O2
O1

O3

[Grice, 1975]
2

Language is about Communication
Goal: refer to O1
Strategy 1: speak the truth
(Maxim of Quality)
on O3

right of O2

Problem: ambiguity

O2
O1

O3

[Grice, 1975]
2

Language is about Communication
Goal: refer to O1
Strategy 1: speak the truth
(Maxim of Quality)
on O3

right of O2

Problem: ambiguity

O2
O1

O3

Strategy 2: also be unambiguous
(Maxims of Quality and Manner)

[Grice, 1975]
2

Language is about Communication
Goal: refer to O1
Strategy 1: speak the truth
(Maxim of Quality)
on O3

right of O2

Problem: ambiguity

O2
O1

O3

Strategy 2: also be unambiguous
(Maxims of Quality and Manner)
on O3
[Grice, 1975]
2

Actual Example

3

Language Game
speaker

4

Language Game
speaker

listener

4

Language Game
speaker

listener

O2
O1

O3

4

Language Game
target

o

speaker

listener

O2
O1

O3

4

Language Game
target

o

speaker

utterance

listener

w

right of O2
O2
O1

O3

4

Language Game
target

o

speaker

utterance

listener

w

ps (w | o)

right of O2
O2
O1

O3

4

Language Game
target

o

speaker

utterance

listener

w

ps (w | o)

right of O2
O2
O1

O3

4

Language Game
target

o

speaker

utterance

listener

w

ps (w | o)

guess

g

right of O2
O2
O1

O3

4

Language Game
target

o

speaker

utterance

listener

w

ps (w | o)

pl (g | w)

guess

g

right of O2
O2
O1

O3

4

Language Game
target

o

speaker
ps (w | o)

utterance

w

listener

guess

pl (g | w)

g

U
utility

U(o, g) = I[o = g] =

1 if o = g
0 otherwise

4

Language Game
target

o

speaker
ps (w | o)

utterance

w

listener

guess

pl (g | w)

g

U
utility

U(o, g) = I[o = g] =

1 if o = g
0 otherwise

EU(s, l) = Es,l [U (o, g)]

4

Speaker Strategies
Assign scores to utterances via:
ps (w|o)

5

Speaker Strategies
Assign scores to utterances via:
ps (w|o)
Two speaker strategies:
1. semantics only

(Maxim of Quality)

5

Speaker Strategies
Assign scores to utterances via:
ps (w|o)
Two speaker strategies:
1. semantics only
2.

semantics + pragmatics

(Maxim of Quality)
(Maxims of Quality + Manner)

5

Semantics Only
Game tree:
right of O2
O1
on O3

6

Semantics Only
Game tree:
right of O2
O1
on O3

ps (w|o) depends only on truth of utterance, does not need to take
listener into account

6

Semantics Only
Game tree:
right of O2
O1
on O3

ps (w|o) depends only on truth of utterance, does not need to take
listener into account
Reﬂex speaker because it does not consider consequence of actions.

6

Semantics + Pragmatics
Maximize wrt. ps (w|o):
EU(s, l) = Es,l [U (o, g)]

7

Semantics + Pragmatics
Maximize wrt. ps (w|o):
EU(s, l) = Es,l [U (o, g)]
ps (w|o) determinstically says:
argmaxw pl (o | w )

7

Semantics + Pragmatics
Maximize wrt. ps (w|o):
EU(s, l) = Es,l [U (o, g)]
ps (w|o) determinstically says:
argmaxw pl (o | w )
Needs embedded model of listener: pl (o|w)

7

Semantics + Pragmatics
Maximize wrt. ps (w|o):
EU(s, l) = Es,l [U (o, g)]
ps (w|o) determinstically says:
argmaxw pl (o | w )
Needs embedded model of listener: pl (o|w)
right
of O2
O1
on O3

7

Semantics + Pragmatics
Maximize wrt. ps (w|o):
EU(s, l) = Es,l [U (o, g)]
ps (w|o) determinstically says:
argmaxw pl (o | w )
Needs embedded model of listener: pl (o|w)
right
of O2
O1

O1
O3

on O3 O1

7

Semantics + Pragmatics
Maximize wrt. ps (w|o):
EU(s, l) = Es,l [U (o, g)]
ps (w|o) determinstically says:
argmaxw pl (o | w )
Needs embedded model of listener: pl (o|w)
right
of O2
O1

O1
O3

on O3 O1
Rational speaker because it is optimal with respect to given listener.

7

Reﬂex vs. Rational
S(L)
w1

S

L

w1
o

w2

g1
o

w2

w3

g2
g3

w3
Reﬂex
(semantics only)

Rational
(semantics + pragmatics)

8

Reﬂex vs. Rational

S()

Reﬂex
(semantics only)

S(L)

Rational
(semantics + pragmatics)

8

Experimental Setup
Google sketchup: 43 rooms, average of 22 objects per room

9

Data Collection with Mechanical Turk

Speaker: o → w

10

Data Collection with Mechanical Turk

Speaker: o → w
Yields annotated data:
{(o1 , w1 ), · · · , (on , wn )}

10

Evaluation with Mechanical Turk
Question: What object is right of O2 ?
O2
O1

O3

Listener: w →g

11

Evaluation with Mechanical Turk
Given ps (w|o), and (o1 , . . . , on ),

12

Evaluation with Mechanical Turk
Given ps (w|o), and (o1 , . . . , on ),
s generates:
(w1 , . . . , wn )

12

Evaluation with Mechanical Turk
Given ps (w|o), and (o1 , . . . , on ),
s generates:
(w1 , . . . , wn )
where:
wi = argmaxw ps (w|oi )

12

Evaluation with Mechanical Turk
Given ps (w|o), and (o1 , . . . , on ),
s generates:
(w1 , . . . , wn )
where:
wi = argmaxw ps (w|oi )
turkers generate:
(g1 , . . . , gn )

12

Evaluation with Mechanical Turk
Given ps (w|o), and (o1 , . . . , on ),
s generates:
(w1 , . . . , wn )
where:
wi = argmaxw ps (w|oi )
turkers generate:
(g1 , . . . , gn )
compute success metric:
Success(s) =

1
n

i

I[oi = gi ]

12

Evaluation with Mechanical Turk
Given ps (w|o), and (o1 , . . . , on ),
s generates:
(w1 , . . . , wn )
where:
wi = argmaxw ps (w|oi )
turkers generate:
(g1 , . . . , gn )
compute success metric:
Success(s) =

1
n

i

I[oi = gi ]

Note: only collecting data and evaluating are done by humans.

12

Results
Literal agents put mass uniformly on true outputs
Speaker

Success

S()

Reﬂex

Literal

4.6%

Literal

33.7%

S(L)

Rational

Rational speaker outperforms reﬂex speaker.

13

Key Points
S(L)

Rational speaker

S()

outperforms reﬂex speaker

14

Key Points
S(L)

Rational speaker

S()

outperforms reﬂex speaker
S(L)

Rational speaker

S(L)

is optimal with respect to listener

14

Key Points
S(L)

Rational speaker

S()

outperforms reﬂex speaker
S(L)

Rational speaker

S(L)

is optimal with respect to listener
S(L)

To improve rational speaker

S(L)

must improve embedded listener

14

Key Points
S(L)

Rational speaker

S()

outperforms reﬂex speaker
S(L)

Rational speaker

S(L)

is optimal with respect to listener
S(L)

To improve rational speaker

S(L)

must improve embedded listener

Up next: extensions for improving the listener model.

14

Listener Extensions
• Training a Listener
• Generating Complex Utterances
• Modeling Listener Confusion

15

Listener Extensions
• Training a Listener
• Generating Complex Utterances
• Modeling Listener Confusion

16

Learned Listener Model
Before: listener was Literal

17

Learned Listener Model
Before: listener was Literal
Now: learn from mturk data:
{(o1 , w1 ), . . . , (on , wn )}

17

Learned Listener Model
Before: listener was Literal
Now: learn from mturk data:
{(o1 , w1 ), . . . , (on , wn )}
Train a log-linear model:
pLearned (g|w; θl ) ∝ exp{θl φ(g, w)}

17

Learned Listener Model
Before: listener was Literal
Now: learn from mturk data:
{(o1 , w1 ), . . . , (on , wn )}
Train a log-linear model:
pLearned (g|w; θl ) ∝ exp{θl φ(g, w)}
S(L)

Use it to deﬁne the rational

Learned speaker

17

Learned Listener Model
Before: listener was Literal
Now: learn from mturk data:
{(o1 , w1 ), . . . , (on , wn )}
Train a log-linear model:
pLearned (g|w; θl ) ∝ exp{θl φ(g, w)}
S(L)

Use it to deﬁne the rational

Learned speaker

We also train ps (w|o; θs ) using the same data and features
S()

to get the reﬂex

Learned speaker

17

Features
The features φ(g, w) are deﬁned between:

18

Features
The features φ(g, w) are deﬁned between:
• guess object g

Features inspired by [Regier, 2001; Tellex, 2009; Landau, 1993]
18

Features
The features φ(g, w) are deﬁned between:
• guess object g
• w = right of O2
w.r

w.o

Features inspired by [Regier, 2001; Tellex, 2009; Landau, 1993]
18

Features
The features φ(g, w) are deﬁned between:
• guess object g
• w = right of O2
w.r

w.o

Features inspired by [Regier, 2001; Tellex, 2009; Landau, 1993]
18

Features
The features φ(g, w) are deﬁned between:
• guess object g
• w = right of O2
w.r

w.o

• g and w.o are bounding boxes

g

w.o
Features inspired by [Regier, 2001; Tellex, 2009; Landau, 1993]
18

Distance Features

g

w.o

19

Distance Features

g

w.o
φdist = value of shortest distance between g and w.o
φtop1 = I[g is closest to w.o]
φtop5 = I[g is among top 5 closest to w.o]
φtop10 = I[g is among top 10 closest to w.o]
19

Containment Features

g

w.o

20

Containment Features

g
w.o ∩ g
w.o
φcont2 = vol(w.o ∩ g) / vol(g )
φcont1 = vol(w.o ∩ g) / vol(w.o)

20

Projection Features

g

w.o

21

Projection Features

v

g

w.o

21

Projection Features

v

21

Projection Features

y

v

x

21

Projection Features

v

y

v
x

21

Projection Features

y

fy
v
x

fx

21

Projection Features

φprojx = fx
φprojy = fy
φprojz = fz
φproj1 = I[fx = max{fx , fy , fz }]
φproj2 = I[fy = max{fx , fy , fz }]
φproj3 = I[fz = max{fx , fy , fz }]

y

fy
v
x

fx

21

Results
Speaker

Success

S()

Reﬂex

Literal

4.6%

Literal

33.7%

S(L)

Rational

22

Results
Speaker

Success

S()

Reﬂex

Literal

4.6%

Literal

33.7%

Learned

38.4%

Learned

52.6%

S(L)

Rational
S()

Reﬂex
S(L)

Rational

22

What’s missing?
Two things are missing from the setup so far.

23

What’s missing?
Two things are missing from the setup so far.
1. Arbitrary descriptors

23

What’s missing?
Two things are missing from the setup so far.
1. Arbitrary descriptors
O2
O1

O3

23

What’s missing?
Two things are missing from the setup so far.
1. Arbitrary descriptors (not today)
O2
O1

O3

2.

23

What’s missing?
Two things are missing from the setup so far.
1. Arbitrary descriptors (not today)
O2
O1

O3

We will not be seeing 100% in this talk.
2.

23

What’s missing?
Two things are missing from the setup so far.
1. Arbitrary descriptors (not today)
O2
O1

O3

We will not be seeing 100% in this talk.
2. Complex utterances

23

What’s missing?
Two things are missing from the setup so far.
1. Arbitrary descriptors (not today)
O2
O1

O3

We will not be seeing 100% in this talk.
2. Complex utterances (coming up)

23

Listener Extensions
• Training a Listener
• Generating Complex Utterances
• Modeling Listener Confusion

24

Complex Utterances
Before: utterances were simple, such as:
right of O2
o

on O3

25

Complex Utterances
Before: utterances were simple, such as:
right of O2
oo
Now: utterances are from grammar:

on O3

25

Complex Utterances
Before: utterances were simple, such as:
right of O2
on O3
ooo
Now: utterances are from grammar:
n → something | O1 | O2 | · · ·
[noun]

25

Complex Utterances
Before: utterances were simple, such as:
right of O2
on O3
oooo
Now: utterances are from grammar:
n → something | O1 | O2 | · · ·
[noun]
r → on | right of | · · ·
[relation]

25

Complex Utterances
Before: utterances were simple, such as:
right of O2
on O3
ooooo
Now: utterances are from grammar:
n → something | O1 | O2 | · · ·
[noun]
r → on | right of | · · ·
[relation]
rp → r np
[relativization]

25

Complex Utterances
Before: utterances were simple, such as:
right of O2
on O3
oooooo
Now: utterances are from grammar:
n → something | O1 | O2 | · · ·
[noun]
r → on | right of | · · ·
[relation]
rp → r np
[relativization]

right of O2

25

Complex Utterances
Before: utterances were simple, such as:
right of O2
on O3
ooooooo
Now: utterances are from grammar:
n → something | O1 | O2 | · · ·
[noun]
r → on | right of | · · ·
[relation]
rp → r np
[relativization]

on something right of O2

25

Complex Utterances
Before: utterances were simple, such as:
right of O2
on O3
ooooooo
Now: utterances are from grammar:
n → something | O1 | O2 | · · ·
[noun]
r → on | right of | · · ·
[relation]
rp → r np
[relativization]
np → n rp∗
[conjunction]

25

Complex Utterances
Before: utterances were simple, such as:
right of O2
on O3
ooooooo
Now: utterances are from grammar:
n → something | O1 | O2 | · · ·
[noun]
r → on | right of | · · ·
[relation]
rp → r np
[relativization]
np → n rp∗
[conjunction]

right of O2 and on O3

25

Example Interpretation
Computing: p(g | on something right of O2)

O2
O1
O3

26

Example Interpretation
Computing: p(g | on something right of O2)

O2
O1
O3

NP

O1 O2 O3

O2

If w is rooted at n, pl (g|w) = I[w = g].
26

Example Interpretation
Computing: p(g | on something right of O2)

O2
O1
O3

RP

O1 O2 O3

R

NP

right of

O2

O1 O2 O3

If w is rooted at rp, recurse on np subtree, use base listener.
26

Example Interpretation
Computing: p(g | on something right of O2)

O2
O1
O3

N

RP

O1 O2 O3

something

O1 O2 O3

R

NP

right of

O2

O1 O2 O3

If w is rooted at n and w = something, pl (g|w) is uniform.
26

Example Interpretation
Computing: p(g | on something right of O2)

O2
O1
O3

NP

N

O1 O2 O3

RP

O1 O2 O3

something

O1 O2 O3

R

NP

right of

O2

O1 O2 O3

If w is rooted at np, recurse on children, multiply and renormalize.
26

Example Interpretation
Computing: p(g | on something right of O2)

O2
O1

RP

O3

O1 O2 O3

R
on

NP

N

O1 O2 O3

RP

O1 O2 O3

something

O1 O2 O3

R

NP

right of

O2

O1 O2 O3

If w is rooted at rp, recurse on np subtree, use base listener.
26

Results
Speaker

Success

S()

Reﬂex

Learned

38.4%

Learned

52.6%

S(L)

Rational

27

Results
Speaker

Success

S()

Reﬂex

Learned

38.4%

Learned

52.6%

Learned compositional

51.0%

S(L)

Rational
S(L)

Rational

27

Results
Speaker

Success

S()

Reﬂex

Learned

38.4%

Learned

52.6%

Learned compositional

51.0%

S(L)

Rational
S(L)

Rational

Problem: introducing complex utterances hurts success

27

Listener Confusion
Observations: success is lower

28

Listener Confusion
Observations: success is lower & all uterances are longer

28

Listener Confusion
Observations: success is lower & all uterances are longer
Is longer always better?

28

Listener Confusion
Observations: success is lower & all uterances are longer
Is longer always better?
Right of the lamp

28

Listener Confusion
Observations: success is lower & all uterances are longer
Is longer always better?
Right of the lamp and on the table

28

Listener Confusion
Observations: success is lower & all uterances are longer
Is longer always better?
Right of the lamp and on the table and below the ceiling

28

Listener Confusion
Observations: success is lower & all uterances are longer
Is longer always better?
Right of the lamp and on the table and below the ceiling and in the room

28

Listener Confusion
Observations: success is lower & all uterances are longer
Is longer always better?
Right of the lamp and on the table and below the ceiling and in the room and etc.

28

Listener Confusion
Observations: success is lower & all uterances are longer
Is longer always better?
Right of the lamp and on the table and below the ceiling and in the room and etc.

Maxim of manner: also be brief

28

Listener Confusion
Observations: success is lower & all uterances are longer
Is longer always better?
Right of the lamp and on the table and below the ceiling and in the room and etc.

Maxim of manner: also be brief
• saves time

28

Listener Confusion
Observations: success is lower & all uterances are longer
Is longer always better?
Right of the lamp and on the table and below the ceiling and in the room and etc.

Maxim of manner: also be brief
• saves time
• prevents confusion

28

Listener Extensions
• Training a Listener
• Generating Complex Utterances
• Modeling Listener Confusion

29

Modeling Listener Confusion
Problem: our model does not match turkers

pl (g | w) = α|w| pl (g | w) + (1 − α|w| )prnd (g | w)
˜
understand

confused

30

Modeling Listener Confusion
Problem: our model does not match turkers
Confused turkers guess randomly.

pl (g | w) = α|w| pl (g | w) + (1 − α|w| )prnd (g | w)
˜
understand

confused

30

Modeling Listener Confusion
Problem: our model does not match turkers
Confused turkers guess randomly.
RP
O1 O2 O3

R
on

NP

N

something

RP

O1 O2 O3

R

NP

right of

O2

pl (g | w) = α|w| pl (g | w) + (1 − α|w| )prnd (g | w)
˜
understand

confused

30

Modeling Listener Confusion
Problem: our model does not match turkers
Confused turkers guess randomly.
RP
O1 O2 O3

R
on

NP

N

something

RP

O1 O2 O3

R

NP

right of

O2

pl (g | w) = α|w| pl (g | w) + (1 − α|w| )prnd (g | w)
˜
understand

confused

30

Modeling Listener Confusion
Problem: our model does not match turkers
Confused turkers guess randomly.
RP
O1 O2 O3

R
on

NP

N

something

RP

O1 O2 O3

R

NP

right of

O2

pl (g | w) = α|w| pl (g | w) + (1 − α|w| )prnd (g | w)
˜
understand

confused

30

Modeling Listener Confusion
Problem: our model does not match turkers
Confused turkers guess randomly.
RP
O1 O2 O3

R
on

NP

N

something

RP

O1 O2 O3

R

NP

right of

O2

pl (g | w) = α|w| pl (g | w) + (1 − α|w| )prnd (g | w)
˜
understand

confused

30

Modeling Listener Confusion
Problem: our model does not match turkers
Confused turkers guess randomly.
RP
O1 O2 O3

R
on

NP

N

something

RP

O1 O2 O3

R

NP

right of

O2

pl (g | w) = α|w| pl (g | w) + (1 − α|w| )prnd (g | w)
˜
understand

confused

30

Modeling Listener Confusion
Problem: our model does not match turkers
Confused turkers guess randomly.
RP
O1 O2 O3

R
on

NP

N

something

RP

O1 O2 O3

R

NP

right of

O2

pl (g | w) = α|w| pl (g | w) + (1 − α|w| )prnd (g | w)
˜
understand

confused

30

Modeling Listener Confusion
Problem: our model does not match turkers
Confused turkers guess randomly.
RP
O1 O2 O3

R
on

NP

N

something

RP

O1 O2 O3

R

NP

right of

O2

pl (g | w) = α|w| pl (g | w) + (1 − α|w| )prnd (g | w)
˜
understand

confused

30

Results
Speaker

Success

S(L)

Rational

Learned

52.6%

Learned compositional

51.0%

S(L)

Rational

31

Results
Speaker

Success

S(L)

Rational

Learned

52.6%

Learned compositional

51.0%

Learned +confusion model

54.5%

S(L)

Rational
S(L)

Rational

31

Final Remarks
Observation: people avoid saying ambiguous utterances

32

Final Remarks
Observation: people avoid saying ambiguous utterances
• Grice (1975) expressed this observation in his maxims

32

Final Remarks
Observation: people avoid saying ambiguous utterances
• Grice (1975) expressed this observation in his maxims
• Parikh (2001) , Stalnaker (2005) , and J¨ger (2008)
a
explore relation between pragmatic models and game theory

32

Final Remarks
Observation: people avoid saying ambiguous utterances
• Grice (1975) expressed this observation in his maxims
• Parikh (2001) , Stalnaker (2005) , and J¨ger (2008)
a
explore relation between pragmatic models and game theory
Meanwhile:
Landau (1993) , Regier (2001) , and Tellex (2009)
have been studying spatial language

32

Final Remarks
Observation: people avoid saying ambiguous utterances
• Grice (1975) expressed this observation in his maxims
• Parikh (2001) , Stalnaker (2005) , and J¨ger (2008)
a
explore relation between pragmatic models and game theory
Meanwhile:
Landau (1993) , Regier (2001) , and Tellex (2009)
have been studying spatial language
Our contribution: we show how a game theoretic pragmatics model
can be used to successfully generate spatial descriptions

32

Thank you!

slides compiled with rﬁg

33

