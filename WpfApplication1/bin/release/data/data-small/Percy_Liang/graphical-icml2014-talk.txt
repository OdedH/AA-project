Estimating Latent Variable Graphical Models with
Moments and Likelihoods
Arun Tejasvi Chaganty
Percy Liang
Stanford University

June 22, 2014

Chaganty, Liang (Stanford University)

Moments and Likelihoods (M58)

June 22, 2014

1 / 25

Introduction

x2

Latent Variable Graphical Models

Gaussian Mixture Models
Latent Dirichlet Allocation

x1

Hidden Markov Models
PCFGs

Chaganty, Liang (Stanford University)

h1

h2

h3

x1

...

x2

x3

Moments and Likelihoods (M58)

...

June 22, 2014

2 / 25

Introduction

−log-likelihood

Parameter Estimation is Hard

parameters
Log-likelihood function is non-convex.

Chaganty, Liang (Stanford University)

Moments and Likelihoods (M58)

June 22, 2014

3 / 25

Introduction

−log-likelihood

Parameter Estimation is Hard

MLE
parameters
Log-likelihood function is non-convex.
MLE is consistent but intractable.

Chaganty, Liang (Stanford University)

Moments and Likelihoods (M58)

June 22, 2014

3 / 25

Introduction

−log-likelihood

Parameter Estimation is Hard

EM
EM
MLE
parameters
Log-likelihood function is non-convex.
MLE is consistent but intractable.
Local methods (EM, gradient descent, . . . ) are tractable but
inconsistent.

Chaganty, Liang (Stanford University)

Moments and Likelihoods (M58)

June 22, 2014

3 / 25

Introduction

−log-likelihood

Parameter Estimation is Hard

MoM

EM

EM
MLE
parameters
Log-likelihood function is non-convex.
MLE is consistent but intractable.
Local methods (EM, gradient descent, . . . ) are tractable but
inconsistent.
Method of moments estimators can be consistent and
computationally-eﬃcient, but require more data.

Chaganty, Liang (Stanford University)

Moments and Likelihoods (M58)

June 22, 2014

3 / 25

Introduction

Consistent estimation for general models
Several estimators based on the method of moments.
Phylogenetic trees: Mossel and Roch 2005.
Hidden Markov models: Hsu, Kakade, and Zhang 2009
Latent trees: Anandkumar et al. 2011
Latent Dirichlet Allocation: Anandkumar et al. 2012
PCFGs: Hsu, Kakade, and Liang 2012
Mixtures of linear regressors Chaganty and Liang 2013
...

Chaganty, Liang (Stanford University)

Moments and Likelihoods (M58)

June 22, 2014

4 / 25

Introduction

Consistent estimation for general models
Several estimators based on the method of moments.
Phylogenetic trees: Mossel and Roch 2005.
Hidden Markov models: Hsu, Kakade, and Zhang 2009
Latent trees: Anandkumar et al. 2011
Latent Dirichlet Allocation: Anandkumar et al. 2012
PCFGs: Hsu, Kakade, and Liang 2012
Mixtures of linear regressors Chaganty and Liang 2013
...

These estimators are applicable only to a speciﬁc type of model.

Chaganty, Liang (Stanford University)

Moments and Likelihoods (M58)

June 22, 2014

4 / 25

Introduction

Consistent estimation for general models
Several estimators based on the method of moments.
Phylogenetic trees: Mossel and Roch 2005.
Hidden Markov models: Hsu, Kakade, and Zhang 2009
Latent trees: Anandkumar et al. 2011
Latent Dirichlet Allocation: Anandkumar et al. 2012
PCFGs: Hsu, Kakade, and Liang 2012
Mixtures of linear regressors Chaganty and Liang 2013
...

These estimators are applicable only to a speciﬁc type of model.
In contrast, EM and gradient descent apply for almost any model.

Chaganty, Liang (Stanford University)

Moments and Likelihoods (M58)

June 22, 2014

4 / 25

Introduction

Consistent estimation for general models
Several estimators based on the method of moments.
Phylogenetic trees: Mossel and Roch 2005.
Hidden Markov models: Hsu, Kakade, and Zhang 2009
Latent trees: Anandkumar et al. 2011
Latent Dirichlet Allocation: Anandkumar et al. 2012
PCFGs: Hsu, Kakade, and Liang 2012
Mixtures of linear regressors Chaganty and Liang 2013
...

These estimators are applicable only to a speciﬁc type of model.
In contrast, EM and gradient descent apply for almost any model.
Note: some work in the observable operator framework does apply to
a more general model class.
Weighted automata: Balle and Mohri 2012.
Junction trees: Song, Xing, and Parikh 2011
...

Chaganty, Liang (Stanford University)

Moments and Likelihoods (M58)

June 22, 2014

4 / 25

Introduction

Consistent estimation for general models
Several estimators based on the method of moments.
Phylogenetic trees: Mossel and Roch 2005.
Hidden Markov models: Hsu, Kakade, and Zhang 2009
Latent trees: Anandkumar et al. 2011
Latent Dirichlet Allocation: Anandkumar et al. 2012
PCFGs: Hsu, Kakade, and Liang 2012
Mixtures of linear regressors Chaganty and Liang 2013
...

These estimators are applicable only to a speciﬁc type of model.
In contrast, EM and gradient descent apply for almost any model.
Note: some work in the observable operator framework does apply to
a more general model class.
Weighted automata: Balle and Mohri 2012.
Junction trees: Song, Xing, and Parikh 2011
...

How can we apply the method of moments to estimate
parameters eﬃciently for a general model?
Chaganty, Liang (Stanford University)

Moments and Likelihoods (M58)

June 22, 2014

4 / 25

Introduction

Setup
h1

Discrete models with k hidden
and d ≥ k observed values.

Chaganty, Liang (Stanford University)

x1

Moments and Likelihoods (M58)

h ∈ {1, 2, · · · , k}

x2

x3

xi ∈ {1, 2, · · · , d}

June 22, 2014

5 / 25

Introduction

Setup
h1

Discrete models with k hidden
and d ≥ k observed values.
Parameters and marginals can
be represented as matrices and
tensors.

Chaganty, Liang (Stanford University)

x1

M12
(M12 )ij

Moments and Likelihoods (M58)

h ∈ {1, 2, · · · , k}

x2

x3

xi ∈ {1, 2, · · · , d}

P(x1 , x2 )
P(x1 = i, x2 = j)

June 22, 2014

5 / 25

Introduction

Setup
h1

Discrete models with k hidden
and d ≥ k observed values.
Parameters and marginals can
be represented as matrices and
tensors.

x1

M12
(M12 )ij

M123
(M123 )ijk

Chaganty, Liang (Stanford University)

Moments and Likelihoods (M58)

h ∈ {1, 2, · · · , k}

x2

x3

xi ∈ {1, 2, · · · , d}

P(x1 , x2 )
P(x1 = i, x2 = j)

P(x1 , x2 , x3 )
P(x1 = i, x2 = j, x3 = k)

June 22, 2014

5 / 25

Introduction

Setup
h1

Discrete models with k hidden
and d ≥ k observed values.
Parameters and marginals can
be represented as matrices and
tensors.

x2

x1

M12

x3

xi ∈ {1, 2, · · · , d}

P(x1 , x2 )
P(x1 = i, x2 = j)

(M12 )ij

M123

h ∈ {1, 2, · · · , k}

P(x1 , x2 , x3 )
P(x1 = i, x2 = j, x3 = k)

(M123 )ijk

O (1|1)
(O
Chaganty, Liang (Stanford University)

P(x1 | h1 )

(1|1)

P(x1 = i | h1 = j)

)ij

Moments and Likelihoods (M58)

June 22, 2014

5 / 25

Introduction

Setup
h1

Discrete models with k hidden
and d ≥ k observed values.
Parameters and marginals can
be represented as matrices and
tensors.
Presented in terms of inﬁnite
data and exact moments.

x2

x1

M12

x3

xi ∈ {1, 2, · · · , d}

P(x1 , x2 )
P(x1 = i, x2 = j)

(M12 )ij

M123

h ∈ {1, 2, · · · , k}

P(x1 , x2 , x3 )
P(x1 = i, x2 = j, x3 = k)

(M123 )ijk

O (1|1)
(O
Chaganty, Liang (Stanford University)

P(x1 | h1 )

(1|1)

P(x1 = i | h1 = j)

)ij

Moments and Likelihoods (M58)

June 22, 2014

5 / 25

Introduction

Setup
aa
h2 ah1 0
a
a

Directed models parameterized
by conditional probability
tables.

h1
a
x1

0
1

1

h2
a
x2

b
x1

b
x2

θ

Chaganty, Liang (Stanford University)

Moments and Likelihoods (M58)

June 22, 2014

6 / 25

Introduction

Setup
aa
h2 ah1 0
a
a

Directed models parameterized
by conditional probability
tables.

h1
a
x1

Undirected models
parameterized as a log-linear
model. Identify modulo A(θ).

Chaganty, Liang (Stanford University)

Moments and Likelihoods (M58)

b
x1

1

h2
a
x2

b
x1

h1
a
x1

0
1

θ

b
x2

h2
a
x2

b
x2

June 22, 2014

6 / 25

Introduction

Setup
aa
h2 ah1 0
a
a

Directed models parameterized
by conditional probability
tables.

h1
a
x1

Undirected models
parameterized as a log-linear
model. Identify modulo A(θ).
Focus on directed models, but
return to undirected models
later.

Chaganty, Liang (Stanford University)

Moments and Likelihoods (M58)

b
x1

1

h2
a
x2

b
x1

h1
a
x1

0
1

θ

b
x2

h2
a
x2

b
x2

June 22, 2014

6 / 25

Introduction

Background: Three-view mixture models aka bottlenecks
Deﬁnition (Bottleneck)
A hidden variable h is a bottleneck
if there exist three observed
variables (views) x1 , x2 , x3 that are
conditionally independent given h.

h1

x1

Chaganty, Liang (Stanford University)

Moments and Likelihoods (M58)

x2

x3

June 22, 2014

7 / 25

Introduction

Background: Three-view mixture models aka bottlenecks
Deﬁnition (Bottleneck)
A hidden variable h is a bottleneck
if there exist three observed
variables (views) x1 , x2 , x3 that are
conditionally independent given h.
Anandkumar et al. 2013
provide an algorithm to
estimate conditional moments
O (i|1) P(xi | h1 ) based on
tensor eigendecomposition.

h1

x1

x2

x3

In general, three views are
necessary for identiﬁability
(Kruskal 1977).
Chaganty, Liang (Stanford University)

Moments and Likelihoods (M58)

June 22, 2014

7 / 25

Introduction

Example: a bridge, take I

Each edge has a set of
parameters.
h1
a
x1

Chaganty, Liang (Stanford University)

Moments and Likelihoods (M58)

h2

b
x1

a
x2

June 22, 2014

b
x2

8 / 25

Introduction

Example: a bridge, take I

Each edge has a set of
parameters.
h1 and h2 are bottlenecks.

h1
a
x1

Chaganty, Liang (Stanford University)

Moments and Likelihoods (M58)

h2

b
x1

a
x2

June 22, 2014

b
x2

8 / 25

Introduction

Example: a bridge, take I

Each edge has a set of
parameters.
h1 and h2 are bottlenecks.
We can learn
a
b
P(x1 |h1 ), P(x1 |h1 ), · · · .

Chaganty, Liang (Stanford University)

Moments and Likelihoods (M58)

h1
a
x1

h2

b
x1

a
x2

June 22, 2014

b
x2

8 / 25

Introduction

Example: a bridge, take I

Each edge has a set of
parameters.
h1 and h2 are bottlenecks.
We can learn
a
b
P(x1 |h1 ), P(x1 |h1 ), · · · .

Chaganty, Liang (Stanford University)

Moments and Likelihoods (M58)

h1
a
x1

h2

b
x1

a
x2

June 22, 2014

b
x2

8 / 25

Introduction

Example: a bridge, take I

Each edge has a set of
parameters.
h1 and h2 are bottlenecks.
We can learn
a
b
P(x1 |h1 ), P(x1 |h1 ), · · · .

Chaganty, Liang (Stanford University)

Moments and Likelihoods (M58)

h1
a
x1

h2

b
x1

a
x2

June 22, 2014

b
x2

8 / 25

Introduction

Example: a bridge, take I

Each edge has a set of
parameters.
h1 and h2 are bottlenecks.
We can learn
a
b
P(x1 |h1 ), P(x1 |h1 ), · · · .
However, we can’t learn
P(h2 |h1 ) this way.

Chaganty, Liang (Stanford University)

Moments and Likelihoods (M58)

h1
a
x1

h2

b
x1

a
x2

June 22, 2014

b
x2

8 / 25

Introduction

Example: a bridge, take II
h1
a
x1

Observe the joint distribution
of x1 , x2 ,

b
x1

a
x2

b
x2

b
a
P(x1 | h1 )P(x2 | h2 )P(h1 , h2 ).

b a
P(x1 , x2 ) =
M12

h2

h1 ,h2

Chaganty, Liang (Stanford University)

O (1|1)

O (2|2)

Z12

Moments and Likelihoods (M58)

June 22, 2014

9 / 25

Introduction

Example: a bridge, take II
h1
a
x1

Observe the joint distribution
of x1 , x2 ,

b
x1

a
x2

b
x2

b
a
P(x1 | h1 )P(x2 | h2 )P(h1 , h2 ).

b a
P(x1 , x2 ) =
M12

h2

h1 ,h2

O (1|1)

O (2|2)

b a
Observed moments P(x1 , x2 )
are linear in the hidden
marginals P(h1 , h2 ).

Chaganty, Liang (Stanford University)

Z12

M12

Moments and Likelihoods (M58)

= O (1|1)

Z12

O (2|2)

June 22, 2014

9 / 25

Introduction

Example: a bridge, take II
h1
a
x1

Observe the joint distribution
of x1 , x2 ,

b
x1

a
x2

b
x2

b
a
P(x1 | h1 )P(x2 | h2 )P(h1 , h2 ).

b a
P(x1 , x2 ) =
M12

h2

h1 ,h2

O (1|1)

O (2|2)

b a
Observed moments P(x1 , x2 )
are linear in the hidden
marginals P(h1 , h2 ).

Solve for P(h1 , h2 ) by
pseudoinversion.

Chaganty, Liang (Stanford University)

Z12

M12

Z12

Moments and Likelihoods (M58)

= O (1|1)
(1|1)†
= O

Z12

O (2|2)

M12

June 22, 2014

O (2|2)†

9 / 25

Introduction

Example: a bridge, take II
h1
a
x1

Observe the joint distribution
of x1 , x2 ,

b
x1

a
x2

b
x2

b
a
P(x1 | h1 )P(x2 | h2 )P(h1 , h2 ).

b a
P(x1 , x2 ) =
M12

h2

h1 ,h2

O (1|1)

O (2|2)

b a
Observed moments P(x1 , x2 )
are linear in the hidden
marginals P(h1 , h2 ).

Solve for P(h1 , h2 ) by
pseudoinversion.

Z12

M12

Z12

= O (1|1)
(1|1)†
= O

Z12

O (2|2)

M12

O (2|2)†

Normalize for P(h2 | h1 ).

Chaganty, Liang (Stanford University)

Moments and Likelihoods (M58)

June 22, 2014

9 / 25

Introduction

Outline
M

P(x) Observed moments

θ Parameters

Chaganty, Liang (Stanford University)

Moments and Likelihoods (M58)

June 22, 2014

10 / 25

Introduction

Outline
M

P(x) Observed moments

O (1|1)

Z

P(x|h) Conditional moments

P(h) Hidden marginals

θ Parameters

Chaganty, Liang (Stanford University)

Moments and Likelihoods (M58)

June 22, 2014

10 / 25

Introduction

Outline
M

P(x) Observed moments
1. Solve bottlenecks
O (1|1)

Z

P(x|h) Conditional moments

P(h) Hidden marginals

θ Parameters

Chaganty, Liang (Stanford University)

Moments and Likelihoods (M58)

June 22, 2014

10 / 25

Introduction

Outline
M

P(x) Observed moments
1. Solve bottlenecks
O (1|1)

P(x|h) Conditional moments
2a. Pseudoinverse

Z

P(h) Hidden marginals

θ Parameters

Chaganty, Liang (Stanford University)

Moments and Likelihoods (M58)

June 22, 2014

10 / 25

Estimating Hidden Marginals

Exclusive Views
Deﬁnition (Exclusive views)
We say hi ∈ S ⊆ h has an
exclusive view xv if
1. There exists some observed
variable xv which is
conditionally independent of
the others (S\{hi }) given hi .

x1

h1
x2

h2

h4
h3

x4

S

x3

Chaganty, Liang (Stanford University)

Moments and Likelihoods (M58)

June 22, 2014

11 / 25

Estimating Hidden Marginals

Exclusive Views
Deﬁnition (Exclusive views)
We say hi ∈ S ⊆ h has an
exclusive view xv if
1. There exists some observed
variable xv which is
conditionally independent of
the others (S\{hi }) given hi .
2. The conditional moment
matrix O (v |i) P(xv | hi ) has
full column rank k and can be
recovered.

Chaganty, Liang (Stanford University)

Moments and Likelihoods (M58)

x1

h1
x2

h2

h4
h3

x4

S

x3

June 22, 2014

11 / 25

Estimating Hidden Marginals

Exclusive Views
Deﬁnition (Exclusive views)
We say hi ∈ S ⊆ h has an
exclusive view xv if
1. There exists some observed
variable xv which is
conditionally independent of
the others (S\{hi }) given hi .
2. The conditional moment
matrix O (v |i) P(xv | hi ) has
full column rank k and can be
recovered.

x1

h1
x2

h2

h4
h3

x4

S

x3

3. A set has exclusive views if
each hi ∈ S has an exclusive
view.
Chaganty, Liang (Stanford University)

Moments and Likelihoods (M58)

June 22, 2014

11 / 25

Estimating Hidden Marginals

Exclusive views give parameters
Given exclusive views, P(x | h), learning cliques is solving a linear
equation!
P(x1 , . . . , xm ) =
M

Chaganty, Liang (Stanford University)

P(x1 |h1 ) · · · P(h1 , · · · , hm )
h1 ,...,hm

O (1|1)

Moments and Likelihoods (M58)

Z

June 22, 2014

12 / 25

Estimating Hidden Marginals

Exclusive views give parameters
Given exclusive views, P(x | h), learning cliques is solving a linear
equation!
P(x1 , . . . , xm ) =

P(x1 |h1 ) · · · P(h1 , · · · , hm )
h1 ,...,hm

M

O (1|1)

Z

O (3|3)
M

=

O (1|1)

Z

Chaganty, Liang (Stanford University)

O (2|2)

Moments and Likelihoods (M58)

June 22, 2014

12 / 25

Estimating Hidden Marginals

Exclusive views give parameters
Given exclusive views, P(x | h), learning cliques is solving a linear
equation!
P(x1 , . . . , xm ) =

P(x1 |h1 ) · · · P(h1 , · · · , hm )
h1 ,...,hm

M

Z

O (1|1)

O (3|3)†

O (3|3)
M

=

O (1|1)

Z

Chaganty, Liang (Stanford University)

O (2|2)

→

Z

= O (1|1)†

Moments and Likelihoods (M58)

M

O (2|2)†

June 22, 2014

12 / 25

Estimating Hidden Marginals

Bottlenecked graphs

When are we assured exclusive
views?

x1

h1
x2

h2

h4
h3

x4

S

x3

Chaganty, Liang (Stanford University)

Moments and Likelihoods (M58)

June 22, 2014

13 / 25

Estimating Hidden Marginals

Bottlenecked graphs

When are we assured exclusive
views?
Theorem: A clique in which
each hidden variable is a
bottleneck has exclusive
views.

x1

h1
x2

h2

h4
h3

x4

S

x3

Chaganty, Liang (Stanford University)

Moments and Likelihoods (M58)

June 22, 2014

13 / 25

Estimating Hidden Marginals

Bottlenecked graphs

When are we assured exclusive
views?
Theorem: A clique in which
each hidden variable is a
bottleneck has exclusive
views.
Follows by graph
independence conditions.

Chaganty, Liang (Stanford University)

Moments and Likelihoods (M58)

x1

h1
x2

h2

h4
h3

x4

S

x3

June 22, 2014

13 / 25

Estimating Hidden Marginals

Bottlenecked graphs

When are we assured exclusive
views?
Theorem: A clique in which
each hidden variable is a
bottleneck has exclusive
views.
Follows by graph
independence conditions.
We say that the clique is
“bottlenecked”.

Chaganty, Liang (Stanford University)

Moments and Likelihoods (M58)

x1

h1
x2

h2

h4
h3

x4

S

x3

June 22, 2014

13 / 25

Estimating Hidden Marginals

Example

a
x1

b
x1

h1

a
x2

h2
b
x2

h3
b
x3

h4
a
x4

Chaganty, Liang (Stanford University)

a
x3

b
x4

Moments and Likelihoods (M58)

June 22, 2014

14 / 25

Estimating Hidden Marginals

Example

a
x1

Bottleneck

b
x1

h1

a
x2

h2
b
x2

h3
b
x3

h4
a
x4

Chaganty, Liang (Stanford University)

a
x3

b
x4

Moments and Likelihoods (M58)

June 22, 2014

14 / 25

Estimating Hidden Marginals

Example

a
x1

b
x1

h1

a
x2

h2
b
x2

h3
b
x3

h4
a
x4

Chaganty, Liang (Stanford University)

a
x3

b
x4

Moments and Likelihoods (M58)

June 22, 2014

14 / 25

Estimating Hidden Marginals

Example

a
x1

b
x1

h1

a
x2

h2
b
x2

Chaganty, Liang (Stanford University)

a
x3

h3
b
x3

h4
a
x4

Bottleneck

b
x4

Moments and Likelihoods (M58)

June 22, 2014

14 / 25

Estimating Hidden Marginals

Example

a
x1

b
x1

h1

a
x2

h2
b
x2

h3
b
x3

h4
a
x4

Chaganty, Liang (Stanford University)

a
x3

b
x4

Moments and Likelihoods (M58)

June 22, 2014

14 / 25

Estimating Hidden Marginals

Example

a
x1

b
x1

h1

a
x2

h2
b
x2

h3
b
x3

h4
a
x4

Chaganty, Liang (Stanford University)

a
x3

b
x4

Moments and Likelihoods (M58)

June 22, 2014

14 / 25

Estimating Hidden Marginals

Example

a
x1

b
x1

h1

a
x2

h2
b
x2

h3
b
x3

h4
a
x4

Chaganty, Liang (Stanford University)

a
x3

b
x4

Moments and Likelihoods (M58)

June 22, 2014

14 / 25

Estimating Hidden Marginals

Example

a
x1

b
x1

h1

a
x2

h2
b
x2

Chaganty, Liang (Stanford University)

a
x3

h3
b
x3

h4
a
x4

Exclusive views

b
x4

Moments and Likelihoods (M58)

June 22, 2014

14 / 25

Estimating Hidden Marginals

Example

Exclusive views

a
x1

b
x1

h1

a
x2

h2
b
x2

h3
b
x3

h4
a
x4

Chaganty, Liang (Stanford University)

a
x3

b
x4

Moments and Likelihoods (M58)

June 22, 2014

14 / 25

Estimating Hidden Marginals

Example

a
x1

b
x1

h1

a
x2

h2
b
x2

h3
b
x3

h4
a
x4

Chaganty, Liang (Stanford University)

a
x3

b
x4

Moments and Likelihoods (M58)

June 22, 2014

14 / 25

Estimating Hidden Marginals

Example

a
x1

b
x1

h1

a
x2

h2
b
x2

h3
b
x3

h4
a
x4

Chaganty, Liang (Stanford University)

a
x3

Exclusive views

b
x4

Moments and Likelihoods (M58)

June 22, 2014

14 / 25

Estimating Hidden Marginals

Example

a
x1

b
x1

h1

a
x2

h2
b
x2

h3
b
x3

h4
a
x4

Chaganty, Liang (Stanford University)

a
x3

b
x4

Moments and Likelihoods (M58)

June 22, 2014

14 / 25

Estimating Hidden Marginals

Example

a
x1

b
x1

h1

a
x2

h2
b
x2

h3
b
x3

h4
a
x4

Chaganty, Liang (Stanford University)

a
x3

b
x4

Moments and Likelihoods (M58)

June 22, 2014

14 / 25

Estimating Hidden Marginals

More Bottlenecked Examples
Hidden Markov models

Latent Tree models

h1

h2

h3

x1

x2

h1

x3

...
h2
a
x2

Chaganty, Liang (Stanford University)

h3
b
x2

Moments and Likelihoods (M58)

a
x3

h4
b
x3

a
x4

b
x4

June 22, 2014

15 / 25

Estimating Hidden Marginals

More Bottlenecked Examples
Hidden Markov models

Latent Tree models

h1

h2

h3

x1

x2

h1

x3

...
h2
a
x2

Chaganty, Liang (Stanford University)

h3
b
x2

Moments and Likelihoods (M58)

a
x3

h4
b
x3

a
x4

b
x4

June 22, 2014

15 / 25

Estimating Hidden Marginals

More Bottlenecked Examples
Hidden Markov models

Latent Tree models

h1

h2

h3

x1

x2

h1

x3

...
h2
a
x2

h3
b
x2

a
x3

h4
b
x3

a
x4

b
x4

Noisy Or (non-example) (Halpern and Sontag 2013)
h1
x1

Chaganty, Liang (Stanford University)

x2

h2
x3

x4

Moments and Likelihoods (M58)

x5

June 22, 2014

15 / 25

Combining moments with likelihood estimators

Outline
M

P(x) Observed moments
1. Solve bottlenecks
O (1|1)

P(x|h) Conditional moments
2a. Pseudoinverse

Z

P(h) Hidden marginals

θ Parameters

Chaganty, Liang (Stanford University)

Moments and Likelihoods (M58)

June 22, 2014

16 / 25

Combining moments with likelihood estimators

Outline
M

P(x) Observed moments
1. Solve bottlenecks
O (1|1)

P(x|h) Conditional moments
2a. Pseudoinverse
2b. Composite likelihood

Z

P(h) Hidden marginals

θ Parameters

Chaganty, Liang (Stanford University)

Moments and Likelihoods (M58)

June 22, 2014

16 / 25

Combining moments with likelihood estimators

Convex marginal likelihoods
h1

The MLE is statistically most
eﬃcient, but usually
non-convex.

a
x1

h2

b
x1

a
x2

b
x2

P(x1 |h1 ) P(x2 |h2 )P(h1 , h2 )

log P(x) = log
h1 ,h2

Chaganty, Liang (Stanford University)

Moments and Likelihoods (M58)

June 22, 2014

17 / 25

Combining moments with likelihood estimators

Convex marginal likelihoods
h1

The MLE is statistically most
eﬃcient, but usually
non-convex.
If we ﬁx the conditional
moments, − log P(x ) is convex
in θ.

a
x1

h2

b
x1

a
x2

b
x2

P(x1 |h1 ) P(x2 |h2 )P(h1 , h2 )

log P(x) = log
h1 ,h2

known

0.12

Composite Likelihood
Pseudoinverse Objective

Objective

0.10

0.08

0.06

0.04

0.02

0.00
0.0

Chaganty, Liang (Stanford University)

Moments and Likelihoods (M58)

0.2

0.4

π1

0.6

0.8

1.0

June 22, 2014

17 / 25

Combining moments with likelihood estimators

Convex marginal likelihoods
h1

The MLE is statistically most
eﬃcient, but usually
non-convex.

No closed form solution, but a
local method like EM is
guaranteed to converge to the
global optimum.

b
x1

a
x2

b
x2

P(x1 |h1 ) P(x2 |h2 )P(h1 , h2 )

log P(x) = log
h1 ,h2

known

0.12

Composite Likelihood
Pseudoinverse Objective
0.10

Objective

If we ﬁx the conditional
moments, − log P(x ) is convex
in θ.

a
x1

h2

0.08

0.06

0.04

0.02

0.00
0.0

Chaganty, Liang (Stanford University)

Moments and Likelihoods (M58)

0.2

0.4

π1

0.6

0.8

1.0

June 22, 2014

17 / 25

Combining moments with likelihood estimators

Composite likelihoods
h1

h2

h3

x1

In general, the full likelihood is
still non-convex.

x2

x3

...

P(x1 | h1 ) P(x2 | h2 )P(x3 | h3 )

log P(x123 ) = log
h1 ,h2 , h3

known

P(h3 | h2 )P(h1 , h2 )

Chaganty, Liang (Stanford University)

Moments and Likelihoods (M58)

June 22, 2014

18 / 25

Combining moments with likelihood estimators

Composite likelihoods
h1

Consider composite likelihood
on a subset of observed
log P(x123 ) = log
variables.

h2

h3

x1

In general, the full likelihood is
still non-convex.

x2

x3

...

P(x1 | h1 ) P(x2 | h2 )P(x3 | h3 )
h1 ,h2 , h3

known

P(h3 | h2 )P(h1 , h2 )

Chaganty, Liang (Stanford University)

Moments and Likelihoods (M58)

June 22, 2014

18 / 25

Combining moments with likelihood estimators

Composite likelihoods
h1

Consider composite likelihood
on a subset of observed
variables.

h2

h3

x1

In general, the full likelihood is
still non-convex.

x2

x3

...

P(x1 | h1 ) P(x2 | h2 )

log P(x12 ) = log
h1 ,h2

known

P(h1 , h2 )

Chaganty, Liang (Stanford University)

Moments and Likelihoods (M58)

June 22, 2014

18 / 25

Combining moments with likelihood estimators

Composite likelihoods
h1

Consider composite likelihood
on a subset of observed
variables.

h3

x2

x3

Moments and Likelihoods (M58)

...

P(x1 | h1 ) P(x2 | h2 )

log P(x12 ) = log

Can be shown that estimation
with composite likelihoods is
consistent (Lindsay 1988).

Chaganty, Liang (Stanford University)

h2

x1

In general, the full likelihood is
still non-convex.

h1 ,h2

known

P(h1 , h2 )

June 22, 2014

18 / 25

Combining moments with likelihood estimators

Composite likelihoods
h1

Consider composite likelihood
on a subset of observed
variables.

h3

x2

x3

...

P(x1 | h1 ) P(x2 | h2 )

log P(x12 ) = log
h1 ,h2

Can be shown that estimation
with composite likelihoods is
consistent (Lindsay 1988).

known

P(h1 , h2 )
101

2

100

ˆ
θ−θ

Asymptotically, the composite
likelihood estimator is more
eﬃcient.

h2

x1

In general, the full likelihood is
still non-convex.

10−1
Pseudoinverse
Composite likelihood
10

Chaganty, Liang (Stanford University)

−2

100

Moments and Likelihoods (M58)

10−1

10−2

10−3

10−4

10−5

June 22, 2014

18 / 25

Combining moments with likelihood estimators

Outline
M

P(x) Observed moments
1. Solve bottlenecks
O (1|1)

P(x|h) Conditional moments
2a. Pseudoinverse
2b. Composite likelihood

Z

P(h) Hidden marginals

θ Parameters

Chaganty, Liang (Stanford University)

Moments and Likelihoods (M58)

June 22, 2014

19 / 25

Combining moments with likelihood estimators

Outline
M

P(x) Observed moments
1. Solve bottlenecks
O (1|1)

P(x|h) Conditional moments
2a. Pseudoinverse
2b. Composite likelihood

Z

P(h) Hidden marginals
3a. Renormalization
3b. Convex optimization
θ Parameters

Chaganty, Liang (Stanford University)

Moments and Likelihoods (M58)

June 22, 2014

19 / 25

Recovering parameters

Recovering parameters in directed models
Conditional probability tables
are the default for a directed
model.
Can be recovered by
normalization:
P(h2 | h1 ) =

aa
h2 ah1 0
a
a

P(h1 , h2 )
.
h2 P(h1 , h2 )

Chaganty, Liang (Stanford University)

h1
a
x1

Moments and Likelihoods (M58)

b
x1

0
1

1

h2
a
x2

b
x2

June 22, 2014

20 / 25

Recovering parameters

Recovering parameters in directed models
Conditional probability tables
are the default for a directed
model.
Can be recovered by
normalization:
P(h2 | h1 ) =

aa
h2 ah1 0
a
a

P(h1 , h2 )
.
h2 P(h1 , h2 )

h1
a
x1

b
x1

0
1

1

h2
a
x2

b
x2

No dependence on tree-width.
Memory, computation and
samples depend linearly on the
size of each clique.

Chaganty, Liang (Stanford University)

Moments and Likelihoods (M58)

June 22, 2014

20 / 25

Recovering parameters

Recovering parameters in undirected log-linear models
Assume a log-linear parameterization,


pθ (x, h) = exp 



θ φ(xC , hC ) − A(θ) .

C∈G

Chaganty, Liang (Stanford University)

Moments and Likelihoods (M58)

June 22, 2014

21 / 25

Recovering parameters

Recovering parameters in undirected log-linear models
Assume a log-linear parameterization,


pθ (x, h) = exp 



θ φ(xC , hC ) − A(θ) .

C∈G

The unsupervised negative log-likelihood is non-convex,
Lunsup (θ)

pθ (x, h)].

Ex∼D [− log
h∈H

Chaganty, Liang (Stanford University)

Moments and Likelihoods (M58)

June 22, 2014

21 / 25

Recovering parameters

Recovering parameters in undirected log-linear models
Assume a log-linear parameterization,




pθ (x, h) = exp 

θ φ(xC , hC ) − A(θ) .

C∈G

The unsupervised negative log-likelihood is non-convex,
Lunsup (θ)

pθ (x, h)].

Ex∼D [− log
h∈H

However, the supervised negative log-likelihood is convex,
E(x,h)∼Dsup [− log pθ (x, h)]

Lsup (θ)





E(x,h)∼Dsup [φ(xC , hC )] + A(θ).

= −θ 
C∈G

Chaganty, Liang (Stanford University)

Moments and Likelihoods (M58)

June 22, 2014

21 / 25

Recovering parameters

Recovering parameters in undirected log-linear models
Recall, the marginals can typically estimated from supervised data.




E(x,h)∼Dsup [φ(xC , hC )] + A(θ).

Lsup (θ) = −θ 
C∈G

µC

Chaganty, Liang (Stanford University)

Moments and Likelihoods (M58)

June 22, 2014

22 / 25

Recovering parameters

Recovering parameters in undirected log-linear models
Recall, the marginals can typically estimated from supervised data.




E(x,h)∼Dsup [φ(xC , hC )] + A(θ).

Lsup (θ) = −θ 
C∈G

µC

However, the marginals can also be consistently estimated by
moments!
P(xC | hC )

µC =
xC ,hC

Chaganty, Liang (Stanford University)

P(hC )

φ(xC , hC ).

cond. moments hidden marginals

Moments and Likelihoods (M58)

June 22, 2014

22 / 25

Recovering parameters

Optimizing pseudolikelihood
Estimating µC : independent of
treewidth.
h1,3
xb
1,3
xa
1,3

h2,3
xb
2,3
xa
2,3

h1,2
xb
1,2
xa
1,2

h2,2
xb
2,2
xa
2,2

h1,1
xb
1,1
xa
1,1

Chaganty, Liang (Stanford University)

Moments and Likelihoods (M58)

h3,3
xb
3,3
xa
3,3

h3,2
xb
3,2
xa
3,2

h2,1
xb
2,1
xa
2,1

h3,1
xb
3,1
xa
3,1

June 22, 2014

23 / 25

Recovering parameters

Optimizing pseudolikelihood
Estimating µC : independent of
treewidth.
Computing A(θ): dependent
on treewidth.
A(θ)

log

h1,3
xb
1,3
xa
1,3

exp θ φ(x, h) .
x,h

h2,3
xb
2,3
xa
2,3

h1,2
xb
1,2
xa
1,2

h2,2
xb
2,2
xa
2,2

h1,1
xb
1,1
xa
1,1

Chaganty, Liang (Stanford University)

Moments and Likelihoods (M58)

h3,3
xb
3,3
xa
3,3

h3,2
xb
3,2
xa
3,2

h2,1
xb
2,1
xa
2,1

h3,1
xb
3,1
xa
3,1

June 22, 2014

23 / 25

Recovering parameters

Optimizing pseudolikelihood
Estimating µC : independent of
treewidth.
Computing A(θ): dependent
on treewidth.
A(θ)

log

h1,3
xb
1,3
xa
1,3

exp θ φ(x, h) .
x,h

Apseudo (θ; N (a))

h1,2
xb
1,2
xa
1,2

Instead, use pseudolikelihood
(Besag 1975) to consistently
estimate distributions over
local neighborhoods.
log

h2,3
xb
2,3
xa
2,3

h2,2
xb
2,2
xa
2,2

h1,1
xb
1,1
xa
1,1

h3,3
xb
3,3
xa
3,3

h3,2
xb
3,2
xa
3,2

h2,1
xb
2,1
xa
2,1

h3,1
xb
3,1
xa
3,1

exp θ φ(xN , hN ) .
a

Chaganty, Liang (Stanford University)

Moments and Likelihoods (M58)

June 22, 2014

23 / 25

Recovering parameters

Outline
M

P(x) Observed moments
1. Solve bottlenecks
O (1|1)

P(x|h) Conditional moments
2a. Pseudoinverse
2b. Composite likelihood

Z

P(h) Hidden marginals
3a. Renormalization
3b. Convex optimization
θ Parameters

Chaganty, Liang (Stanford University)

Moments and Likelihoods (M58)

June 22, 2014

24 / 25

Conclusions

Conclusions

M

P(x)

Observed moments
1. Solve bottlenecks

An algorithm for any
bottlenecked discrete
graphical model.

O (1|1)

P(x|h)

Conditional moments

2a. Pseudoinverse
2b. Composite likelihood
Z

P(h)

Hidden marginals
3a. Renormalization
3b. Convex optimization

θ

Chaganty, Liang (Stanford University)

Parameters

Moments and Likelihoods (M58)

June 22, 2014

25 / 25

Conclusions

Conclusions

M

P(x)

1. Solve bottlenecks

An algorithm for any
bottlenecked discrete
graphical model.
Combine moment estimators
with likelihood estimators.

O (1|1)

P(x|h)

Conditional moments

2a. Pseudoinverse
2b. Composite likelihood
Z

P(h)

Hidden marginals
3a. Renormalization
3b. Convex optimization

θ

Chaganty, Liang (Stanford University)

Observed moments

Parameters

Moments and Likelihoods (M58)

June 22, 2014

25 / 25

Conclusions

Conclusions

M

P(x)

1. Solve bottlenecks

An algorithm for any
bottlenecked discrete
graphical model.
Combine moment estimators
with likelihood estimators.

O (1|1)

P(x|h)

Conditional moments

2a. Pseudoinverse
2b. Composite likelihood
Z

P(h)

Hidden marginals
3a. Renormalization
3b. Convex optimization

Extends to log-linear models.
θ

Chaganty, Liang (Stanford University)

Observed moments

Parameters

Moments and Likelihoods (M58)

June 22, 2014

25 / 25

Conclusions

Conclusions

M

P(x)

1. Solve bottlenecks

An algorithm for any
bottlenecked discrete
graphical model.
Combine moment estimators
with likelihood estimators.

O (1|1)

Chaganty, Liang (Stanford University)

P(x|h)

Conditional moments

2a. Pseudoinverse
2b. Composite likelihood
Z

P(h)

Hidden marginals
3a. Renormalization
3b. Convex optimization

Extends to log-linear models.
Eﬃciently learns models with
high-treewidth.

Observed moments

θ

Parameters

Moments and Likelihoods (M58)

June 22, 2014

25 / 25

Conclusions

Conclusions

M

P(x)

1. Solve bottlenecks

An algorithm for any
bottlenecked discrete
graphical model.
Combine moment estimators
with likelihood estimators.

O (1|1)

P(x|h)

Conditional moments

2a. Pseudoinverse
2b. Composite likelihood
Z

P(h)

Hidden marginals
3a. Renormalization
3b. Convex optimization

Extends to log-linear models.
Eﬃciently learns models with
high-treewidth.

Observed moments

θ

Parameters
directed

undirected
Chaganty, Liang (Stanford University)

Moments and Likelihoods (M58)

June 22, 2014

25 / 25

Conclusions

Conclusions

M

P(x)

1. Solve bottlenecks

An algorithm for any
bottlenecked discrete
graphical model.
Combine moment estimators
with likelihood estimators.

O (1|1)

Conditional moments

P(x|h)

2a. Pseudoinverse
2b. Composite likelihood
Z

P(h)

Hidden marginals
3a. Renormalization
3b. Convex optimization

Extends to log-linear models.
Eﬃciently learns models with
high-treewidth.

Observed moments

θ

Parameters
lda

xx

directed
noisy-or
x

3 view

undirected
Chaganty, Liang (Stanford University)

Moments and Likelihoods (M58)

June 22, 2014

25 / 25

Conclusions

Conclusions

M

1. Solve bottlenecks

An algorithm for any
bottlenecked discrete
graphical model.
Combine moment estimators
with likelihood estimators.

Observed moments

P(x)

O (1|1)

Conditional moments

2a. Pseudoinverse
2b. Composite likelihood
Z

Hidden marginals

P(h)

3a. Renormalization
3b. Convex optimization

Extends to log-linear models.
Eﬃciently learns models with
high-treewidth.

P(x|h)

θ

Parameters
lda

xx

directed
noisy-or
x

3 view

bottlenecked

undirected
Chaganty, Liang (Stanford University)

Moments and Likelihoods (M58)

June 22, 2014

25 / 25

Conclusions

Conclusions

M

1. Solve bottlenecks

An algorithm for any
bottlenecked discrete
graphical model.
Combine moment estimators
with likelihood estimators.

Observed moments

P(x)

O (1|1)

Conditional moments

2a. Pseudoinverse
2b. Composite likelihood
Z

Hidden marginals

P(h)

3a. Renormalization
3b. Convex optimization

Extends to log-linear models.
Eﬃciently learns models with
high-treewidth.

P(x|h)

θ

Parameters
lda
trees
grids

xx

directed
noisy-or
x

3 view

bottlenecked

undirected
Chaganty, Liang (Stanford University)

Moments and Likelihoods (M58)

June 22, 2014

25 / 25

Conclusions

Conclusions

M

1. Solve bottlenecks

An algorithm for any
bottlenecked discrete
graphical model.
Combine moment estimators
with likelihood estimators.

Observed moments

P(x)

O (1|1)

Conditional moments

2a. Pseudoinverse
2b. Composite likelihood
Z

Hidden marginals

P(h)

3a. Renormalization
3b. Convex optimization

Extends to log-linear models.
Eﬃciently learns models with
high-treewidth.

P(x|h)

θ

Parameters
lda
trees
grids

xx

directed
noisy-or
x

3 view

bottlenecked

undirected
Chaganty, Liang (Stanford University)

Moments and Likelihoods (M58)

June 22, 2014

25 / 25

Conclusions

Conclusions

M

1. Solve bottlenecks

An algorithm for any
bottlenecked discrete
graphical model.
Combine moment estimators
with likelihood estimators.

Observed moments

P(x)

O (1|1)

Thank you! Poster: M58

Conditional moments

2a. Pseudoinverse
2b. Composite likelihood
Z

Hidden marginals

P(h)

3a. Renormalization
3b. Convex optimization

Extends to log-linear models.
Eﬃciently learns models with
high-treewidth.

P(x|h)

θ

Parameters
lda
trees
grids

xx

directed
noisy-or
x

3 view

bottlenecked

undirected
Chaganty, Liang (Stanford University)

Moments and Likelihoods (M58)

June 22, 2014

25 / 25

