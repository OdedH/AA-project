A Permutation-Augmented Sampler
for DP Mixture Models
ICML 2007

Corvallis, Oregon

June 21, 2007
Percy Liang

Michael I. Jordan

Ben Taskar

UC Berkeley

UC Berkeley

U Penn

Introduction
Dirichlet process mixture models:
• Clustering applications:
– natural language processing, e.g. [Blei, et. al, 2004;
Daume, Marcu, 2005; Goldwater, et. al, 2006; Liang,
et. al, 2007]
– vision, e.g. [Sudderth, et. al, 2006]
– bioinformatics, e.g. [Xing, et. al, 2004]

• Nonparametric: number of clusters adapts to data
• Current inference based on local moves

2

Introduction
Dirichlet process mixture models:
• Clustering applications:
– natural language processing, e.g. [Blei, et. al, 2004;
Daume, Marcu, 2005; Goldwater, et. al, 2006; Liang,
et. al, 2007]
– vision, e.g. [Sudderth, et. al, 2006]
– bioinformatics, e.g. [Xing, et. al, 2004]

• Nonparametric: number of clusters adapts to data
• Current inference based on local moves
Outline:
• DP mixture model
• Permutation-augmented model ⇒ global moves
• Experiments
2

[Ferguson, 1973; Antoniak, 1974]

Dirichlet processes
G

DP mixture model
G ∼ DP(α0, G0)
For each data point i = 1, . . . , n:
θi ∼ G
xi ∼ F (θi)

θi
xi
n

3

[Ferguson, 1973; Antoniak, 1974]

Dirichlet processes
G

DP mixture model
G ∼ DP(α0, G0)
For each data point i = 1, . . . , n:
θi ∼ G
xi ∼ F (θi)

θi
xi
n

Deﬁnition: G0 = a distribution on Θ, α0 = concentration parameter.
G is a draw from a Dirichlet process, denoted G ∼ DP(α0, G0)

3

[Ferguson, 1973; Antoniak, 1974]

Dirichlet processes
G

DP mixture model
G ∼ DP(α0, G0)
For each data point i = 1, . . . , n:
θi ∼ G
xi ∼ F (θi)

θi
xi
n

Deﬁnition: G0 = a distribution on Θ, α0 = concentration parameter.
⇔

G is a draw from a Dirichlet process, denoted G ∼ DP(α0, G0)
(G(A1), . . . , G(AK )) ∼ Dirichlet(α0G0(A1), . . . , α0G0(AK ))
for all partitions (A1, . . . , AK ) of Θ.
A1
A3

A2

Θ

A4
3

Inference
Representations:
• Chinese restaurant process: marginalize G
• Stick-breaking representation: explicitly represent G

4

Inference
Representations:
• Chinese restaurant process: marginalize G
• Stick-breaking representation: explicitly represent G
Previous algorithms:
• Collapsed Gibbs sampling [Escobar, West, 1995]
• Blocked Gibbs sampling [Ishwaran, James, 2001]
• Split-merge sampling [Jain, Neal, 2000; Dahl, 2003]
• Variational [Blei, Jordan, 2005; Kurihara, et. al, 2007]
• A-star search [Daume, 2007]

4

[Pitman, 2002]

Chinese restaurant process
G ∼ DP(α0, G0) is discrete (with probability 1)
Marginalize out G ⇒ induces clustering C
Each cluster c ∈ C is a subset of {1, . . . , n}
Example: C = {{1}, {2, 3, 5}, {4}}

5

[Pitman, 2002]

Chinese restaurant process
G ∼ DP(α0, G0) is discrete (with probability 1)
Marginalize out G ⇒ induces clustering C
Each cluster c ∈ C is a subset of {1, . . . , n}
Example: C = {{1}, {2, 3, 5}, {4}}
...

p(i ∈ c) =

|c|
i−1+α0
α0
i−1+α0

if c old
if c new

probability:
5

[Pitman, 2002]

Chinese restaurant process
G ∼ DP(α0, G0) is discrete (with probability 1)
Marginalize out G ⇒ induces clustering C
Each cluster c ∈ C is a subset of {1, . . . , n}
Example: C = {{1}, {2, 3, 5}, {4}}
1
...

p(i ∈ c) =
probability:

|c|
i−1+α0
α0
i−1+α0

if c old
if c new

α0
0+α0
5

[Pitman, 2002]

Chinese restaurant process
G ∼ DP(α0, G0) is discrete (with probability 1)
Marginalize out G ⇒ induces clustering C
Each cluster c ∈ C is a subset of {1, . . . , n}
Example: C = {{1}, {2, 3, 5}, {4}}
1
2
...

p(i ∈ c)
probability:

|c|
= i−1+α0
α0
i−1+α0
α0
α0
0+α0 1+α0

if c old
if c new

5

[Pitman, 2002]

Chinese restaurant process
G ∼ DP(α0, G0) is discrete (with probability 1)
Marginalize out G ⇒ induces clustering C
Each cluster c ∈ C is a subset of {1, . . . , n}
Example: C = {{1}, {2, 3, 5}, {4}}
1
2
...
3
p(i ∈ c)
probability:

|c|
if
i−1+α0
=
α0
if
i−1+α0
α0
α0
1
0+α0 1+α0 2+α0

c old
c new

5

[Pitman, 2002]

Chinese restaurant process
G ∼ DP(α0, G0) is discrete (with probability 1)
Marginalize out G ⇒ induces clustering C
Each cluster c ∈ C is a subset of {1, . . . , n}
Example: C = {{1}, {2, 3, 5}, {4}}
1
2
...
3
4
p(i ∈ c)
probability:

|c|
if c old
i−1+α0
=
α0
if c new
i−1+α0
α0
α0
α0
1
0+α0 1+α0 2+α0 3+α0
5

[Pitman, 2002]

Chinese restaurant process
G ∼ DP(α0, G0) is discrete (with probability 1)
Marginalize out G ⇒ induces clustering C
Each cluster c ∈ C is a subset of {1, . . . , n}
Example: C = {{1}, {2, 3, 5}, {4}}
1
2
...
3
5
4
p(i ∈ c)
probability:

|c|
if c old
i−1+α0
=
α0
if c new
i−1+α0
α0
α0
α0
1
2
0+α0 1+α0 2+α0 3+α0 4+α0
5

[Antoniak, 1974]

CRP prior over clusterings
Previous example: p(C) =

α0
α0
α0
1
2
0+α0 1+α0 2+α0 3+α0 4+α0

6

[Antoniak, 1974]

CRP prior over clusterings
Previous example: p(C) =

α0
α0
α0
1
2
0+α0 1+α0 2+α0 3+α0 4+α0

In general:
1
p(C) =
AF(α0, n)

α0(|c| − 1)!
c∈C

AF(α0, n) = α0(α0 + 1) · · · (α0 + n − 1) is ascending factorial

Key: p(C) decomposes over clusters c

6

DP mixture model via the CRP

G
θi
xi
n

7

DP mixture model via the CRP
Each cluster (table) c has a dish θ.
Data points (customers) generated i.i.d. given dish.
Assuming conjugacy, we can marginalize out θ.
G
θi
xi
n

7

DP mixture model via the CRP
Each cluster (table) c has a dish θ.
Data points (customers) generated i.i.d. given dish.
Assuming conjugacy, we can marginalize out θ.
G
C
θi
x
xi
n

7

DP mixture model via the CRP
Each cluster (table) c has a dish θ.
Data points (customers) generated i.i.d. given dish.
Assuming conjugacy, we can marginalize out θ.
1
p(C) =
AF(α0, n)

G
C
θi
x
xi
n

α0(|c| − 1)!
c∈C

p(x | C) =

F (xi; θ)G0(dθ)
c∈C

i∈c
def

=p(xc )

Key: p(C) and p(x | C) decompose over clusters c
7

Posterior inference
C
Goal: compute p(C | x)
x
• Exact inference: sum over exponential number of
clusterings

8

Posterior inference
C
Goal: compute p(C | x)
x
• Exact inference: sum over exponential number of
clusterings
• Collapsed Gibbs sampler: change C one assignment at
a time

8

Posterior inference
C
Goal: compute p(C | x)
x
• Exact inference: sum over exponential number of
clusterings
• Collapsed Gibbs sampler: change C one assignment at
a time
• Split-merge sampler: change C two clusters at a time

8

Posterior inference
C
Goal: compute p(C | x)
x
• Exact inference: sum over exponential number of
clusterings
• Collapsed Gibbs sampler: change C one assignment at
a time
• Split-merge sampler: change C two clusters at a time
• Permutation-augmented sampler: can change all of C

8

Local optima
Collapsed Gibbs can get stuck in local optima
one collapsed
Gibbs move

Hard to reach this state:

9

Augmenting with a permutation
C
x

π

Sampler: alternate between sampling C and π

10

Augmenting with a permutation
C
x

π

Sampler: alternate between sampling C and π
Why augment?
• Conditioned on π, can use
dynamic programming to
eﬃciently sample all of C

10

Augmenting with a permutation
C
x

π

Sampler: alternate between sampling C and π
Why augment?
• Conditioned on π, can use
dynamic programming to
eﬃciently sample all of C
• If sample in augmented model,
can marginalize out (ignore) π to
recover original model
10

Augmenting with a permutation
C
x

π

Sampler: alternate between sampling C and π
Why augment?
{{1}, {2, 3, 5}, {4}}
• Conditioned on π, can use
dynamic programming to
eﬃciently sample all of C
• If sample in augmented model,
can marginalize out (ignore) π to
recover original model
10

Augmenting with a permutation
C
x

π

Sampler: alternate between sampling C and π
Why augment?
{{1}, {2, 3, 5}, {4}}
• Conditioned on π, can use
sample π | C
dynamic programming to
41523
eﬃciently sample all of C
• If sample in augmented model,
can marginalize out (ignore) π to
recover original model
10

Augmenting with a permutation
C
x

π

Sampler: alternate between sampling C and π
Why augment?
{{1}, {2, 3, 5}, {4}}
• Conditioned on π, can use
sample π | C
dynamic programming to
41523
eﬃciently sample all of C
sample C | π, x
{{4, 1}, {5}, {2, 3}}
• If sample in augmented model,
can marginalize out (ignore) π to
recover original model
10

Augmenting with a permutation
C
x

π

Sampler: alternate between sampling C and π
Why augment?
{{1}, {2, 3, 5}, {4}}
• Conditioned on π, can use
sample π | C
dynamic programming to
41523
eﬃciently sample all of C
sample C | π, x
{{4, 1}, {5}, {2, 3}}
• If sample in augmented model,
can marginalize out (ignore) π to
sample π | C
recover original model
54132
10

Sampling the permutation
C
p(π | C, x)
x

π

11

Sampling the permutation
C
p(π | C, x)
x

π

What’s p(π | C)?
Let Π(C) = permutations consistent with C (all clusters contiguous
in permutation)
Example:
Clustering C = {{1, 3}, {2}}
Consistent permutations:
132 312 213 231 123 321

11

Sampling the permutation
C
p(π | C, x)
x

π

What’s p(π | C)?
Let Π(C) = permutations consistent with C (all clusters contiguous
in permutation)
Example:
Clustering C = {{1, 3}, {2}}
Consistent permutations:
132 312 213 231 123 321
p(π | C) = uniform over Π(C)
1
=
if π ∈ Π(C), else 0.
|C|! c∈C |c|!
11

Sampling the clustering
C
p(C | π, x) ∝ p(C)p(x | C)p(π | C)
x

π
Number of consistent clusterings C: 2n−1

Example:
Permutation π = 312
Consistent clusterings C:
{3}, {1}, {2}
{3, 1}, {2}
{3}, {1, 2}
{3, 1, 2}

12

Sampling the clustering
C
p(C | π, x) ∝ p(C)p(x | C)p(π | C)
x

π

13

Sampling the clustering
C
p(C | π, x) ∝ p(C)p(x | C)p(π | C)
x

π
1
p(C) =
AF(α0, n)
p(x | C) =

α0(|c| − 1)!
c∈C

p(xc)
c∈C

1[π ∈ Π(C)]
p(π | C) =
|C|! c∈C |c|!

13

Sampling the clustering
C
p(C | π, x) ∝ p(C)p(x | C)p(π | C)
x

π
1
p(C) =
AF(α0, n)
p(x | C) =

α0(|c| − 1)!
c∈C

p(xc)
c∈C

1[π ∈ Π(C)]
p(π | C) =
|C|! c∈C |c|!

1[π ∈ Π(C)]
p(C, π, x) =
AF(α0, n)|C|!
def

=A(|C|)

c∈C

α0p(xc)
|c|
def

=B(c)
13

DPDP
p(C, π, x) = A(|C|)

B(c)
c∈C

Goal: p(π, x) =

n
K=1 A(K)

B(c)
C:π∈Π(C),|C|=K c∈C
def

=g(n,K)

14

DPDP
p(C, π, x) = A(|C|)

B(c)
c∈C

Goal: p(π, x) =

n
K=1 A(K)

B(c)
C:π∈Π(C),|C|=K c∈C
def

=g(n,K)

g(r, K) = sum over clusterings of 1 . . . r with K clusters
g(r, K) =

1

r
m=1 g(r

− m, K − 1)B({πr−m+1, . . . , πr })
B({πr−m+1, . . . , πr })
r−m
r ···
g(r − m, K − 1)
g(r, K)

Running time: O(n3), space: O(n2)
14

Optimizations
Current running time: O(n3), space: O(n2)
p(C, π, x) = A(|C|)

B(c)
c∈C

• Remove dependence on |C| to get MH proposal ⇒
O(n2) dynamic program
• Use a beam ⇒ O(n) time
Final running time: empirically O(n), space: O(n)

15

Data-dependent permutations
C
x

π

16

Data-dependent permutations
C
x

π

Goal: use data x to guide permutation—place similar
points near each other

16

Data-dependent permutations
C
x

π

Goal: use data x to guide permutation—place similar
points near each other
Two possible p(π | C, x):
• Markov Gibbs scans
• Random projections

16

Random projections
How to sample from p(π | C, x):
• Choose a random direction u
• Project points onto u ⇒ induces permutation
• Note: keep clusters contiguous in permutation
1
2 u
3
4
Permutation induced by projection u: 3 1 2 4

17

Random projections
How to sample from p(π | C, x):
• Choose a random direction u
• Project points onto u ⇒ induces permutation
• Note: keep clusters contiguous in permutation
1
2 u
3
4
Permutation induced by projection u: 3 1 2 4
Computing p(π | C, x) is hard; ignore it ⇒ stochastic
hill-climbing algorithm
17

Experimental setup
Interleave diﬀerent moves to form hybrid samplers:
Gibbs
Gibbs+SplitMerge
Gibbs+Perm
Gibbs+SplitMerge+Perm

Collapsed Gibbs [Escobar, West, 1995]
With split-merge [Dahl, 2003]
With permutation (this paper)
With all three moves

18

Experimental setup
Interleave diﬀerent moves to form hybrid samplers:
Gibbs
Gibbs+SplitMerge
Gibbs+Perm
Gibbs+SplitMerge+Perm

Collapsed Gibbs [Escobar, West, 1995]
With split-merge [Dahl, 2003]
With permutation (this paper)
With all three moves

• Run on synthetic Gaussians and two real-world datasets
• Evaluate on log-probability of clustering

18

Synthetic Gaussians
Setup: generate mixture of Gaussians: 10,000 points, 10–80
dimensions, 20–160 true clusters
(g) 40 dimensions, 40 true clusters

(f) 160 true clusters, 40 dimensions
-650000

log probability

log probability

-650000

-700000

-750000

-800000

-700000

-750000

-800000

-850000

-850000
-900000
0

50

100

150

seconds

200

250

0

200

400

600

800

1000

1200

seconds

Gibbs
Gibbs+SplitMerge
Gibbs+Perm
Gibbs+SplitMerge+Perm

19

Synthetic Gaussians
Setup: generate mixture of Gaussians: 10,000 points, 10–80
dimensions, 20–160 true clusters
(g) 40 dimensions, 40 true clusters

(f) 160 true clusters, 40 dimensions
-650000

log probability

log probability

-650000

-700000

-750000

-800000

-700000

-750000

-800000

-850000

-850000
-900000
0

50

100

150

seconds

200

250

0

200

400

600

800

1000

1200

seconds

Gibbs
Gibbs+SplitMerge
Gibbs+Perm
Gibbs+SplitMerge+Perm

• Gibbs+Perm signiﬁcantly outperforms Gibbs
• Gibbs+Perm outperforms Gibbs+SplitMerge,
especially when there are many clusters
19

AP dataset
2246 points, 10,473 dimensions [multinomial model]
(j) AP
log probability

-3.57e+06
-3.58e+06
-3.59e+06
-3.6e+06
-3.61e+06

Gibbs
Gibbs+SplitMerge
Gibbs+Perm
Gibbs+SplitMerge+Perm

-3.62e+06
-3.63e+06
-3.64e+06
-3.65e+06
0

500

1000

1500

2000

2500

3000

seconds

Gibbs+SplitMerge outperforms Gibbs+Perm
Gibbs+SplitMerge+Perm performs best
20

MNIST dataset
10,000 points, 50 dimensions (obtained via PCA on pixels)
[Gaussian model]
(i) MNIST
log probability

-1.252e+06

-1.254e+06

-1.256e+06

Gibbs
Gibbs+SplitMerge
Gibbs+Perm
Gibbs+SplitMerge+Perm

-1.258e+06

-1.26e+06

0

500

1000

1500

2000

2500

3000

seconds

Gibbs+Perm outperforms Gibbs+SplitMerge
Gibbs+SplitMerge+Perm performs best
21

Conclusions
• Inference algorithms for DP mixtures suﬀer from local
minima when they make small moves
• Key idea: can use dynamic programming to sum over
all clusterings consistent with a permutation
• Random projections yields eﬀective stochastic
hill-climbing algorithm

22

Conclusions
• Inference algorithms for DP mixtures suﬀer from local
minima when they make small moves
• Key idea: can use dynamic programming to sum over
all clusterings consistent with a permutation
• Random projections yields eﬀective stochastic
hill-climbing algorithm
What sampler should I use for my data?
• Gibbs is good at reﬁning clusterings
• Split-merge is good when there are few clusters
• Permutation-augmented is good at changing many
clusters at once
22

Conclusions
• Inference algorithms for DP mixtures suﬀer from local
minima when they make small moves
• Key idea: can use dynamic programming to sum over
all clusterings consistent with a permutation
• Random projections yields eﬀective stochastic
hill-climbing algorithm
What sampler should I use for my data?
• Gibbs is good at reﬁning clusterings
• Split-merge is good when there are few clusters
• Permutation-augmented is good at changing many
clusters at once
Combining all three often leads to best performance.
22

