Structured Bayesian Nonparametric Models
with Variational Inference
ACL Tutorial
Prague, Czech Republic
June 24, 2007
Percy Liang and Dan Klein

Probabilistic modeling of NLP
• Document clustering
• Topic modeling
• Language modeling
• Part-of-speech induction
• Parsing and grammar induction
• Word segmentation
• Word alignment
• Document summarization
• Coreference resolution
• etc.
2

Probabilistic modeling of NLP
• Document clustering
• Topic modeling
• Language modeling
• Part-of-speech induction
• Parsing and grammar induction
• Word segmentation
• Word alignment
• Document summarization
• Coreference resolution
• etc.
Recent interest in Bayesian nonparametric methods
2

A motivating example
How many clusters?

K

training
log-likelihood

test
log-likelihood

3

A motivating example
How many clusters?

K
1

training
log-likelihood

test
log-likelihood

-364

3

A motivating example
How many clusters?

K
1
2

training
log-likelihood

test
log-likelihood

-364
-204

3

A motivating example
How many clusters?

K
1
2
4

training
log-likelihood

test
log-likelihood

-364
-204
-82

3

A motivating example
How many clusters?

K
1
2
4
12

training
log-likelihood

test
log-likelihood

-364
-204
-82
21

3

A motivating example
How many clusters?

K
1
2
4
12
20

training
log-likelihood

test
log-likelihood

-364
-204
-82
21
86

3

A motivating example
How many clusters?

K
1
2
4
12
20

training
log-likelihood

test
log-likelihood

-364
-204
-82
21
86

3

A motivating example
How many clusters?

K
1
2
4
12
20

training
log-likelihood

test
log-likelihood

-364
-204
-82
21
86

3

A motivating example
How many clusters?

K
1
2
4
12
20

training
log-likelihood

test
log-likelihood

-364
-204
-82
21
86

-368

3

A motivating example
How many clusters?

K
1
2
4
12
20

training
log-likelihood

test
log-likelihood

-364
-204
-82
21
86

-368
-206

3

A motivating example
How many clusters?

K
1
2
4
12
20

training
log-likelihood

test
log-likelihood

-364
-204
-82
21
86

-368
-206
-128

3

A motivating example
How many clusters?

K
1
2
4
12
20

training
log-likelihood

test
log-likelihood

-364
-204
-82
21
86

-368
-206
-128
-147

3

A motivating example
How many clusters?

K
1
2
4
12
20

training
log-likelihood

test
log-likelihood

-364
-204
-82
21
86

-368
-206
-128
-147
-173

3

A motivating example
How many clusters?

K
1
2
4
12
20

training
log-likelihood

test
log-likelihood

-364
-204
-82
21
86

-368
-206
-128
-147
-173

3

Model selection: traditional solutions
Deﬁne an sequence of models of increasing complexity
Θ1

4

Model selection: traditional solutions
Deﬁne an sequence of models of increasing complexity
Θ1

Θ2

4

Model selection: traditional solutions
Deﬁne an sequence of models of increasing complexity
Θ1

Θ2

Θ3

Θ4

...

4

Model selection: traditional solutions
Deﬁne an sequence of models of increasing complexity
Θ1

Θ2

Θ3

Θ4

...

• Cross-validation: select a model based on
likelihood on heldout set

4

Model selection: traditional solutions
Deﬁne an sequence of models of increasing complexity
Θ1

Θ2

Θ3

Θ4

...

• Cross-validation: select a model based on
likelihood on heldout set
• Bayesian model selection: use marginal likelihood or
minimum description length

4

Model selection: traditional solutions
Deﬁne an sequence of models of increasing complexity
Θ1

Θ2

Θ3

Θ4

...

• Cross-validation: select a model based on
likelihood on heldout set
• Bayesian model selection: use marginal likelihood or
minimum description length
Discrete optimization: combinatorial search over
models
4

A nonparametric Bayesian approach
Deﬁne one model with an inﬁnite number of clusters but
penalize the use of more clusters.
Θ

5

A nonparametric Bayesian approach
Deﬁne one model with an inﬁnite number of clusters but
penalize the use of more clusters.
Θ

This penalty is accomplished with the Dirichlet process.

5

A nonparametric Bayesian approach
Deﬁne one model with an inﬁnite number of clusters but
penalize the use of more clusters.
Θ

This penalty is accomplished with the Dirichlet process.
Continuous optimization: model complexity governed by
magnitude of parameters

5

A nonparametric Bayesian approach
Deﬁne one model with an inﬁnite number of clusters but
penalize the use of more clusters.
Θ

This penalty is accomplished with the Dirichlet process.
Continuous optimization: model complexity governed by
magnitude of parameters
Advantages:
• Works on structured models
• Allows EM-like tools
5

True or false?
1. Being Bayesian is just about having priors.
2. Bayesian methods are slow.
3. Nonparametric means no parameters.

4. Variational inference is complicated and foreign.

6

True or false?
1. Being Bayesian is just about having priors.
Being Bayesian is about managing uncertainty.
2. Bayesian methods are slow.
3. Nonparametric means no parameters.

4. Variational inference is complicated and foreign.

6

True or false?
1. Being Bayesian is just about having priors.
Being Bayesian is about managing uncertainty.
2. Bayesian methods are slow.
Bayesian methods can be just as fast as EM.
3. Nonparametric means no parameters.

4. Variational inference is complicated and foreign.

6

True or false?
1. Being Bayesian is just about having priors.
Being Bayesian is about managing uncertainty.
2. Bayesian methods are slow.
Bayesian methods can be just as fast as EM.
3. Nonparametric means no parameters.
Nonparametric means the number of
eﬀective parameters grows adaptively with
the data.
4. Variational inference is complicated and foreign.

6

True or false?
1. Being Bayesian is just about having priors.
Being Bayesian is about managing uncertainty.
2. Bayesian methods are slow.
Bayesian methods can be just as fast as EM.
3. Nonparametric means no parameters.
Nonparametric means the number of
eﬀective parameters grows adaptively with
the data.
4. Variational inference is complicated and foreign.
Variational inference is a natural extension of EM.
6

Tutorial outline
• Part I
– Distributions and Bayesian principles
– Variational Bayesian inference
– Mean-ﬁeld for mixture models
• Part II
– Emulating DP-like qualities with ﬁnite mixtures
– DP mixture model
– Other views of the DP
• Part III
– Structured models
– Survey of related methods
– Survey of applications

7

Roadmap
• Part I
– Distributions and Bayesian principles
– Variational Bayesian inference
– Mean-ﬁeld for mixture models
• Part II
– Emulating DP-like qualities with ﬁnite mixtures
– DP mixture model
– Other views of the DP
• Part III
– Structured models
– Survey of related methods
– Survey of applications

Part I

8

Roadmap
• Part I
– Distributions and Bayesian principles
– Variational Bayesian inference
– Mean-ﬁeld for mixture models
• Part II
– Emulating DP-like qualities with ﬁnite mixtures
– DP mixture model
– Other views of the DP
• Part III
– Structured models
– Survey of related methods
– Survey of applications

Part I / Distributions and Bayesian principles

9

Two paradigms
Traditional (frequentist) approach
data x

maximum likelihood

Part I / Distributions and Bayesian principles

θ∗

θ

10

Two paradigms
Traditional (frequentist) approach
data x

maximum likelihood

θ∗

θ

Bayesian approach
data x

Bayesian inference

Part I / Distributions and Bayesian principles

q(θ)

θ

10

Two paradigms
Traditional (frequentist) approach
data x

maximum likelihood

θ∗

θ

Bayesian approach
data x

Bayesian inference

q(θ)

θ

An example:
x= H T H H
coin ﬂips (data):
probability of H (parameter): θ = 3 ? 4 ?
4 6

Part I / Distributions and Bayesian principles

10

Two paradigms
Traditional (frequentist) approach
data x

maximum likelihood

θ∗

θ

Bayesian approach
data x

Bayesian inference

q(θ)

θ

An example:
x= H T H H
coin ﬂips (data):
probability of H (parameter): θ = 3 ? 4 ?
4 6

We need a notion of distributions over parameters...
Part I / Distributions and Bayesian principles

10

Multinomial distribution
Most parameters in NLP are distributions p(z) over discrete objects
z ∈ {1, . . . , K}.
Possible p(z)s are the points φs on the simplex. For K = 3:
1

φ2
0

Part I / Distributions and Bayesian principles

0

φ1

1

11

Multinomial distribution
Most parameters in NLP are distributions p(z) over discrete objects
z ∈ {1, . . . , K}.
Possible p(z)s are the points φs on the simplex. For K = 3:
1



φ=



0
 1 
0

φ2
0
1

2

3

Part I / Distributions and Bayesian principles

0

φ1

1

11

Multinomial distribution
Most parameters in NLP are distributions p(z) over discrete objects
z ∈ {1, . . . , K}.
Possible p(z)s are the points φs on the simplex. For K = 3:
1



φ=



0
 1 
0



φ2
0
1

2

3

Part I / Distributions and Bayesian principles

φ=
0

φ1

1


0.2
 0.5 
0.3
1

2

3

11

Multinomial distribution
Most parameters in NLP are distributions p(z) over discrete objects
z ∈ {1, . . . , K}.
Possible p(z)s are the points φs on the simplex. For K = 3:
1



φ=



0
 1 
0



φ2
0
1

2

3

φ=
0

φ1

1


0.2
 0.5 
0.3
1

2

3

p(z)

Part I / Distributions and Bayesian principles

11

Multinomial distribution
Most parameters in NLP are distributions p(z) over discrete objects
z ∈ {1, . . . , K}.
Possible p(z)s are the points φs on the simplex. For K = 3:
1



φ=



0
 1 
0



φ2
0
1

2

p(z)

3

φ=
0

φ1

1


0.2
 0.5 
0.3
1

⇒

Part I / Distributions and Bayesian principles

2

3

p(z | φ)

11

Multinomial distribution
Most parameters in NLP are distributions p(z) over discrete objects
z ∈ {1, . . . , K}.
Possible p(z)s are the points φs on the simplex. For K = 3:
1



φ=



0
 1 
0



φ2
0
1

2

p(z)

3

φ=
0

φ1

1


0.2
 0.5 
0.3
1

⇒

Part I / Distributions and Bayesian principles

p(z | φ)

⇒

2

3

φz

11

Multinomial distribution
Most parameters in NLP are distributions p(z) over discrete objects
z ∈ {1, . . . , K}.
Possible p(z)s are the points φs on the simplex. For K = 3:
1



φ=



0
 1 
0



φ2
0
1

2

3

φ=
0

φ1

1


0.2
 0.5 
0.3
1

2

3

⇒
⇒
φz
p(z)
p(z | φ)
A random draw z from a multinomial distribution is written:
z ∼ Multinomial(φ).

Part I / Distributions and Bayesian principles

11

Multinomial distribution
Most parameters in NLP are distributions p(z) over discrete objects
z ∈ {1, . . . , K}.
Possible p(z)s are the points φs on the simplex. For K = 3:
1



φ=



0
 1 
0



φ2
0
1

2

3

φ=
0

φ1

1


0.2
 0.5 
0.3
1

2

3

⇒
⇒
φz
p(z)
p(z | φ)
A random draw z from a multinomial distribution is written:
z ∼ Multinomial(φ).
If we draw some number of independent samples from
Multinomial(φ), the probability of observing cz counts of observation
z is proportional to:
φ1c1 · · · φK cK
Part I / Distributions and Bayesian principles

11

Distributions over multinomial parameters
A Dirichlet distribution is a distribution over multinomial
parameters φ in the simplex.

Part I / Distributions and Bayesian principles

12

Distributions over multinomial parameters
A Dirichlet distribution is a distribution over multinomial
parameters φ in the simplex.
Like a Gaussian, there’s a notion of mean and variance.
Diﬀerent means:
1

1

1

1

1

φ2

φ2

φ2

φ2

φ2

0

0

φ1

1

0

0

φ1

Part I / Distributions and Bayesian principles

1

0

0

φ1

1

0

0

φ1

1

0

0

φ1

1

12

Distributions over multinomial parameters
A Dirichlet distribution is a distribution over multinomial
parameters φ in the simplex.
Like a Gaussian, there’s a notion of mean and variance.
Diﬀerent means:
1

1

1

1

1

φ2

φ2

φ2

φ2

φ2

0

0

φ1

1

0

0

φ1

1

0

0

φ1

1

0

0

φ1

1

0

0

φ1

1

0

φ1

1

Diﬀerent variances:
1

1

1

1

1

φ2

φ2

φ2

φ2

φ2

0

0

φ1

1

0

0

φ1

Part I / Distributions and Bayesian principles

1

0

0

φ1

1

0

0

φ1

1

0

12

Dirichlet distribution
A Dirichlet is speciﬁed by concentration parameters:
α = (α1, . . . , αK ), αz ≥ 0

Part I / Distributions and Bayesian principles

13

Dirichlet distribution
A Dirichlet is speciﬁed by concentration parameters:
α = (α1, . . . , αK ), αz ≥ 0
Mean:

α
α
P 1 ,..., P n
z αz
z αz

Variance: larger αs → smaller variance

Part I / Distributions and Bayesian principles

13

Dirichlet distribution
A Dirichlet is speciﬁed by concentration parameters:
α = (α1, . . . , αK ), αz ≥ 0
Mean:

α
α
P 1 ,..., P n
z αz
z αz

Variance: larger αs → smaller variance
A Dirichlet draw φ is written φ ∼ Dirichlet(α),
which means p(φ | α) ∝ φ1α1−1 · · · φK αK −1

Part I / Distributions and Bayesian principles

13

Dirichlet distribution
A Dirichlet is speciﬁed by concentration parameters:
α = (α1, . . . , αK ), αz ≥ 0
Mean:

α
α
P 1 ,..., P n
z αz
z αz

Variance: larger αs → smaller variance
A Dirichlet draw φ is written φ ∼ Dirichlet(α),
which means p(φ | α) ∝ φ1α1−1 · · · φK αK −1
The Dirichlet distribution assigns probability mass to multinomials
that are likely to yield pseudocounts (α1 − 1, . . . , αK − 1).

Part I / Distributions and Bayesian principles

13

Dirichlet distribution
A Dirichlet is speciﬁed by concentration parameters:
α = (α1, . . . , αK ), αz ≥ 0
Mean:

α
α
P 1 ,..., P n
z αz
z αz

Variance: larger αs → smaller variance
A Dirichlet draw φ is written φ ∼ Dirichlet(α),
which means p(φ | α) ∝ φ1α1−1 · · · φK αK −1
The Dirichlet distribution assigns probability mass to multinomials
that are likely to yield pseudocounts (α1 − 1, . . . , αK − 1).
Mode:

Part I / Distributions and Bayesian principles

P α1 −1 , . . . , P αn −1
z (αz −1)
z (αz −1)

13

Draws from Dirichlet distributions

Dirichlet(1,1,1)
1

φ2
0

0

φ1

1

Part I / Distributions and Bayesian principles

14

Draws from Dirichlet distributions

Dirichlet(1,1,1)
1

φ2
0

0

φ1

1

Dirichlet(5,10,8)
1

φ2
0

0

φ1

1

Part I / Distributions and Bayesian principles

14

Draws from Dirichlet distributions
Dirichlet(.5,.5,.5)
1

φ2
0

0

φ1

1

Dirichlet(1,1,1)
1

φ2
0

0

φ1

1

Dirichlet(5,10,8)
1

φ2
0

0

φ1

1

Part I / Distributions and Bayesian principles

14

The Bayesian paradigm
Bayesian inference

data x

q(θ)

What is q(θ)?

Part I / Distributions and Bayesian principles

15

The Bayesian paradigm
Bayesian inference

data x

q(θ)

What is q(θ)?
def

q(θ) = p(θ | x) =
posterior

Part I / Distributions and Bayesian principles

1
p(x)

p(θ) p(x | θ)
prior

likelihood

15

Posterior updating for Dirichlet distributions
Model:
φ ∼ Dirichlet3(0.5, 0.5, 0.5)
α
x ∼ Multinomial7(φ)
3

Part I / Distributions and Bayesian principles

16

Posterior updating for Dirichlet distributions
Model:
φ ∼ Dirichlet3(0.5, 0.5, 0.5)
α
x ∼ Multinomial7(φ)
3
1

Prior: p(φ)

∝ φA0.5−1 φB 0.5−1 φC 0.5−1

φB
0

Part I / Distributions and Bayesian principles

0

φA

1

16

Posterior updating for Dirichlet distributions
Model:
φ ∼ Dirichlet3(0.5, 0.5, 0.5)
α
x ∼ Multinomial7(φ)
3
1

Prior: p(φ)

∝ φA0.5−1 φB 0.5−1 φC 0.5−1

φB
0

Likelihood: p(x | φ) = φA2

Part I / Distributions and Bayesian principles

φB 4

φC 1

0

φA

1

A B B C A B B

16

Posterior updating for Dirichlet distributions
Model:
φ ∼ Dirichlet3(0.5, 0.5, 0.5)
α
x ∼ Multinomial7(φ)
3
1

Prior: p(φ)

∝ φA0.5−1 φB 0.5−1 φC 0.5−1

φB
0

Likelihood: p(x | φ) = φA2

φB 4

φC 1

0

φA

1

A B B C A B B
1

Posterior: p(φ | x) ∝ φA2.5−1 φB 4.5−1 φC 1.5−1

φB
0

0

φA

1

Result: p(φ | x) = Dirichlet(2.5, 4.5, 1.5)
Part I / Distributions and Bayesian principles

16

A two-component mixture model
φ1 ∼ Dirichlet(1, 1)
A

B

A

B

φ2 ∼ Dirichlet(1, 1)

Part I / Distributions and Bayesian principles

17

A two-component mixture model
φ1 ∼ Dirichlet(1, 1)
A

B

A

B

φ2 ∼ Dirichlet(1, 1)
zi ∼ Multinomial( 1 , 1 )
2 2

Part I / Distributions and Bayesian principles

2

17

A two-component mixture model
φ1 ∼ Dirichlet(1, 1)
A

B

A

B

φ2 ∼ Dirichlet(1, 1)
zi ∼ Multinomial( 1 , 1 )
2 2
xi ∼ Multinomial5(φzi )

Part I / Distributions and Bayesian principles

2
A

A

B

B

A

17

A two-component mixture model
Observed data:

φ1 ∼ Dirichlet(1, 1)
A

x1 =
x2 =
x3 =

B

φ2 ∼ Dirichlet(1, 1)
A

zi ∼ Multinomial( 1 , 1 )
2 2
xi ∼ Multinomial5(φzi )

Part I / Distributions and Bayesian principles

B

A

B

B

B

B

A

B

B

B

B

B

A

A

A

A

2
A

A

B

B

A

17

A two-component mixture model
Observed data:

φ1 ∼ Dirichlet(1, 1)
A

x1 =
x2 =
x3 =

B

φ2 ∼ Dirichlet(1, 1)
A

zi ∼ Multinomial( 1 , 1 )
2 2
xi ∼ Multinomial5(φzi )

Part I / Distributions and Bayesian principles

B

2
A

A

B

A

B

B

B

B

A

B

B

B

B

B

A

A

A

A

Unknown parameters:
B

A

θ = (φ1, φ2)

17

A two-component mixture model
Observed data:

φ1 ∼ Dirichlet(1, 1)
A

x1 =
x2 =
x3 =

B

φ2 ∼ Dirichlet(1, 1)
A

zi ∼ Multinomial( 1 , 1 )
2 2
xi ∼ Multinomial5(φzi )

B

2
A

A

B

A

B

B

B

B

A

B

B

B

B

B

A

A

A

A

Unknown parameters:
B

A

θ = (φ1, φ2)

p(θ | x)
1.0
0.8
0.6

φ2

0.4
0.2
0.0
0.0

0.2

0.4

0.6

0.8

1.0

φ1
Part I / Distributions and Bayesian principles

17

A two-component mixture model
Observed data:

φ1 ∼ Dirichlet(1, 1)
A

x1 =
x2 =
x3 =

B

φ2 ∼ Dirichlet(1, 1)
A

zi ∼ Multinomial( 1 , 1 )
2 2
xi ∼ Multinomial5(φzi )

B

2
A

A

B

A

B

B

B

B

A

B

B

B

B

B

A

A

A

A

Unknown parameters:
B

A

θ = (φ1, φ2)

p(θ | x)
1.0

• True posterior p(θ | x) has symmetries
• φ1 explains x1, x2 and φ2 explains x3
in upper mode (or vice-versa in lower mode)
• The component explaining x3
has higher uncertainty

0.8
0.6

φ2

0.4
0.2
0.0
0.0

0.2

0.4

0.6

0.8

1.0

φ1
Part I / Distributions and Bayesian principles

17

Using the posterior for prediction
Setup:
Assume a joint probabilistic model p(θ)p(x, y | θ)
over input x, output y, parameters θ.
Training examples: (x1, y1), . . . , (xn, yn)
Test input: xnew

Part I / Distributions and Bayesian principles

18

Using the posterior for prediction
Setup:
Assume a joint probabilistic model p(θ)p(x, y | θ)
over input x, output y, parameters θ.
Training examples: (x1, y1), . . . , (xn, yn)
Test input: xnew
Traditional: ynew∗ = argmaxynew p(ynew | xnew, θ)

Part I / Distributions and Bayesian principles

18

Using the posterior for prediction
Setup:
Assume a joint probabilistic model p(θ)p(x, y | θ)
over input x, output y, parameters θ.
Training examples: (x1, y1), . . . , (xn, yn)
Test input: xnew
Traditional: ynew∗ = argmaxynew p(ynew | xnew, θ)
Bayes-optimal prediction:
ynew∗ = argmaxynew p(ynew | xnew, {(xi, yi)})

Part I / Distributions and Bayesian principles

18

Using the posterior for prediction
Setup:
Assume a joint probabilistic model p(θ)p(x, y | θ)
over input x, output y, parameters θ.
Training examples: (x1, y1), . . . , (xn, yn)
Test input: xnew
Traditional: ynew∗ = argmaxynew p(ynew | xnew, θ)
Bayes-optimal prediction:
ynew∗ = argmaxynew p(ynew | xnew, {(xi, yi)})
Explicitly write out the integrated parameters:
p(ynew | xnew, {(xi, yi)}) =

p(ynew | xnew, θ) p(θ | {(xi, yi)}) dθ
posterior

Part I / Distributions and Bayesian principles

18

Using the posterior for prediction
Setup:
Assume a joint probabilistic model p(θ)p(x, y | θ)
over input x, output y, parameters θ.
Training examples: (x1, y1), . . . , (xn, yn)
Test input: xnew
Traditional: ynew∗ = argmaxynew p(ynew | xnew, θ)
Bayes-optimal prediction:
ynew∗ = argmaxynew p(ynew | xnew, {(xi, yi)})
Explicitly write out the integrated parameters:
p(ynew | xnew, {(xi, yi)}) =

p(ynew | xnew, θ) p(θ | {(xi, yi)}) dθ
posterior

We can plug in an approximate posterior:
p(ynew | xnew, {(xi, yi)}) =
Part I / Distributions and Bayesian principles

p(ynew | xnew, θ)qapprox(θ)dθ
18

Roadmap
• Part I
– Distributions and Bayesian principles
– Variational Bayesian inference
– Mean-ﬁeld for mixture models
• Part II
– Emulating DP-like qualities with ﬁnite mixtures
– DP mixture model
– Other views of the DP
• Part III
– Structured models
– Survey of related methods
– Survey of applications

Part I / Variational Bayesian inference

19

Variational Bayesian inference
Goal of Bayesian inference: compute posterior p(θ, z | x)

Part I / Variational Bayesian inference

20

Variational Bayesian inference
Goal of Bayesian inference: compute posterior p(θ, z | x)
Variational inference is a framework for approximating the:
true posterior with the best from a set of distributions Q:

Part I / Variational Bayesian inference

20

Variational Bayesian inference
Goal of Bayesian inference: compute posterior p(θ, z | x)
Variational inference is a framework for approximating the:
true posterior with the best from a set of distributions Q:
q ∗ = argminq∈Q KL(q(θ, z)||p(θ, z | x))
Q
q ∗(θ, z)

Part I / Variational Bayesian inference

p(θ, z | x)

20

Types of variational approximations
∗

q = argmin KL(q(θ, z)||p(θ, z | x))
q∈Q

What types of Q can we consider?
q(θ, z)

Part I / Variational Bayesian inference

=

q(θ)

q(z)

21

Types of variational approximations
∗

q = argmin KL(q(θ, z)||p(θ, z | x))
q∈Q

What types of Q can we consider?
q(θ, z)

=

q(θ)

q(z)

Hard EM

Part I / Variational Bayesian inference

21

Types of variational approximations
∗

q = argmin KL(q(θ, z)||p(θ, z | x))
q∈Q

What types of Q can we consider?
q(θ, z)

=

q(θ)

q(z)

Hard EM
EM

Part I / Variational Bayesian inference

21

Types of variational approximations
∗

q = argmin KL(q(θ, z)||p(θ, z | x))
q∈Q

What types of Q can we consider?
q(θ, z)

=

q(θ)

q(z)

Hard EM
EM
mean-ﬁeld
Part I / Variational Bayesian inference

21

Heuristic derivation of mean-ﬁeld
Algorithm template: iterate between the E-step and M-step.
Let us ﬁrst heuristically derive the mean-ﬁeld algorithm.
E-step
M-step

Part I / Variational Bayesian inference

22

Heuristic derivation of mean-ﬁeld
Algorithm template: iterate between the E-step and M-step.
Let us ﬁrst heuristically derive the mean-ﬁeld algorithm.
E-step
M-step
Find best z
z∗ = argmaxz p(z | θ ∗, x)

Part I / Variational Bayesian inference

22

Heuristic derivation of mean-ﬁeld
Algorithm template: iterate between the E-step and M-step.
Let us ﬁrst heuristically derive the mean-ﬁeld algorithm.
E-step
M-step
Find best z
Find best θ
z∗ = argmaxz p(z | θ ∗, x)
θ ∗ = argmaxθ p(θ | z∗, x)

Part I / Variational Bayesian inference

22

Heuristic derivation of mean-ﬁeld
Algorithm template: iterate between the E-step and M-step.
Let us ﬁrst heuristically derive the mean-ﬁeld algorithm.
E-step
M-step
Find best z
Find best θ
z∗ = argmaxz p(z | θ ∗, x)
θ ∗ = argmaxθ p(θ | z∗, x)
Find best distrib. over z
q(z)∗ ∝ p(z | θ, x)

Part I / Variational Bayesian inference

22

Heuristic derivation of mean-ﬁeld
Algorithm template: iterate between the E-step and M-step.
Let us ﬁrst heuristically derive the mean-ﬁeld algorithm.
E-step
M-step
Find best z
Find best θ
z∗ = argmaxz p(z | θ ∗, x)
θ ∗ = argmaxθ p(θ | z∗, x)
Find best distrib. over z
q(z)∗ ∝ p(z | θ, x)

Part I / Variational Bayesian inference

Take distrib. into account
θ ∗ = argmaxθ z q(z)p(θ | z, x)

22

Heuristic derivation of mean-ﬁeld
Algorithm template: iterate between the E-step and M-step.
Let us ﬁrst heuristically derive the mean-ﬁeld algorithm.
E-step
M-step
Find best z
Find best θ
z∗ = argmaxz p(z | θ ∗, x)
θ ∗ = argmaxθ p(θ | z∗, x)
Find best distrib. over z
q(z)∗ ∝ p(z | θ, x)

Part I / Variational Bayesian inference

Take distrib. into account
θ ∗ = argmaxθ z q(z)p(θ | z, x)
θ ∗ = argmaxθ z p(θ | z, x)q(z)

22

Heuristic derivation of mean-ﬁeld
Algorithm template: iterate between the E-step and M-step.
Let us ﬁrst heuristically derive the mean-ﬁeld algorithm.
E-step
M-step
Find best z
Find best θ
z∗ = argmaxz p(z | θ ∗, x)
θ ∗ = argmaxθ p(θ | z∗, x)
Find best distrib. over z
q(z)∗ ∝ p(z | θ, x)

Take distrib. into account
θ ∗ = argmaxθ z q(z)p(θ | z, x)
θ ∗ = argmaxθ z p(θ | z, x)q(z)
Find best distrib. over θ
q(θ)∗ ∝ z p(θ | z, x)q(z)

Part I / Variational Bayesian inference

22

Heuristic derivation of mean-ﬁeld
Algorithm template: iterate between the E-step and M-step.
Let us ﬁrst heuristically derive the mean-ﬁeld algorithm.
E-step
M-step
Find best z
Find best θ
z∗ = argmaxz p(z | θ ∗, x)
θ ∗ = argmaxθ p(θ | z∗, x)
Find best distrib. over z
q(z)∗ ∝ p(z | θ, x)

Take distrib. into account
θ ∗ = argmaxθ z q(z)p(θ | z, x)
θ ∗ = argmaxθ z p(θ | z, x)q(z)

Take distrib. into account
q(z)∗ ∝ θ p(z | θ, x)q(θ )

Find best distrib. over θ
q(θ)∗ ∝ z p(θ | z, x)q(z)

Part I / Variational Bayesian inference

22

Heuristic derivation of mean-ﬁeld
Algorithm template: iterate between the E-step and M-step.
Let us ﬁrst heuristically derive the mean-ﬁeld algorithm.
E-step
M-step
Find best z
Find best θ
z∗ = argmaxz p(z | θ ∗, x)
θ ∗ = argmaxθ p(θ | z∗, x)
Find best distrib. over z
q(z)∗ ∝ p(z | θ, x)

Take distrib. into account
θ ∗ = argmaxθ z q(z)p(θ | z, x)
θ ∗ = argmaxθ z p(θ | z, x)q(z)

Take distrib. into account
q(z)∗ ∝ θ p(z | θ, x)q(θ )

Find best distrib. over θ
q(θ)∗ ∝ z p(θ | z, x)q(z)

Inﬁnite product over θ has closed form if q(θ) is a Dirichlet.

Part I / Variational Bayesian inference

22

Formal derivation of mean-ﬁeld
q ∗ = argmin KL(q(θ, z)||p(θ, z | x))
q∈Q

Steps:
1. Formulate as an optimization problem (variational
principle)
2. Relax the optimization problem (e.g., mean-ﬁeld)
3. Solve using coordinate-wise descent

Part I / Variational Bayesian inference

23

Kullback-Leibler divergence
KL measures how diﬀerent two distributions p and q are.
Deﬁnition:
def

q(θ)
KL(q||p) = Eq log p(θ)

Part I / Variational Bayesian inference

24

Kullback-Leibler divergence
KL measures how diﬀerent two distributions p and q are.
Deﬁnition:
def

q(θ)
KL(q||p) = Eq log p(θ)

An important property:
KL(q||p) ≥ 0
KL(q||p) = 0 if and only if q = p

Part I / Variational Bayesian inference

24

Kullback-Leibler divergence
KL measures how diﬀerent two distributions p and q are.
Deﬁnition:
def

q(θ)
KL(q||p) = Eq log p(θ)

An important property:
KL(q||p) ≥ 0
KL(q||p) = 0 if and only if q = p
KL is asymmetric:
Assuming KL(q||p) < ∞,
p(θ) = 0 ⇒ q(θ) = 0 [q“ ⊂ ”p]

Part I / Variational Bayesian inference

24

Minimizing with respect to KL
Since KL = 0 exactly when two arguments are equal, we have:
p = argminq∈Qall KL(q||p) and p = argminq∈Qall KL(p||q)
where Qall is all possible distributions.

Part I / Variational Bayesian inference

25

Minimizing with respect to KL
Since KL = 0 exactly when two arguments are equal, we have:
p = argminq∈Qall KL(q||p) and p = argminq∈Qall KL(p||q)
where Qall is all possible distributions.
Asymmetry revealed when we replace Qall with Q

Part I / Variational Bayesian inference

Qall.

25

Minimizing with respect to KL
Since KL = 0 exactly when two arguments are equal, we have:
p = argminq∈Qall KL(q||p) and p = argminq∈Qall KL(p||q)
where Qall is all possible distributions.
Asymmetry revealed when we replace Qall with Q

Qall.

0.3

Let p = 0.1

and Q = all possible Gaussians

p

9.8

Part I / Variational Bayesian inference

19.6

25

Minimizing with respect to KL
Since KL = 0 exactly when two arguments are equal, we have:
p = argminq∈Qall KL(q||p) and p = argminq∈Qall KL(p||q)
where Qall is all possible distributions.
Asymmetry revealed when we replace Qall with Q

Qall.

0.3

Let p = 0.1

and Q = all possible Gaussians

p

9.8

19.6
0.4
p
q1

q1 = argmin KL(q||p) (mode-ﬁnding): 0.2
q∈Q
9.8

Part I / Variational Bayesian inference

19.6

25

Minimizing with respect to KL
Since KL = 0 exactly when two arguments are equal, we have:
p = argminq∈Qall KL(q||p) and p = argminq∈Qall KL(p||q)
where Qall is all possible distributions.
Asymmetry revealed when we replace Qall with Q

Qall.

0.3

Let p = 0.1

and Q = all possible Gaussians

p

9.8

19.6
0.4
p
q1

q1 = argmin KL(q||p) (mode-ﬁnding): 0.2
q∈Q
9.8

19.6

0.3
p
q2

q2 = argmin KL(p||q) (mean-ﬁnding): 0.1
q∈Q
9.8

Part I / Variational Bayesian inference

19.6

25

Step 1: formulate as optimization problem
Variational principle: write the quantity we wish to compute
as the solution of an optimization problem:
∗ def

q = argmin KL(q(θ) || p(θ | x)),
q∈Qall

where Qall is set of all distributions over parameters.

Part I / Variational Bayesian inference

26

Step 1: formulate as optimization problem
Variational principle: write the quantity we wish to compute
as the solution of an optimization problem:
∗ def

q = argmin KL(q(θ) || p(θ | x)),
q∈Qall

where Qall is set of all distributions over parameters.
Solution: q ∗(θ) = p(θ | x), which achieves KL = 0.
This is not very useful yet because q ∗ is just as hard to
compute...

Part I / Variational Bayesian inference

26

Step 1: formulate as optimization problem
Variational principle: write the quantity we wish to compute
as the solution of an optimization problem:
∗ def

q = argmin KL(q(θ) || p(θ | x)),
q∈Qall

where Qall is set of all distributions over parameters.
Solution: q ∗(θ) = p(θ | x), which achieves KL = 0.
This is not very useful yet because q ∗ is just as hard to
compute...
Normalization of p not needed:
argminq KL(q(θ)||p(θ | x)) = argminq KL(q(θ)||p(θ, x))
Part I / Variational Bayesian inference

26

Step 2: relax the optimization problem
p(θ | x)
1.0
0.8

∗ def

q = argmin KL(q(θ) || p(θ | x))

φ2

q∈Qall

0.6
0.4
0.2
0.0
0.0 0.2 0.4 0.6 0.8 1.0

Qall

φ1
Optimal q ∗(θ)
1.0
0.8

p

0.6

φ2

Qall = all distributions

0.4
0.2
0.0
0.0

Part I / Variational Bayesian inference

0.2

0.4

0.6

φ1

0.8

1.0

27

Step 2: relax the optimization problem
p(θ | x)
1.0
0.8

∗ def

q = argmin KL(q(θ) || p(θ | x))

φ2

q∈Qdeg

0.6
0.4
0.2
0.0
0.0 0.2 0.4 0.6 0.8 1.0

Qall

φ1

Qdeg

Degenerate q ∗(θ)

q∗

1.0
0.8

p

0.6

φ2

Qdeg = q : q(θ) = δθ ∗ (θ)

0.4
0.2
0.0
0.0

Part I / Variational Bayesian inference

0.2

0.4

0.6

φ1

0.8

1.0

27

Step 2: relax the optimization problem
p(θ | x)
1.0
0.8

∗ def

q = argmin KL(q(θ) || p(θ | x))

φ2

q∈Qmf

0.6
0.4
0.2
0.0
0.0 0.2 0.4 0.6 0.8 1.0

Qall
Qdeg

φ1

Qmf

Mean-ﬁeld q ∗(θ)
1.0

q

∗

0.8

p

0.6

φ2

n

Qmf = q : q(θ) =

qi(θi)
i=1

0.4
0.2
0.0
0.0

Part I / Variational Bayesian inference

0.2

0.4

0.6

φ1

0.8

1.0

27

Step 3: coordinate-wise descent
Goal: minimize KL(q(θ)||p(θ | x)) subject to q ∈ Q
Assume: q(θ) =

i qi (θi )

Part I / Variational Bayesian inference

28

Step 3: coordinate-wise descent
Goal: minimize KL(q(θ)||p(θ | x)) subject to q ∈ Q
Assume: q(θ) =

i qi (θi )

Algorithm: for each i, optimize qi(θi) holding all other
coordinates q−i(θ−i) ﬁxed.

Part I / Variational Bayesian inference

28

Step 3: coordinate-wise descent
Goal: minimize KL(q(θ)||p(θ | x)) subject to q ∈ Q
Assume: q(θ) =

i qi (θi )

Algorithm: for each i, optimize qi(θi) holding all other
coordinates q−i(θ−i) ﬁxed.
∗
If qis degenerate (qi(θi) = δθi (θi)):

∗
∗
θi = argmaxθi p(θi | θ−i, x)

Part I / Variational Bayesian inference

28

Step 3: coordinate-wise descent
Goal: minimize KL(q(θ)||p(θ | x)) subject to q ∈ Q
Assume: q(θ) =

i qi (θi )

Algorithm: for each i, optimize qi(θi) holding all other
coordinates q−i(θ−i) ﬁxed.
∗
If qis degenerate (qi(θi) = δθi (θi)):

∗
∗
θi = argmaxθi p(θi | θ−i, x)

If qis non-degenerate:
qi(θi) ∝ exp{Eq−i log p(θi | θ−i, x)}

Part I / Variational Bayesian inference

28

Mean-ﬁeld recipe
qi(θi) ∝ exp{Eq−i log p(θi | θ−i, x)}
∝ exp{Eq−i log p(θ, x)}

Part I / Variational Bayesian inference

29

Mean-ﬁeld recipe
qi(θi) ∝ exp{Eq−i log p(θi | θ−i, x)}
∝ exp{Eq−i log p(θ, x)}
Recipe for updating qi:
1. Write down all the unobserved variables in the model
(including parameters and latent variables)
2. For each variable θi:
a. Write down only the factors of the joint distribution
that contain θi
b. Set qi ∝ exp{Eq−i log(those factors)}

Part I / Variational Bayesian inference

29

Non-Bayesian variational inference
···
···

• The E-step (computing p(z | x)) is sometimes
intractable when there are long-range dependencies

Part I / Variational Bayesian inference

30

Non-Bayesian variational inference
···
···

• The E-step (computing p(z | x)) is sometimes
intractable when there are long-range dependencies
• Variational EM: approximate E-step using
argminq KL(q(z)||p(z | x))

Part I / Variational Bayesian inference

30

Non-Bayesian variational inference
···
···

• The E-step (computing p(z | x)) is sometimes
intractable when there are long-range dependencies
• Variational EM: approximate E-step using
argminq KL(q(z)||p(z | x))
• Variational EM ﬁts into the general variational
framework: one true posterior, various approximating
families

Part I / Variational Bayesian inference

30

Roadmap
• Part I
– Distributions and Bayesian principles
– Variational Bayesian inference
– Mean-ﬁeld for mixture models
• Part II
– Emulating DP-like qualities with ﬁnite mixtures
– DP mixture model
– Other views of the DP
• Part III
– Structured models
– Survey of related methods
– Survey of applications

Part I / Mean-ﬁeld for mixture models

31

Bayesian ﬁnite mixture model
β
zi

φz
z

xi
i

Parameters: θ = (β, φ) = (β1, . . . , βK , φ1, . . . , φK )
Hidden variables: z = (z1, . . . , zn)
Observed data: x = (x1, . . . , xn)
K

K

p(θ, z, x) = p(β)

n

p(zi | β)p(xi | φzi )

p(φz )
z=1

i=1

n

Part I / Mean-ﬁeld for mixture models

32

Bayesian ﬁnite mixture model
β
zi

φz
z

xi
i

Parameters: θ = (β, φ) = (β1, . . . , βK , φ1, . . . , φK )
Hidden variables: z = (z1, . . . , zn)
Observed data: x = (x1, . . . , xn)
K

K

p(θ, z, x) = p(β)

n

p(zi | β)p(xi | φzi )

p(φz )
z=1

i=1

n

β ∼ DirichletK (α , . . . , α )
For each component z ∈ {1, . . . , K}:
φz ∼ G0
For each data point i ∈ {1, . . . , n}:
zi ∼ Multinomial(β)
xi ∼ F (φzi )
Part I / Mean-ﬁeld for mixture models

Prior over component parameters:
G0 (e.g., DirichletV (α , . . . , α ))
Data model:
F (e.g., Multinomial)

32

Bayesian ﬁnite mixture model
Running example: document clustering

Part I / Mean-ﬁeld for mixture models

33

Bayesian ﬁnite mixture model
Running example: document clustering

β ∼ DirichletK (α , . . . , α ) [draw component probabilities]

Part I / Mean-ﬁeld for mixture models

33

Bayesian ﬁnite mixture model
Running example: document clustering

β ∼ DirichletK (α , . . . , α ) [draw component probabilities]
K = 2 clusters

β:
1

Part I / Mean-ﬁeld for mixture models

2

33

Bayesian ﬁnite mixture model
Running example: document clustering

β ∼ DirichletK (α , . . . , α ) [draw component probabilities]
K = 2 clusters

β:
1

2

For each component (cluster) z ∈ {1, . . . , K}:
φz ∼ DirichletV (α , . . . , α ) [draw component parameter]

Part I / Mean-ﬁeld for mixture models

33

Bayesian ﬁnite mixture model
Running example: document clustering

β ∼ DirichletK (α , . . . , α ) [draw component probabilities]
K = 2 clusters

β:
1

2

For each component (cluster) z ∈ {1, . . . , K}:
φz ∼ DirichletV (α , . . . , α ) [draw component parameter]
V = 3 word types

φ1:

φ2:
A

Part I / Mean-ﬁeld for mixture models

B

C

A

B

C

33

Bayesian ﬁnite mixture model
Running example: document clustering

β ∼ DirichletK (α , . . . , α ) [draw component probabilities]
K = 2 clusters

β:
1

2

For each component (cluster) z ∈ {1, . . . , K}:
φz ∼ DirichletV (α , . . . , α ) [draw component parameter]
V = 3 word types

φ1:

φ2:
A

B

C

A

B

C

For each data point (document) i ∈ {1, . . . , n}:
zi ∼ MultinomialK (β) [choose component]
xi ∼ Multinomialm(φzi ) [generate data]
V

Part I / Mean-ﬁeld for mixture models

33

Bayesian ﬁnite mixture model
Running example: document clustering

β ∼ DirichletK (α , . . . , α ) [draw component probabilities]
K = 2 clusters

β:
1

2

For each component (cluster) z ∈ {1, . . . , K}:
φz ∼ DirichletV (α , . . . , α ) [draw component parameter]
V = 3 word types

φ1:

φ2:
A

B

C

A

B

C

For each data point (document) i ∈ {1, . . . , n}:
zi ∼ MultinomialK (β) [choose component]
xi ∼ Multinomialm(φzi ) [generate data]
V
n = 5 documents
m = 4 words/doc
Part I / Mean-ﬁeld for mixture models

z1 : 2 z2 : 2 z3 : 1 z4 : 2 z5 : 1
x1: C C A C x2: C C A C x3: A A B B x4: C A C C x5: A C B B
33

Hard EM for the ﬁnite mixture model
K

Model: p(θ, z, x) = p(β)

p(zi | β)p(xi | φzi )

p(φz )
z=1

Part I / Mean-ﬁeld for mixture models

n

i=1

34

Hard EM for the ﬁnite mixture model
K

Model: p(θ, z, x) = p(β)

n

z=1

Approximation: q(θ, z) = q(β)
δβ ∗ ( β )

Part I / Mean-ﬁeld for mixture models

p(zi | β)p(xi | φzi )

p(φz )
i=1
K
z=1

q(φz )
δφ∗ (φz )
z

n
i=1

q(zi)
δz ∗ (zi )
i

34

Hard EM for the ﬁnite mixture model
K

Model: p(θ, z, x) = p(β)

n

p(zi | β)p(xi | φzi )

p(φz )
z=1

Approximation: q(θ, z) = q(β)
δβ ∗ ( β )

i=1
K
z=1

q(φz )
δφ∗ (φz )
z

n
i=1

q(zi)
δz ∗ (zi )
i

E-step:
For each data point i ∈ {1, . . . , n}:
zi∗ = argmaxzi p(zi | β ∗, x) = argmaxzi p(zi | β ∗)p(xi | φzi )

Part I / Mean-ﬁeld for mixture models

34

Hard EM for the ﬁnite mixture model
K

Model: p(θ, z, x) = p(β)

n

p(zi | β)p(xi | φzi )

p(φz )
z=1

Approximation: q(θ, z) = q(β)
δβ ∗ ( β )

i=1
K
z=1

q(φz )

n
i=1

δφ∗ (φz )
z

q(zi)
δz ∗ (zi )
i

E-step:
For each data point i ∈ {1, . . . , n}:
zi∗ = argmaxzi p(zi | β ∗, x) = argmaxzi p(zi | β ∗)p(xi | φzi )
M-step:
β ∗ = argmaxβ p(β | z∗, x) = argmaxβ p(β)

Part I / Mean-ﬁeld for mixture models

n
∗
p(zi
i=1

| β)

34

Hard EM for the ﬁnite mixture model
K

n

Model: p(θ, z, x) = p(β)

p(zi | β)p(xi | φzi )

p(φz )
z=1

i=1

Approximation: q(θ, z) = q(β)

K
z=1

δβ ∗ ( β )

q(φz )

n
i=1

δφ∗ (φz )
z

q(zi)
δz ∗ (zi )
i

E-step:
For each data point i ∈ {1, . . . , n}:
zi∗ = argmaxzi p(zi | β ∗, x) = argmaxzi p(zi | β ∗)p(xi | φzi )
M-step:
β ∗ = argmaxβ p(β | z∗, x) = argmaxβ p(β)

n
∗
p(zi
i=1

| β)

For each component z ∈ {1, . . . , K}:
φ∗
z

= argmaxφ p(φ)

Part I / Mean-ﬁeld for mixture models

n
i=1 p(xi

∗
1[zi =z]

| φ)

34

Mean-ﬁeld for the ﬁnite mixture model
K

Model: p(θ, z, x) = p(β)

n

p(zi | β)p(xi | φzi )

p(φz )
z=1

i=1

M-step: optimize q(φz ) and q(β)

Part I / Mean-ﬁeld for mixture models

35

Mean-ﬁeld for the ﬁnite mixture model
K

Model: p(θ, z, x) = p(β)

n

p(zi | β)p(xi | φzi )

p(φz )
z=1

i=1

M-step: optimize q(φz ) and q(β)
p(β | z, x)q(z)

q(β) ∝
z

Part I / Mean-ﬁeld for mixture models

35

Mean-ﬁeld for the ﬁnite mixture model
K

Model: p(θ, z, x) = p(β)

n

p(zi | β)p(xi | φzi )

p(φz )
z=1

i=1

M-step: optimize q(φz ) and q(β)
p(β | z, x)q(z)

q(β) ∝
z

n

K

p(zi | β)q(zi)

∝ p(β)
i=1 zi =1

Part I / Mean-ﬁeld for mixture models

35

Mean-ﬁeld for the ﬁnite mixture model
K

Model: p(θ, z, x) = p(β)

n

p(zi | β)p(xi | φzi )

p(φz )
z=1

i=1

M-step: optimize q(φz ) and q(β)
p(β | z, x)q(z)

q(β) ∝
z

n

K

K

p(zi | β)q(zi) = p(β)

∝ p(β)
i=1 zi =1

Part I / Mean-ﬁeld for mixture models

βzi

Pn

i=1 q(zi )

zi =1

35

Mean-ﬁeld for the ﬁnite mixture model
K

Model: p(θ, z, x) = p(β)

n

p(zi | β)p(xi | φzi )

p(φz )
z=1

i=1

M-step: optimize q(φz ) and q(β)
p(β | z, x)q(z)

q(β) ∝

n

z

K

K

p(zi | β)q(zi) = p(β)

∝ p(β)

βzi

Pn

i=1 q(zi )

i=1 zi =1

Deﬁne expected
Recall the prior:

zi =1
n
counts of component z: Cz = i=1 qzi (z)
k
p(β) = Dirichlet(β; α) ∝ z=1 βz α−1

Part I / Mean-ﬁeld for mixture models

35

Mean-ﬁeld for the ﬁnite mixture model
K

Model: p(θ, z, x) = p(β)

n

p(zi | β)p(xi | φzi )

p(φz )
z=1

i=1

M-step: optimize q(φz ) and q(β)
p(β | z, x)q(z)

q(β) ∝

n

z

K

K

p(zi | β)q(zi) = p(β)

∝ p(β)

βzi

Pn

i=1 q(zi )

i=1 zi =1

Deﬁne expected
Recall the prior:

zi =1
n
counts of component z: Cz = i=1 qzi (z)
k
p(β) = Dirichlet(β; α) ∝ z=1 βz α−1
Prior: K βz α−1
z=1

Part I / Mean-ﬁeld for mixture models

35

Mean-ﬁeld for the ﬁnite mixture model
K

Model: p(θ, z, x) = p(β)

n

p(zi | β)p(xi | φzi )

p(φz )
z=1

i=1

M-step: optimize q(φz ) and q(β)
p(β | z, x)q(z)

q(β) ∝

n

z

K

K

p(zi | β)q(zi) = p(β)

∝ p(β)

βzi

Pn

i=1 q(zi )

i=1 zi =1

Deﬁne expected
Recall the prior:

zi =1
n
counts of component z: Cz = i=1 qzi (z)
k
p(β) = Dirichlet(β; α) ∝ z=1 βz α−1
Prior: K βz α−1
z=1
“Likelihood”: K βz Cz
z=1

Part I / Mean-ﬁeld for mixture models

35

Mean-ﬁeld for the ﬁnite mixture model
K

Model: p(θ, z, x) = p(β)

n

p(zi | β)p(xi | φzi )

p(φz )
z=1

i=1

M-step: optimize q(φz ) and q(β)
p(β | z, x)q(z)

q(β) ∝

n

z

K

K

p(zi | β)q(zi) = p(β)

∝ p(β)

βzi

Pn

i=1 q(zi )

i=1 zi =1

Deﬁne expected
Recall the prior:

zi =1
n
counts of component z: Cz = i=1 qzi (z)
k
p(β) = Dirichlet(β; α) ∝ z=1 βz α−1
Prior: K βz α−1
z=1
“Likelihood”: K βz Cz
z=1
“Posterior”: K βz α−1+Cz
z=1

Part I / Mean-ﬁeld for mixture models

35

Mean-ﬁeld for the ﬁnite mixture model
K

Model: p(θ, z, x) = p(β)

n

p(zi | β)p(xi | φzi )

p(φz )
z=1

i=1

M-step: optimize q(φz ) and q(β)
p(β | z, x)q(z)

q(β) ∝

n

z

K

K

p(zi | β)q(zi) = p(β)

∝ p(β)

βzi

Pn

i=1 q(zi )

i=1 zi =1

Deﬁne expected
Recall the prior:

zi =1
n
counts of component z: Cz = i=1 qzi (z)
k
p(β) = Dirichlet(β; α) ∝ z=1 βz α−1
Prior: K βz α−1
z=1
“Likelihood”: K βz Cz
z=1
“Posterior”: K βz α−1+Cz
z=1

Conclusion: q(β) = Dirichlet(β; α + C)
Part I / Mean-ﬁeld for mixture models

35

Mean-ﬁeld for the ﬁnite mixture model
K

Model: p(θ, z, x) = p(β)

n

p(zi | β)p(xi | φzi )

p(φz )
z=1

i=1

E-step: optimize q(zi)
p(zi | θ, x)q(θ )

q(zi) ∝
θ

Part I / Mean-ﬁeld for mixture models

36

Mean-ﬁeld for the ﬁnite mixture model
K

Model: p(θ, z, x) = p(β)

n

p(zi | β)p(xi | φzi )

p(φz )
z=1

i=1

E-step: optimize q(zi)
p(zi | θ, x)q(θ )

q(zi) ∝
θ

p(zi | β)q(β)

∝
β

p(xi | φzi )q(φzi )
φzi

W (zi )

Part I / Mean-ﬁeld for mixture models

W (xi |zi )

36

Mean-ﬁeld for the ﬁnite mixture model
K

Model: p(θ, z, x) = p(β)

n

p(zi | β)p(xi | φzi )

p(φz )
z=1

i=1

E-step: optimize q(zi)
p(zi | θ, x)q(θ )

q(zi) ∝
θ

p(zi | β)q(β)

∝
β

p(xi | φzi )q(φzi )
φzi

W (zi )

W (xi |zi )

W (zi)W (xi | zi) is like p(zi)p(xi | zi), but multinomial weights W
takes into account uncertainty in θ.

Part I / Mean-ﬁeld for mixture models

36

Mean-ﬁeld: computing multinomial weights
W (z) =

p(z | β)q(β) = exp{Eq(β) log p(z | β)}
β

Part I / Mean-ﬁeld for mixture models

37

Mean-ﬁeld: computing multinomial weights
W (z) =

p(z | β)q(β) = exp{Eq(β) log p(z | β)}
β

What’s q(β)?
n

E-step: expected counts: Cz = i=1 qzi (z)
M-step: q(β) = Dirichlet(β; α + C )
α

Part I / Mean-ﬁeld for mixture models

37

Mean-ﬁeld: computing multinomial weights
W (z) =

p(z | β)q(β) = exp{Eq(β) log p(z | β)}
β

What’s q(β)?
n

E-step: expected counts: Cz = i=1 qzi (z)
M-step: q(β) = Dirichlet(β; α + C )
α

Mean-ﬁeld multinomial weight:
W (z) =

exp(Ψ(αz ))
exp(Ψ(

K
z =1 αz ))

Compare with EM (q(β) = δβ ∗ (β) is degenerate):
W (z) =

Part I / Mean-ﬁeld for mixture models

β∗
z

=

αz −1
K
z =1 (αz

−1)
37

Mean-ﬁeld: computing multinomial weights
W (z) =

p(z | β)q(β) = exp{Eq(β) log p(z | β)}
β

What’s q(β)?
n

E-step: expected counts: Cz = i=1 qzi (z)
M-step: q(β) = Dirichlet(β; α + C )
α

Mean-ﬁeld multinomial weight:
W (z) =

exp(Ψ(αz ))
exp(Ψ(

K
z =1 αz ))

Compare with EM (q(β) = δβ ∗ (β) is degenerate):
W (z) =

β∗
z

=

αz −1
K
z =1 (αz

−1)

Ψ(·) is the digamma function and is easy to compute.
Part I / Mean-ﬁeld for mixture models

37

Mean-ﬁeld overview
E-step: q(zi) ∝ W (zi)W (xi | zi), W (z) =
M-step: q(β) = Dirichlet(β; α + C), Cz =

Part I / Mean-ﬁeld for mixture models

exp Ψ(αz +Cz )
P
exp Ψ( K=1 αz +Cz )
z
n
i=1 qzi (z)

38

Mean-ﬁeld overview
E-step: q(zi) ∝ W (zi)W (xi | zi), W (z) =
M-step: q(β) = Dirichlet(β; α + C), Cz =

exp Ψ(αz +Cz )
P
exp Ψ( K=1 αz +Cz )
z
n
i=1 qzi (z)

EM
E-step
q(zi)

θ∗

C
M-step

Part I / Mean-ﬁeld for mixture models

38

Mean-ﬁeld overview
E-step: q(zi) ∝ W (zi)W (xi | zi), W (z) =

exp Ψ(αz +Cz )
P
exp Ψ( K=1 αz +Cz )
z
n
i=1 qzi (z)

M-step: q(β) = Dirichlet(β; α + C), Cz =
EM

Mean-ﬁeld

E-step

E-step

θ∗

C
M-step

Part I / Mean-ﬁeld for mixture models

q(zi)

W

C

q(zi)

q(θ)
M-step

38

Roadmap
• Part I
– Distributions and Bayesian principles
– Variational Bayesian inference
– Mean-ﬁeld for mixture models
• Part II
– Emulating DP-like qualities with ﬁnite mixtures
– DP mixture model
– Other views of the DP
• Part III
– Structured models
– Survey of related methods
– Survey of applications

Part II

39

Roadmap
• Part I
– Distributions and Bayesian principles
– Variational Bayesian inference
– Mean-ﬁeld for mixture models
• Part II
– Emulating DP-like qualities with ﬁnite mixtures
– DP mixture model
– Other views of the DP
• Part III
– Structured models
– Survey of related methods
– Survey of applications

Part II / Emulating DP-like qualities with ﬁnite mixtures

40

Interpreting mean-ﬁeld through exp(Ψ(·))
EM with α = 1 (maximum likelihood):
Cz
W (z) = K
z =1 Cz

Part II / Emulating DP-like qualities with ﬁnite mixtures

41

Interpreting mean-ﬁeld through exp(Ψ(·))
EM with α = 1 (maximum likelihood):
Cz
W (z) = K
z =1 Cz

2.0
1.6

x
exp(Ψ(·))

1.2
0.8
0.4

0.4

0.8

1.2

1.6

2.0

x

exp(Ψ(x)) x − 0.5
(for x > 1)

Part II / Emulating DP-like qualities with ﬁnite mixtures

41

Interpreting mean-ﬁeld through exp(Ψ(·))
EM with α = 1 (maximum likelihood):
Cz
W (z) = K
z =1 Cz
Mean-ﬁeld with α = 1:
exp(Ψ(1+Cz ))
W (z) =
K
exp(Ψ(1+ z =1 Cz ))
(

2.0
1.6

x
exp(Ψ(·))

1.2
0.8
0.4

adding 0.5)
0.4

0.8

1.2

1.6

2.0

x

exp(Ψ(x)) x − 0.5
(for x > 1)

Part II / Emulating DP-like qualities with ﬁnite mixtures

41

Interpreting mean-ﬁeld through exp(Ψ(·))
EM with α = 1 (maximum likelihood):
Cz
W (z) = K
z =1 Cz
Mean-ﬁeld with α = 1:
exp(Ψ(1+Cz ))
W (z) =
K
exp(Ψ(1+ z =1 Cz ))
(

1.6

x
exp(Ψ(·))

1.2
0.8
0.4

adding 0.5)

Mean-ﬁeld with α = 0:
exp(Ψ(Cz ))
W (z) =
K
exp(Ψ( z =1 Cz ))
(

2.0

0.4

0.8

1.2

1.6

2.0

x

exp(Ψ(x)) x − 0.5
(for x > 1)

subtracting 0.5)

Part II / Emulating DP-like qualities with ﬁnite mixtures

41

The rich get richer
What happens when α = 0?
E-step: q(zi) ∝ W (zi)W (xi | zi)
M-step: W (z) =

exp(Ψ(Cz ))
PK
exp(Ψ( z =1 Cz ))

Part II / Emulating DP-like qualities with ﬁnite mixtures

n
i=1 q(zi )
C −0.5
PK z
( z =1 Cz )−0.5

Cz =

42

The rich get richer
What happens when α = 0?
E-step: q(zi) ∝ W (zi)W (xi | zi)
M-step: W (z) =

exp(Ψ(Cz ))
PK
exp(Ψ( z =1 Cz ))

n
i=1 q(zi )
C −0.5
PK z
( z =1 Cz )−0.5

Cz =

Eﬀect of mean-ﬁeld with α = 0 on component probabilities β?
Thought experiment: ignore W (xi | zi).

Part II / Emulating DP-like qualities with ﬁnite mixtures

42

The rich get richer
What happens when α = 0?
E-step: q(zi) ∝ W (zi)W (xi | zi)
M-step: W (z) =

exp(Ψ(Cz ))
PK
exp(Ψ( z =1 Cz ))

n
i=1 q(zi )
C −0.5
PK z
( z =1 Cz )−0.5

Cz =

Eﬀect of mean-ﬁeld with α = 0 on component probabilities β?
Thought experiment: ignore W (xi | zi).
10
8

counts Cz

z
z
z
z

6
4
2
10

20

=1
=2
=3
=4

When subtract 0.5,
small counts are hurt
more than large ones
(like a regressive tax)

30

iteration

Part II / Emulating DP-like qualities with ﬁnite mixtures

42

The rich get richer
What happens when α = 0?
E-step: q(zi) ∝ W (zi)W (xi | zi)
M-step: W (z) =

exp(Ψ(Cz ))
PK
exp(Ψ( z =1 Cz ))

n
i=1 q(zi )
C −0.5
PK z
( z =1 Cz )−0.5

Cz =

Eﬀect of mean-ﬁeld with α = 0 on component probabilities β?
Thought experiment: ignore W (xi | zi).
10
8

counts Cz

z
z
z
z

6
4
2
10

20

=1
=2
=3
=4

When subtract 0.5,
small counts are hurt
more than large ones
(like a regressive tax)

30

iteration

The algorithm achieves sparsity by choosing one component.
In general, data term ﬁghts the sparsity prior.
Part II / Emulating DP-like qualities with ﬁnite mixtures

42

The rich get even richer
E-step: q(zi) ∝ W (zi)W (xi | zi)
M-step: W (j | z) =

exp(Ψ(Czj ))
P
exp(Ψ( V =1 Czj ))
j

Part II / Emulating DP-like qualities with ﬁnite mixtures

Czj =
(

n
i=1 qzi (z)xij
Czj −0.5

PV
j =1

Czj )−0.5

43

The rich get even richer
E-step: q(zi) ∝ W (zi)W (xi | zi)
M-step: W (j | z) =

exp(Ψ(Czj ))
P
exp(Ψ( V =1 Czj ))
j

Czj =
(

n
i=1 qzi (z)xij
Czj −0.5

PV
j =1

Czj )−0.5

Thought experiment: ignore W (zi), focus on W (xi | zi).
Suppose C1A = 20, C1B = 20, C2A = 0.5, C2B = 0.2

Part II / Emulating DP-like qualities with ﬁnite mixtures

43

The rich get even richer
E-step: q(zi) ∝ W (zi)W (xi | zi)
M-step: W (j | z) =

Czj =

exp(Ψ(Czj ))
P
exp(Ψ( V =1 Czj ))
j

(

n
i=1 qzi (z)xij
Czj −0.5

PV
j =1

Czj )−0.5

Thought experiment: ignore W (zi), focus on W (xi | zi).
Suppose C1A = 20, C1B = 20, C2A = 0.5, C2B = 0.2
W (A | 1)
W (A | 2)
EM:
Mean-ﬁeld:

20
20+20

= 0.5

eΨ(20+1)
eΨ(20+20+1)

= 0.494

Part II / Emulating DP-like qualities with ﬁnite mixtures

0.5
0.5+0.2

= 0.71

eΨ(0.5+1)
eΨ(0.5+0.2+1)

= 0.468

43

The rich get even richer
E-step: q(zi) ∝ W (zi)W (xi | zi)
M-step: W (j | z) =

Czj =

exp(Ψ(Czj ))
P
exp(Ψ( V =1 Czj ))
j

(

n
i=1 qzi (z)xij
Czj −0.5

PV
j =1

Czj )−0.5

Thought experiment: ignore W (zi), focus on W (xi | zi).
Suppose C1A = 20, C1B = 20, C2A = 0.5, C2B = 0.2
W (A | 1)
W (A | 2)
20
20+20

EM:
Mean-ﬁeld:

= 0.5

eΨ(20+1)
eΨ(20+20+1)

= 0.494

0.5
0.5+0.2

= 0.71

eΨ(0.5+1)
eΨ(0.5+0.2+1)

= 0.468

For observation A: EM prefers component 2
mean-ﬁeld prefers 1

Part II / Emulating DP-like qualities with ﬁnite mixtures

43

The rich get even richer
E-step: q(zi) ∝ W (zi)W (xi | zi)
M-step: W (j | z) =

Czj =

exp(Ψ(Czj ))
P
exp(Ψ( V =1 Czj ))
j

(

n
i=1 qzi (z)xij
Czj −0.5

PV
j =1

Czj )−0.5

Thought experiment: ignore W (zi), focus on W (xi | zi).
Suppose C1A = 20, C1B = 20, C2A = 0.5, C2B = 0.2
W (A | 1)
W (A | 2)
EM:
Mean-ﬁeld:

20
20+20

= 0.5

eΨ(20+1)
eΨ(20+20+1)

= 0.494

0.5
0.5+0.2

= 0.71

eΨ(0.5+1)
eΨ(0.5+0.2+1)

= 0.468

For observation A: EM prefers component 2
mean-ﬁeld prefers 1
Key property: multinomial weights are not normalized,
allowing global tradeoﬀs between components.
Part II / Emulating DP-like qualities with ﬁnite mixtures

43

Example: IBM model 1 for word alignment
we deemed it inadvisable to attend the meeting and so informed COJO .
nous ne avons pas cru bon de assister ` la r´union et en avons inform´ le COJO en cons´quence .
a
e
e
e

Part II / Emulating DP-like qualities with ﬁnite mixtures

44

Example: IBM model 1 for word alignment
we deemed it inadvisable to attend the meeting and so informed COJO .
nous ne avons pas cru bon de assister ` la r´union et en avons inform´ le COJO en cons´quence .
a
e
e
e

Part II / Emulating DP-like qualities with ﬁnite mixtures

44

Example: IBM model 1 for word alignment
we deemed it inadvisable to attend the meeting and so informed COJO .
nous ne avons pas cru bon de assister ` la r´union et en avons inform´ le COJO en cons´quence .
a
e
e
e

n

p(zi)p(xi | ezi ; φ)

p(x, z; φ) =
i=1

Part II / Emulating DP-like qualities with ﬁnite mixtures

44

Example: IBM model 1 for word alignment
we deemed it inadvisable to attend the meeting and so informed COJO .
nous ne avons pas cru bon de assister ` la r´union et en avons inform´ le COJO en cons´quence .
a
e
e
e

n

p(zi)p(xi | ezi ; φ)

p(x, z; φ) =
i=1

E-step: qzi (j) ∝ p(xi | ej )

Ce,x =

Part II / Emulating DP-like qualities with ﬁnite mixtures

i,j:ej =e,xi =x

qzi (j)

44

Example: IBM model 1 for word alignment
we deemed it inadvisable to attend the meeting and so informed COJO .
nous ne avons pas cru bon de assister ` la r´union et en avons inform´ le COJO en cons´quence .
a
e
e
e

n

p(zi)p(xi | ezi ; φ)

p(x, z; φ) =
i=1

E-step: qzi (j) ∝ p(xi | ej )
M-step: W (x | e) =

Ce,x =
i,j:ej =e,xi =x

qzi (j)

Ce,x
x Ce,x

Part II / Emulating DP-like qualities with ﬁnite mixtures

44

Example: IBM model 1 for word alignment
we deemed it inadvisable to attend the meeting and so informed COJO .
nous ne avons pas cru bon de assister ` la r´union et en avons inform´ le COJO en cons´quence .
a
e
e
e

n

p(zi)p(xi | ezi ; φ)

p(x, z; φ) =
i=1

E-step: qzi (j) ∝ p(xi | ej )
M-step: W (x | e) =

Ce,x =
i,j:ej =e,xi =x

qzi (j)

Ce,x
x Ce,x

Garbage collectors problem: rare source words have large
probabilities for target words in the translation.

Part II / Emulating DP-like qualities with ﬁnite mixtures

44

Example: IBM model 1 for word alignment
we deemed it inadvisable to attend the meeting and so informed COJO .
nous ne avons pas cru bon de assister ` la r´union et en avons inform´ le COJO en cons´quence .
a
e
e
e

n

p(zi)p(xi | ezi ; φ)

p(x, z; φ) =
i=1

E-step: qzi (j) ∝ p(xi | ej )
M-step: W (x | e) =

Ce,x =
i,j:ej =e,xi =x

qzi (j)

Ce,x
exp(Ψ(Ce,x))
x Ce,x exp(Ψ(
x Ce,x ))

Garbage collectors problem: rare source words have large
probabilities for target words in the translation.

Part II / Emulating DP-like qualities with ﬁnite mixtures

44

Example: IBM model 1 for word alignment
we deemed it inadvisable to attend the meeting and so informed COJO .
nous ne avons pas cru bon de assister ` la r´union et en avons inform´ le COJO en cons´quence .
a
e
e
e

n

p(zi)p(xi | ezi ; φ)

p(x, z; φ) =
i=1

E-step: qzi (j) ∝ p(xi | ej )
M-step: W (x | e) =

Ce,x =
i,j:ej =e,xi =x

qzi (j)

Ce,x
exp(Ψ(Ce,x))
x Ce,x exp(Ψ(
x Ce,x ))

Garbage collectors problem: rare source words have large
probabilities for target words in the translation.
Quick experiment: EM: 20.3 AER, mean-ﬁeld: 19.0 AER
Part II / Emulating DP-like qualities with ﬁnite mixtures

44

An approximate Dirichlet process (DP) mixture model
Take ﬁnite mixture model, deﬁne p(β) = Dirichlet( α0 , . . . , α0 )
K
K

Part II / Emulating DP-like qualities with ﬁnite mixtures

45

An approximate Dirichlet process (DP) mixture model
Take ﬁnite mixture model, deﬁne p(β) = Dirichlet( α0 , . . . , α0 )
K
K
Theorem:
As K → ∞, ﬁnite mixture model → DP mixture model.

Part II / Emulating DP-like qualities with ﬁnite mixtures

45

An approximate Dirichlet process (DP) mixture model
Take ﬁnite mixture model, deﬁne p(β) = Dirichlet( α0 , . . . , α0 )
K
K
Theorem:
As K → ∞, ﬁnite mixture model → DP mixture model.
As α0 → 0, mean-ﬁeld enters rich-gets-richer regime.
K

Part II / Emulating DP-like qualities with ﬁnite mixtures

45

An approximate Dirichlet process (DP) mixture model
Take ﬁnite mixture model, deﬁne p(β) = Dirichlet( α0 , . . . , α0 )
K
K
Theorem:
As K → ∞, ﬁnite mixture model → DP mixture model.
As α0 → 0, mean-ﬁeld enters rich-gets-richer regime.
K
How to Bayesianify/DPify your EM algorithm
In the M-step of EM code where counts are normalized,
exp P
replace P Cz with exp Ψ(Ψ(Cz ) ) .
C
C
z

z

Part II / Emulating DP-like qualities with ﬁnite mixtures

z

z

45

Roadmap
• Part I
– Distributions and Bayesian principles
– Variational Bayesian inference
– Mean-ﬁeld for mixture models
• Part II
– Emulating DP-like qualities with ﬁnite mixtures
– DP mixture model
– Other views of the DP
• Part III
– Structured models
– Survey of related methods
– Survey of applications

Part II / DP mixture model

46

The limit of ﬁnite mixtures?
Theoretical goal:
Deﬁne a nonparametric (inﬁnite) mixture model.

Part II / DP mixture model

47

The limit of ﬁnite mixtures?
Theoretical goal:
Deﬁne a nonparametric (inﬁnite) mixture model.
First attempt:
Look at β ∼ Dirichlet( α0 , . . . , α0 ) with α0 = 1:
K
K
K→∞
K=2 K=3

K=4

Part II / DP mixture model

K=5

K=6

K=7

K=8

−→

?

47

The limit of ﬁnite mixtures?
Theoretical goal:
Deﬁne a nonparametric (inﬁnite) mixture model.
First attempt:
Look at β ∼ Dirichlet( α0 , . . . , α0 ) with α0 = 1:
K
K
K→∞
K=2 K=3

K=4

K=5

K=6

K=7

Problem: for each component z, Eβz =

Part II / DP mixture model

K=8

1
K

−→

?

→ 0.

47

The limit of ﬁnite mixtures?
Theoretical goal:
Deﬁne a nonparametric (inﬁnite) mixture model.
First attempt:
Look at β ∼ Dirichlet( α0 , . . . , α0 ) with α0 = 1:
K
K
K→∞
K=2 K=3

K=4

K=5

K=6

K=7

Problem: for each component z, Eβz =

K=8

1
K

−→

?

→ 0.

The issue is that the Dirichlet is symmetric.
We need an asymmetric approach, where the large
components are ﬁrst on average.
Part II / DP mixture model

47

Size-biased permutation
Solution: take a size-biased permutation of the components:

Part II / DP mixture model

48

Size-biased permutation
Solution: take a size-biased permutation of the components:
• Generate β ∼ Dirichlet( α0 , . . . , α0 ):
K
K

β1

Part II / DP mixture model

β2

β3 β4

β5

β6

β7

β8

48

Size-biased permutation
Solution: take a size-biased permutation of the components:
• Generate β ∼ Dirichlet( α0 , . . . , α0 ):
K
K
•S ←∅
• For z = 1, . . . , K:
– Choose j ∼ Multinomial(β ) conditioned on j ∈ S
– βz ← βj
– S ← S ∪ {j}
β1

β2

β3 β4

β5

β6

β7

β8

β1

Part II / DP mixture model

48

Size-biased permutation
Solution: take a size-biased permutation of the components:
• Generate β ∼ Dirichlet( α0 , . . . , α0 ):
K
K
•S ←∅
• For z = 1, . . . , K:
– Choose j ∼ Multinomial(β ) conditioned on j ∈ S
– βz ← βj
– S ← S ∪ {j}
β1

β2

β1

Part II / DP mixture model

β3 β4

β5

β6

β7

β8

β2

48

Size-biased permutation
Solution: take a size-biased permutation of the components:
• Generate β ∼ Dirichlet( α0 , . . . , α0 ):
K
K
•S ←∅
• For z = 1, . . . , K:
– Choose j ∼ Multinomial(β ) conditioned on j ∈ S
– βz ← βj
– S ← S ∪ {j}
β1

β2

β1

Part II / DP mixture model

β3 β4

β2

β5

β6

β7

β8

β3

48

Size-biased permutation
Solution: take a size-biased permutation of the components:
• Generate β ∼ Dirichlet( α0 , . . . , α0 ):
K
K
•S ←∅
• For z = 1, . . . , K:
– Choose j ∼ Multinomial(β ) conditioned on j ∈ S
– βz ← βj
– S ← S ∪ {j}
β1

β2

β1

Part II / DP mixture model

β3 β4

β2

β5

β3

β6

β7

β4

β5

β8

β6

β7

β8

48

Size-biased permutation
Solution: take a size-biased permutation of the components:
• Generate β ∼ Dirichlet( α0 , . . . , α0 ):
K
K
•S ←∅
• For z = 1, . . . , K:
– Choose j ∼ Multinomial(β ) conditioned on j ∈ S
– βz ← βj
– S ← S ∪ {j}
β1

β2

β1

β3 β4

β2

β5

β6

β3

β7

β4

β5

β8

β6

β7

β8

Stick-breaking characterization of distribution of β:
Deﬁne vz = PKβz
as the fraction of the tail-end of the stick
z =z

βz

Fact: vz ∼ Beta(1 + α0 , α0 − zα0 ).
K
K
Part II / DP mixture model

48

The stick-breaking (GEM) distribution
vz ∼ Beta(1 + α0 , α0 − zα0 )
K
K
Now we can take the limit:

K→∞

vz ∼ Beta(1, α0)

Part II / DP mixture model

49

The stick-breaking (GEM) distribution
vz ∼ Beta(1 + α0 , α0 − zα0 )
K
K
Now we can take the limit:

K→∞

vz ∼ Beta(1, α0)
1:1 relationship between stick-breaking proportions v = (v1, v2, . . . )
and stick-breaking probabilities β = (β1, β2, . . . )

Part II / DP mixture model

49

The stick-breaking (GEM) distribution
vz ∼ Beta(1 + α0 , α0 − zα0 )
K
K
Now we can take the limit:

K→∞

vz ∼ Beta(1, α0)
1:1 relationship between stick-breaking proportions v = (v1, v2, . . . )
and stick-breaking probabilities β = (β1, β2, . . . )
v1

Part II / DP mixture model

1 − v1

1

49

The stick-breaking (GEM) distribution
vz ∼ Beta(1 + α0 , α0 − zα0 )
K
K
Now we can take the limit:

K→∞

vz ∼ Beta(1, α0)
1:1 relationship between stick-breaking proportions v = (v1, v2, . . . )
and stick-breaking probabilities β = (β1, β2, . . . )
1 − v1

v1
v2

Part II / DP mixture model

1 − v2

1
(1 − v1)

49

The stick-breaking (GEM) distribution
vz ∼ Beta(1 + α0 , α0 − zα0 )
K
K
Now we can take the limit:

K→∞

vz ∼ Beta(1, α0)
1:1 relationship between stick-breaking proportions v = (v1, v2, . . . )
and stick-breaking probabilities β = (β1, β2, . . . )
1 − v1

v1
v2

1

1 − v2
v3

(1 − v1)
1 − v3
...

Part II / DP mixture model

(1 − v1)(1 − v2)

49

The stick-breaking (GEM) distribution
vz ∼ Beta(1 + α0 , α0 − zα0 )
K
K
Now we can take the limit:

K→∞

vz ∼ Beta(1, α0)
1:1 relationship between stick-breaking proportions v = (v1, v2, . . . )
and stick-breaking probabilities β = (β1, β2, . . . )
1 − v1

v1
v2

1

1 − v2
v3

(1 − v1)
1 − v3
...

vz =

βz
βz + βz+1 + · · ·

(1 − v1)(1 − v2)

βz = (1 − v1) · · · (1 − vz−1)vz

Write β ∼ GEM(α0) to denote the stick-breaking distribution.
Part II / DP mixture model

49

Examples of the GEM distribution
vz ∼ Beta(1, α0)
As α0 increases, sticks decay slower ⇒ more clusters
GEM(0.3)
1

φ2
0

0

φ1 1

Part II / DP mixture model

50

Examples of the GEM distribution
vz ∼ Beta(1, α0)
As α0 increases, sticks decay slower ⇒ more clusters
GEM(0.3)
1

φ2
0

0

φ1 1

GEM(1)
1

φ2
0

0

φ1 1

Part II / DP mixture model

50

Examples of the GEM distribution
vz ∼ Beta(1, α0)
As α0 increases, sticks decay slower ⇒ more clusters
GEM(0.3)
1

φ2
0

0

φ1 1

GEM(1)
1

φ2
0

0

φ1 1

GEM(3)
1

φ2
0

0 φ1 1
Part II / DP mixture model

50

A cautionary tale about point estimates
Question: what is the most likely value of β ∼ GEM(1.2)?

Part II / DP mixture model

51

A cautionary tale about point estimates
Question: what is the most likely value of β ∼ GEM(1.2)?
1.0
0.9

K
z=1 Beta(vz ; 1, 1.2)

p(v) =
For each z, best vz = 0, so best βz = 0.

0.8
0.7

Beta(x; 1, 1.2)

0.6

0.2

0.4

0.6

0.8

1.0

x

Part II / DP mixture model

51

A cautionary tale about point estimates
Question: what is the most likely value of β ∼ GEM(1.2)?
1.0
0.9

K
z=1 Beta(vz ; 1, 1.2)

p(v) =
For each z, best vz = 0, so best βz = 0.

0.8
0.7

Beta(x; 1, 1.2)

0.6

0.2

0.4

0.6

0.8

1.0

x

But in typical draws, components decay...
A contradiction?

Part II / DP mixture model

51

A cautionary tale about point estimates
Question: what is the most likely value of β ∼ GEM(1.2)?
1.0
0.9

K
z=1 Beta(vz ; 1, 1.2)

p(v) =
For each z, best vz = 0, so best βz = 0.

0.8
0.7

Beta(x; 1, 1.2)

0.6

0.2

0.4

0.6

0.8

1.0

x

But in typical draws, components decay...
A contradiction? No!
• Problem: mode not representative of entire distribution
• Solution: need inference algorithms that work with
entire distribution

Part II / DP mixture model

51

DP mixture model
Finite

mixture model

β ∼ DirichletK (α, . . . , α)
For each component z ∈ {1, . . . , K}
φz ∼ G0
For each data point i ∈ {1, . . . , n}:
zi ∼ Multinomial(β)
xi ∼ F (φzi )

Part II / DP mixture model

52

DP mixture model
Finite DP mixture model
β ∼ DirichletK (α, . . . , α) GEM(α)
For each component z ∈ {1, . . . , K} {1, 2, . . . }:
φz ∼ G0
For each data point i ∈ {1, . . . , n}:
zi ∼ Multinomial(β)
xi ∼ F (φzi )

Part II / DP mixture model

52

DP mixture model
Finite DP mixture model
β ∼ DirichletK (α, . . . , α) GEM(α)
For each component z ∈ {1, . . . , K} {1, 2, . . . }:
φz ∼ G0
For each data point i ∈ {1, . . . , n}:
zi ∼ Multinomial(β)
xi ∼ F (φzi )
Mean-ﬁeld inference [Blei, Jordan, 2005]:
Approximation in terms of stick-breaking proportions v:
∞
q(β) = z=1 q(vz )

Part II / DP mixture model

52

DP mixture model
Finite DP mixture model
β ∼ DirichletK (α, . . . , α) GEM(α)
For each component z ∈ {1, . . . , K} {1, 2, . . . }:
φz ∼ G0
For each data point i ∈ {1, . . . , n}:
zi ∼ Multinomial(β)
xi ∼ F (φzi )
Mean-ﬁeld inference [Blei, Jordan, 2005]:
Approximation in terms of stick-breaking proportions v:
∞
q(β) = z=1 q(vz )
How to deal with an inﬁnite number of parameters?
Choose a truncation level K and force q(vK ) = 1.
Now q(vz ) and q(φz ) for z > K are irrelevant.
Part II / DP mixture model

52

Roadmap
• Part I
– Distributions and Bayesian principles
– Variational Bayesian inference
– Mean-ﬁeld for mixture models
• Part II
– Emulating DP-like qualities with ﬁnite mixtures
– DP mixture model
– Other views of the DP
• Part III
– Structured models
– Survey of related methods
– Survey of applications

Part II / Other views of the DP

53

Three views of the Dirichlet process

Part II / Other views of the DP

54

Three views of the Dirichlet process

king
brea

cess
pro

]
1994
Stickethuraman,
[S

Part II / Other views of the DP

54

Three views of the Dirichlet process

Chine

se re
staur
ant p
[Pitm
roces
an, 2
002]
s
...

Part II / Other views of the DP

king
brea

cess
pro

]
1994
Stickethuraman,
[S

54

Three views of the Dirichlet process
Stochastic process
[Ferguson, 1973]
A1

Chine

A3

se re
staur
ant p
[Pitm
roces
an, 2
002]
s
...

Part II / Other views of the DP

A2
A4

king
brea

cess
pro

]
1994
Stickethuraman,
[S

54

Deﬁnition of the Dirichlet process
DP mixture model
β ∼ GEM(α)
For each component z ∈ {1, 2, . . . }:
φz ∼ G0
For each data point i ∈ {1, . . . , n}:
zi ∼ Multinomial(β)
xi ∼ F (φzi )

Part II / Other views of the DP

55

Deﬁnition of the Dirichlet process
DP mixture model
β ∼ GEM(α)
For each component z ∈ {1, 2, . . . }:
φz ∼ G0
For each data point i ∈ {1, . . . , n}:
zi ∼ Multinomial(β)
xi ∼ F (φzi )
∞

G = z=1 βz δφz fully speciﬁes the
parameters.

Part II / Other views of the DP

55

Deﬁnition of the Dirichlet process
DP mixture model
β ∼ GEM(α)
For each component z ∈ {1, 2, . . . }:
φz ∼ G0

}

G ∼ DP(α, G0)

For each data point i ∈ {1, . . . , n}:
zi ∼ Multinomial(β)
}ψi ∼ G
xi ∼ F (φzi )
}xi ∼ F (ψi)
∞

G = z=1 βz δφz fully speciﬁes the
parameters.
Write G ∼ DP(α, G0) to mean G has
a Dirichlet process prior.

Part II / Other views of the DP

55

Deﬁnition of the Dirichlet process
DP mixture model
β ∼ GEM(α)
For each component z ∈ {1, 2, . . . }:
φz ∼ G0

}

G ∼ DP(α, G0)

For each data point i ∈ {1, . . . , n}:
zi ∼ Multinomial(β)
}ψi ∼ G
xi ∼ F (φzi )
}xi ∼ F (ψi)
∞

G = z=1 βz δφz fully speciﬁes the
parameters.
Write G ∼ DP(α, G0) to mean G has
a Dirichlet process prior.

Part II / Other views of the DP

1

1

φB

φB

0

0

φA

G0

1

0

0

φA

1

G

55

Stochastic process deﬁnition
⇔

G ∼ DP(α, G0)
Finite partition property:
(G(A1), . . . , G(Am)) ∼ Dirichlet(αG0(A1), . . . , αG0(Am))
for all partitions (A1, . . . , Am) of Ω,
where Ω is the set of all possible parameters.
1

A1
A3

A2
A4

Ω
Example: Ω = φB
0

0

φA

1

(the set of multinomial parameters)

Part II / Other views of the DP

56

Stochastic process deﬁnition
⇔

G ∼ DP(α, G0)
Finite partition property:
(G(A1), . . . , G(Am)) ∼ Dirichlet(αG0(A1), . . . , αG0(Am))
for all partitions (A1, . . . , Am) of Ω,
where Ω is the set of all possible parameters.
1

A1
A3

A2
A4

Ω
Example: Ω = φB
0

0

φA

1

(the set of multinomial parameters)
When Ω is ﬁnite, Dirichlet process = Dirichlet distribution.

Part II / Other views of the DP

56

Stochastic process deﬁnition
⇔

G ∼ DP(α, G0)
Finite partition property:
(G(A1), . . . , G(Am)) ∼ Dirichlet(αG0(A1), . . . , αG0(Am))
for all partitions (A1, . . . , Am) of Ω,
where Ω is the set of all possible parameters.
1

A1
A3

A2
A4

Ω
Example: Ω = φB
0

0

φA

1

(the set of multinomial parameters)
When Ω is ﬁnite, Dirichlet process = Dirichlet distribution.
Signiﬁcance: theoretical properties, compact notation, deﬁning HDPs
Part II / Other views of the DP

56

Chinese restaurant process
What is the distribution of ψi | ψ1, . . . , ψi−1 marginalizing out G?

Part II / Other views of the DP

57

Chinese restaurant process
What is the distribution of ψi | ψ1, . . . , ψi−1 marginalizing out G?
Metaphor: customer = data point, dish = parameter,
table = cluster

Part II / Other views of the DP

57

Chinese restaurant process
What is the distribution of ψi | ψ1, . . . , ψi−1 marginalizing out G?
Metaphor: customer = data point, dish = parameter,
table = cluster
A Chinese restaurant has an inﬁnite number of tables.
Customer i:
• joins the table of a previous customer j and shares the dish, or
• starts a new table and randomly picks a new dish from G0
...

Part II / Other views of the DP

57

Chinese restaurant process
What is the distribution of ψi | ψ1, . . . , ψi−1 marginalizing out G?
Metaphor: customer = data point, dish = parameter,
table = cluster
A Chinese restaurant has an inﬁnite number of tables.
Customer i:
• joins the table of a previous customer j and shares the dish, or
• starts a new table and randomly picks a new dish from G0
1
α0
. . . probability: 0+α0

Part II / Other views of the DP

57

Chinese restaurant process
What is the distribution of ψi | ψ1, . . . , ψi−1 marginalizing out G?
Metaphor: customer = data point, dish = parameter,
table = cluster
A Chinese restaurant has an inﬁnite number of tables.
Customer i:
• joins the table of a previous customer j and shares the dish, or
• starts a new table and randomly picks a new dish from G0
1
2
α0
α0
. . . probability: 0+α0 1+α0

Part II / Other views of the DP

57

Chinese restaurant process
What is the distribution of ψi | ψ1, . . . , ψi−1 marginalizing out G?
Metaphor: customer = data point, dish = parameter,
table = cluster
A Chinese restaurant has an inﬁnite number of tables.
Customer i:
• joins the table of a previous customer j and shares the dish, or
• starts a new table and randomly picks a new dish from G0
1
2
α0
α0
1
. . . probability: 0+α0 1+α0 2+α0
3

Part II / Other views of the DP

57

Chinese restaurant process
What is the distribution of ψi | ψ1, . . . , ψi−1 marginalizing out G?
Metaphor: customer = data point, dish = parameter,
table = cluster
A Chinese restaurant has an inﬁnite number of tables.
Customer i:
• joins the table of a previous customer j and shares the dish, or
• starts a new table and randomly picks a new dish from G0
1
2
α0
α0
α0
1
. . . probability: 0+α0 1+α0 2+α0 3+α0
3
4

Part II / Other views of the DP

57

Chinese restaurant process
What is the distribution of ψi | ψ1, . . . , ψi−1 marginalizing out G?
Metaphor: customer = data point, dish = parameter,
table = cluster
A Chinese restaurant has an inﬁnite number of tables.
Customer i:
• joins the table of a previous customer j and shares the dish, or
• starts a new table and randomly picks a new dish from G0
1
2
α0
α0
α0
1
2
. . . probability: 0+α0 1+α0 2+α0 3+α0 4+α0
3
5
4

Part II / Other views of the DP

57

Chinese restaurant process
What is the distribution of ψi | ψ1, . . . , ψi−1 marginalizing out G?
Metaphor: customer = data point, dish = parameter,
table = cluster
A Chinese restaurant has an inﬁnite number of tables.
Customer i:
• joins the table of a previous customer j and shares the dish, or
• starts a new table and randomly picks a new dish from G0
1
2
α0
α0
α0
1
2
. . . probability: 0+α0 1+α0 2+α0 3+α0 4+α0
3
5
4
In symbols:
ψi | ψ1, . . . , ψi−1 ∼

Part II / Other views of the DP

1
α+i−1

i−1
j=1 δψj

+ αG0

57

Chinese restaurant process
What is the distribution of ψi | ψ1, . . . , ψi−1 marginalizing out G?
Metaphor: customer = data point, dish = parameter,
table = cluster
A Chinese restaurant has an inﬁnite number of tables.
Customer i:
• joins the table of a previous customer j and shares the dish, or
• starts a new table and randomly picks a new dish from G0
1
2
α0
α0
α0
1
2
. . . probability: 0+α0 1+α0 2+α0 3+α0 4+α0
3
5
4
In symbols:
ψi | ψ1, . . . , ψi−1 ∼

1
α+i−1

i−1
j=1 δψj

+ αG0

Rich-gets-richer property: tables with more customers get more.
Signiﬁcance: leads to eﬃcient sampling algorithms.
Part II / Other views of the DP

57

Roadmap
• Part I
– Distributions and Bayesian principles
– Variational Bayesian inference
– Mean-ﬁeld for mixture models
• Part II
– Emulating DP-like qualities with ﬁnite mixtures
– DP mixture model
– Other views of the DP
• Part III
– Structured models
– Survey of related methods
– Survey of applications

Part III

58

Roadmap
• Part I
– Distributions and Bayesian principles
– Variational Bayesian inference
– Mean-ﬁeld for mixture models
• Part II
– Emulating DP-like qualities with ﬁnite mixtures
– DP mixture model
– Other views of the DP
• Part III
– Structured models
– Survey of related methods
– Survey of applications

Part III / Structured models

59

Building DP-based structured models
DP
single mixture model

Part III / Structured models

60

Building DP-based structured models
DP
single mixture model

HDP
several mixture models
sharing same components

Part III / Structured models

60

Building DP-based structured models
DP
single mixture model

HDP
several mixture models
sharing same components

HDP-HMM
each state has a
mixture model over next states
Part III / Structured models

HDP-PCFG
each state has a
mixture model over pairs of states
60

[Teh, et al., 2006]

Hierarchical DP mixture models
Suppose we have a collection of J mixture models.
Goal: share common inventory of mixture components

Part III / Structured models

61

[Teh, et al., 2006]

Hierarchical DP mixture models
Suppose we have a collection of J mixture models.
Goal: share common inventory of mixture components
β
π1

...

πJ

z1i

φz

zJi
∞

z

x1i
i

xJi

...
n

i

n

• Component parameters {φz } are shared globally
• Each mixture model has individual component
probabilities πj tied via β
Part III / Structured models

61

Hierarchical DP mixture models
β
πj
φz

zji

z

xji
i
j

n

∞

β ∼ GEM(α)
For each component z ∈ {1, 2, . . . }:
φz ∼ G0
For each group j ∈ {1, . . . , J}:
π j ∼ DP(α , β)
For each data point i ∈ {1, . . . , n}:
zji ∼ Multinomial(π j )
xji ∼ F (φzji )

J

Part III / Structured models

62

Hierarchical DP mixture models
β
πj
φz

zji

z

xji
i
j

n

∞

β ∼ GEM(α)
For each component z ∈ {1, 2, . . . }:
φz ∼ G0
For each group j ∈ {1, . . . , J}:
π j ∼ DP(α , β)
For each data point i ∈ {1, . . . , n}:
zji ∼ Multinomial(π j )
xji ∼ F (φzji )

J

β determines the rough component probabilities
π j ∼ DP(α , β) makes π j close to β

Part III / Structured models

62

Sharing component probabilities
Draw top-level component probabilities β ∼ GEM(α):
α=2

Part III / Structured models

63

Sharing component probabilities
Draw top-level component probabilities β ∼ GEM(α):
α=2
Draw group-level component probabilities π ∼ DP(α , β):
α =1

Part III / Structured models

63

Sharing component probabilities
Draw top-level component probabilities β ∼ GEM(α):
α=2
Draw group-level component probabilities π ∼ DP(α , β):
α =1
α =5

Part III / Structured models

63

Sharing component probabilities
Draw top-level component probabilities β ∼ GEM(α):
α=2
Draw group-level component probabilities π ∼ DP(α , β):
α =1
α =5
α = 20

Part III / Structured models

63

Sharing component probabilities
Draw top-level component probabilities β ∼ GEM(α):
α=2
Draw group-level component probabilities π ∼ DP(α , β):
α =1
α =5
α = 20
α = 100
As α increases, the more π resembles β.

Part III / Structured models

63

Mean-ﬁeld inference for the HDP
β
πj
φz

zji

z

xji
i
j

n

∞

β ∼ GEM(α)
For each component z ∈ {1, 2, . . . }:
φz ∼ G0
For each group j ∈ {1, . . . , J}:
π j ∼ DP(α , β)
For each data point i ∈ {1, . . . , n}:
zji ∼ Multinomial(π j )
xji ∼ F (φzji )

J

Mean-ﬁeld approximation: q(β)

Part III / Structured models

∞
z=1 q(φz )

J
j=1 q(π j )

n
i=1 q(zi )

64

Mean-ﬁeld inference for the HDP
β
πj
φz

zji

z

xji
i
j

n

∞

β ∼ GEM(α)
For each component z ∈ {1, 2, . . . }:
φz ∼ G0
For each group j ∈ {1, . . . , J}:
π j ∼ DP(α , β)
For each data point i ∈ {1, . . . , n}:
zji ∼ Multinomial(π j )
xji ∼ F (φzji )

J

Mean-ﬁeld approximation: q(β)

∞
z=1 q(φz )

J
j=1 q(π j )

n
i=1 q(zi )

• As in variational DP, truncate β at level K
• π j ∼ DP(α , β) reduces to ﬁnite K-dimensional
Dirichlet π j ∼ Dirichlet(α , β), so we can use ﬁnite
variational techniques for updating q(π).
• Let q(β) = δβ ∗ (β) for tractability, optimize with gradient descent
Part III / Structured models

64

[Beal, et al., 2002; Teh, et al., 2006]

HDP hidden Markov models
β
z1

z2

z3

···

x1

x2

x3

···

πz
φz
z

∞

β ∼ GEM(α)
For each state z ∈ {1, 2, . . . }:
φz ∼ G0
π z ∼ DP(α , β)
For each time step i ∈ {1, . . . , n}:
zi+1 ∼ Multinomial(π zi )
xi ∼ F (φzi )

Each state z is a component and has the following:
• π z : transition parameters
• φz : emission parameters

Part III / Structured models

65

[Beal, et al., 2002; Teh, et al., 2006]

HDP hidden Markov models
β
z1

z2

z3

···

x1

x2

x3

···

πz
φz
z

∞

β ∼ GEM(α)
For each state z ∈ {1, 2, . . . }:
φz ∼ G0
π z ∼ DP(α , β)
For each time step i ∈ {1, . . . , n}:
zi+1 ∼ Multinomial(π zi )
xi ∼ F (φzi )

Each state z is a component and has the following:
• π z : transition parameters
• φz : emission parameters
Key:
β ∼ GEM(α) speciﬁes which states will be used (global)
π z ∼ DP(α , β) speciﬁes distribution over next states (per state)
Part III / Structured models

65

Mean-ﬁeld inference for the HDP-HMM
β
z1

z2

z3

···

x1

x2

x3

···

πz
φz
z

∞

β ∼ GEM(α)
For each state z ∈ {1, 2, . . . }:
φz ∼ G0
π z ∼ DP(α , β)
For each time step i ∈ {1, . . . , n}:
zi+1 ∼ Multinomial(π zi )
xi ∼ F (φzi )

Structured mean-ﬁeld approximation: q(β)

Part III / Structured models

∞
z=1 q(φz )q(π z )

q(z)

66

Mean-ﬁeld inference for the HDP-HMM
β
z1

z2

z3

···

x1

x2

x3

···

πz
φz
z

∞

β ∼ GEM(α)
For each state z ∈ {1, 2, . . . }:
φz ∼ G0
π z ∼ DP(α , β)
For each time step i ∈ {1, . . . , n}:
zi+1 ∼ Multinomial(π zi )
xi ∼ F (φzi )

Structured mean-ﬁeld approximation: q(β)

∞
z=1 q(φz )q(π z )

q(z)

EM:
E-step: run forward-backward using p(z | z, π) = πzz
M-step: normalize transition counts: πzz ∝ C(z → z )

Part III / Structured models

66

Mean-ﬁeld inference for the HDP-HMM
β
z1

z2

z3

···

x1

x2

x3

···

πz
φz
z

∞

β ∼ GEM(α)
For each state z ∈ {1, 2, . . . }:
φz ∼ G0
π z ∼ DP(α , β)
For each time step i ∈ {1, . . . , n}:
zi+1 ∼ Multinomial(π zi )
xi ∼ F (φzi )

Structured mean-ﬁeld approximation: q(β)

∞
z=1 q(φz )q(π z )

q(z)

EM:
E-step: run forward-backward using p(z | z, π) = πzz
M-step: normalize transition counts: πzz ∝ C(z → z )
Mean-ﬁeld:
E-step: run forward-backward using multinomial weights W (z, z )
M-step: compute multinomial weights given transition counts:
exp Ψ(α βz + C(z → z ))
W (z, z ) =
exp Ψ(α + C(z → ∗))
Top-level: optimize q(β) = δβ ∗ (β) using gradient descent

Part III / Structured models

66

[Liang, et al., 2007]

HDP probabilistic context-free grammars
β ∼ GEM(α)
For each symbol z ∈ {1, 2, . . . }:
φT ∼ Dirichlet
z
φE ∼ G0
z
φB ∼ DP(α , ββ T )
z
For each node i in the parse tree:
ti ∼ Multinomial(φTi ) [rule type]
z
If ti = Binary-Production:
(zL(i), zR(i)) ∼ Multinomial(φB ) [children symbols]
zi
If ti = Emission:
xi ∼ Multinomial(φE ) [terminal symbol]
zi

Part III / Structured models

β

z1

φB
z
z2

x2

φT
z
φE
z
z

z3

x3

∞

67

[Liang, et al., 2007]

HDP probabilistic context-free grammars
β ∼ GEM(α)
For each symbol z ∈ {1, 2, . . . }:
φT ∼ Dirichlet
z
φE ∼ G0
z
φB ∼ DP(α , ββ T )
z
For each node i in the parse tree:
ti ∼ Multinomial(φTi ) [rule type]
z
If ti = Binary-Production:
(zL(i), zR(i)) ∼ Multinomial(φB ) [children symbols]
zi
If ti = Emission:
xi ∼ Multinomial(φE ) [terminal symbol]
zi

HDP-HMM
At each i... transition and emit

Part III / Structured models

β

z1

φB
z
z2

x2

φT
z
φE
z
z

z3

x3

∞

HDP-PCFG
produce or emit

67

[Liang, et al., 2007]

HDP probabilistic context-free grammars
β ∼ GEM(α)
For each symbol z ∈ {1, 2, . . . }:
φT ∼ Dirichlet
z
φE ∼ G0
z
φB ∼ DP(α , ββ T )
z
For each node i in the parse tree:
ti ∼ Multinomial(φTi ) [rule type]
z
If ti = Binary-Production:
(zL(i), zR(i)) ∼ Multinomial(φB ) [children symbols]
zi
If ti = Emission:
xi ∼ Multinomial(φE ) [terminal symbol]
zi

β

z1

φB
z
z2

x2

φT
z
φE
z
z

z3

x3

∞

HDP-HMM
HDP-PCFG
At each i... transition and emit produce or emit
DP is over...
1 next state
2 children symbols
Part III / Structured models

67

[Liang, et al., 2007]

HDP probabilistic context-free grammars
β ∼ GEM(α)
For each symbol z ∈ {1, 2, . . . }:
φT ∼ Dirichlet
z
φE ∼ G0
z
φB ∼ DP(α , ββ T )
z
For each node i in the parse tree:
ti ∼ Multinomial(φTi ) [rule type]
z
If ti = Binary-Production:
(zL(i), zR(i)) ∼ Multinomial(φB ) [children symbols]
zi
If ti = Emission:
xi ∼ Multinomial(φE ) [terminal symbol]
zi

β

z1

φB
z
z2

x2

φT
z
φE
z
z

z3

x3

∞

HDP-HMM
HDP-PCFG
At each i... transition and emit produce or emit
DP is over...
1 next state
2 children symbols
Variational inference: modiﬁed inside-outside
Part III / Structured models

67

Prediction revisited
Train on x; now get test xnew, want to ﬁnd best znew.
p(znew | xnew)

Part III / Structured models

68

Prediction revisited
Train on x; now get test xnew, want to ﬁnd best znew.
p(znew | xnew) = Ep(θ|x)p(znew | xnew, θ)

Part III / Structured models

68

Prediction revisited
Train on x; now get test xnew, want to ﬁnd best znew.
p(znew | xnew) = Ep(θ|x)p(znew | xnew, θ)
Eq(θ)p(znew | xnew, θ)

Part III / Structured models

68

Prediction revisited
Train on x; now get test xnew, want to ﬁnd best znew.
p(znew | xnew) = Ep(θ|x)p(znew | xnew, θ)
Eq(θ)p(znew | xnew, θ)
Contrast with training (has log):
q(znew) ∝ exp Eq(θ) log p(znew | xnew, θ)

Part III / Structured models

68

Prediction revisited
Train on x; now get test xnew, want to ﬁnd best znew.
p(znew | xnew) = Ep(θ|x)p(znew | xnew, θ)
Eq(θ)p(znew | xnew, θ)
Contrast with training (has log):
q(znew) ∝ exp Eq(θ) log p(znew | xnew, θ)
To ﬁnd argmaxznew Eq(θ)p(znew | xnew, θ), consider all znew:
intractable when znew is a parse tree because cannot use
dynamic programming (unlike training)

Part III / Structured models

68

Prediction revisited
Train on x; now get test xnew, want to ﬁnd best znew.
p(znew | xnew) = Ep(θ|x)p(znew | xnew, θ)
Eq(θ)p(znew | xnew, θ)
Contrast with training (has log):
q(znew) ∝ exp Eq(θ) log p(znew | xnew, θ)
To ﬁnd argmaxznew Eq(θ)p(znew | xnew, θ), consider all znew:
intractable when znew is a parse tree because cannot use
dynamic programming (unlike training)
Approximations:
• Use mode of q(θ)
• Maximize expected log-likelihood as in training
• Reranking: get an n-best list using proxy; choose best one
Part III / Structured models

68

Roadmap
• Part I
– Distributions and Bayesian principles
– Variational Bayesian inference
– Mean-ﬁeld for mixture models
• Part II
– Emulating DP-like qualities with ﬁnite mixtures
– DP mixture model
– Other views of the DP
• Part III
– Structured models
– Survey of related methods
– Survey of applications

Part III / Survey of related methods

69

Top-down and bottom-up
Model

DP mixture models, HMMs, PCFGs

priors

Inference
algorithm

EM, mean-ﬁeld, sampling

smoothing, discounting

Data

Part III / Survey of related methods

words, sentences, parse trees

70

Why doesn’t my variational DP work?
Modeling issues
• Setting concentration parameters is still an art.
• Is the DP even the right prior to use?

Part III / Survey of related methods

71

Why doesn’t my variational DP work?
Modeling issues
• Setting concentration parameters is still an art.
• Is the DP even the right prior to use?
Inference issues
• Mean-ﬁeld is an approximation of the true posterior.
• The coordinate-wise descent algorithm for optimizing
the mean-ﬁeld objective is susceptible to local minima
problems, just as is EM.

Part III / Survey of related methods

71

Other inference methods
We focused on variational inference using the stick-breaking
construction.

Part III / Survey of related methods

72

Other inference methods
We focused on variational inference using the stick-breaking
construction.
Algorithm:
• Variational (mean-ﬁeld, expectation propagation, etc.):
deterministic, fast, converge to local optima
• Sampling (Gibbs, split-merge, etc.): converges to true
posterior (but don’t know when), could be stuck in
local optima for a long time

Part III / Survey of related methods

72

Other inference methods
We focused on variational inference using the stick-breaking
construction.
Algorithm:
• Variational (mean-ﬁeld, expectation propagation, etc.):
deterministic, fast, converge to local optima
• Sampling (Gibbs, split-merge, etc.): converges to true
posterior (but don’t know when), could be stuck in
local optima for a long time
Representation:
• Stick-breaking construction: simple, concrete
• Chinese restaurant process: works well for simple
mixture models
Part III / Survey of related methods

72

The Bayesian elephant

Even just the elephant posterior is intractable.
Mean-ﬁeld guy: “feels like smoothing/discounting”
Sampling guy: “feels like stochastic hill-climbing”
A common property: rich gets richer

Part III / Survey of related methods

73

Other nonparametric priors
• Pitman-Yor process: component probabilities less
sparse compared to the DP; yields power-law
distributions useful for language modeling. [Pitman,
Yor, 1997; Teh, 2006]
• Beta process / Indian buﬀet process: allows data
points to belong to more than one cluster; inﬁnite
number of latent features, each data point generated
using a ﬁnite subset. [Griﬃths, Ghahramani, 2007;
Thibaux, Jordan, 2007]
• etc.
Many of these priors also have stochastic process, Chinese
restaurant, stick-breaking counterparts.
Part III / Survey of related methods

74

Other types of problems
Clustering (unsupervised)

parametric
nonparametric

non-Bayesian
k-means, EM
agglomerative clustering

Part III / Survey of related methods

Bayesian
Bayesian mixture models
Dirichlet processes

75

Other types of problems
Clustering (unsupervised)

parametric
nonparametric

non-Bayesian
k-means, EM
agglomerative clustering

Bayesian
Bayesian mixture models
Dirichlet processes

Classiﬁcation (supervised)

parametric
nonparametric

non-Bayesian
logistic regression, SVMs
nearest neighbors, kernel methods

Part III / Survey of related methods

Bayesian
Bayesian logistic regression
Gaussian processes

75

Roadmap
• Part I
– Distributions and Bayesian principles
– Variational Bayesian inference
– Mean-ﬁeld for mixture models
• Part II
– Emulating DP-like qualities with ﬁnite mixtures
– DP mixture model
– Other views of the DP
• Part III
– Structured models
– Survey of related methods
– Survey of applications

Part III / Survey of applications

76

Bayesian trends in NLP
number of hits

60
48
36
24
12

90 91 92 93 94 95 96 97 98 99 00 01 02 03 04 05 06

year

“Bayesian”

Part III / Survey of applications

77

Bayesian trends in NLP
60

48

48

number of hits

number of hits

60

36
24
12

36
24
12

90 91 92 93 94 95 96 97 98 99 00 01 02 03 04 05 06

90 91 92 93 94 95 96 97 98 99 00 01 02 03 04 05 06

year

year

“Bayesian”

Part III / Survey of applications

“Dirichlet”

77

Bayesian trends in NLP
60

48

48

number of hits

number of hits

60

36
24
12

36
24
12

90 91 92 93 94 95 96 97 98 99 00 01 02 03 04 05 06

90 91 92 93 94 95 96 97 98 99 00 01 02 03 04 05 06

year

year

“Bayesian”

“Dirichlet”

number of hits

60
48
36
24
12

90 91 92 93 94 95 96 97 98 99 00 01 02 03 04 05 06

year

“prior”
Part III / Survey of applications

77

Bayesian trends in NLP
60

48

48

number of hits

number of hits

60

36
24
12

36
24
12

90 91 92 93 94 95 96 97 98 99 00 01 02 03 04 05 06

90 91 92 93 94 95 96 97 98 99 00 01 02 03 04 05 06

year

year

“Bayesian”

“Dirichlet”
60

48

48

number of hits

number of hits

60

36
24
12

36
24
12

90 91 92 93 94 95 96 97 98 99 00 01 02 03 04 05 06

90 91 92 93 94 95 96 97 98 99 00 01 02 03 04 05 06

year

year

“prior”
Part III / Survey of applications

“posterior”
77

Applications
• Topic modeling
– ﬁnite Bayesian model; variational [Blei, et al., 2003]
– HDP-based model; sampling [Teh, et al., 2006]
• Language modeling
– Pitman-Yor ⇒ power-law; sampling [Goldwater, et al., 2005]
– Kneser-Ney ⇔ Pitman-Yor; sampling [Teh, 2006]
• POS induction using a ﬁnite Bayesian HMM
– Collapsed sampling [Goldwater, Griﬃths, 2007]
– Variational [Johnson, 2007]
• Parsing using nonparametric grammars
– Collapsed sampling [Johnson, et al., 2006]
– Collapsed sampling [Finkel, et al., 2007]
– Variational stick-breaking representation [Liang, et al., 2007]
• Coreference resolution
– Supervised clustering; collapsed sampling [Daume, Marcu, 2005]
– HDP-based model; sampling [Haghighi, Klein, 2007]
Part III / Survey of applications

78

Conclusions
• Bayesian methods are about keeping uncertainty in
parameters

79

Conclusions
• Bayesian methods are about keeping uncertainty in
parameters
• Variational inference: hard EM → EM → mean-ﬁeld
– Represent a distribution over parameters

79

Conclusions
• Bayesian methods are about keeping uncertainty in
parameters
• Variational inference: hard EM → EM → mean-ﬁeld
– Represent a distribution over parameters
• The nonparametric Dirichlet process prior penalizes use
of extra clusters
– Inference algorithms have rich-gets-richer property

79

Conclusions
• Bayesian methods are about keeping uncertainty in
parameters
• Variational inference: hard EM → EM → mean-ﬁeld
– Represent a distribution over parameters
• The nonparametric Dirichlet process prior penalizes use
of extra clusters
– Inference algorithms have rich-gets-richer property
• Structured models can be built from nonparametric
Bayesian parts

79

Resources
• References
• Derivations
• Glossary and notation

Resources / References

80

References (DP theory)
1. T. S. Ferguson. A Bayesian analysis of some nonparametric problems. Annals
of Statistics, 1973. Deﬁnes the Dirichlet process using stochastic processes.
2. D. Blackwell and J. B. MacQueen. Ferguson distributions via P´lya urn
o
schemes. Annals of Statistics, 1973. Connection between DP and the P´lya
o
urn.
3. J. Pitman. Combinatorial stochastic processes. UC Berkeley, 2002. A
comprehensive set of tutorial notes.
4. C. E. Antoniak. Mixtures of Dirichlet processes with applications to Bayesian
nonparametric problems. Annals of Statistics, 1974.
5. M. Beal and Z. Ghahramani and C. Rasmussen. The inﬁnite hidden Markov
model. NIPS, 2002. Introduces the (inﬁnite) HDP-HMMs.
6. Y. W. Teh and M. I. Jordan and M. Beal and D. Blei. Hierarchical Dirichlet
processes. JASA, 2006. Sets up the HDP, a mechanism of tying diﬀerent DPs
together (gives another treatment of HDP-HMMs).

Resources / References

81

References (variational inference)
1. D. Blei and A. Ng and M. I. Jordan. Latent Dirichlet allocation. JMLR, 2003.
Uses variational inference for LDA.
2. D. Blei and M. I. Jordan. Variational inference for Dirichlet process mixtures.
Bayesian Analysis, 2005. First mean-ﬁeld algorithm for DP mixtures
(stick-breaking representation).
3. K. Kurihara and M. Welling and N. Vlassis. Accelerated variational Dirichlet
mixture models. NIPS, 2007. Use KD-trees to speed up inference for DP
mixtures (stick-breaking representation).
4. Y. W. Teh and D. Newman and M. Welling. A collapsed variational Bayesian
inference algorithm for Latent Dirichlet Allocation. NIPS, 2007. Variational
inference in the collapsed CRP representation for LDA.
5. K. Kurihara and M. Welling and Y. W. Teh. Collapsed variational Dirichlet
process mixture models. IJCAI, 2007. Compares several variational inference
algorithms.

Resources / References

82

References (Bayesian applications)
1. S. Goldwater and T. Griﬃths. A fully Bayesian approach to unsupervised
part-of-speech tagging. ACL, 2007.
2. M. Johnson. Why doesn’t EM ﬁnd good HMM POS-taggers?.
EMNLP/CoNLL, 2007.
3. K. Kurihara and T. Sato. An application of the variational Bayesian approach
to probabilistic context-free grammars. International Joint Conference on
Natural Language Processing Workshop Beyond Shallow Analyses, 2004.
4. K. Kurihara and T. Sato. Variational Bayesian grammar induction for natural
language. International Colloquium on Grammatical Inference, 2006.
5. M. Johnson and T. Griﬃths and S. Goldwater. Adaptor grammars: a
framework for specifying compositional nonparametric Bayesian models. NIPS,
2006.
6. H. Daume and D. Marcu. Bayesian query-focused summarization.
COLING/ACL, 2006.

Resources / References

83

References (nonparametric applications)
1. M. Johnson and T. Griﬃths and S. Goldwater. Adaptor grammars: a
framework for specifying compositional nonparametric Bayesian models. NIPS,
2006.
2. J. R. Finkel and T. Grenager and C. Manning. The inﬁnite tree. ACL, 2007.
3. P. Liang and S. Petrov and M. I. Jordan and D. Klein. The inﬁnite PCFG using
hierarchical Dirichlet processes. EMNLP/CoNLL, 2007.
4. S. Goldwater and T. Griﬃths and M. Johnson. Interpolating between types and
tokens by estimating power-law generators. NIPS, 2005.
5. Y. W. Teh. A hierarchical Bayesian language model based on Pitman-Yor
processes. COLING/ACL, 2006.
6. H. Daume and D. Marcu. A Bayesian model for supervised clustering with the
Dirichlet process prior. JMLR, 2005.
7. S. Goldwater and T. Griﬃths and M. Johnson. Contextual dependencies in
unsupervised word segmentation. COLING/ACL, 2006.
8. A. Haghighi and D. Klein. Unsupervised coreference resolution in a
nonparametric Bayesian model. ACL, 2007.
Resources / References

84

Resources
• References
• Derivations
• Glossary and notation

Resources / Derivations

85

KL-divergence and normalization
def

KL(q||p) = Eq log

q(θ)
p(θ)

Usually, we only know p up to normalization:
Example: p = p(θ | x), p = p(θ, x), Z = p(x)
˜
p (unnormalized distribution: tractable)
˜
p=
Z (normalization constant: intractable)

Resources / Derivations

86

KL-divergence and normalization
def

KL(q||p) = Eq log

q(θ)
p(θ)

Usually, we only know p up to normalization:
Example: p = p(θ | x), p = p(θ, x), Z = p(x)
˜
p (unnormalized distribution: tractable)
˜
p=
Z (normalization constant: intractable)
No problem, due to the following identity:
KL(q||p) = KL(q||˜) + log Z
p

Resources / Derivations

86

KL-divergence and normalization
def

KL(q||p) = Eq log

q(θ)
p(θ)

Usually, we only know p up to normalization:
Example: p = p(θ | x), p = p(θ, x), Z = p(x)
˜
p (unnormalized distribution: tractable)
˜
p=
Z (normalization constant: intractable)
No problem, due to the following identity:
KL(q||p) = KL(q||˜) + log Z
p
• argminq KL(q||p) = argminq KL(q||˜)
p
• Since KL ≥ 0, we get a lower bound on log Z as a bonus:
log Z ≥ −KL(q||˜)
p
Resources / Derivations

intractable

tractable

86

KL-divergence and normalization
KL(q||p)

(1)

1. This is the quantity we want to minimize.

Resources / Derivations

87

KL-divergence and normalization
KL(q||p)

(1)

q(θ)
= Eq [log
]
p(θ)

(2)

2. Expand the deﬁnition of KL.

Resources / Derivations

87

KL-divergence and normalization
KL(q||p)
= Eq [log

(1)
q(θ)
]
p(θ)

= Eq [log q(θ) − log p(θ)]

(2)

(3)

3. log turns division into subtraction.

Resources / Derivations

87

KL-divergence and normalization
KL(q||p)

(1)
q(θ)
]
p(θ)

(2)

= Eq [log q(θ) − log p(θ)]

(3)

= Eq [log q(θ)] − Eq [log p(θ)]

(4)

= Eq [log

4. Use linearity of expectation.

Resources / Derivations

87

KL-divergence and normalization
KL(q||p)

(1)
q(θ)
]
p(θ)

(2)

= Eq [log q(θ) − log p(θ)]

(3)

= Eq [log q(θ)] − Eq [log p(θ)]

(4)

= −H(q) − Eq [log p(θ)]

(5)

= Eq [log

5. Substitute deﬁnition of entropy (see next
slide).
Resources / Derivations

87

KL-divergence and normalization
KL(q||p)

(1)
q(θ)
]
p(θ)

(2)

= Eq [log q(θ) − log p(θ)]

(3)

= Eq [log q(θ)] − Eq [log p(θ)]

(4)

= −H(q) − Eq [log p(θ)]

(5)

p(θ)
˜
= −H(q) − Eq [log
]
Z

(6)

= Eq [log

6. Expand p into the tractable p(θ) and
˜
intractable Z.
Resources / Derivations

87

KL-divergence and normalization
KL(q||p)

(1)
q(θ)
]
p(θ)

(2)

= Eq [log q(θ) − log p(θ)]

(3)

= Eq [log q(θ)] − Eq [log p(θ)]

(4)

= −H(q) − Eq [log p(θ)]

(5)

p(θ)
˜
]
Z

(6)

= −H(q) − Eq [log p(θ)] +log Z
˜

(7)

= Eq [log

= −H(q) − Eq [log

free energy

7. Use property of log, linearity of expectation.

Resources / Derivations

87

KL-divergence and normalization
KL(q||p)

(1)
q(θ)
]
p(θ)

(2)

= Eq [log q(θ) − log p(θ)]

(3)

= Eq [log q(θ)] − Eq [log p(θ)]

(4)

= −H(q) − Eq [log p(θ)]

(5)

p(θ)
˜
]
Z
= −H(q) − Eq [log p(θ)] +log Z
˜

(6)

= Eq [log

= −H(q) − Eq [log

(7)

free energy

= KL(q||˜) + log Z
p

(8)

8. Reverse the derivation without log Z.

Resources / Derivations

87

KL-divergence and normalization
KL(q||p)

(1)
q(θ)
]
p(θ)

(2)

= Eq [log q(θ) − log p(θ)]

(3)

= Eq [log q(θ)] − Eq [log p(θ)]

(4)

= −H(q) − Eq [log p(θ)]

(5)

p(θ)
˜
]
Z
= −H(q) − Eq [log p(θ)] +log Z
˜

(6)

= Eq [log

= −H(q) − Eq [log

(7)

free energy

= KL(q||˜) + log Z
p

Resources / Derivations

(8)

87

Entropy
Entropy measures the amount of uncertainty in a distribution.
def

Deﬁnition: H(q) = − Eq log q(θ) = − q(θ) log q(θ)dθ
Property: H(q) ≥ 0
Suppose q is fully-factorized (as in mean-ﬁeld): q(θ) =

n
i=1 qi (θi )

Then the entropy decomposes:
H(q)

(1)

1. The entropy of the entire distribution.

Resources / Derivations

88

Entropy
Entropy measures the amount of uncertainty in a distribution.
def

Deﬁnition: H(q) = − Eq log q(θ) = − q(θ) log q(θ)dθ
Property: H(q) ≥ 0
n
i=1 qi (θi )

Suppose q is fully-factorized (as in mean-ﬁeld): q(θ) =
Then the entropy decomposes:
H(q)

= −Eq [log

(1)
n
i=1 qi (θi )]

(2)

2. Expand deﬁnition of entropy and mean-ﬁeld
distribution.
Resources / Derivations

88

Entropy
Entropy measures the amount of uncertainty in a distribution.
def

Deﬁnition: H(q) = − Eq log q(θ) = − q(θ) log q(θ)dθ
Property: H(q) ≥ 0
n
i=1 qi (θi )

Suppose q is fully-factorized (as in mean-ﬁeld): q(θ) =
Then the entropy decomposes:
H(q)

(1)
n

= −Eq [log i=1 qi(θi)]
n
= i=1 −Eq [log qi(θi)]

(2)

(3)

3. log turns products into sum; linearity of
expectation.
Resources / Derivations

88

Entropy
Entropy measures the amount of uncertainty in a distribution.
def

Deﬁnition: H(q) = − Eq log q(θ) = − q(θ) log q(θ)dθ
Property: H(q) ≥ 0
n
i=1 qi (θi )

Suppose q is fully-factorized (as in mean-ﬁeld): q(θ) =
Then the entropy decomposes:
H(q)

(1)
n
i=1 qi (θi )]

(2)

n
i=1 −Eq [log qi (θi )]

(3)

= −Eq [log
=

=

n
i=1 −Eqi [log qi (θi )]

(4)

4. The expectation only involves θi.

Resources / Derivations

88

Entropy
Entropy measures the amount of uncertainty in a distribution.
def

Deﬁnition: H(q) = − Eq log q(θ) = − q(θ) log q(θ)dθ
Property: H(q) ≥ 0
n
i=1 qi (θi )

Suppose q is fully-factorized (as in mean-ﬁeld): q(θ) =
Then the entropy decomposes:
H(q)

(1)

= −Eq [log
=
=

=

n
i=1 qi (θi )]

n
i=1 −Eq [log qi (θi )]
n
i=1 −Eqi [log qi (θi )]

n
i=1 H(qi )

(2)
(3)
(4)

(5)

5. Deﬁnition of entropy.

Resources / Derivations

88

Entropy
Entropy measures the amount of uncertainty in a distribution.
def

Deﬁnition: H(q) = − Eq log q(θ) = − q(θ) log q(θ)dθ
Property: H(q) ≥ 0
n
i=1 qi (θi )

Suppose q is fully-factorized (as in mean-ﬁeld): q(θ) =
Then the entropy decomposes:
H(q)

(1)

= −Eq [log
=
=

=

Resources / Derivations

n
i=1 qi (θi )]

n
i=1 −Eq [log qi (θi )]
n
i=1 −Eqi [log qi (θi )]

n
i=1 H(qi )

(2)
(3)
(4)

(5)

88

Formal derivation of mean-ﬁeld updates
Goal: optimize each qi holding q−i ﬁxed.
Strategy: manipulate KL(q||p), throwing away terms that do not
depend on qi.
KL(q||p)
(1)

1. This is the quantity we want to minimize.

Resources / Derivations

89

Formal derivation of mean-ﬁeld updates
Goal: optimize each qi holding q−i ﬁxed.
Strategy: manipulate KL(q||p), throwing away terms that do not
depend on qi.
KL(q||p)

(1)

= −H(q) − Eq [log p(θ)]

(2)

2. Expand KL using a previous derivation. Write p(θ | x) as
p(θ) to simplify notation.

Resources / Derivations

89

Formal derivation of mean-ﬁeld updates
Goal: optimize each qi holding q−i ﬁxed.
Strategy: manipulate KL(q||p), throwing away terms that do not
depend on qi.
KL(q||p)

(1)

= −H(q) − Eq [log p(θ)]

(2)

−H(qj ) − Eq [log p(θ)]

=

(3)

j

3. The entropy term decouples because q is fully-factorized.

Resources / Derivations

89

Formal derivation of mean-ﬁeld updates
Goal: optimize each qi holding q−i ﬁxed.
Strategy: manipulate KL(q||p), throwing away terms that do not
depend on qi.
KL(q||p)

(1)

= −H(q) − Eq [log p(θ)]

(2)

−H(qj ) − Eq [log p(θ)]

=

(3)

j

= −H(qi) − Eq [log p(θ)] + C

(4)

4. Only the entropy of qi matters; the constant C absorbs the
other terms.

Resources / Derivations

89

Formal derivation of mean-ﬁeld updates
Goal: optimize each qi holding q−i ﬁxed.
Strategy: manipulate KL(q||p), throwing away terms that do not
depend on qi.
KL(q||p)

(1)

= −H(q) − Eq [log p(θ)]

(2)

−H(qj ) − Eq [log p(θ)]

=

(3)

j

= −H(qi) − Eq [log p(θ)] + C

(4)

= −H(qi) − Eqi [Eq−i log p(θ)] + C

(5)

5. Expand the expectation as an iterated conditional
expectation.

Resources / Derivations

89

Formal derivation of mean-ﬁeld updates
Goal: optimize each qi holding q−i ﬁxed.
Strategy: manipulate KL(q||p), throwing away terms that do not
depend on qi.
KL(q||p)

(1)

= −H(q) − Eq [log p(θ)]

(2)

−H(qj ) − Eq [log p(θ)]

=

(3)

j

= −H(qi) − Eq [log p(θ)] + C

(4)

= −H(qi) − Eqi [Eq−i log p(θ)] + C

(5)

= −H(qi) − Eqi [log(exp Eq−i log p(θ))] + C

(6)

6. Applying log exp has no eﬀect, but puts the expression in a
convenient form.

Resources / Derivations

89

Formal derivation of mean-ﬁeld updates
Goal: optimize each qi holding q−i ﬁxed.
Strategy: manipulate KL(q||p), throwing away terms that do not
depend on qi.
KL(q||p)

(1)

= −H(q) − Eq [log p(θ)]

(2)

−H(qj ) − Eq [log p(θ)]

=

(3)

j

= −H(qi) − Eq [log p(θ)] + C

(4)

= −H(qi) − Eqi [Eq−i log p(θ)] + C

(5)

= −H(qi) − Eqi [log(exp Eq−i log p(θ))] + C

(6)

= KL(qi||exp Eq−i log p(θ)) + C

(7)

7. Recognize the expression as a KL divergence.

Resources / Derivations

89

Formal derivation of mean-ﬁeld updates
Goal: optimize each qi holding q−i ﬁxed.
Strategy: manipulate KL(q||p), throwing away terms that do not
depend on qi.
KL(q||p)

(1)

= −H(q) − Eq [log p(θ)]

(2)

−H(qj ) − Eq [log p(θ)]

=

(3)

j

= −H(qi) − Eq [log p(θ)] + C

(4)

= −H(qi) − Eqi [Eq−i log p(θ)] + C

(5)

= −H(qi) − Eqi [log(exp Eq−i log p(θ))] + C

(6)

= KL(qi||exp Eq−i log p(θ)) + C

(7)

Recall that KL is minimized when the two arguments are equal.
Conclusion: argminqi KL(q||p) ∝ exp{Eq−i log p(θ)}.
Resources / Derivations

89

Derivation of exp Ψ
def

Goal: show W (z) = exp{EDirichlet(φ;α) log φz } =

exp{Ψ(αz )}
P
exp{Ψ( z αz }

Write the Dirichlet distribution in exponential family form:
p(φ | α) = exp

αz log φz −
z

log Γ(αz ) − log Γ
z

αz
z

Suﬃcient statistics: log φz
Log-partition function:

z

log Γ(αz ) − log Γ

z

αz

Fact: mean of suﬃcient statistics = derivative of log-partition function
Deﬁnition: the digamma function is Ψ(x) =
∂A(α)
E log φz =
= Ψ(αz ) −
Ψ(αz )
∂αz

∂ log Γ(x)
∂x

z

Exponentiating both sides:

Resources / Derivations

exp{Ψ(αz )}
exp{E log φz } =
exp{Ψ( z αz }

90

Resources
• References
• Derivations
• Glossary and notation

Resources / Glossary and notation

91

Glossary
Bayesian inference: A methodology whereby a prior over
parameters is combined with the likelihood of observed data to
produce a posterior using the laws of probability.
Chinese restaurant process: The distribution of the clustering
induced by draws from the Dirichlet process (marginalizing out
component probabilities and parameters).
Conjugacy: Two distributions are conjugate (e.g., Dirichlet and
multinomial).
Dirichlet distribution: A distribution over (parameters of) ﬁnite
probability distributions.
Dirichlet process: A distribution over arbitrary distributions
(generalizes the Dirichlet distribution).

Resources / Glossary and notation

92

Glossary
Likelihood: The probability of observing the data.
Marginalization: Integrating or summing out unobserved quantities
in a model.
Marginal likelihood: The probability of the observed data,
marginalizing out unknown parameters. This quantity is lower
bounded in variational inference.
Markov Chain Monte Carlo: A randomized approximate inference
algorithm with nice asymptotic properties.
Mean-ﬁeld: A fully-factorized approximation for variational
inference.

Resources / Glossary and notation

93

Glossary
Nonparametric: Refers to models where the number of eﬀective
parameters can grow with the amount of data. The Dirichlet
process is an example of a nonparametric prior.
Posterior: The distribution over unknown quantities in a model
(parameters) conditioned on the observed data.
Prior: A distribution over unknown quantities in the model
(parameters) before observing data.
Stick-breaking representation: A constructive deﬁnition of the
Dirichlet process prior.
Variational Bayes: Variational inference for computing the
posterior in Bayesian models.

Resources / Glossary and notation

94

Notation
θ All parameters (β, φ)
z All hidden variables
βz Probability of component z
vz Stick-breaking proportion of component z
φz Parameters of component z
zi Component that point i is assigned to
xi Data point i
α0 Concentration parameter of the Dirichlet process prior
G0 Base distribution parameter of the Dirichlet process prior
G A draw from the Dirichlet process
ψi Component parameters used to generate point i

Resources / Glossary and notation

95

