Estimating Latent-Variable Graphical Models using Moments and Likelihoods

Arun Tejasvi Chaganty
Percy Liang
Stanford University, Stanford, CA, USA

CHAGANTY @ CS . STANFORD . EDU
PLIANG @ CS . STANFORD . EDU

Abstract
Recent work on the method of moments enable
consistent parameter estimation, but only for certain types of latent-variable models. On the other
hand, pure likelihood objectives, though more
universally applicable, are difﬁcult to optimize.
In this work, we show that using the method of
moments in conjunction with composite likelihood yields consistent parameter estimates for a
much broader class of discrete directed and undirected graphical models, including loopy graphs
with high treewidth. Speciﬁcally, we use tensor
factorization to reveal information about the hidden variables. This allows us to construct convex
likelihoods which can be globally optimized to
recover the parameters.

xa
1

xb
1

xa
1

xb
1

h1

h1

xa
2

xa
1

xa
3

h2
xb
2
xa
4

xb
1

h1

h2

h3

xb
2

xb
3

h4

xa
1

xb
1

h1

xa
2

xa
3

h3
xa
2

xa
3

h2
xb
2

θ

xb
3

h4
xa
4

xb
3

h3

xb
4

xb
4

1. G ET C ONDITIONALS

2. G ET M ARGINALS

3. G ET PARAMETERS

Figure 1. Overview of our approach: (i) we use tensor factorization to learn the conditional moments for each hidden variable;
(ii) we optimize a composite likelihood to recover the hidden
marginals; and (iii) we optimize another likelihood objective to
the model parameters. Both likelihood objectives are convex.

of models, and are thus not as broadly applicable as EM.

1. Introduction
Latent-variable graphical models provide compact representations of data and have been employed across many
ﬁelds (Ghahramani & Beal, 1999; Jaakkola & Jordan,
1999; Blei et al., 2003; Quattoni et al., 2004; Haghighi &
Klein, 2006). However, learning these models remains a
difﬁcult problem due to the non-convexity of the negative
log-likelihood. Local methods such as expectation maximization (EM) are the norm, but are susceptible to local
optima.
Recently, unsupervised learning techniques based on the
spectral method of moments have offered a refreshing perspective on this learning problem (Mossel & Roch, 2005;
Hsu et al., 2009; Bailly et al., 2010; Song et al., 2011;
Anandkumar et al., 2011; 2012b;a; Hsu et al., 2012; Balle
& Mohri, 2012). These methods exploit the linear algebraic
properties of the model to factorize moments of the observed data distribution into parameters, providing strong
theoretical guarantees. However, they apply to a limited set
Proceedings of the 31 st International Conference on Machine
Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s).

In this paper, we show that a much broader class of discrete directed and undirected graphical models can be consistently estimated: speciﬁcally those in which each hidden
variable has three conditionally independent observed variables (“views”). Our key idea is to leverage the method
of moments, not to directly provide a consistent parameter estimate as in previous work, but as constraints on a
likelihood-based objective. Notably, our method applies to
latent undirected log-linear models with high treewidth.
The essence of our approach is illustrated in Figure 1,
which contains three steps. First, we identify three views
for each hidden variable hi (for example, xa , xb and xa
1
1
3
are conditionally independent given h1 ) and use the tensor
factorization algorithm of Anandkumar et al. (2013) to estimate the conditional moments P(xa | hi ) and P(xb | hi )
i
i
for each i (Section 3). Second, we optimize a composite marginal likelihood to recover the marginals over subsets of hidden nodes (e.g., P(h2 , h3 , h4 )). Normally, such
a marginal likelihood objective would be non-convex, but
given the conditional moments, we obtain a convex objective, which can be globally optimized using EM (see Sections 4 and 4.2). So far, our method has relied only on the
conditional independence structure of the model and applies generically to both directed and undirected models.

Estimating Latent-Variable Graphical Models using Moments and Likelihoods

The ﬁnal step of turning hidden marginals into model parameters requires some specialization. In the directed case,
this is simple normalization; in the undirected case, we
need to solve another convex optimization problem (Section 5).

2. Setup

then u ⊗ v ∈ Rd×k . For an -th order tensor T ∈ Rd×...×d
and vectors v1 , · · · , v ∈ Rd , deﬁne the application:
T (v1 , · · · , v ) =

T [i]v1 [i1 ] · · · v [i ].

Analogously, for matrices M1 ∈ Rd×k , · · · , M ∈ Rd×k :
T (M1 , · · · , M )[j] =

Let G be a discrete graphical model with observed variables
x = (x1 , . . . , xL ) and hidden variables h = (h1 , . . . , hM ).
We assume that the domains of the variables are xv ∈ [d]
for all v ∈ [L] and hi ∈ [k] for all i ∈ [M ], where [n] =
{1, . . . , n}. Let X
[d]L and H
[k]M be the joint
domains of x and h, respectively.

i

i

T [i]M1 [i1 , j1 ] · · · M [i , j ].

We will use P(·) to denote various moment tensors constructed from the true data distribution pθ∗ (x, h):
Mi

P(xi ),

Mij

P(xi , xj ),

Mijk

P(xi , xj , xk ).

Here, Mi , Mij , Mijk are tensors of orders 1, 2, 3 in
Rd , Rd×d , Rd×d×d . Next, we deﬁne the hidden marginals:

For undirected models G, let G denote a set of cliques,
where each clique C ⊆ x ∪ h is a subset of nodes. The joint
distribution is given by an exponential family: pθ (x, h) ∝
C∈G exp(θ φC (xC , hC )), where θ is the parameter vector, and φC (xC , hC ) is the local feature vector which only
depends on the observed (xC ) and hidden (hC ) variables in
clique C. Also deﬁne N (a) = {b = a : ∃C ⊇ {a, b}} to be
the neighbors of variable a.

2.1. Assumptions

For directed models G, deﬁne pθ (x, h) = a∈x∪h pθ (a |
Pa(a)), where Pa(a) ⊆ x ∪ h are the parents of a variable
a. The parameters θ are the conditional probability tables
of each variable, and the cliques are G = {{a} ∪ Pa(a) :
a ∈ x ∪ h}.

In this section, we state technical assumptions that hold for
the rest of the paper, but that we feel are not central to our
main ideas. The ﬁrst one ensures that all realizations of
each hidden variable are possible:
Assumption 1 (Non-degeneracy). The marginal distribution of each hidden variable hi has full support: P(hi ) 0.

Problem statement This paper focuses on the problem
of parameter estimation: We are given n i.i.d. examples of
the observed variables D = (x(1) , . . . , x(n) ), where each
x(i) ∼ pθ∗ for some true parameters θ∗ . Our goal is to
ˆ
produce a parameter estimate θ that approximates θ∗ .
The standard estimation procedure is maximum likelihood:
Lunsup (θ)

log pθ (x) =
x∈D

pθ (x, h). (1)

log
x∈D

h∈H

Maximum likelihood is statistically efﬁcient, but in general
computationally intractable because marginalizing over
hidden variables h yields a non-convex objective. In practice, one uses local optimization procedures (e.g., EM or LBFGS) on the marginal likelihood, but these can get stuck
in local optima. We will later return to likelihoods, but let
us ﬁrst describe a method of moments approach for parameter estimation. To do this, let’s introduce some notation.
Notation We use the notation [·] to indicate indexing; for
example, M [i] is the i-th row of a matrix M and M [i, j]
is the (i, j)-th element of M . For a tensor T ∈ Rd×···×d
and a vector i = (i1 , . . . , i ), deﬁne the projection T [i] =
T [i1 , . . . , i ].
We use ⊗ to denote the tensor product: if u ∈ Rd , v ∈ Rk ,

Zi

P(hi ),

Zij

P(hi , hj ),

Zijk

P(hi , hj , hk ).

These are tensors of orders 1, 2, 3 in Rk , Rk×k , Rk×k×k .
Finally, we deﬁne conditional moments O(v|i)
P(xv |
hi ) ∈ Rd×k for each v ∈ [L] and i ∈ [M ].

Next, we assume the graphical model only has conditional
independences given by the graph:
Assumption 2 (Faithful). For any hidden variables
a, b, c ∈ h such that an active trail1 connects a and b conditioned on c, we have that a and b are dependent given
c.
Finally, we assume the graphical model is in a canonical
form in which all observed variables are leaves:
Assumption 3 (Canonical form). For each observed variable xv , there exists exactly one C ∈ G such that C =
{xv , hi } for some hidden node hi .
The following lemma shows that this is not a real assumption (see the appendix for the proof):
Lemma 1 (Reduction to canonical form). Every graphical
model can be transformed into canonical form. There is a
one-to-one correspondence between the parameters of the
transformed and original models.
Finally, for clarity, we will derive our algorithms using exact moments of the true distribution pθ∗ . In practice, we
would use moments estimated from data D.

1
See Koller & Friedman (2009) for a deﬁnition. We do not
condition on observed variables.

Estimating Latent-Variable Graphical Models using Moments and Likelihoods

3. Bottlenecks
We start by trying to reveal some information about the
hidden variables that will be used by subsequent sections. Speciﬁcally, we review how the tensor factorization
method of Anandkumar et al. (2013) can be used to recover
the conditional moments O(v|i) P(xv | hi ). The key notion is that of a bottleneck:
Deﬁnition 1 (Bottleneck). A hidden variable hi is said to
be a bottleneck if (i) there exists three observed variables
(views), xv1 , xv2 , xv3 , that are conditionally independent
given hi (Figure 2(a)), and (ii) each O(v|i) P(xv | hi ) ∈
Rd×k has full column rank k for each v ∈ {v1 , v2 , v3 }. We
say that a subset of hidden variables S ⊆ h is bottlenecked
if every h ∈ S is a bottleneck. We say that a graphical
model G is bottlenecked if all its hidden variables are bottlenecks.
For example, in Figure 1, xa , xb , xa are views of the bot1
1
2
tleneck h1 , and xa , xb , xb are views of the bottleneck h2 .
2
2
1
Therefore, the clique {h1 , h2 } is bottlenecked. Note that
views are allowed to overlap.
The full rank assumption on the conditional moments
O(v|i) = P(xv | hi ) ensures that all states of hi “behave
differently.” In particular, the conditional distribution of
one state cannot be a mixture of that of other states.
Anandkumar et al. (2012a) provide an efﬁcient tensor factorization algorithm for estimating P(xv | hi ):
Theorem 1 (Tensor factorization). Let hi ∈ h be a bottleneck with views xv1 , xv2 , xv3 . Then there exists an algorithm G ET C ONDITIONALS that returns consistent estimates of O(v|i) for each v ∈ {v1 , v2 , v3 } up to relabeling
of the hidden variables.

x1

h1

h1
x2

h2

h4
h3

x1

x2

(a) Bottleneck

x3

x4

S

x3

(b) Exclusive views

Figure 2. (a) A bottleneck h1 has three conditionally independent
views x1 , x2 , x3 . (b) A bidependent subset S has exclusive views
{x1 , x2 , x3 , x4 }.

singular value of O(v|i) . Note that σk (O(v|i) ) can become
quite small if hi and xv are connected via many intermediate hidden variables.2
The tensor factorization method attacks the heart of the
non-convexity in latent-variable models, providing some
information about the hidden variables in the form of
the conditional moments O(v|i) = P(xv | hi ). Note
that G ET C ONDITIONALS only examines the conditional
independence structure of the graphical model, not its
parametrization.
If i is the single parent of v (e.g., P(xa | h1 ) in Figure 1),
1
then this conditional moment is a parameter of the model,
but this is in general not the case (e.g., P(xa | h1 )). Fur2
thermore, there are other parameters (e.g., P(h4 | h2 , h3 ))
which we do not have a handle on yet. In general, there
is a gap between the conditional moments and the model
parameters, which we will address in the next two sections.

4. Recovering hidden marginals

To simplify notation, consider the example in Figure 2(a)
where h1 = 1, v1 = 1, v2 = 2, v3 = 3. The observed
moments M12 , M23 , M13 and M123 can be factorized as
follows:

Having recovered conditional moments O(v|i)
P(xv |
hi ), we now seek to compute the marginal distribution of
sets of hidden variables ZS P(hS ).

π (1) [h]O(v|1) [h] ⊗ O(v |1) [h]

Example To gain some intuition, consider the directed
grid model from Figure 1. We can express the observed
marginals M12 P(xa , xa ) ∈ Rd×d as a linear function of
1
2
the hidden marginals Z12 P(h1 , h2 ) ∈ Rk×k , where the
linear coefﬁcients are based on the conditional moments
O(1|1) , O(2|2) ∈ Rd×k :

Mvv =
h

M123 =
h

π (1) [h]O(1|1) [h] ⊗ O(2|1) [h] ⊗ O(3|1) [h].

The G ET C ONDITIONALS algorithm ﬁrst computes a
whitening matrix W ∈ Rd×k such that W M12 W =
Ik×k , and uses W to transform M123 into a symmetric
orthogonal tensor. Then a robust tensor power method is
used to extract the eigenvectors of the whitened M123 ; unwhitening yields the columns of O(3|1) (up to permutation).
The other conditional moments can be recovered similarly.
The resulting estimate of O(v|i) based on n data points con1
verges at a rate of n− 2 with a constant that depends poly(v|i) −1
nomially on σk (O
) , the inverse of the k-th largest

M12 = O(1|1) Z12 O(2|2) .
We can then solve for Z12 by matrix inversion:
Z12 = O(1|1)† M12 O(2|2)† .
2
To see this, suppose h1 has a view xv via a chain: h1 −
h2 · · · − ht − xv . In this example, if σk (P(hi+1 | hi )) = ak for
each i = 1, · · · , t − 1, then σk (O(v|1) ) = at σk (O(v|t) ).
k

Estimating Latent-Variable Graphical Models using Moments and Likelihoods

4.1. Exclusive views
For which subsets of hidden nodes can we recover the
marginals? The following deﬁnition offers a characterization:
Deﬁnition 2 (Exclusive views). Let S ⊆ h be a subset of
hidden variables. We say hi ∈ S has an exclusive view xv
if the two conditions hold: (i) there exists some observed
variable xv which is conditionally independent of the others S\{hi } given hi (Figure 2(b)), and (ii) the conditional
moment matrix O(v|i)
P(xv | hi ) has full column rank
k and can be recovered. We say that S has the exclusive
views property if every hi ∈ S has an exclusive view.
Estimating hidden marginals We now show that if a
subset of hidden variables S has the exclusive views property, then we can recover the marginal distribution P(hS ).
Consider any S = {hi1 , . . . , him } with the exclusive views
property. Let xvj be an exclusive view for hij in S and deﬁne V = {xv1 , . . . , xvm }. By the exclusive views property,
the marginal over the observed variables P(xV ) factorizes
according to the marginal over the hidden variables P(hS )
times the conditional moments:
MV

hS

views property allows recovery of hidden marginals. But
we will now show that the latter property is in fact implied
by the former property for special sets of hidden variables,
which we call bidependent sets (in analogy with biconnected components), in which conditioning on one variable
does not break the set apart:
Deﬁnition 3 (Bidependent set). We say that a subset of
nodes S is bidependent if conditioned on any a ∈ S, there
is an active trail between any other two nodes b, c ∈ S.
Note that all cliques are bidependent, but bidependent sets can have more conditional independences (e.g.,
{h1 , h2 , h3 } in Figure 2(b)). This will be important in Section 5.1.
Bidependent sets are signiﬁcant because they guarantee exclusive views if they are bottlenecked:

P(xV )
=

Algorithm 1 G ET M ARGINALS (pseudoinverse)
Input: Hidden subset S = {hi1 , . . . , him } with exclusive
views V = {xv1 , . . . , xvm } and conditional moments
O(vj |ij ) = P(xvj | hij ).
Output: Marginals ZS = P(hS ).
†
†
Return ZS ← MV (O(v1 |i1 ) , . . . , O(vm |im ) ).

P(hS ) P(xv1 | hi1 ) · · · P(xvm | him )

= ZS (O(v1 |i1 ) , . . . , O(vm |im ) )

Lemma 2 (Bottlenecked implies exclusive views). Let S ⊆
h be a bidependent subset of hidden variables. If S is bottlenecked, then S has the exclusive views property.

= ZS (O),
where O = O(v1 |i1 ) ⊗ · · · ⊗ O(vm |im ) is the tensor product
of all the conditional moments. Vectorizing, we have that
m
m
m
m
ZS ∈ Rk , MV ∈ Rd , and O ∈ Rd ×k . Since each
O(v|i) has full column rank k, the tensor product O has full
column rank k m . Succinctly, MV (which can be estimated
directly from data) is a linear function of ZS (what we seek
to recover). We can solve for the hidden marginals ZS simply by multiplying MV by the pseudoinverse of O:
†

†

ZS = MV (O(v1 |i1 ) , · · · , O(vm |im ) ).
Algorithm 1 summarizes the procedure, G ET M ARGINALS.
Given ZS , the conditional probability tables for S can easily be obtained via renormalization.
Theorem 2 (Hidden marginals from exclusive views). If
S ⊆ x is a subset of hidden variables with the exclusive views property, then Algorithm 1 recovers the
marginals ZS = P(hS ) up to a global relabeling of
the hidden variables determined by the labeling from
G ET C ONDITIONALS.
Relationship to bottlenecks The bottleneck property allows recovery of conditional moments, and the exclusive

Proof. Let S be a bidependent subset and ﬁx any h0 ∈ S.
Since h0 is a bottleneck, it has three conditionally independent views, say x1 , x2 , x3 without loss of generality. For
condition (i), we will show that at least one of the views
is conditionally independent of S\{h0 } given h0 . For the
sake of contradiction, suppose that each observed variable
xi is conditionally dependent on some hi ∈ S\{h0 } given
h0 , for i ∈ {1, 2, 3}. Then conditioned on h0 , there is an
active trail between h1 and h2 because S is biconnected.
This means there is also an active trail x1 − h1 − h2 − x2
conditioned on h0 . Since the graphical model is faithful
by assumption, we have x1 ⊥ x2 | h0 , contradicting the
fact that x1 and x2 are conditionally independent given
h0 . To show condition (ii), assume, without loss of generality, that x1 is an exclusive view. Then we can recover
O(1|0) = P(x1 | h0 ) via G ET C ONDITIONALS.
Remarks. Note that having only two independent views
for each hi ∈ S is sufﬁcient for condition (i) of the exclusive views property, while three is needed for condition (ii).
The bottleneck property (Deﬁnition 1) can also be relaxed
if some cliques share parameters (see examples below).
Our method extends naturally to the case in which the observed variables are real-valued (xv ∈ Rd ), as long as the

Estimating Latent-Variable Graphical Models using Moments and Likelihoods
h1

h1

h2

x1

x2

h2

...

h3
x3

xa
2

(a) Hidden Markov model

x2

xb
2

xa
3

h4
xb
3

xa
4

xb
4

(b) Tree model

h1
x1

h3

h2
x3

x4

x5

(c) Noisy-or model

two changes to circumvent non-convexity: The ﬁrst is that
we already have the conditional moments from tensor decomposition, so effectively a subset of the parameters are
ﬁxed. However, this alone is not enough, for the full likelihood is still non-convex. The second change is that we will
optimize a composite likelihood objective (Lindsay, 1988)
rather than the full likelihood.
Consider a subset of hidden nodes S = {hi1 , . . . , him },
with exclusive views V = {xv1 , . . . , xvm }. The expected
composite log-likelihood over xV given parameters ZS
P(hS ) with respect to the true distribution MV can be written as follows:
Lcl (ZS )

Figure 3. (a) and (b): graphical models that satisfy the exclusive
views property; (c) a graphical model that does not.

hidden variables remain discrete. In this setting, the conditional moments O(v|i)
E(xv | hi ) ∈ Rd×k would no
longer be distributions but general rank k matrices.
Example: hidden Markov model. In the HMM (Figure
3(a)), h2 is a bottleneck, so we can recover O
P(x2 |
h2 ). While the ﬁrst hidden variable h1 is not a bottleneck, it still has an exclusive view x1 with respect to the
clique {h1 , h2 }, assuming parameter sharing across emissions (P(x1 | h1 ) = O).
Example: latent tree model. In the latent tree model
(Figure 3(b)), h1 is not directly connected to an observed
variable, but it is still a bottleneck, with views xa , xa , xa ,
2
3
4
for example. The clique {h1 , h2 } has exclusive views
{xa , xa }.
2
3
Non-example In Figure 3(c), h1 does not have exclusive
views. Without parameter sharing, the techniques in this
paper are insufﬁcient. In the special case where the graphical model represents a binary-valued noisy-or network, we
can use the algorithm of Halpern & Sontag (2013), which
ﬁrst learns h2 and subtracts off its inﬂuence, thereby making h1 a bottleneck.
4.2. Composite likelihood
So far, we have provided a method of moments estimator which used (i) tensor decomposition to recover conditional moments and (ii) matrix pseudoinversion to recover
the hidden marginals. We will now improve statistical efﬁciency by replacing (ii) with a convex likelihood-based
objective.
Of course, optimizing the original marginal likelihood
(Equation 1) is subject to local optima. However, we make

E[log P(xV )]
= E[log
hS

P(hS ) P(xV | hS )]

= E[log ZS (O(v1 |i1 ) [xv1 ], · · · , O(vm |im ) [xvm ])]

= E[log ZS (O[xV ])].

(2)

The ﬁnal expression is an expectation over the log of a linear function of ZS , which is concave in ZS . Unlike maximum likelihood in fully-observed settings, we do not have
a closed-form solution, so we use EM to optimize it. However, since the function is concave, EM is guaranteed to
converge to the global maximum. Algorithm 2 summarizes
our algorithm.
Algorithm 2 G ET M ARGINALS (composite likelihood)
Input: Hidden subset S = {hi1 , . . . , him } with exclusive
views V = {xv1 , . . . , xvm } and conditional moments
O(vj |ij ) = P(xvj | hij ).
Output: Marginals ZS = P(hS ).
Return ZS = arg maxZS ∈∆km −1 E[log ZS (O[xV ])].

4.3. Statistical efﬁciency
We have proposed two methods for estimating the hidden
marginals ZS given the conditional moments O, one based
on computing a simple pseudoinverse, and the other based
ˆ pi
on composite likelihood. Let ZS denote the pseudoinverse
ˆ cl
estimator and ZS denote the composite likelihood estima3
tor.
The Cram´ r-Rao lower bound tells us that maximum likee
lihood yields the most statistically efﬁcient composite estimator for ZS given access to only samples of xV .4 Let
us go one step further and quantify the relative efﬁciency
3

For simplicity, assume that O is known. In practice, O would
be estimated via tensor factorization.
4
Of course, we could improve statistical efﬁciency by maximizing the likelihood of all of x, but this would lead to a nonconvex optimization problem.

Estimating Latent-Variable Graphical Models using Moments and Likelihoods

of the pseudoinverse estimator compared to the composite
likelihood estimator.

Note that z and µ are constrained to lie on simplexes ∆k−1
and ∆d−1 , respectively. To avoid constraints, we reparameterize z and µ using z ∈ Rk−1 and µ ∈ Rd−1 :
µ=

µ
1−1 µ

z=

z
.
1−1 z

2

100

ˆ
θ−θ

Abusing notation slightly, think of MV as just a ﬂat multinomial over dm outcomes and ZS as a multinomial over k m
m
m
outcomes, where the two are related by O ∈ Rd ×k . We
will not need to access the internal tensor structure of MV
and ZS , so to simplify the notation, let m = 1 and deﬁne
µ = MV ∈ Rd , z = ZS ∈ Rk , and O = O ∈ Rd×k . The
hidden marginals z and observed marginals µ are related
via µ = Oz.

101

10−1
Pseudoinverse
Composite likelihood
10−2 0
10

10−1

10−2

10−3

10−4

10−5

ˆ
Figure 4. Comparison of parameter estimation error ( θ − θ 2 )
versus error in moments ( ) for a hidden Markov model with k =
2 hidden and d = 5 observed values. Empirical moments M123
were generated by adding Gaussian noise, N (0, I), to expected
moments M123 . Results are averaged over 400 trials.

In this representation, µ and z are related as follows,
µ
O¬d,¬k
=
Od,¬k
1−1 µ

O¬d,k
Od,k

z
1−1 z

µ = (O¬d,¬k − O¬d,k 1 ) z + O¬d,k .
O
pi

The pseudoinverse estimator is deﬁned as z = O† (µ −
˜
˜
O¬d,k ), and the composite likelihood estimator is given by
cl
ˆ
z = arg maxz E[ (x; z)], where (x; z) = log(µ[x]) is
˜
the log-likelihood function.
First, we compute the asymptotic variances of the two estimators.
Lemma 3 (Asymptotic variances). The asymptotic varipi

ances of the pseudoinverse estimator z and composite
˜
cl
likelihood estimator z are:
˜
Σpi = O† (D − µµ )O† ,
Σcl = O (D−1 + d−1 11 )O
where D

diag(µ) and d

−1

,

1 − 1 µ.

Next, let us compare the relative efﬁciencies of the two es1
cl
pi −1
). From the Cram´ r-Rao
e
timators: epi
k−1 tr(Σ (Σ )
bound (van der Vaart, 1998), we know that Σcl Σpi . This
implies that the relative efﬁciency, epi , lies between 0 and 1,
and when epi = 1, the pseudoinverse estimator is said to be
(asymptotically) efﬁcient. To gain intuition, let us explore
two special cases:
Lemma 4 (Relative efﬁciency when O is invertible). When
O is invertible, the asymptotic variances of the pseudoinverse and composite likelihood estimators are equal, Σcl =
Σpi , and the relative efﬁciency is 1.

Lemma 5 (Relative efﬁciency with uniform observed
marginals). Let the observed marginals µ be uniform: µ =
1
d 1. The efﬁciency of the pseudoinverse estimator is:
epi = 1 −

1
1U 2
k − 1 1 + 1U

1−

2

1
d − 1U

2

,

(3)

where 1U
OO† 1, the projection of 1 onto the column
space of O. Note that 0 ≤ 1U 2 ≤ k − 1.
2
When 1U 2 = 0, the pseudoinverse estimator is efﬁcient:
epi = 1. When 1U 2 > 0 and d > k, the pseudoinverse
estimator is strictly inefﬁcient. In particular, if 1U 2 =
2
k − 1, and we get:
epi = 1 −

1
k

1−

1
1+d−k

.

(4)

Based on Equation 3 and Equation 4, we see that the pseudoinverse gets progressively worse compared to the composite likelihood as the gap between k and d increases for
the special case wherein the observed moments are uniformly distributed. For instance, when k = 2 and d → ∞,
the efﬁcency of the pseudolikelihood estimator is half that
of the composite likelihood estimator. Empirically, we observe that the composite likelihood estimator also leads to
more accurate estimates in general non-asymptotic regimes
(see Figure 4).

5. Recovering parameters
We have thus far shown how to recover the conditional moments O(v|i) = P(xv | hi ) for each exclusive view xv of
each hidden variable hi , as well as the hidden marginals
ZS = P(hS ) for each bidependent subset of hidden variables S. Now all that remains to be done is to recover the
parameters.

Estimating Latent-Variable Graphical Models using Moments and Likelihoods
h1,3
xb
1,3

h2,3
xb
2,3

xa
1,3

xa
2,3

h1,2
xb
1,2
xa
1,2

xa
3,3

h2,2
xb
2,2
xa
2,2

h1,1
xb
1,1
xa
1,1

Algorithm 3 G ET PARAMETERS

h3,3
xb
3,3

h3,2
xb
3,2
xa
3,2

h2,1
xb
2,1
xa
2,1

h3,1
xb
3,1
xa
3,1

Figure 5. Example: undirected grid model where each hidden
variable has two conditionally independent observations. This
model has high treewidth, but we can estimate it efﬁciently using pseudolikelihood.

Since our graphical model is in canonical form (Assumption 3), all cliques C ∈ G either consist of hidden variables hC or are of the form {xv , hi }. The key observation
is that the clique marginals are actually sufﬁcient statistics of the model pθ . How we turn these clique marginals
{P(xC , hC )}C∈G into parameters θ depends on the exact
model parametrization.
For directed models, the parameters are simply the local
conditional tables pθ (a | Pa(a)) for each clique C = {a} ∪
Pa(a). These conditional distributions can be obtained by
simply normalizing ZC for each assignment of Pa(a).
For undirected log-linear models, the canonical parameters
θ cannot be obtained locally, but we can construct a global
convex optimization problem to solve for θ. Suppose we
were able to observe h. Then we could optimize the supervised likelihood, which is concave:
Lsup (θ)

E(x,h)∼pθ∗ [log pθ (x, h)]
=θ

E[φ(xC , hC )]
C∈G

− A(θ).

(5)

E[φ(xC , hC )] =

Remark If we have exclusive views for only a subset of
the cliques, we can still obtain the expected features µC for
those cliques and use posterior regularization (Graca et al.,
¸
2008), measurements (Liang et al., 2009), or generalized
expectation criteria (Mann & McCallum, 2008) to encourage Epθ [φ(xC , hC )] to match µC . The resulting objective
functions would be non-convex, but we expect local optima
to be less of an issue.
5.1. Pseudolikelihood
While we now have a complete algorithm for estimating
directed and undirected models, optimizing the full likelihood (Equation 5) can still be computationally intractable
for undirected models with high treewidth due to the intractability of the log-partition function A(θ). One can
employ various variational approximations of A(θ) (Wainwright & Jordan, 2008), but these generally lead to inconsistent estimates of θ. We thus turn to an older idea of
pseudolikelihood (Besag, 1975). The pseudolikelihood objective is a sum over the log-probability of each variable a
given its neighbors N (a):
Lpseudo (θ)

E(x,h)∼pθ∗
a∈x∪h

Of course we don’t have supervised data, but we do have
the marginals P(xC , hC ), from which we can easily compute the expected features:
µC

Input: Conditional moments O(v|i) = P(xv | hi ) and hidden marginals ZS = P(hS ).
Output: Parameters θ.
if G is directed then
Normalize P(a, Pa(a)) for a ∈ x ∪ h.
else if G is undirected with low treewidth then
Compute features µC for C ∈ G (Equation 6).
Optimize full likelihood (Equation 5).
else if G is undirected with high treewidth then
Compute features µ{a}∪N (a) for a ∈ h (Equation 8).
Optimize pseudolikelihood (Equation 7).
end if

P(xC , hC )φ(xC , hC ).

(6)

xC ,hC

Therefore, we can optimize the supervised likelihood objective without actually having any supervised data! In the
ﬁnite data regime, the method of moments yields the estimate µmom which approximates the true µC . In supervised
ˆC
learning, we obtain a different estimate µsup of µC based
ˆC
on an empirical average over data points. In the limit of
inﬁnite data, both estimators converge to µC .

log pθ (a | N (a)) . (7)

In the fully-supervised setting, it is well-known that pseudolikelihood provides consistent estimates which are computationally efﬁcient but less statistically efﬁciency.5
Let φa,N (a) (a, N (a)) = C a φC (xC , hC ) denote the sum
over cliques C that contain a; note that φa,N (a) only depends on a and its neighbors N (a). We can write each
conditional log-likelihood from Equation 7 as:
pθ (a | N (a)) = exp(θ φa,N (a) (a, N (a)) − Aa (θ; N (a))),
where

the

conditional

log-partition

function

5
Coincidentally, this is the same high-level motivation for using method of moments in the ﬁrst place.

Estimating Latent-Variable Graphical Models using Moments and Likelihoods

Aa (θ; N (a)) = log α∈[k] exp(θ φa,N (a) (α, N (a)))
involves marginalizing only over the single variable a.
If we knew the marginals for each neighborhood,
µa,N (a)

E[φa,N (a) (a, N (a))],

(8)

then we would be able to optimize the pseudolikelihood
objective again without having access to any labeled data.
Unfortunately, {a} ∪ N (a) does not always have exclusive views. For example, consider a = h1 and N (a) =
{h2 , h3 , h4 } in Figure 3(b).
However, we can decompose {a} ∪ N (a) as follows: conditioning on a partitions N (a) into independent subsets; let B(a) be the collection of these subsets, which we will call sub-neighborhoods. For example, B(h1 ) = {{h2 }, {h3 }, {h4 }} in Figure 3(b) and
B(h2,2 ) = {{h1,2 , h2,3 , h3,2 , h2,1 }} contains a single subneighborhood in Figure 5.
A key observation is that for each sub-neighborhood B ∈
B(a), each {a} ∪ B is bidependent: conditioning on a does
not introduce new independencies within B by construction of B(a), and conditioning on any b ∈ B does not either
since every other b ∈ B\{b} is connected to a. Assuming G is bottlenecked, by Lemma 2 we have that {a} ∪ B
has exclusive views. Hence, we can recover P(a, B) for
each a and B ∈ B(a). Based on conditional independence of the sub-neighborhoods B given a, we have that
P(a, N (a)) = P(a) B∈B(a) P(B | a). This allows us to
compute the expected features µa,N (a) and use them in the
optimization of the pseudolikelihood objective.
Note that our pseudolikelihood-based approach does depend exponentially on the size of the sub-neighborhoods,
which could be exceed the largest clique size. Therefore,
each node essentially should have low degree or locally exhibit a lot of conditional independence. On the positive
side, we can handle graphical models with high treewidth;
neither sample nor computational complexity necessarily
depends on the treewidth. For example, an n×n grid model
has a treewidth of n, but the degree is at most 4.

6. Discussion
For latent-variable models, there has been tension between
local optimization of likelihood, which is broadly applicable but offers no global theoretical guarantees, and the
spectral method of moments, which provides consistent estimators but are limited to models with special structure.
The purpose of this work is to show that the two methods
can be used synergistically to produce consistent estimates
for a broader class of directed and undirected models.
Our approach provides consistent estimates for a family of
models in which each hidden variable is a bottleneck—that

is, it has three conditionally independent observations. This
bottleneck property of Anandkumar et al. (2013) has been
exploited in many other contexts, including latent Dirichlet
allocation (Anandkumar et al., 2012b), mixture of spherical
Gaussians (Hsu & Kakade, 2013), probabilistic grammars
(Hsu et al., 2012), noisy-or Bayesian networks (Halpern
& Sontag, 2013), mixture of linear regressions (Chaganty
& Liang, 2013), and others. Each of these methods can
be viewed as “preprocessing” the given model into a form
that exposes the bottleneck or tensor factorization structure.
The model parameters correspond directly to the solution
of the factorization.
In contrast, the bottlenecks in our graphical models are
given by assumption, but the conditional distribution of
the observations given the bottleneck can be quite complex. Our work can therefore be viewed as “postprocessing”, where the conditional moments recovered from tensor
factorization are used to further obtain the hidden marginals
and eventually the parameters. Along the way, we developed the notion of exclusive views and bidependent sets,
which characterize conditions under which the conditional
moments can reveal the dependency structure between hidden variables. We also made use of custom likelihood functions which were constructed to be easy to optimize.
Another prominent line of work in the method of moments
community has focused on recovering observable operator representations (Jaeger, 2000; Hsu et al., 2009; Bailly
et al., 2010; Balle & Mohri, 2012). These methods allow
prediction of new observations, but do not recover the actual parameters of the model, making them difﬁcult to use
in conjunction with likelihood-based models. Song et al.
(2011) proposed an algorithm to learn observable operator representations for latent tree graphical models, like
the one in Figure 3(b), assuming the graph is bottlenecked.
Their approach is similar to our ﬁrst step of learning conditional moments, but they only consider trees. Parikh et al.
(2012) extended this approach to general graphical models
which are bottlenecked using a latent junction tree representation. Consequently, the size of the observable representations is exponential in the treewidth. In contrast, our
algorithm only constructs moments of the order of size of
the cliques (and sub-neighborhoods for pseudolikelihood),
which can be much smaller.
An interesting direction is to examine the necessity of the
bottleneck property. Certainly, three views is in general
needed to ensure identiﬁability (Kruskal, 1977), but requiring each hidden variable to be a bottleneck is stronger
than what we would like. We hope that by judiciously
leveraging likelihood-based methods in conjunction with
the method of moments, we can generate new hybrid techniques for estimating even richer classes of latent-variable
models.

Estimating Latent-Variable Graphical Models using Moments and Likelihoods

References
Anandkumar, A., Chaudhuri, K., Hsu, D., Kakade, S. M.,
Song, L., and Zhang, T. Spectral methods for learning
multivariate latent tree structure. In Advances in Neural
Information Processing Systems (NIPS), 2011.
Anandkumar, A., Hsu, D., and Kakade, S. M. A method of
moments for mixture models and hidden Markov models. In Conference on Learning Theory (COLT), 2012a.
Anandkumar, A., Liu, Y., Hsu, D., Foster, D. P., and
Kakade, S. M. A spectral algorithm for latent dirichlet
allocation. In Advances in Neural Information Processing Systems (NIPS), pp. 917–925, 2012b.
Anandkumar, A., Ge, R., Hsu, D., Kakade, S. M., and Telgarsky, M. Tensor decompositions for learning latent
variable models. Technical report, ArXiv, 2013.
Bailly, R., Habrard, A., and Denis, F. A spectral approach
for probabilistic grammatical inference on trees. In Algorithmic Learning Theory, pp. 74–88. Springer, 2010.
Balle, B. and Mohri, M. Spectral learning of general
weighted automata via constrained matrix completion.
In Advances in Neural Information Processing Systems
(NIPS), pp. 2159–2167, 2012.
Besag, J. The analysis of non-lattice data. The Statistician,
24:179–195, 1975.
Blei, D., Ng, A., and Jordan, M. I. Latent Dirichlet allocation. Journal of Machine Learning Research, 3:993–
1022, 2003.
Chaganty, A. and Liang, P. Spectral experts for estimating
mixtures of linear regressions. In International Conference on Machine Learning (ICML), 2013.
Ghahramani, Z. and Beal, M. J. Variational inference for
Bayesian mixtures of factor analysers. In Advances in
Neural Information Processing Systems, 1999.
Graca, J., Ganchev, K., and Taskar, B. Expectation maxi¸
mization and posterior constraints. In Advances in Neural Information Processing Systems (NIPS), 2008.
Haghighi, A. and Klein, D. Prototype-driven learning for
sequence models. In North American Association for
Computational Linguistics (NAACL), 2006.
Halpern, Y. and Sontag, D. Unsupervised learning of noisyor Bayesian networks. In Uncertainty in Artiﬁcial Intelligence (UAI), pp. 272–281, 2013.
Hsu, D. and Kakade, S. M. Learning mixtures of spherical
Gaussians: Moment methods and spectral decompositions. In Innovations in Theoretical Computer Science
(ITCS), 2013.

Hsu, D., Kakade, S. M., and Zhang, T. A spectral algorithm
for learning hidden Markov models. In Conference on
Learning Theory (COLT), 2009.
Hsu, D., Kakade, S. M., and Liang, P. Identiﬁability and
unmixing of latent parse trees. In Advances in Neural
Information Processing Systems (NIPS), 2012.
Jaakkola, T. S and Jordan, M. I. Variational probabilistic
inference and the QMR-DT network. Journal of Artiﬁcial Intelligence Research, 10:291–322, 1999.
Jaeger, H. Observable operator models for discrete stochastic time series. Neural Computation, 2000.
Koller, D. and Friedman, N. Probabilistic graphical models: principles and techniques. MIT press, 2009.
Kruskal, J. B. Three-way arrays: Rank and uniqueness of
trilinear decompositions, with application to arithmetic
complexity and statistics. Linear Algebra and Applications, 18:95–138, 1977.
Liang, P., Jordan, M. I., and Klein, D. Learning from measurements in exponential families. In International Conference on Machine Learning (ICML), 2009.
Lindsay, B. Composite likelihood methods. Contemporary
Mathematics, 80:221–239, 1988.
Mann, G. and McCallum, A. Generalized expectation criteria for semi-supervised learning of conditional random
ﬁelds. In Human Language Technology and Association
for Computational Linguistics (HLT/ACL), pp. 870–878,
2008.
Mossel, E. and Roch, S. Learning nonsingular phylogenies
and hidden markov models. In Theory of computing, pp.
366–375. ACM, 2005.
Parikh, A., Song, L., Ishteva, M., Teodoru, G., and Xing,
E. A spectral algorithm for latent junction trees. In Uncertainty in Artiﬁcial Intelligence (UAI), 2012.
Quattoni, A., Collins, M., and Darrell, T. Conditional random ﬁelds for object recognition. In Advances in Neural
Information Processing Systems (NIPS), 2004.
Song, Le, Xing, E. P, and Parikh, A. P. A spectral algorithm for latent tree graphical models. In International
Conference on Machine Learning (ICML), 2011.
van der Vaart, A. W. Asymptotic statistics. Cambridge
University Press, 1998.
Wainwright, M. and Jordan, M. I. Graphical models, exponential families, and variational inference. Foundations
and Trends in Machine Learning, 1:1–307, 2008.

Estimating Latent-Variable Graphical Models using Moments and Likelihoods

h1

h1

h2
xv

x v1
x v2

h4

Proof for Lemma 3. First, let us look at the asymptotic
variance of the pseudoinverse estimator z pi = O† (µ −
ˆ
˜
n
1
O¬d,k ). Note that µ = n i=1 xi , where each xi is
ˆ
an independent draw from the multinomial distribution µ.
Hence the variance of µ is (D −µµ ) where D diag(µ).
ˆ
Recall that µ is just the ﬁrst d − 1 entries of µ, so the vari˜
ˆ
ance of µ is (D − µµ ) where D
˜
diag(µ). Since z is
just a linear transformation of µ, the asymptotic variance of

h2
hnew
h4

x v3

Figure 6. Reduction to canonical form.

pi

z is:
˜
Σpi = O† Var(µ)O†
˜

A. Proofs
Due to space limitations, we have omitted some proofs
from the main body of the paper. The proofs are provided
below.

= O† (D − µµ )O† .
Now, let us look at the variance of the composite likelihood estimator. Using the delta-method (van der Vaart,
cl

A.1. Lemma 1
Proof. Let xv be an observed variable which is contained
in more than one clique or in cliques of size larger than
2. We apply the following simple transformation (see Figure 6 for directed models): ﬁrst, replace xv with a a new
hidden variable hnew ; for directed models, this means that
the parents and children of xv become the parents and children of hnew . Second, create three fresh observed variables
xv1 , xv2 , xv3 , connecting them to hnew , and making all new
nodes to deterministically take on identical values. We add
three copies so that hnew is guaranteed to be a bottleneck.
By construction, there is a one-to-one mapping between the
joint distributions of the old and new graphical models, and
thus the parameters as well. We repeatedly apply this procedure until the graphical model is in canonical form.

1998) we have that the asymptotic variance of z
˜
ˆ
arg maxz E[ (x; z)] is,
Σcl = E[

2

(x; z ∗ )]−1 Var[

= log ex

O¬d,k
O
z + ex
1 − 1 O¬d,k
−1 O

Taking the ﬁrst derivative,

In Section 4.2, we compared the asymptotic variance
of the composite likelihood estimator with that of the pseudoinverse estimator, Σpi , for a subset of hidden variables S.
S
Now we will derive these asymptotic variances in detail.

=
where D

Σ = O (D
where D

diag(µ) and d

−1

+d

11 )O

1 − 1 µ.

O
−1 O

ex

D−1 ex ,

E[

(x; z)] =

O
−1 O

D−1 E[ex ]

=

O
−1 O

D−1 µ

=
−1

,

(9)

It is easily veriﬁed that the expectation of the ﬁrst derivative
is indeed 0:

O
−1 O

pi

−1

1
O
µ[x] −1 O

diag(µ).

Lemma (Asymptotic variances). The asymptotic variances of the pseudoinverse estimator z and composite
˜
cl
likelihood estimator z are:
˜

,

where ex is an indicator vector on x.

Σcl
S

cl

(x; z ∗ )]−1 ,

(x; z) = log(µ[x])

(x; z) =

Σpi = O† (D − µµ )O† ,

2

where (x; z) is the log-likelihood of the observations x
given parameters z. We can write (x; z) in terms of z and
O as,

A.2. Lemma 3

Recall, that in Section 4.2 we simpliﬁed notation by taking m = 1 and ﬂattening the moments MV and hidden
marginals ZS into vectors µ ∈ Rd and z ∈ Rk respectively.
The conditional moments, O, is a now matrix O ∈ Rd×k
and the hidden marginals z and observed marginals µ are
related via µ = Oz.

(x; z ∗ )] E[

=

1

=O 1−O 1
= 0.

Estimating Latent-Variable Graphical Models using Moments and Likelihoods

Taking the second derivative,
2

(x; z) =

=

1
O
µ[x]2 −1 O
O
−1 O

We will make repeated use of the Sherman-Morrison formula to simplify matrix inverses:
O
−1 O

ex ex

(A + αuv )−1 = A−1 −

O
. (10)
−1 O

D−1 ex ex D−1

Var[

(x; z ∗ )] = −

O
−1 O

D−1 E[ex ex ]D−1

O
−1 O

(x; z ∗ )] =

2

O
−1 O

D−1 E[ex ex ]D−1

O
−1 O

O
−1 O

D−1 DD−1

=

=
=

O
−1 O

D−1
0

O
−1 O

= Var[

Proof. Given that O is invertible we can simplify the expression of the asymptotic variance of the composite likelihood estimator, Σcl , as follows:
Σcl = O (D−1 + d−1 11 )O

(x; z ∗ )]−1 Var[

D−

2

(x; z ∗ )]−1

D11 D
d + 1 D1

O−

O− .

Note that D1 = µ and d = 1 − 1 µ. This gives us,
Σcl = O−1 D −

(x; z ∗ )] E[

µµ
1−1 µ+1 µ

O−

= O−1 (D − µµ )O−
= Σpi .

(x; z ∗ )]−1

= O D−1 O + d−1 O 11 O

= O−1

−1

−1

= O−1 D−1 − d−1 11

O D−1 O + d−1 O 11 O,

Finally, the asymptotic variance of Σcl is,
2

Lemma 6 (Relative efﬁciency when O is invertible). When
O is invertible, the asymptotic variances of the pseudoinverse and composite likelihood estimators are equal, Σcl =
Σpi , and the relative efﬁciency is 1.

O
−1 O

0
d−1

where D = diag(µ) and d = 1 − 1 µ are the diagonal
elements of D. As expected, E[ 2 (x)] = − Var[ (x)]
because z is a maximum likelihood estimator.
ˆ

Σcl = E[

where A is an invertible matrix, u, v are vectors and α is
a scalar constant. Unless otherwise speciﬁed, we u to
denote the Euclidean norm of a vector u.
First, let us consider the case where O:

From Equation 9 and Equation 10, we get

E[

A−1 uv A−1
,
α−1 + v A−1 u

−1

.

Given our assumptions, 1
µ
0. Consequently, D is
invertible and the asymptotic variance is ﬁnite.
A.3. Comparing the pseudoinverse and composite
likelihood estimators
In Lemma 3, we derived concrete expressions for the
asymptotic variances of the pseudoinverse and composite
likelihood estimators, Σpi and Σcl respectively. In this section, we will use the asymptotic variances to compare the
two estimators for two special cases.
Recall that the relative efﬁciency of the pseudoinverse estimator with respect to the composite likelihood estimator
e
is epi = 1 tr(Σcl (Σpi )−1 ), where k = k − 1. The Cram´ rk
Rao lower bound tells us that Σcl
Σpi : thus the relative
efﬁciency epi lies between 0 and 1. When epi = 1, the
pseudoinverse estimator is said to be efﬁcient.

Next, we consider the case where the observed moments µ
is the uniform distribution.
Lemma 7 (Relative efﬁciency with uniform observed mo1
ments). Let the observed marginals µ be uniform: µ = d 1.
The efﬁciency of the pseudoinverse estimator is,
epi = 1 −

1
1U 2
2
k − 1 1 + 1U

2
2

1−

1
d − 1U

2
2

,

(11)

where 1U
OO† 1, the projection of 1 onto the column
space of O. Note that 0 ≤ 1U 2 ≤ k − 1.
2
When 1U 2 = 0, the pseudoinverse estimator is efﬁcient:
epi = 1. When 1U 2 > 0 and d > k, the pseudoinverse
estimator is strictly inefﬁcient. In particular, if 1U 2 =
2
k − 1, and we get:
epi = 1 −

1
k

1−

1
1+d−k

.

(12)

Estimating Latent-Variable Graphical Models using Moments and Likelihoods

Proof. Next, let us consider the case where the moments
1
1
are the uniform distribution, where µ = d 1 and D = d I.
cl
The expressions for Σ can be simpliﬁed as follows,
Σcl = O (dI + d11 )O

1
d

O† O† −

=

(O† O† O )11 (OO† O† )

1
d

O† O† −

1 + OO† 1

=

1
d

O† O† −

O† 11 O†
1 + 1U 2

=

†

O† O† −

k

tr(I) +

−

1
k
1
k

1
k

O† 11 O†
1 + 1U 2

O 11 O
d − 1U 2
O† O† O 11 O
d − 1U 2

tr

tr

O† 11 O† O O
1 + 1U 2

tr

O† 11 O† O 11 O
(d − 1U 2 )(1 + 1U 2 )

Next we apply the property that the trace is invariant under
cyclic permutations,

2

,

epi = 1 +
−1

−

d − 1 O† (O† O† )−1 O† 1

.

Using the properties (O† O† )−1 = O O and O OO† =
O , we get,
†

O OO 11 O

†

O O

d − 1 O† O OO† 1
O 11 O
O† O1 2

O 11 O
=d O O+
d − 1U 2

1

1U 2
k d − 1U
1

k (1 + 1U
1U 2
=1−
k(1 + 1U

(O† O† )−1 O† 11 O† (O† O† )−1

.

2
2

Note that OO† is a symmetric projection matrix and thus,
OO† = (OO† ) and OO† = (OO† )(OO† ). Then,

= d (O† O† )−1

d−

1 O† O 1 2
1 OO† 1
−
k d − 1U 2
k 1 + 1U
†
(1 O O 1)2
1
.
−
k (d − 1U 2 )(1 + 1U 2 )

epi = 1 +

O†

1
1 † †
O O − 2 O† 11 O†
d
d

=d O O+

1

†

Next, we can simplify the expression for (Σpi )−1 ,

(Σpi )−1 = d O O +

1
d

tr

−

where 1U OO† 1 = O† O 1 is the projection of 1 onto
the column space of O.

+

k

tr(Σcl (Σpi )−1 )

O† 11 O†

Σcl =

(Σpi )−1 =

k
1

d O O+

where we have used the property (O O)
= O O
in the last step. Next, we use the pseudoinverse property,
OO† O† = O† ,

I
11
− 2
d
d

1

,

1 + (1 OO† )(O† O 1)
−1

Σpi = O†

epi =

−1

−1
1
=
O O + O 11 O
d
(O O)−1 O 11 O(O O)−1
1
(O O)−1 −
=
d
1 + 1 O(O O)−1 O 1

=

Now, we are ready to study the relative efﬁciency.

2

−

1U 2
k 1 + 1U
1

1U 4
2 )(d −

1U

1−

2)

2

2)

1
d − 1U

2

.

Note that 1U is the projection of 1 on to a k-dimensional
subspace, thus, 0 ≤ 1U 2 ≤ k. When 1U = 0, the
relative efﬁciency epi is 1: the pseudoinverse estimator is
efﬁcient. When 1U > 0 and d > k, the pseudoinverse
estimator is strictly inefﬁcient.
Consider the case when 1U
efﬁciency is,
epi = 1 −

1

2

= k. Then, the relative

1−

1

k+1
d−k
1
1
=1−
1−
k
1+d−k

.

