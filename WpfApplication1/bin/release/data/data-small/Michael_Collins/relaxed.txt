Online Learning of Relaxed CCG Grammars for Parsing to Logical Form
Luke S. Zettlemoyer and Michael Collins
MIT CSAIL
lsz@csail.mit.edu,mcollins@csail.mit.edu

Abstract
We consider the problem of learning to
parse sentences to lambda-calculus representations of their underlying semantics and
present an algorithm that learns a weighted
combinatory categorial grammar (CCG). A
key idea is to introduce non-standard CCG
combinators that relax certain parts of the
grammar—for example allowing ﬂexible
word order, or insertion of lexical items—
with learned costs. We also present a new,
online algorithm for inducing a weighted
CCG. Results for the approach on ATIS
data show 86% F-measure in recovering
fully correct semantic analyses and 95.9%
F-measure by a partial-match criterion, a
more than 5% improvement over the 90.3%
partial-match ﬁgure reported by He and
Young (2006).

1

Introduction

Recent work (Mooney, 2007; He and Young, 2006;
Zettlemoyer and Collins, 2005) has developed learning algorithms for the problem of mapping sentences
to underlying semantic representations. In one such
approach (Zettlemoyer and Collins, 2005) (ZC05),
the input to the learning algorithm is a training set
consisting of sentences paired with lambda-calculus
expressions. For instance, the training data might
contain the following example:
Sentence:
list ﬂights to boston
Logical Form: λx.f light(x) ∧ to(x, boston)

In this case the lambda-calculus expression denotes
the set of all ﬂights that land in Boston. In ZC05
it is assumed that training examples do not include
additional information, for example parse trees or

a) on may four atlanta to denver delta ﬂight 257
λx.month(x, may) ∧ day number(x, f ourth)∧
f rom(x, atlanta) ∧ to(x, denver)∧
airline(x, delta air lines) ∧ f light(x)∧
f light number(x, 257)
b) show me information on american airlines from fort worth
texas to philadelphia
λx.airline(x, american airlines)∧
f rom(x, f ort worth) ∧ to(x, philadelphia)
c) okay that one’s great too now we’re going to go on april
twenty second dallas to washington the latest nighttime
departure one way
argmax(λx.f light(x) ∧ f rom(x, dallas)∧
to(x, washington) ∧ month(x, april)∧
day number(x, 22) ∧ during(x, night)∧
one way(x), λy.depart time(y))

Figure 1: Three sentences from the ATIS domain.
other derivations. The output from the learning algorithm is a combinatory categorial grammar (CCG),
together with parameters that deﬁne a log-linear distribution over parses under the grammar. Experiments show that the approach gives high accuracy on
two database-query problems, introduced by Zelle
and Mooney (1996) and Tang and Mooney (2000).
The use of a detailed grammatical formalism such
as CCG has the advantage that it allows a system to
handle quite complex semantic effects, such as coordination or scoping phenomena. In particular, it
allows us to leverage the considerable body of work
on semantics within these formalisms, for example
see Carpenter (1997). However, a grammar based
on a formalism such as CCG can be somewhat rigid,
and this can cause problems when a system is faced
with spontaneous, unedited natural language input,
as is commonly seen in natural language interface
applications. For example, consider the sentences
shown in ﬁgure 1, which were taken from the ATIS
travel-planning domain (Dahl et al., 1994). These
sentences exhibit characteristics which present signiﬁcant challenges to the approach of ZC05. For ex-

ample, the sentences have quite ﬂexible word order,
and include telegraphic language where some words
are effectively omitted.
In this paper we describe a learning algorithm that
retains the advantages of using a detailed grammar,
but is highly effective in dealing with phenomena
seen in spontaneous natural language, as exempliﬁed by the ATIS domain. A key idea is to extend
the approach of ZC05 by allowing additional nonstandard CCG combinators. These combinators relax certain parts of the grammar—for example allowing ﬂexible word order, or insertion of lexical
items—with learned costs for the new operations.
This approach has the advantage that it can be seamlessly integrated into CCG learning algorithms such
as the algorithm described in ZC05.
A second contribution of the work is a new, online algorithm for CCG learning. The approach involves perceptron training of a model with hidden
variables. In this sense it is related to the algorithm
of Liang et al. (2006). However it has the additional twist of also performing grammar induction
(lexical learning) in an online manner. In our experiments, we show that the new algorithm is considerably more efﬁcient than the ZC05 algorithm; this
is important when training on large training sets, for
example the ATIS data used in this paper.
Results for the approach on ATIS data show 86%
F-measure accuracy in recovering fully correct semantic analyses, and 95.9% F-measure by a partialmatch criterion described by He and Young (2006).
The latter ﬁgure contrasts with a ﬁgure of 90.3% for
the approach reported by He and Young (2006).1
Results on the Geo880 domain also show an improvement in accuracy, with 88.9% F-measure for
the new approach, compared to 87.0% F-measure
for the method in ZC05.

2

Background

2.1 Semantics
Training examples in our approach consist of sentences paired with lambda-calculus expressions. We
use a version of the lambda calculus that is closely
related to the one presented by Carpenter (1997).
There are three basic types: t, the type of truth val1
He and Young (2006) do not give results for recovering
fully correct parses.

ues; e, the type for entities; and r, the type for real
numbers. Functional types are deﬁned by specifying their input and output types, for example e, t
is the type of a function from entities to truth values. In general, declarative sentences have a logical
form of type t. Question sentences generally have
functional types.2 Each expression is constructed
from constants, logical connectors, quantiﬁers and
lambda functions.
2.2

Combinatory Categorial Grammars

Combinatory categorial grammar (CCG) is a syntactic theory that models a wide range of linguistic
phenomena (Steedman, 1996; Steedman, 2000).
The core of a CCG grammar is a lexicon Λ. For
example, consider the lexicon
ﬂights
to
boston

:=
:=
:=

N : λx.f light(x)
(N \N )/N P : λy.λf.λx.f (x) ∧ to(x, y)
N P : boston

Each entry in the lexicon is a pair consisting of a
word and an associated category. The category contains both syntactic and semantic information. For
example, the ﬁrst entry states that the word ﬂights
can have the category N : λx.f light(x). This category consists of a syntactic type N , together with
the semantics λx.f light(x). In general, the semantic entries for words in the lexicon can consist of any
lambda-calculus expression. Syntactic types can either be simple types such as N , N P , or S, or can be
more complex types that make use of slash notation,
for example (N \N )/N P .
CCG makes use of a set of combinators which
are used to combine categories to form larger pieces
of syntactic and semantic structure. The simplest
such rules are the functional application rules:
A/B : f B : g
B : g A\B : f

⇒
⇒

A : f (g)
A : f (g)

(>)
(<)

The ﬁrst rule states that a category with syntactic
type A/B can be combined with a category to the
right of syntactic type B to create a new category
of type A. It also states that the new semantics
will be formed by applying the function f to
the expression g. The second rule handles arguments to the left. Using these rules, we can parse the
2
For example, many question sentences have semantics of
type e, t , as in λx.f light(x) ∧ to(x, boston).

following phrase to create a new category of type N :
ﬂights

to

boston

N
λx.f light(x)

(N \N )/N P
λy.λf.λx.f (x) ∧ to(x, y)

NP
boston

(N \N )
λf.λx.f (x) ∧ to(x, boston)

N
λx.f light(x) ∧ to(x, boston)

>

<

The top-most parse operations pair each word with a
corresponding category from the lexicon. The later
steps are labeled −> (for each instance of forward
application) or −< (for backward application).
A second set of combinators in CCG grammars
are the rules of functional composition:
A/B : f B/C : g
B\C : g A\B : f

⇒
⇒

A/C : λx.f (g(x))
A\C : λx.f (g(x))

(> B)
(< B)

These rules allow for an unrestricted notion of constituency that is useful for modeling coordination
and other linguistic phenomena. As we will see, they
also turn out to be useful when modeling constructions with relaxed word order, as seen frequently in
domains such as ATIS.
In addition to the application and composition
rules, we will also make use of type raising and coordination combinators. A full description of these
combinators goes beyond the scope of this paper.
Steedman (1996; 2000) presents a detailed description of CCG.
2.3 Log-Linear CCGs
We can generalize CCGs to weighted, or probabilistic, models as follows. Our models are similar to
several other approaches (Ratnaparkhi et al., 1994;
Johnson et al., 1999; Lafferty et al., 2001; Collins,
2004; Taskar et al., 2004). We will write x to denote a sentence, and y to denote a CCG parse for a
sentence. We use GEN(x; Λ) to refer to all possible CCG parses for x under some CCG lexicon Λ.
We will deﬁne f(x, y) ∈ Rd to be a d-dimensional
feature–vector that represents a parse tree y paired
with an input sentence x. In principle, f could include features that are sensitive to arbitrary substructures within the pair (x, y). We will deﬁne
w ∈ Rd to be a parameter vector. The optimal parse
for a sentence x under parameters w and lexicon Λ
is then deﬁned as
y ∗ (x) = arg

max

y∈GEN(x;Λ)

w · f(x, y) .

Assuming sufﬁciently local features3 in f, search for
y ∗ can be achieved using dynamic-programmingstyle algorithms, typically with some form of beam
search.4 Training a model of this form involves
learning the parameters w and potentially also the
lexicon Λ. This paper focuses on a method for learning a (w, Λ) pair from a training set of sentences
paired with lambda-calculus expressions.
2.4

Zettlemoyer and Collins 2005

We now give a description of the approach of Zettlemoyer and Collins (2005). This method will form
the basis for our approach, and will be one of the
baseline models for the experimental comparisons.
The input to the ZC05 algorithm is a set of training examples (xi , zi ) for i = 1 . . . n. Each xi is
a sentence, and each zi is a corresponding lambdaexpression. The output from the algorithm is a pair
(w, Λ) specifying a set of parameter values, and a
CCG lexicon. Note that for a given training example
(xi , zi ), there may be many possible parses y which
lead to the correct semantics zi .5 For this reason
the training problem is a hidden-variable problem,
where the training examples contain only partial information, and the CCG lexicon and parse derivations must be learned without direct supervision.
A central part of the ZC05 approach is a function
GENLEX(x, z) which maps a sentence x together
with semantics z to a set of potential lexical entries.
The function GENLEX is deﬁned through a set of
rules—see ﬁgure 2—that consider the expression z,
and generate a set of categories that may help in
building the target semantics z. An exhaustive set
of lexical entries is then generated by taking all categories generated by the GENLEX rules, and pairing them with all possible sub-strings of the sentence
x. Note that our lexicon can contain multi-word entries, where a multi-word string such as New York
can be paired with a CCG category. The ﬁnal out3

For example, features which count the number of lexical
entries of a particular type, or features that count the number of
applications of a particular CCG combinator.
4
In our experiments we use a parsing algorithm that is similar to a CKY-style parser with dynamic programming. Dynamic
programming is used but each entry in the chart maintains a full
semantic expression, preventing a polynomial-time algorithm;
beam search is used to make the approach tractable.
5
This problem is compounded by the fact that the lexicon
is unknown, so that many of the possible hidden derivations
involve completely spurious lexical entries.

Input Trigger
constant c
arity one predicate p
arity one predicate p
arity two predicate p2
arity two predicate p2
arity one predicate p1
literal with arity two predicate p2
and constant second argument c
arity two predicate p2
an arg max / min with second
argument arity one function f
arity one function f
arity one function f
no trigger

Rules

Example categories produced from the logical form
arg max(λx.f light(x) ∧ f rom(x, boston), λx.cost(x))
N P : boston
N : λx.f light(x)
S\N P : λx.f light(x)
(S\N P )/N P : λx.λy.f rom(y, x)
(S\N P )/N P : λx.λy.f rom(x, y)
N/N : λg.λx.f light(x) ∧ g(x)

Output Category
NP : c
N : λx.p(x)
S\N P : λx.p(x)
(S\N P )/N P : λx.λy.p2 (y, x)
(S\N P )/N P : λx.λy.p2 (x, y)
N/N : λg.λx.p1 (x) ∧ g(x)
N/N : λg.λx.p2 (x, c) ∧ g(x)

N/N : λg.λx.f rom(x, boston) ∧ g(x)

(N \N )/N P : λy.λg.λx.p2 (x, y) ∧ g(x)

(N \N )/N P : λy.λg.λx.f rom(x, y) ∧ g(x)

N P/N : λg. arg max / min(g, λx.f (x))

N P/N : λg. arg max(g, λx.cost(x))

S/N P : λx.f (x)
(N \N )/N P : λy.λf.λx.g(x) ∧ f (x) > < y
/
S/N P : λx.x, S/N : λf.λx.f (x)

S/N P : λx.cost(x)
(N \N )/N P : λy.λf.λx.g(x) ∧ cost(x) > y
S/N P : λx.x, S/N : λf.λx.f (x)

Figure 2: Rules used in GENLEX. Each row represents a rule. The ﬁrst column lists the triggers that identify some sub-structure

within a logical form. The second column lists the category that is created. The third column lists categories that are created when
the rule is applied to the logical form at the top of this column. We use the 10 rules described in ZC05 and add two new rules,
listed in the last two rows above. This ﬁrst new rule is instantiated for greater than (>) and less than (<) comparisions. The second
new rule has no trigger; it is always applied. It generates categories that are used to learn lexical entries for semantically vacuous
sentence preﬁxes such as the phrase show me information on in the example in ﬁgure 1(b).

put from GENLEX(x, z) is a large set of potential
lexical entries, with the vast majority of those entries being spurious. The algorithm in ZC05 embeds
GENLEX within an overall learning approach that
simultaneously selects a small subset of all entries
generated by GENLEX and estimates parameter values w. Zettlemoyer and Collins (2005) present more
complete details. In section 4.2 we describe a new,
online algorithm that uses GENLEX.

3

Parsing Extensions: Combinators

This section describes a set of CCG combinators
which we add to the conventional CCG combinators
described in section 2.2. These additional combinators are natural extensions of the forward application, forward composition, and type-raising rules
seen in CCG. We ﬁrst describe a set of combinators that allow the parser to signiﬁcantly relax constraints on word order. We then describe a set of
type-raising rules which allow the parser to cope
with telegraphic input (in particular, missing function words). In both cases these additional rules
lead to signiﬁcantly more parses for any sentence
x given a lexicon Λ. Many of these parses will be
suspect from a linguistic perspective; broadening the
set of CCG combinators in this way might be considered a dangerous move. However, the learning
algorithm in our approach can learn weights for the
new rules, effectively allowing the model to learn to
use them only in appropriate contexts; in the experiments we show that the rules are highly effective
additions when used within a weighted CCG.

3.1

Application and Composition Rules

The ﬁrst new combinators we consider are the
relaxed functional application rules:
A\B : f B : g
B : g A/B : f

⇒
⇒

A : f (g)
A : f (g)

( )
( )

These are variants of the original application
rules, where the slash direction on the principal categories (A/B or A\B) is reversed.6 These rules allow simple reversing of regular word order, for example
ﬂights

one way

N
λx.f light(x)

N/N
λf.λx.f (x) ∧ one way(x)

N
λx.f light(x) ∧ one way(x)

Note that we can recover the correct analysis for this
fragment, with the same lexical entries as those used
for the conventional word order, one-way ﬂights.
A second set of new combinators are the relaxed
functional composition rules:
A\B : f B/C : g
B\C : g A/B : f

⇒
⇒

A/C : λx.f (g(x))
A\C : λx.f (g(x))

(
(

B)
B)

These rules are variantions of the standard functional composition rules, where the slashes of the
principal categories are reversed.
6
Rules of this type are non-standard in the sense that they
violate Steedman’s Principle of Consistency (2000); this principle states that rules must be consistent with the slash direction
of the principal category. Steedman (2000) only considers rules
that do not violate this principle—for example, crossed composition rules, which we consider later, and which Steedman also
considers, do not violate this principle.

An important point is that that these new composition and application rules can deal with quite ﬂexible word orders. For example, take the fragment to
washington the latest ﬂight. In this case the parse is
to washington

the latest

ﬂight

N \N
λf.λx.f (x)∧
to(x, washington)

N P/N
λf. arg max(f,
λy.depart time(y))

N
λx.f light(x)

In practice, in our experiments most rules of this
form have p as the semantics of some preposition,
for example from or to. A typical example of a use
of this rule would be the following:
ﬂights

boston

to new york

N
λx.f light(x)

NP
bos

N \N
λf.λx.f (x)
∧to(x, new york)

B

TR

N P \N
λf. arg max(λx.f (x)∧
to(x, washington), λy.depart time(y))

N \N
λf.λx.f (x) ∧ f rom(x, bos)

N
λf.λx.f light(x) ∧ f rom(x, bos)

NP
arg max(λx.f light(x) ∧ to(x, washington),
λy.depart time(y))

Note that in this case the substring the latest has category N P/N , and this prevents a naive parse where
the latest ﬁrst combines with ﬂight, and to washington then combines with the latest ﬂight. The functional composition rules effectively allow the latest
to take scope over ﬂight and to washington, in spite
of the fact that the latest appears between the two
other sub-strings. Examples like this are quite frequent in domains such as ATIS.
We add features in the model which track the occurrences of each of these four new combinators.
Speciﬁcally, we have four new features in the definition of f; each feature tracks the number of times
one of the combinators is used in a CCG parse. The
model learns parameter values for each of these features, allowing it to learn to penalise these rules to
the correct extent.
3.2 Additional Rules of Type-Raising
We now describe new CCG operations designed to
deal with cases where words are in some sense missing in the input. For example, in the string ﬂights
Boston to New York, one style of analysis would
assume that the preposition from had been deleted
from the position before Boston.
The ﬁrst set of rules is generated from the following role-hypothesising type shifting rules template:
NP : c

⇒

N \N : λf.λx.f (x) ∧ p(x, c)

(TR )

This rule can be applied to any N P with semantics
c, and any arity-two function p such that the second
argument of p has the same type as c. By “any” aritytwo function, we mean any of the arity-two functions seen in training data. We deﬁne features within
the feature-vector f that are sensitive to the number
of times these rules are applied in a parse; a separate
feature is deﬁned for each value of p.

<

<

N
λx.f light(x) ∧ to(x, new york) ∧ f rom(x, bos)

The second rule we consider is the null-head type
shifting rule:
N \N : f

⇒

N : f (λx.true)

(TN )

This rule allows parses of fragments such as American Airlines from New York, where there is again a
word that is in some sense missing (it is straightforward to derive a parse for American Airlines ﬂights
from New York). The analysis would be as follows:
American Airlines
N/N
λf.λx.f (x) ∧ airline(x, aa)

from New York
N \N
λf.λx.f (x) ∧ f rom(x, new york)
N
λx.f rom(x, new york)

N
λx.airline(x, aa) ∧ f rom(x, new york)

TN

>

The new rule effectively allows the prepositional phrase from New York to type-shift to
an entry with syntactic type N and semantics
λx.f rom(x, new york), representing the set of all
things from New York.7
We introduce a single additional feature which
counts the number of times this rule is used.
3.3

Crossed Composition Rules

Finally, we include crossed functional composition
rules:
A/B : f B\C : g
B/C : g A\B : f

⇒
⇒

A\C : λx.f (g(x))
A/C : λx.f (g(x))

(>B× )
(<B× )

These rules are standard CCG operators but they
were not used by the parser described in ZC05.
When used in unrestricted contexts, they can signiﬁcantly relax word order. Again, we address this
7

Note that we do not analyze this prepositional phrase as
having the semantics λx.f light(x) ∧ f rom(x, new york)—
although in principle this is possible—as the f light(x) predicate is not necessarily implied by this utterance.

dallas

to

washington

the latest

on

friday

NP
dallas

(N \N )/N P
λy.λf.λx.f (x)
∧to(x, y)

NP
washington

N P/N
λf. arg max(f,
λy.depart time(y))

(N \N )/N P
λy.λf.λx.f (x)
∧day(x, y)

NP
f riday

TR

N \N
λf.λx.f (x) ∧ f rom(x, dallas)

>

N \N
λf.λx.f (x) ∧ to(x, washington)

N \N
λf.λx.f (x) ∧ f rom(x, dallas) ∧ to(x, washington)

>

N \N
λf.λx.f (x) ∧ day(x, f riday)

<B

N
λx.day(x, f riday)

TN

B

N P \N
λf. arg max(λx.f (x) ∧ f rom(x, dallas) ∧ to(x, washington), λy.depart time(y))
NP
arg max(λx.day(x, f riday) ∧ f rom(x, dallas) ∧ to(x, washington), λy.depart time(y))

Figure 3: A parse with the ﬂexible parser.

problem by introducing features that count the number of times they are used in a parse.8
3.4 An Example
As a ﬁnal point, to see how these rules can interact
in practice, see ﬁgure 3. This example demonstrates
the use of the relaxed application and composition
rules, as well as the new type-raising rules.

4

Learning

This section describes an approach to learning in our
model. We ﬁrst deﬁne the features used and then describe a new online learning algorithm for the task.
4.1 Features in the Model
Section 2.3 described the use of a function f(x, y)
which maps a sentence x together with a CCG parse
y to a feature vector. As described in section 3,
we introduce features for the new CCG combinators. In addition, we follow ZC05 in deﬁning features which track the number of times each lexical
item in Λ is used. For example, we would have one
feature tracking the number of times the lexical entry
f lights := N : λx.f lights(x) is used in a parse,
and similar features for all other members of Λ.
Finally, we introduce new features which directly
consider the semantics of a parse. For each predicate
f seen in training data, we introduce a feature that
counts the number of times f is conjoined with itself
at some level in the logical form. For example, the
expression λx.f light(x) ∧ f rom(x, new york) ∧
f rom(x, boston) would trigger the new feature for
8

In general, applications of the crossed composition rules
can be lexically governed, as described in work on Multi-Modal
CCG (Baldridge, 2002). In the future we would like to incorporate more ﬁne-grained lexical distinctions of this type.

the f rom predicate signaling that the logical-form
describes ﬂights with more than one origin city. We
introduce similar features which track disjunction as
opposed to conjunction.
4.2

An Online Learning Algorithm

Figure 4 shows a learning algorithm that takes a
training set of (xi , zi ) pairs as input, and returns
a weighted CCG (i.e., a pair (w, Λ)) as its output.
The algorithm is online, in that it visits each example in turn, and updates both w and Λ if necessary. In Step 1 on each example, the input xi is
parsed. If it is parsed correctly, the algorithm immediately moves to the next example. In Step 2,
the algorithm temporarily introduces all lexical entries seen in GENLEX(xi , zi ), and ﬁnds the highest
scoring parse that leads to the correct semantics zi .
A small subset of GENLEX(xi , zi )—namely, only
those lexical entries that are contained in the highest
scoring parse—are added to Λ. In Step 3, a simple
perceptron update (Collins, 2002) is performed. The
hypothesis is parsed again with the new lexicon, and
an update to the parameters w is made if the resulting parse does not have the correct logical form.
This algorithm differs from the approach in ZC05
in a couple of important respects. First, the ZC05 algorithm performed learning of the lexicon Λ at each
iteration in a batch method, requiring a pass over the
entire training set. The new algorithm is fully online,
learning both Λ and w in an example-by-example
fashion. This has important consequences for the
efﬁciency of the algorithm. Second, the parameter
estimation method in ZC05 was based on stochastic
gradient descent on a log-likelihood objective function. The new algorithm makes use of perceptron

Inputs: Training examples {(xi , zi ) : i = 1 . . . n} where
each xi is a sentence, each zi is a logical form. An initial
lexicon Λ0 . Number of training iterations, T .
Deﬁnitions: GENLEX(x, z) takes as input a sentence x and
a logical form z and returns a set of lexical items as described in section 2.4. GEN(x; Λ) is the set of all parses
for x with lexicon Λ. GEN(x, z; Λ) is the set of all parses
for x with lexicon Λ, which have logical form z. The
function f(x, y) represents the features described in section 4.1. The function L(y) maps a parse tree y to its
associated logical form.
Initialization: Set parameters w to initial values described in
section 6.2. Set Λ = Λ0 .
Algorithm:
• For t = 1 . . . T, i = 1 . . . n :
Step 1: (Check correctness)
• Let y ∗ = arg maxy∈GEN(xi ;Λ) w · f(xi , y) .
• If L(y ∗ ) = zi , go to the next example.

Step 2: (Lexical generation)
•
•
•
•

Set λ = Λ ∪ GENLEX(xi , zi ) .
Let y ∗ = arg maxy∈GEN(xi ,zi ;λ) w · f(xi , y) .
Deﬁne λi to be the set of lexical entries in y ∗ .
Set lexicon to Λ = Λ ∪ λi .

Step 3: (Update parameters)

• Let y = arg maxy∈GEN(xi ;Λ) w · f(xi , y) .
• If L(y ) = zi :

• Set w = w + f(xi , y ∗ ) − f(xi , y ) .

Output: Lexicon Λ together with parameters w.

Figure 4: An online learning algorithm.
updates, which are simpler and cheaper to compute.
As in ZC05, the algorithm assumes an initial lexicon Λ0 that contains two types of entries. First, we
compile entries such as Boston := N P : boston
for entities such as cities, times and month-names
that occur in the domain or underlying database. In
practice it is easy to compile a list of these atomic
entities. Second, the lexicon has entries for some
function words such as wh-words, and determiners.9

5

Related Work

There has been a signiﬁcant amount of previous work on learning to map sentences to underlying semantic representations. A wide variety
9

Our assumption is that these entries are likely to be domain
independent, so it is simple enough to compile a list that can
be reused in new domains. Another approach, which we may
consider in the future, would be to annotate a small subset of
the training examples with full CCG derivations, from which
these frequently occurring entries could be learned.

of techniques have been considered including approaches based on machine translation techniques
(Papineni et al., 1997; Ramaswamy and Kleindienst,
2000; Wong and Mooney, 2006), parsing techniques
(Miller et al., 1996; Ge and Mooney, 2006), techniques that use inductive logic programming (Zelle
and Mooney, 1996; Thompson and Mooney, 2002;
Tang and Mooney, 2000; Kate et al., 2005), and
ideas from string kernels and support vector machines (Kate and Mooney, 2006; Nguyen et al.,
2006). In our experiments we compare to He and
Young (2006) on the ATIS domain and Zettlemoyer
and Collins (2005) on the Geo880 domain, because these systems currently achieve the best performance on these problems.
The approach of Zettlemoyer and Collins (2005)
was presented in section 2.4. He and Young (2005)
describe an algorithm that learns a probabilistic
push-down automaton that models hierarchical dependencies but can still be trained on a data set that
does not have full treebank-style annotations. This
approach has been integrated with a speech recognizer and shown to be robust to recognition errors
(He and Young, 2006).
There is also related work in the CCG literature. Clark and Curran (2003) present a method for
learning the parameters of a log-linear CCG parsing model from fully annotated normal–form parse
trees. Watkinson and Manandhar (1999) present an
unsupervised approach for learning CCG lexicons
that does not represent the semantics of the training sentences. Bos et al. (2004) present an algorithm that learns CCG lexicons with semantics
but requires fully–speciﬁed CCG derivations in the
training data. Bozsahin (1998) presents work on using CCG to model languages with free word order.
In addition, there is related work that focuses on
modeling child language learning. Siskind (1996)
presents an algorithm that learns word-to-meaning
mappings from sentences that are paired with a set
of possible meaning representations. Villavicencio
(2001) describes an approach that learns a categorial
grammar with syntactic and semantic information.
Both of these approaches use sentences from childdirected speech, which differ signiﬁcantly from the
natural language interface queries we consider.
Finally, there is work on manually developing
parsing techniques to improve robustness (Carbonell

and Hayes, 1983; Seneff, 1992). In contrast, our approach is integrated into a learning framework.

6

Experiments

The main focus of our experiments is on the ATIS
travel planning domain. For development, we used
4978 sentences, split into a training set of 4500 examples, and a development set of 478 examples. For
test, we used the ATIS NOV93 test set which contains 448 examples. To create the annotations, we
created a script that maps the original SQL annotations provided with the data to lambda-calculus expressions.
He and Young (2006) previously reported results
on the ATIS domain, using a learning approach
which also takes sentences paired with semantic annotations as input. In their case, the semantic structures resemble context-free parses with semantic (as
opposed to syntactic) non-terminal labels. In our experiments we have used the same split into training and test data as He and Young (2006), ensuring that our results are directly comparable. He and
Young (2006) report partial match ﬁgures for their
parser, based on precision and recall in recovering
attribute-value pairs. (For example, the sentence
ﬂights to Boston would have a single attribute-value
entry, namely destination = Boston.) It is simple for us to map from lambda-calculus expressions
to attribute-value entries of this form; for example,
the expression to(x, Boston) would be mapped to
destination = Boston. He and Young (2006) gave
us their data and annotations, so we can directly
compare results on the partial-match criterion. We
also report accuracy for exact matches of lambdacalculus expressions, which is a stricter criterion.
In addition, we report results for the method on
the Geo880 domain. This allows us to compare
directly to the previous work of Zettlemoyer and
Collins (2005), using the same split of the data into
training and test sets of sizes 600 and 280 respectively. We use cross-validation of the training set, as
opposed to a separate development set, for optimization of parameters.
6.1 Improving Recall
The simplest approach to the task is to train the
parser and directly apply it to test sentences. In our

experiments we will see that this produces results
which have high precision, but somewhat lower recall, due to some test sentences failing to parse (usually due to words in the test set which were never
observed in training data). A simple strategy to alleviate this problem is as follows. If the sentence fails
to parse, we parse the sentence again, this time allowing parse moves which can delete words at some
cost. The cost of this deletion operation is optimized
on development data. This approach can signiﬁcantly improve F-measure on the partial-match criterion in particular. We report results both with and
without this second pass strategy.
6.2

Parameters in the Approach

The algorithm in ﬁgure 4 has a number of parameters, the set {T, α, β, γ}, which we now describe.
The values of these parameters were chosen to optimize the performance on development data. T is
the number of passes over the training set, and was
set to be 4. Each lexical entry in the initial lexicon
Λ0 has an associated feature which counts the number of times this entry is seen in a parse. The initial
parameter value in w for all features of this form
was chosen to be some value α. Each of the new
CCG rules—the application, composition, crossedcomposition, and type-raising rules described in section 3—has an associated parameter. We set all of
these parameters to the same initial value β. Finally,
when new lexical entries are added to Λ (in step 2
of the algorithm), their initial weight is set to some
value γ. In practice, optimization on development
data led to a positive value for α, and negative values for β and γ.
6.3

Results

Table 1 shows accuracy for the method by the exactmatch criterion on the ATIS test set. The two pass
strategy actually hurts F-measure in this case, although it does improve recall of the method.
Table 2 shows results under the partial-match criterion. The results for our approach are higher
than those reported by He and Young (2006) even
without the second, high-recall, strategy. With the
two-pass strategy our method has more than halved
the F-measure error rate, giving improvements from
90.3% F-measure to 95.9% F-measure.
Table 3 shows results on the Geo880 domain. The

Single-Pass Parsing
Two-Pass Parsing

Precision
90.61
85.75

Recall
81.92
84.6

F1
86.05
85.16

Table 1: Exact-match accuracy on the ATIS test set.
Single-Pass Parsing
Two-Pass Parsing
He and Young (2006)

Precision
96.76
95.11
–

Recall
86.89
96.71
–

F1
91.56
95.9
90.3

Table 2: Partial-credit accuracy on the ATIS test set.
new method gives improvements in performance
both with and without the two pass strategy, showing
that the new CCG combinators, and the new learning algorithm, give some improvement on even this
domain. The improved performance comes from a
slight drop in precision which is offset by a large increase in recall.
Table 4 shows ablation studies on the ATIS data,
where we have selectively removed various aspects
of the approach, to measure their impact on performance. It can be seen that accuracy is seriously degraded if the new CCG rules are removed, or if the
features associated with these rules (which allow the
model to penalize these rules) are removed.
Finally, we report results concerning the efﬁciency of the new online algorithm as compared to
the ZC05 algorithm. We compared running times
for the new algorithm, and the ZC05 algorithm, on
the geography domain, with both methods making
4 passes over the training data. The new algorithm
took less than 4 hours, compared to over 12 hours
for the ZC05 algorithm. The main explanation for
this improved performance is that on many training
examples,10 in step 1 of the new algorithm a correct parse is found, and the algorithm immediately
moves on to the next example. Thus GENLEX is
not required, and in particular parsing the example
with the large set of entries generated by GENLEX
is not required.

7

Discussion

We presented a new, online algorithm for learning a combinatory categorial grammar (CCG), together with parameters that deﬁne a log-linear parsing model. We showed that the use of non-standard
CCG combinators is highly effective for parsing sen10

Measurements on the Geo880 domain showed that in the 4
iterations, 83.3% of all parses were successful at step 1.

Single-Pass Parsing
Two-Pass Parsing
ZC05

Precision
95.49
91.63
96.25

Recall
83.2
86.07
79.29

F1
88.93
88.76
86.95

Table 3: Exact-match accuracy on the Geo880 test set.
Full Online Method
Without control features
Without relaxed word order
Without word insertion

Precision
87.26
70.33
82.81
77.31

Recall
74.44
42.45
63.98
56.94

F1
80.35
52.95
72.19
65.58

Table 4: Exact-match accuracy on the ATIS development set
for the full algorithm and restricted versions of it. The second row reports results of the approach without the features
described in section 3 that control the use of the new combinators. The third row presents results without the combinators
from section 3.1 that relax word order. The fourth row reports
experiments without the type-raising combinators presented in
section 3.2.

tences with the types of phenomena seen in spontaneous, unedited natural language. The resulting system achieved signiﬁcant accuracy improvements in
both the ATIS and Geo880 domains.
Acknowledgements
We would like to thank Yulan He and Steve Young
for their help with obtaining the ATIS data set. We
also acknowledge the support for this research. Luke
Zettlemoyer was funded by a Microsoft graduate
research fellowship and Michael Collins was supported by the National Science Foundation under
grants 0347631 and DMS-0434222.

References
Jason Baldridge. 2002. Lexically Speciﬁed Derivational Control in Combinatory Categorial Grammar. Ph.D. thesis,
University of Edinburgh.
Johan Bos, Stephen Clark, Mark Steedman, James R. Curran,
and Julia Hockenmaier. 2004. Wide-coverage semantic representations from a CCG parser. In Proceedings of the 20th
International Conference on Computational Linguistics.
Cem Bozsahin. 1998. Deriving the predicate-argument structure for a free word order language. In Proceedings of the
36th Annual Meeting of the Association for Computational
Linguistics.
Jaime G. Carbonell and Philip J. Hayes. 1983. Recovery strategies for parsing extragrammatical language. American Journal of Computational Linguistics, 9.
Bob Carpenter. 1997. Type-Logical Semantics. The MIT Press.
Stephen Clark and James R. Curran. 2003. Log-linear models
for wide-coverage CCG parsing. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.

Michael Collins. 2002. Discriminative training methods for
hidden markov models: Theory and experiments with perceptron algorithms. In Proceedings of the 2002 Conference
on Empirical Methods in Natural Language Processing.
Michael Collins. 2004. Parameter estimation for statistical
parsing models: Theory and practice of distribution-free
methods. In Harry Bunt, John Carroll and Giorgio Satta,
editors, New Developments in Parsing Technology. Kluwer.
Deborah A. Dahl, Madeleine Bates, Michael Brown, William
Fisher, Kate Hunicke-Smith, David Pallett, Christine Pao,
Alexander Rudnicky, and Elizabeth Shriberg. 1994. Expanding the scope of the atis task: the atis-3 corpus. In ARPA
Human Language Technology Workshop.
Ruifang Ge and Raymond J. Mooney. 2006. Discriminative
reranking for semantic parsing. In Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions.
Yulan He and Steve Young. 2005. Semantic processing using
the hidden vector state model. Computer Speech and Language.
Yulan He and Steve Young. 2006. Spoken language understanding using the hidden vector state model. Speech Communication Special Issue on Spoken Language Understanding for Conversational Systems.
Mark Johnson, Stuart Geman, Steven Canon, Zhiyi Chi, and
Stefan Riezler. 1999. Estimators for stochastic “uniﬁcationbased” grammars. In Proceedings of the Association for
Computational Linguistics.
Rohit J. Kate and Raymond J. Mooney. 2006. Using stringkernels for learning semantic parsers. In Proceedings of the
44th Annual Meeting of the Association for Computational
Linguistics.
Rohit J. Kate, Yuk Wah Wong, and Raymond J. Mooney. 2005.
Learning to transform natural to formal languages. In Proceedings of the 20th National Conference on Artiﬁcial Intelligence.
John Lafferty, Andrew McCallum, and Fernando Pereira. 2001.
Conditional random ﬁelds: Probabilistic models for segmenting and labeling sequence data. In Proceedings of the
18th International Conference on Machine Learning.
Percy Liang, Alexandre Bouchard-Cˆ t´ , Dan Klein, and Ben
oe
Taskar. 2006. An end-to-end discriminative approach to machine translation. In Proceedings of the 44th Annual Meeting
of the Association for Computational Linguistics.
Scott Miller, David Stallard, Robert J. Bobrow, and Richard L.
Schwartz. 1996. A fully statistical approach to natural language interfaces. In Proceedings of the Association for Computational Linguistics.
Raymond J. Mooney. 2007. Learning for semantic parsing. In
Computational Linguistics and Intelligent Text Processing:
Proceedings of the 8th International Conference.
Le-Minh Nguyen, Akira Shimazu, and Xuan-Hieu Phan. 2006.
Semantic parsing with structured SVM ensemble classiﬁcation models. In Proceedings of the COLING/ACL 2006 Main
Conference Poster Sessions.

K. A. Papineni, S. Roukos, and T. R. Ward. 1997. Featurebased language understanding. In Proceedings of European
Conference on Speech Communication and Technology.
Ganesh N. Ramaswamy and Jan Kleindienst. 2000. Hierarchical feature-based translation for scalable natural language
understanding. In Proceedings of 6th International Conference on Spoken Language Processing.
Adwait Ratnaparkhi, Salim Roukos, and R. Todd Ward. 1994.
A maximum entropy model for parsing. In Proceedings of
the International Conference on Spoken Language Processing.
Stephanie Seneff. 1992. Robust parsing for spoken language
systems. In Proceedings of the IEEE Conference on Acoustics, Speech, and Signal Processing.
Jeffrey M. Siskind. 1996. A computational study of crosssituational techniques for learning word-to-meaning mappings. Cognition, 61(2-3).
Mark Steedman. 1996. Surface Structure and Interpretation.
The MIT Press.
Mark Steedman. 2000. The Syntactic Process. The MIT Press.
Lappoon R. Tang and Raymond J. Mooney. 2000. Automated
construction of database interfaces: Integrating statistical
and relational learning for semantic parsing. In Joint Conference on Empirical Methods in Natural Language Processing
and Very Large Corpora.
Ben Taskar, Dan Klein, Michael Collins, Daphne Koller, and
Christopher Manning. 2004. Max-margin parsing. In Proceedings of the Conference on Empirical Methods in Natural
Language Processing.
Cynthia A. Thompson and Raymond J. Mooney. 2002. Acquiring word-meaning mappings for natural language interfaces.
Journal of Artiﬁcial Intelligence Research, 18.
Aline Villavicencio. 2001. The acquisition of a uniﬁcationbased generalised categorial grammar. Ph.D. thesis, University of Cambridge.
Stephen Watkinson and Suresh Manandhar. 1999. Unsupervised lexical learning with categorial grammars using the
LLL corpus. In Proceedings of the 1st Workshop on Learning Language in Logic.
Yuk Wah Wong and Raymond Mooney. 2006. Learning for semantic parsing with statistical machine translation. In Proceedings of the Human Language Technology Conference of
the NAACL.
John M. Zelle and Raymond J. Mooney. 1996. Learning to
parse database queries using inductive logic programming.
In Proceedings of the 14th National Conference on Artiﬁcial
Intelligence.
Luke S. Zettlemoyer and Michael Collins. 2005. Learning
to map sentences to logical form: Structured classiﬁcation
with probabilistic categorial grammars. In Proceedings of
the 21st Conference on Uncertainty in Artiﬁcial Intelligence.

