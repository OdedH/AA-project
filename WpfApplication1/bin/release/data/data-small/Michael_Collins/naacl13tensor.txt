Approximate PCFG Parsing Using Tensor Decomposition
Shay B. Cohen
Department of Computer Science
Columbia University, USA
scohen@cs.columbia.edu

Giorgio Satta
Department of Information Engineering
University of Padua, Italy
satta@dei.unipd.it

Michael Collins
Department of Computer Science
Columbia University, USA
mcollins@cs.columbia.edu
Abstract
We provide an approximation algorithm for
PCFG parsing, which asymptotically improves time complexity with respect to the input grammar size, and prove upper bounds on
the approximation quality. We test our algorithm on two treebanks, and get signiﬁcant improvements in parsing speed.

1

Introduction

The problem of speeding-up parsing algorithms
based on probabilistic context-free grammars
(PCFGs) has received considerable attention in
recent years. Several strategies have been proposed,
including beam-search, best-ﬁrst and A∗ . In this
paper we focus on the standard approach of approximating the source PCFG in such a way that parsing
accuracy is traded for efﬁciency.
Nederhof (2000) gives a thorough presentation
of old and novel ideas for approximating nonprobabilistic CFGs by means of ﬁnite automata,
on the basis of specialized preprocessing of selfembedding structures. In the probabilistic domain,
approximation by means of regular grammars is also
exploited by Eisner and Smith (2005), who ﬁlter
long-distance dependencies on-the-ﬂy.
Beyond ﬁnite automata approximation, Charniak
et al. (2006) propose a coarse-to-ﬁne approach in
which an approximated (not necessarily regular)
PCFG is used to construct a parse forest for the input sentence. Some statistical parameters are then
computed on such a structure, and exploited to ﬁlter
parsing with the non-approximated grammar. The
approach can also be iterated at several levels. In
the non-probabilistic setting, a similar ﬁltering ap-

proach was also proposed by Boullier (2003), called
“guided parsing.”
In this paper we rely on an algebraic formulation
of the inside-outside algorithm for PCFGs, based on
a tensor formulation developed for latent-variable
PCFGs in Cohen et al. (2012). We combine the
method with known techniques for tensor decomposition to approximate the source PCFG, and develop
a novel algorithm for approximate PCFG parsing.
We obtain improved time upper bounds with respect
to the input grammar size for PCFG parsing, and
provide error upper bounds on the PCFG approximation, in contrast with existing heuristic methods.

2

Preliminaries

This section introduces the special representation for
probabilistic context-free grammars that we adopt in
this paper, along with the decoding algorithm that
we investigate. For an integer i ≥ 1, we let [i] =
{1, 2, . . . , i}.
2.1

Probabilistic Context-Free Grammars

We consider context-free grammars (CFGs) in
Chomsky normal form, and denote them as
(N , L, R) where:
• N is the ﬁnite set of nonterminal symbols, with
m = |N |, and L is the ﬁnite set of words (lexical tokens), with L ∩ N = ∅ and with n = |L|.
• R is a set of rules having the form a → b c,
a, b, c ∈ N , or the form a → x, a ∈ N and
x ∈ L.
A probabilistic CFG (PCFG) is a CFG associated
with a set of parameters deﬁned as follows:
• For each (a → b c) ∈ R, we have a parameter
p(a → b c | a).

• For each (a → x) ∈ R, we have a parameter
p(a → x | a).
• For each a ∈ N , we have a parameter πa ,
which is the probability of a being the root
symbol of a derivation.
The parameters above satisfy the following normalization conditions:
p(a → b c | a) +

p(a → x | a) = 1,

(a→x)∈R

(a→b c)∈R

for each a ∈ N , and a∈N πa = 1.
The probability of a tree τ deriving a sentence in
the language, written p(τ ), is calculated as the product of the probabilities of all rule occurrences in τ ,
times the parameter πa where a is the symbol at the
root of τ .
2.2

Ta,b,c

p(a → b c | a).

Similarly, we denote by Q ∈ Rm×n a matrix such
that:
Qa,x p(a → x | a).
The root probabilities are denoted using a vector π ∈
Rm×1 such that πa is deﬁned as before.

Tensor Form of PCFGs

A three-dimensional tensor C ∈ R(m×m×m) is a
set of m3 parameters Ci,j,k for i, j, k ∈ [m]. In what
follows, we associate with each tensor three functions, each mapping a pair of vectors in Rm into a
vector in Rm .
Deﬁnition 1 Let C ∈ R(m×m×m) be a tensor.
Given two vectors y 1 , y 2 ∈ Rm , we let C(y 1 , y 2 )
be the m-dimensional row vector with components:
[C(y 1 , y 2 )]i =

tensor D ∈ Rm×m×m where Di,j,k = xi yj zk (this
is analogous to the outer product: [xy ]i,j = xi yj ).
We extend the parameter set of our PCFG such
that p(a → b c | a) = 0 for all a → b c not in R,
and p(a → x | a) = 0 for all a → x not in R. We
also represent each a ∈ N by a unique index in [m],
and we represent each x ∈ L by a unique index in
[n]: it will always be clear from the context whether
these indices refer to a nonterminal in N or else to a
word in L.
In this paper we assume a tensor representation
for the parameters p(a → b c | a), and we denote by
T ∈ Rm×m×m a tensor such that:

1 2
Ci,j,k yj yk .

2.3

Minimum Bayes-Risk Decoding

Let z = x1 · · · xN be some input sentence; we write
T (z) to denote the set of all possible trees for z. It
is often the case that parsing aims to ﬁnd the highest scoring tree τ ∗ for z according to the underlying
PCFG, also called the “Viterbi parse:”
τ ∗ = argmax p(τ )
τ ∈T (z)

j∈[m],k∈[m]

(y 1 , y 2 )

We also let C(1,2)
be the m-dimensional column vector with components:
[C(1,2) (y 1 , y 2 )]k =

1 2
Ci,j,k yi yj .
i∈[m],j∈[m]

Finally, we let C(1,3) (y 1 , y 2 ) be the m-dimensional
column vector with components:
[C(1,3) (y 1 , y 2 )]j =

1 2
Ci,j,k yi yk .
i∈[m],k∈[m]

For two vectors x, y ∈ Rm we denote by x y ∈
the Hadamard product of x and y, i.e., [x y]i =
xi yi . Finally, for vectors x, y, z ∈ Rm , xy z is the
Rm

Goodman (1996) noted that Viterbi parsers do not
optimize the same metric that is usually used for
parsing evaluation (Black et al., 1991). He suggested an alternative algorithm, which he called the
“Labelled Recall Algorithm,” which aims to ﬁx this
issue.
Goodman’s algorithm has two phases. In the ﬁrst
phase it computes, for each a ∈ N and for each substring xi · · · xj of z, the marginal µ(a, i, j) deﬁned
as:
µ(a, i, j) =

p(τ ).
τ ∈T (z) : (a,i,j)∈τ

Here we write (a, i, j) ∈ τ if nonterminal a spans
words xi · · · xj in the parse tree τ .

Inputs: Sentence x1 · · · xN , PCFG (N , L, R), parameters T ∈ R(m×m×m) , Q ∈ R(m×n) , π ∈
R(m×1) .
Data structures:
• Each µ(a, i, j) ∈ R for a ∈ N , i, j ∈ [N ],
i ≤ j, is a marginal probability.
• Each γ i,j ∈ R for i, j ∈ [N ], i ≤ j, is the highest score for a tree spanning substring xi · · · xj .
Algorithm:
(Marginals) ∀a ∈ N , ∀i, j ∈ [N ], i ≤ j, compute
the marginals µ(a, i, j) using the inside-outside
algorithm.
(Base case) ∀i ∈ [N ],
γ i,i =

max

µ(a, i, i)

(a→xi )∈R

(Maximize Labelled Recall) ∀i, j ∈ [N ], i < j,
γ i,j = max µ(a, i, j) + max γ i,k + γ k+1,j
a∈N

i≤k<j

Figure 1: The labelled recall algorithm from Goodman
(1996). The algorithm in this ﬁgure ﬁnds the highest
score for a tree which maximizes labelled recall. The actual parsing algorithm would use backtrack pointers in
the score computation to return a tree. These are omitted
for simplicity.

The second phase includes a dynamic programming algorithm which ﬁnds the tree τ ∗ that maximizes the sum over marginals in that tree:
τ ∗ = argmax
τ ∈T (z)

µ(a, i, j).
(a,i,j)∈τ

Goodman’s algorithm is described in Figure 1.
As Goodman notes, the complexity of the second
phase (“Maximize Labelled Recall,” which is also
referred to as “minimum Bayes risk decoding”) is
O(N 3 + mN 2 ). There are two nested outer loops,
each of order N , and inside these, there are two separate loops, one of order m and one of order N ,
yielding this computational complexity. The reason

for the linear dependence on the number of nonterminals is the lack of dependence on the actual grammar rules, once the marginals are computed.
In its original form, Goodman’s algorithm does
not enforce that the output parse trees are included in
the tree language of the PCFG, that is, certain combinations of children and parent nonterminals may
violate the rules in the grammar. In our experiments
we departed from this, and changed Goodman’s algorithm by incorporating the grammar into the dynamic programming algorithm in Figure 1. The reason this is important for our experiments is that we
binarize the grammar prior to parsing, and we need
to enforce the links between the split nonterminals
(in the binarized grammar) that refer to the same
syntactic category. See Matsuzaki et al. (2005) for
more details about the binarization scheme we used.
This step changes the dynamic programming equation of Goodman to be linear in the size of the grammar (ﬁgure 1). However, empirically, it is the insideoutside algorithm which takes most of the time to
compute with Goodman’s algorithm. In this paper
we aim to asymptotically reduce the time complexity of the calculation of the inside-outside probabilities using an approximation algorithm.

3

Tensor Formulation of the
Inside-Outside Algorithm

At the core of our approach lies the observation that
there is a (multi)linear algebraic formulation of the
inside-outside algorithm. It can be represented as a
series of tensor, matrix and vector products. A similar observation has been made for latent-variable
PCFGs (Cohen et al., 2012) and hidden Markov
models, where only matrix multiplication is required
(Jaeger, 2000). Cohen and Collins (2012) use this
observation together with tensor decomposition to
improve the speed of latent-variable PCFG parsing.
The representation of the inside-outside algorithm
in tensor form is given in Figure 2. For example,
if we consider the recursive equation for the inside
probabilities (where αi,j is a vector varying over the
nonterminals in the grammar, describing the inside
probability for each nonterminal spanning words i
to j):
j−1

αi,j =

T (αi,k , αk+1,j )
k=i

Inputs: Sentence x1 · · · xN , PCFG (N , L, R), parameters T ∈ R(m×m×m) , Q ∈ R(m×n) , π ∈
R(m×1) .
Data structures:

deﬁned recursively as follows:
j−1
i,k
k+1,j
Ta,b,c × αb × αc

[αi,j ]a =
k=i b,c

• Each αi,j ∈ R1×m , i, j ∈ [N ], i ≤ j, is a row
vector of inside terms ranging over a ∈ N .
• Each β i,j ∈ Rm×1 , i, j ∈ [N ], i ≤ j, is a
column vector of outside terms ranging over
a ∈ N.
• Each µ(a, i, j) ∈ R for a ∈ N , i, j ∈ [N ],
i ≤ j, is a marginal probability.
Algorithm:
(Inside base case) ∀i ∈ [N ], ∀(a → xi ) ∈ R,
[αi,i ]a = Qa,x
(Inside recursion) ∀i, j ∈ [N ], i < j,
j−1

αi,j =

T (αi,k , αk+1,j )

j−1

k=i b,c

which is exactly the recursive deﬁnition of the inside
algorithm. The correctness of the outside recursive
equations follows very similarly.
The time complexity of the algorithm in this case
is O(m3 N 3 ). To see this, observe that each tensor
application takes time O(m3 ). Furthermore, the tensor T is applied O(N ) times in the computation of
each vector αi,j and β i,j . Finally, we need to compute a total of O(N 2 ) inside and outside vectors, one
for each substring of the input sentence.

4

k=i

(Outside base case) ∀a ∈ N ,
[β 1,N ]a = πa
(Outside recursion) ∀i, j ∈ [N ], i ≤ j,
i−1

β i,j =

T(1,2) (β k,j , αk,i−1 )+

k=1
N

T(1,3) (β i,k , αj+1,k )
k=j+1

(Marginals) ∀a ∈ N , ∀i, j ∈ [N ], i ≤ j,

i,k
k+1,j
p(a → b c | a) × αb × αc
,

=

Tensor Decomposition for the
Inside-Outside Algorithm

In this section, we detail our approach to approximate parsing using tensor decomposition.
4.1

Tensor Decomposition

In the formulation of the inside-outside algorithm
based on tensor T , each vector αi,j and β i,j consists
of m elements, where computation of each element
requires time O(m2 ). Therefore, the algorithm has a
O(m3 ) multiplicative factor in its time complexity,
which we aim to reduce by means of an approximate
algorithm.
Our approximate method relies on a simple observation. Given an integer r ≥ 1, assume that
the tensor T has the following special form, called
“Kruskal form:”

µ(a, i, j) = [αi,j ]a · [β i,j ]a

r

T =
Figure 2: The tensor form of the inside-outside algorithm,
for calculation of marginal terms µ(a, i, j).

and then apply the tensor product from Deﬁnition 1
to this equation, we get that coordinate a in αi,j is

λi ui vi wi .

(1)

i=1

In words, T is the sum of r tensors, where each
tensor is obtained as the product of three vectors
ui , vi and wi , together with a scalar λi . Exact
Kruskal decomposition of a tensor is not necessarily
unique. See Kolda and Bader (2009) for discussion
of uniqueness of tensor decomposition.

Consider now two vectors y 1 , y 2 ∈ Rm , associated with the inside probabilities for the left (y 1 ) and
right child (y 2 ) of a given node in a parse tree. Let
us introduce auxiliary arrays U, V, W ∈ Rr×m , with
the i-th row being ui , vi and wi , respectively. Let
also λ = (λ1 , . . . , λr ). Using the decomposition in
Eq. 1 within Deﬁnition 1 we can express the array
T (y 1 , y 2 ) as:
r
1

2

T (y , y ) =

λi ui vi wi

(y 1 , y 2 ) =

i=1
r

λi ui (vi y 1 )(wi y 2 ) =
i=1

U (λ

V y1

W y2)

.

(2)

The total complexity of the computation in Eq. 2
is now O(rm). It is well-known that an exact tensor
decomposition for T can be achieved with r = m2
(Kruskal, 1989). In this case, there is no computational gain in using Eq. 2 for the inside calculation.
The minimal r required for an exact tensor decomposition can be smaller than m2 . However, identifying that minimal r is NP-hard (Høastad, 1990).
In this section we focused on the computation of the inside probabilities through vectors
T (αi,k , αk+1,j ). Nonetheless, the steps above can
be easily adapted for the computation of the outside
probabilities through vectors T(1,2) (β k,j , αk,i−1 )
and T(1,3) (β i,k , αj+1,k ).
4.2

Approximate Tensor Decomposition

The PCFG tensor T will not necessarily have the exact decomposed form in Eq. 1. We suggest to approximate the tensor T by ﬁnding the closest tensor
according to some norm over Rm×m×m .
An example of such an approximate decomposition is the canonical polyadic decomposition
(CPD), also known as CANDECOMP/PARAFAC
decomposition (Carroll and Chang, 1970; Harshman, 1970; Kolda and Bader, 2009). Given an integer r, least squares CPD aims to ﬁnd the nearest
tensor in Kruskal form, minimizing squared error.
More formally, for a given tensor D ∈ Rm×m×m ,
2
let ||D||F =
i,j,k Di,j,k . Let the set of tensors in

Kruskal form Cr be:
r

Cr ={C ∈ R

m×m×m

|C=

λi ui vi wi
i=1
m

s.t. λi ∈ R, ui , vi , wi ∈ R ,
||ui ||2 = ||vi ||2 = ||wi ||2 = 1}.
ˆ
The least squares CPD of C is a tensor C such
ˆ ∈ argmin ˆ
ˆ F . Here, we treat
that C
C∈Cr ||C − C||
the argmin as a set because there could be multiple
solutions which achieve the same accuracy.
There are various algorithms to perform CPD,
such as alternating least squares, direct linear decomposition, alternating trilinear decomposition and
pseudo alternating least squares (Faber et al., 2003)
and even algorithms designed for sparse tensors (Chi
and Kolda, 2011). Most of these algorithms treat
the problem of identifying the approximate tensor as
an optimization problem. Generally speaking, these
optimization problems are hard to solve, but they
work quite well in practice.
4.3

Parsing with Decomposed Tensors

Equipped with the notion of tensor decomposition,
we can now proceed with approximate tensor parsing in two steps. The ﬁrst is approximating the tensor using a CPD algorithm, and the second is applying the algorithms in Figure 1 and Figure 2 to do
parsing, while substituting all tensor product computations with the approximate O(rm) operation of
tensor product.
This is not sufﬁcient to get a signiﬁcant speed-up
in parsing time. Re-visiting Eq. 2 shows that there
are additional ways to speed-up the tensor application T in the context of the inside-outside algorithm.
The ﬁrst thing to note is that the projections V y 1
and W y 2 in Eq. 2 can be cached, and do not have
to be re-calculated every time the tensor is applied.
Here, y 1 and y 2 will always refer to an outside or
an inside probability vector over the nonterminals in
the grammar. Caching these projections means that
after each computation of an inside or outside probability, we can immediately project it to the necessary
r-dimensional space, and then re-use this computation in subsequent application of the tensor.
The second thing to note is that the U projection in T can be delayed, because of the rule of

distributivity. For example, the step in Figure 2
that computes the inside probability αi,j can be reformulated as follows (assuming an exact decomposition of T ):
j−1

α

i,j

T (αi,k , αk+1,j )

=
k=i
j−1

=

Deﬁne Z(a, N ) =

ˆ

τ ∈T (N ) [ξ(τ )]a .

In addition,

ˆ
deﬁne D(a, N ) =
τ ∈T (N ) |[ξ(τ )]a − [ξ(τ )]a |
and deﬁne F (a, N ) = D(a, N )/Z(a, N ). Deˆ
ﬁne ∆ = ||T − T ||F .
Last, deﬁne ν =
min(a→b c)∈R p(a → b c | a). Then, the following
lemma holds:
Lemma 1 For any a and any N , it holds:

U (λ

V αi,k

W αk+1,j )
D(a, N ) ≤ Z(a, N ) (1 + ∆/ν)N − 1

k=1
j−1

=U

(λ

V αi,k

W αk+1,j ) . (3)

k=1

This means that projection through U can be done
outside of the loop over splitting points in the sentence. Similar reliance on distributivity can be used
to speed-up the outside calculations as well.
The caching speed-up and the delayed projection
speed-up make the approximate inside-outside computation asymptotically faster. While na¨ve applicaı
tion of the tensor yields an inside algorithm which
runs in time O(rmN 3 ), the improved algorithm
runs in time O(rN 3 + rmN 2 ).

5

Quality of Approximate Tensor Parsing

In this section, we give the main approximation result, that shows that the probability distribution induced by the approximate tensor is close to the original probability distribution, if the distance between
the approximate tensor and the rule probabilities is
not too large.
Denote by T (N ) the set of trees in the tree language of the PCFG with N words (any nonterminal can be the root of the tree). Let T (N ) be the
set of pairs of trees τ = (τ1 , τ2 ) such that the total number of binary rules combined in τ1 and τ2 is
N − 2 (this means that the total number of words
ˆ
combined is N ). Let T be the approximate tensor
for T . Denote the probability distribution induced
ˆ
by T by p. 1 Deﬁne the vector ξ(τ ) such that
ˆ
[ξ(τ )]a = Ta,b,c · p(τ1 | b) · p(τ2 | c) where the root
τ1 is nonterminal b and the root of τ2 is c. Similarly,
ˆ
ˆ
deﬁne [ξ(τ )]a = Ta,b,c · p(τ1 | b) · p(τ2 | c).
ˆ
ˆ
ˆ
Here, p does not have to be a distribution, because T could
ˆ
have negative values, in principle, and its slices do not have to
normalize to 1. However, we just treat p as a function that maps
ˆ
ˆ
trees to products of values according to T .

Proof sketch: The proof is by induction on N .
Assuming that 1 + F (b, k) ≤ (1 + ∆/ν)k and
1 + F (c, N − k − 1) ≤ (1 + ∆/ν)N −k−1 for F
deﬁned as above (this is the induction hypothesis), it
can be shown that the lemma holds.
Lemma 2 The following holds for any N :
|ˆ(τ ) − p(τ )| ≤ m (1 + ∆/ν)N − 1
p
τ ∈T (N )

Proof sketch: Using H¨ lder’s inequality and
o
Lemma 1 and the fact that Z(a, N ) ≤ 1, it follows
that:
ˆ
|[ξ(τ )]a − [ξ(τ )]a |

|ˆ(τ ) − p(τ )| ≤
p
τ ∈T (N )

τ ∈T (N ),a

≤

Z(a, N )

(1 + ∆/ν)N − 1

a

≤ m (1 + ∆/ν)N − 1

Then, the following is a result that explains how
accuracy changes as a function of the quality of the
tensor approximation:
Theorem 1 For any N , and < 1/4, it holds that if
ν
, then:
∆≤
2N m

1

|ˆ(τ ) − p(τ )| ≤
p
τ ∈T (N )

(4)

Proof sketch: This is the result of applying Lemma 2
together with the inequality (1 + y/t)t − 1 ≤ 2y for
any t > 0 and y ≤ 1/2.
We note that Theorem 1 also implicitly bounds
the difference between a marginal µ(a, i, j) and its
approximate version. A marginal corresponds to a
sum over a subset of summands in Eq. 4.
A question that remains at this point is to decide
whether for a given grammar, the optimal ν that can
be achieved is large or small. We deﬁne:

ˆ
∆∗ = ||T − T ||F ≤ ||T − Tr ||F
r
m2
∗
∗
λ∗ u∗ (vi ) (wi ) ||F
i i

= ||
i=r+1
m2

m2

|λ∗ |
i

≤

·

∗
||u∗ (vi )
i

i=r+1

∗
(wi )

||F

|λ∗ |
i

=
i=r+1

as required.
ˆ
∆∗ = min ||T − T ||F
r
ˆ
T ∈Cr

(5)

The following theorem gives an upper bound on
the value of ∆∗ based on an intrinsic property of the
r
grammar, or more speciﬁcally T . It relies on the
fact that for three-dimensional tensors, where each
dimension is of length m, there exists an exact decomposition of T using m2 components.
Theorem 2 Let:
m2
∗
∗
λ∗ u∗ (vi ) (wi )
i i

T =
i=1

be an exact Kruskal decomposition of T such that
∗
∗
||u∗ ||2 = ||vi ||2 = ||wi || = 1 and λ∗ ≥ λ∗ for
i
i
i+1
i ∈ [m2 − 1]. Then, for a given r, it holds:
m2

∆∗
r

|λ∗ |
i

≤
i=r+1

ˆ
Proof: Let T be a tensor that achieves the minimum
in Eq. 5. Deﬁne:
r
∗
∗
λ∗ u∗ (vi ) (wi )
i i

Tr =
i=1

Then, noting that ∆∗ is a minimizer of the norm
r
ˆ
difference between T and T and then applying the
triangle inequality and then Cauchy-Schwartz inequality leads to the following chain of inequalities:

6

Experiments

In this section, we describe experiments that demonstrate the trade-off between the accuracy of the tensor approximation (and as a consequence, the accuracy of the approximate parsing algorithm) and parsing time.
Experimental Setting We compare the tensor approximation parsing algorithm versus the vanilla
Goodman algorithm. Both algorithms were implemented in Java, and the code for both is almost identical, except for the set of instructions which computes the dynamic programming equation for propagating the beliefs up in the tree. This makes the
clocktime comparison reliable for drawing conclusions about the speed of the algorithms. Our implementation of the vanilla parsing algorithm is linear in the size of the grammar (and not cubic in the
number of nonterminals, which would give a worse
running time).
In our experiments, we use the method described
in Chi and Kolda (2011) for tensor decomposition.2
This method is fast, even for large tensors, as long
as they are sparse. Such is the case with the tensors
for our grammars.
We use two treebanks for our comparison: the
Penn treebank (Marcus et al., 1993) and the Arabic
treebank (Maamouri et al., 2004). With the Penn
treebank, we use sections 2–21 for training a maximum likelihood model and section 22 for parsing,
while for the Arabic treebank we divide the data into
2

We use the implementation given in Sandia’s Matlab Tensor Toolbox, which can be downloaded at http:
//www.sandia.gov/˜tgkolda/TensorToolbox/
index-2.5.html.

two sets, of size 80% and 20%, one is used for training a maximum likelihood model and the other is
used for parsing.
The number of binary rules in the treebank grammar is 7,240. The number of nonterminals is 112
and the number of preterminals is 259.3 The number of binary rules in the Arabic treebank is signiﬁcantly smaller and consists of 232 rules. We run all
parsing experiments on sentences of length ≤ 40.
The number of nonterminals is 48 and the number
of preterminals is 81.
Results Table 1 describes the results of comparing the tensor decomposition algorithm to the vanilla
PCFG parsing algorithm.
The ﬁrst thing to note is that the running time of
the parsing algorithm is linear in r. This indeed
validates the asymptotic complexity of the insideoutside component in Goodman’s algorithm with the
approximate tensors. It also shows that most of the
time during parsing is spent on the inside-outside algorithm, and not on the dynamic programming algorithm which follows it.
In addition, compared to the baseline which uses
a vanilla CKY algorithm (linear in the number of
rules), we get a speed up of a factor of 4.75 for
Arabic (r = 140) and 6.5 for English (r = 260)
while retaining similar performance. Perhaps more
surprising is that using the tensor approximation actually improves performance in several cases. We
hypothesize that the cause of this is that the tensor
decomposition requires less parameters to express
the rule probabilities in the grammar, and therefore
leads to better generalization than a vanilla maximum likelihood estimate.
We include results for a more complex model for
Arabic, which uses horizontal Markovization of order 1 and vertical Markovization of order 2 (Klein
and Manning, 2003). This grammar includes 2,188
binary rules. Parsing exhaustively using this grammar takes 1.30 seconds per sentence (on average)
with an F1 measure of 64.43. Parsing with tensor
decomposition for r = 280 takes 0.62 seconds per
sentence (on average) with an F1 measure of 64.05.
3

Unary rules are removed by collapsing non-terminal
chains. This increased the number of preterminals.

7

Discussion

In this section, we brieﬂy touch on several other topics related to tensor approximation.
7.1

Approximating the Probability of a String

The probability of a sentence z under a PCFG is deﬁned as p(z) = τ ∈T (z) p(τ ), and can be approximated using the algorithm in Section 4.3, running
in time O(rN 3 + rmN 2 ). Of theoretical interest,
we discuss here a time O(rN 3 + r2 N 2 ) algorithm,
which is more convenient when r < m.
Observe that in Eq. 3 vector αi,j always appears
within one of the two terms V αi,j and W αi,j in
Rr×1 , whose dimensions are independent of m. We
can therefore use Eq. 3 to compute V αi,j as
j−1

V αi,j = V U

(λ

V αi,k

W αk+1,j ) ,

k=1

where V U is a Rr×r matrix that can be computed
off-line, i.e., independently of z. A symmetrical relation can be used to compute W αi,j . Finally, we
can write
N −1

p(z) = π U

(λ

V α1,k

W αk+1,N ) ,

k=1

where π U is a R1×r vector that can again be computed off-line. This algorithm then runs in time
O(rN 3 + r2 N 2 ).
7.2

Applications to Dynamic Programming

The approximation method presented in this paper is
not limited to PCFG parsing. A similar approximation method has been used for latent-variable PCFGs
(Cohen and Collins, 2012), and in general, tensor approximation can be used to speed-up insideoutside algorithms for general dynamic programming algorithms or weighted logic programs (Eisner
et al., 2004; Cohen et al., 2011). In the general case,
the dimension of the tensors will not be necessarily
just three (corresponding to binary rules), but can be
of a higher dimension, and therefore the speed gain
can be even greater. In addition, tensor approximation can be used for computing marginals of latent
variables in graphical models.
For example, the complexity of the forwardbackward algorithm for HMMs can be reduced to

c
abi
Ar
sh
gli
En

rank (r)
speed
F1
speed
F1

baseline
0.57
63.78
3.89
71.07

20
0.04
51.80
0.15
57.83

60
0.06
58.39
0.21
61.67

100
0.1
63.63
0.30
68.28

140
0.12
63.77
0.37
69.63

180
0.16
63.88
0.44
70.30

220
0.19
63.82
0.52
70.82

260
0.22
63.84
0.60
71.42

300
0.26
63.80
0.70
71.28

340
0.28
63.88
0.79
71.13

Table 1: Results for the Arabic and English treebank of parsing using a vanilla PCFG with and without tensor decomposition. Speed is given in seconds per sentence.

be linear in the number of states (as opposed to
quadratic) and linear in the rank used in an approximate singular-value decomposition (instead of tensor decomposition) of the transition and emission
matrices.
7.3

Tighter (but Slower) Approximation Using
Singular Value Decomposition

The accuracy of the algorithm depends on the ability
of the tensor decomposition algorithm to decompose
the tensor with a small reconstruction error. The decomposition algorithm is performed on the tensor T
which includes all rules in the grammar.
Instead, one can approach the approximation by
doing a decomposition for each slice of T separately
using singular value decomposition. This will lead
to a more accurate approximation, but will also lead
to an extra factor of m during parsing. This factor
is added because now there is not a single U , V and
W , but instead there are such matrices for each nonterminal in the grammar.

8

Conclusion

We described an approximation algorithm for probabilistic context-free parsing. The approximation algorithm is based on tensor decomposition performed
on the underlying rule table of the CFG grammar.
The approximation algorithm leads to signiﬁcant
speed-up in PCFG parsing, with minimal loss in performance.

References
E. Black, S. Abney, D. Flickenger, C. Gdaniec, R. Grishman, P Harrison, D. Hindle, R. Ingria, F. Jelinek,
J. Klavans, M. Liberman, M. Marcus, S. Roukos,
B. Santorini, and T. Strzalkowski. 1991. A procedure
for quantitatively comparing the syntactic coverage of

English grammars. In Proceedings of DARPA Workshop on Speech and Natural Language.
P. Boullier. 2003. Guided earley parsing. In 8th International Workshop on Parsing Technologies, pages
43–54.
J. D. Carroll and J. J. Chang. 1970. Analysis of individual differences in multidimensional scaling via an
N-way generalization of Eckart-Young decomposition.
Psychometrika, 35:283–319.
E. Charniak, M. Johnson, M. Elsner, J. Austerweil,
D. Ellis, I. Haxton, C. Hill, R. Shrivaths, J. Moore,
M. Pozar, and T. Vu. 2006. Multilevel coarse-to-ﬁne
pcfg parsing. In Proceedings of HLT-NAACL.
E. C. Chi and T. G. Kolda. 2011. On tensors, sparsity, and nonnegative factorizations. arXiv:1112.2414
[math.NA], December.
S. B. Cohen and M. Collins. 2012. Tensor decomposition for fast parsing with latent-variable PCFGs. In
Proceedings of NIPS.
S. B. Cohen, R. J. Simmons, and N. A. Smith. 2011.
Products of weighted logic programs. Theory and
Practice of Logic Programming, 11(2–3):263–296.
S. B. Cohen, K. Stratos, M. Collins, D. F. Foster, and
L. Ungar. 2012. Spectral learning of latent-variable
PCFGs. In Proceedings of ACL.
J. Eisner and N. A. Smith. 2005. Parsing with soft and
hard constraints on dependency length. In Proceedings of IWPT, Parsing ’05.
J. Eisner, E. Goldlust, and N. A. Smith. 2004. Dyna: A
declarative language for implementing dynamic programs. In Proc. of ACL (companion volume).
N. M. Faber, R. Bro, and P. Hopke. 2003. Recent developments in CANDECOMP/PARAFAC algorithms: a
critical review. Chemometrics and Intelligent Laboratory Systems, 65(1):119–137.
J. Goodman. 1996. Parsing algorithms and metrics. In
Proceedings of ACL.
R. A. Harshman. 1970. Foundations of the PARAFAC
procedure: Models and conditions for an “explanatory” multi-modal factor analysis. UCLA working papers in phoentics, 16:1–84.
J. Høastad. 1990. Tensor rank is NP-complete. Algorithms, 11:644–654.

H. Jaeger. 2000. Observable operator models for discrete stochastic time series. Neural Computation,
12(6):1371–1398.
D. Klein and C. D. Manning. 2003. Accurate unlexicalized parsing. In Proceedings of ACL.
T. G. Kolda and B. W. Bader. 2009. Tensor decompositions and applications. SIAM Rev., 51:455–500.
J. B. Kruskal. 1989. Rank, decomposition, and uniqueness for 3-way and N-way arrays. In R. Coppi and
S. Bolasco, editors, Multiway Data Analysis, pages 7–
18.
M. Maamouri, A. Bies, T. Buckwalter, and W. Mekki.
2004. The Penn Arabic Treebank: Building a largescale annotated Arabic corpus. In Proceedings NEMLAR.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a large annotated corpus of English: The Penn treebank. Computational Linguistics,
19(2):313–330.
T. Matsuzaki, Y. Miyao, and J. Tsujii. 2005. Probabilistic CFG with latent annotations. In Proceedings
of ACL.
M.-J. Nederhof. 2000. Practical experiments with regular approximation of context-free languages. Computational Linguistics, 26(1):17–44.

