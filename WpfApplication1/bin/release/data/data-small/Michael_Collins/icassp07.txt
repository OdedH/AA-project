TRIGGER-BASED LANGUAGE MODELING
USING A LOSS-SENSITIVE PERCEPTRON ALGORITHM
Natasha Singh-Miller

Michael Collins

MIT CSAIL
32 Vassar St., Cambridge, MA 02139
natashas@csail.mit.edu

MIT CSAIL
32 Vassar St., Cambridge, MA 02139
mcollins@csail.mit.edu

ABSTRACT
Discriminative language models using n-gram features have been
shown to be effective in reducing speech recognition word error
rates. In this paper we describe a method for incorporating discourselevel triggers into a discriminative language model. Triggers are
features identifying re-occurrence of words within a conversation.
We introduce triggers that are speciﬁc to particular unigrams and
bigrams, as well as “back off” trigger features that allow generalizations to be made across different unigrams. We train our model
using a new loss-sensitive variant of the perceptron algorithm that
makes effective use of information from multiple hypotheses in an
n-best list. We train and test on the Switchboard data set and show a
0.5 absolute reduction in WER over a baseline discriminative model
which uses n-gram features alone, and a 1.5 absolute reduction in
WER over the baseline recognizer.
Index Terms— Perceptrons, Speech recognition, Natural languages
1. INTRODUCTION
Previous work on discriminative language modeling [1] has considered models where the optimal string w∗ for a given acoustic input
a is deﬁned as follows:
w∗ = arg max (β log Pl (w) + log Pa (a|w) + α, Φ(a, w) )
¯
w

In this approach, a standard language model, Pl , and an acoustic
model, Pa , are used alongside a linear correction term α, Φ(a, w) .1
¯
Φ(a, w) is a feature-vector representation of the pair (a, w), and α
¯
is a parameter vector of the same dimensionality as Φ(a, w). The
parameters α are estimated using discriminative methods (e.g. the
¯
perceptron algorithm). Improvements in word error rate (WER) have
been observed by incorporating both n-gram and syntactic features
within Φ(a, w) [1, 2].
In this paper we consider two extensions to the discriminative
language modeling approach. Our ﬁrst contribution is to describe a
method for including trigger features [3, 4] within the deﬁnition of
Φ(a, w). Trigger features are designed to model the fact that content
words are more likely to be used repeatedly within a single conversation than to occur evenly spread throughout all speech. For example
the word “Uzbekistan” may occur very rarely, but within the context of a conversation where it has already occurred, the likelihood
of seeing “Uzbekistan” again increases dramatically. To capture this
1 β is a positive constant that determines the relative weight of the language and acoustic models. We use x, y to denote the inner product of two
vectors x and y.

phenomenon in our model, a trigger feature can be deﬁned that indicates the number of times in a conversation that “Uzbekistan” is
seen preceded by a previous instance of “Uzbekistan”. In addition to
lexically-speciﬁc trigger features, we also introduce backoff trigger
features where content words are placed into different equivalence
classes based on their TF-IDF scores [5]. The use of lexicalized trigger features within a generative language model, i.e., a model that attempts to estimate Pl (w), is described in [3, 4]. However, our use of
trigger features in a discriminative language model is arguably simpler and more direct—in particular, the parameter estimation method
is more closely related to optimizing WER.
Our second contribution is to introduce a new loss-sensitive variant of the perceptron algorithm for the estimation of α. This percep¯
tron is similar in form to that proposed by [7] for multiclass classiﬁcation, however it explicitly models the loss of selecting different hypotheses, and also takes into account the fact that multiple
hypotheses may be considered optimal. In contrast to the work in
[1], this perceptron algorithm makes updates based on averaging the
contribution from a larger number of hypotheses, potentially making
much better use of the information in the hypothesis set.
We tested our model on the Switchboard corpus using the recognizer of [6] and the discriminative language model of [1] as baselines. Our model demonstrates a 0.5 absolute reduction in WER over
the model in [1], and a 1.5 absolute reduction in WER over the baseline recognizer of [6].
2. FEATURES
In this section, we describe how to extend the discriminative model
described above in order to include trigger features in the model. We
will use the following deﬁnitions:
• a1 . . . an represents a sequence of acoustic inputs constituting a single conversation.
• GEN(ai ) denotes the set of n-best hypotheses produced by
the baseline recognizer for the acoustic input ai .
• vi designates the transcription of ai we use to construct histories for identifying triggering events.
• hi = {v1 , . . . , vi−1 } is the history of ai .
• Φ(ai , w, hi ) is a feature-vector representation. We assume
that the score assigned by the generative model is the ﬁrst
feature in this vector (i.e., Φ1 (a, w, h) = log Pa (a|w) +
βPl (w)).
• The resulting decoding model is:
∗
¯
wi = arg max α, Φ(ai , w, hi )
w

For training α, we assume that the baseline speech recognizer
¯
can be used to generate an n-best list of candidate hypotheses for any
acoustic input. During training, vi is the least errorful hypothesis
in GEN(ai ). During decoding, vi is the best scoring hypothesis
under the generative model for each ai . We also experimented with
deﬁning vi to be the hypotheses selected while decoding, but this
gave neglible differences in performance.
The baseline discriminative model and our new model both include the following features. The ﬁrst feature is the score assigned by
the recognizer as described above. The remaining features include
unigram, bigram, and trigram features. As one example, a trigram
feature would be
Φ2 (a, w, h) = number of times the dog barked appears in w
Similar features are deﬁned for all unigrams, bigrams, and trigrams
seen in the n-best lists of the training data.

words are sorted by increasing score and divided into ten equal-sized
bins. In practice bin0 consists of roughly the hundred most common
words in speech (e.g. a, with, go, etc.). Since these words are so
frequent, we anticipate that their trigger features will behave differently from the other words in the vocabulary. We create the other
ten bins in this graded manner because we anticipate that different
content levels will result in different trigger behavior.
One feature for each bin is then added to the model. Suppose
Φw for any word w in the vocabulary is a trigger feature for that
word (for example, ΦU zbekistan would be deﬁned as in the example
above). For each binb , for b = 0 . . . 10, we deﬁne a feature as
follows:
Φb (a, w, h) =

X

Φv (a, w, h)

v∈binb

The feature Φb counts the number of triggering events involving the
words in binb . These features allow the model to learn a general
preference for triggering events involving each of the 11 bins.

2.1. Trigger Features
We augment the baseline model with trigger features designed to
capture information about the re-occurrence of words. These features operate at the discourse level in that they depend upon the
words of the current candidate hypothesis as well as all other words
that have occurred in previous utterances in the conversation. The
unigram trigger features, created for all unigrams seen in the training data, are of the following form.
Φ3 (a, w, h) =
1 iff: (a) Uzbekistan is seen in w at least twice;
or (b) Uzbekistan is seen in w once and is seen at
least once in the history h
0 otherwise
In addition to unigram features, we include bigram trigger features.
For example, we might have a feature that is similar to Φ3 above, but
tests for the bigram San Francisco. Features of this form are created
for all bigrams seen in training data.
Since the above features are lexicalized—i.e., there is a separate feature for each distinct unigram or bigram—some may be very
sparse within our training set. To counteract this shortcoming we
introduce a set of backoff trigger features. Each word w in the vocabulary is assigned to one of eleven bins based on its TF-IDF score
[5]. The TF-IDF score is deﬁned as follows for any word w and
conversation d, where w is seen in d:
n
score(w, d) = (1 + log(tfwd ))(log
)
dfw
Here dfw is the number of conversations in which the word w occurs,
n is the total number of conversations in training data, and tfwd is
the number of times word w occurs in conversation d. The score
for a word w, which we will denote as score(w), is the average of
score(w, d) over all conversations d that contain w. The function
score(w) attempts to measure the degree to which the word w is a
content word (and thus is likely to be a good trigger feature). We
calculated TF-IDF scores for each word seen in the training data,
using the 4,800 transcribed conversation sides in the Switchboard
training set as documents.2
Words are then placed into bins according to their score. Words
with score(w) less than 1.0 are assigned to bin0 . All remaining
2 Note that we used the reference transcriptions for calculating TF-IDF
scores, as opposed to the outputs from the baseline recognizer.

3. TRAINING: PERCEPTRON
Figure 1 show the loss-sensitive perceptron algorithm we use for
training α. This perceptron is similar in form to the perceptrons pro¯
posed by [7] for multiclass classiﬁcation and by [8] for reranking.
The perceptron is loss-sensitive in two ways. First, the perceptron
enforces a margin that scales linearly with increases in loss. Second, the perceptron recognizes that there may be multiple hypotheses with minimal loss that should all be considered optimal.
In a given n-best list, GEN(ai ), there may be one or more optimal hypotheses. For example, the correct transcription may not be
present in the list, but there may be several hypotheses each with
only one error, while all the other hypotheses have two or more errors. We denote the set of lowest error hypotheses of GEN(ai ) by
Gi . In terms of performance, all members of Gi are considered optimal choices by the discriminative model.
Let Bi = GEN(ai ) − Gi , i.e. the set of all non-optimal hypotheses in GEN(ai ). Each hypothesis in Bi will display different
numbers and types of errors. The following loss function is used to
indicate the badness of each member of Bi :
∆i (b) = edits(b) − edits(g) where g is any member of Gi
This loss function is simply the additional number of errors introduced by a hypothesis over the number of errors present in an optimal hypothesis. Note that all members of Gi have a loss of 0, while
all members of Bi have a loss of 1 or greater.
We deﬁne a margin that scales as λ∆i (b) where λ ≥ 0 is a parameter we select. Scaling the margin with the loss was originally
proposed by [9], who give statistical bounds justifying this. Intuitively, the idea is to ensure that hypotheses with a large number of
errors are more strongly separated from the members of Gi . In the
experiments presented in this paper λ is always set to 1.0. We deﬁne
the two sets Ci ⊆ Gi and Ei ⊆ Bi in Figure 1 which consist of optimal and non-optimal hypotheses, respectively, that violate the scaled
P
margin. We then construct two new vectors c∈Ci τ (c)Φi (c) and
P
e∈Ei τ (e)Φi (e), which are used to train the perceptron in the
usual way. The values of τ must meet the constraints described in
Figure 1. The ﬁrst four constraints insure that the weights used to
create the representative samples are all non-negative and sum to 1.
The ﬁnal constraint insures that the newly constructed average samples still violate the margin constraint in an averaged sense.

Note that the training examples used as input to the algorithm
are constructed in the following way. a1 . . . am is a sequence of
acoustic representations formed by concatenating all conversations
in the training data. The histories hi are constructed as follows. We
∗
take wi to be the member of Gi that is scored highest by the generative model. We deﬁne the history, hi , for utterance ai to be the
∗
∗
∗
sequence wi−l , wi−l+1 , . . . , wi−1 where l is the number of previous utterances which belong in the current conversation.
There are many methods for selecting the values of τ . In this
paper we consider the following simple deﬁnition:
1
∀c ∈ Ci ,τ (c) =
|Ci |
X vc (e)
∀e ∈ Ei ,τ (e) =
total
|Ci |vc
c∈Ci

1 if α, (Φi (c) − Φi (e)) < λ∆i (e)
¯
vc (e) =
0
otherwise
X
total
vc
=
vc (e)
e∈Ei

Essentially all the hypotheses in Ci receive an equal positive weight.
The weights of the hypotheses in Ei are assigned based on the values
vc (e). If vc (e) is 1 for many correct hypotheses c, τ (e) will be
relatively high.
The more standard perceptron used in the baseline model can be
thought of as a special case of this perceptron in which λ = 0 and
the τ values are assigned as follows. We designate some c ∈ Gi
as the single best hypothesis (for the baseline, the hypothesis in Gi
with the best recognizer score). We update only if c ∈ Ci . We set
τ (c ) = 1 and τ (e) = 1 where e is the member of Ei for which
α, (Φi (c ) − Φi (e)) is the lowest. All other τ values are set to 0.
¯
We can prove some useful properties for the perceptron in Figure
1. Consider the case where the training data is linearly separable,
or more speciﬁcally there exists some vector U and some maximal
margin δ > 0 such that ||U|| = 1 and the following constraint holds
for all i:
U, (Φi (g) − Φi (b)) ≥ δ∆i (b)

∀b ∈ Bi , ∀g ∈ Gi

It can be shown that in a ﬁnite number of iterations, given that
the values for τ satisfy the given constraints, the perceptron in Figure
1 learns a model α that separates the data as follows:3
¯
α
¯
, (Φi (g)
||α||
¯

where γ =

− Φi (b)) ≥ γ∆i (b)

λ
2λ+ 4R
s

2

∀b ∈ Bi , ∀g ∈ Gi

× δ, R is an upper bound on the maximum

length of a sample feature vector, and s is the minimum size of the
loss seen on an error. (For our loss function we have s = 1.) Note
δ
that as λ → ∞, γ → 2 .
4. EXPERIMENTS
We use the recognizer of [6] as our baseline recognizer (base-G) and
to generate 1000-best lists used by the discriminative models. The
discriminative model used in [1] also serves as a baseline (base-D).
We train the rerankers using Switchboard [10], Switchboard Cellular
[11], and CallHome [12] data. Rich Transcription 2002 (rt02) [13]
data was used for development. Rich Transcription 2003 (rt03) [14]
3 For a proof of this result see the appendix in the online version of this
paper.

Input: An integer T specifying the number of training iterations. A sequence of inputs a1 . . . am . A function GEN(ai )
that produces an n-best list of outputs for the input ai . A
mapping hi that represents the history for ai . A function
∆i (w) that represents the loss of selecting output w for the
sample ai . ∆i must always be non-negative and there must be
at least one member of GEN(ai ) with a loss equal to 0.
Deﬁnitions: Gi = {w|w ∈ GEN(ai ) and ∆i (w) = 0}
Bi = GEN(ai ) − Gi
Let Φi (w) be shorthand for Φ(ai , w, hi )
Algorithm:
α ← 0 λ ← 1.0
¯
For t = 1 to T , i = 1 to m
• Ci = {c|c ∈ Gi and ∃z such that z ∈ Bi and
α, Φi (c) − Φi (z) < λ∆i (z)}
¯
• Ei = {e|e ∈ Bi and ∃y such that y ∈ Gi and
α, Φi (y) − Φi (e) < λ∆i (e)}
¯
• If |Ci | = 0, deﬁne a function τ over Ci ∪ Ei such that
the following constraints hold:
P
*
c∈Ci τ (c) = 1
P
*
e∈Ei τ (e) = 1
* ∀c ∈ Ci , τ (c) ≥ 0

* ∀e ∈ Ei , τ (e) ≥ 0
“P
”
P
* α,
¯
τ (c)Φi (c) − e∈Ei τ (e)Φi (e) <
c∈Ci
“P
”
λ
e∈Ei τ (e)∆i (e)

Update the parameters:
P
P
α ← α + c∈Ci τ (c)Φi (c) − e∈Ei τ (e)Φi (e)
¯
¯

Output: The parameters α.
¯

Fig. 1. The perceptron algorithm we propose for reranking speech
recognition output. In our experiments we used the averaged parameters from the perceptron, see [1] for details.

data was used for testing. The training set consisted of 5533 conversation sides (individual speakers in a conversation), or about 3.3
million words. The development set consisted of 120 conversation
sides (6081 sentences) and the test set consisted of 144 conversation
sides (9050 sentences).
The perceptron trains very quickly, usually converging within
three passes over the training data, and we optimize the exact number
of iterations using the development set. We report results for the
test set only for the baseline models base-D and base-G, and for the
model that produces the best results on rt02.
We tested several combinations of the trigger features and report
results in Table 1. We ﬁnd that including all three types of trigger features—unigram self-triggers, bigram self-triggers, and backoff triggers—gives us the best results on the development set. This
model gives us a 0.4% absolute reduction in WER over base-D and
a 1.2% absolute reduction in WER over base-G on the development
set. This optimal model also achieves a 0.5% absolute reduction in
WER over base-D for the test set and a 1.5% absolute reduction in
WER over base-G. The results on rt03 are signiﬁcant with p < 0.01

Features
Base-G
Base-D
Loss-sensitive perceptron: n-grams
+ unigram self-triggers
+ bigram self-triggers
+ backoff unigram self-triggers
+ unigram and bigram self-triggers
+ unigram and backoff unigram self-triggers
+ unigram, bigram, and backoff unigram selftriggers

rt02
37.0
36.2
36.0
36.0
36.0
35.9
35.8
35.9
35.8

rt03
36.4
35.4
35.3

0.18
6.96
12.21
13.64

bin4
bin5
bin6
bin7

13.51
14.62
18.44
2.65

bin8
bin9
bin10

WEATHER
WEAR
SOMEONE
PARTS
SOMEPLACE

WANNA
LAKE
ICE
TRUCK
UH-HUH

WOOD
WAR
ALRIGHT
RIDE
WOMEN

Table 3. The 20 unigram trigger features with highest weight in the
ﬁnal model.
34.9

Table 1. Results of base-G, base-D, and our discriminative model
on the development set (rt02) and the test set (rt03).
bin0
bin1
bin2
bin3

GONNA
OIL
ONE’S
TRUST
READING

3.61
16.00
8.76

gether make a substantial gain over the baseline model. Additionally we present a perceptron algorithm used to train the discriminative model that shows improvements as well. Overall, the WER
on the test set was reduced by 0.5 over the baseline discriminative
model, and by 1.5 over the baseline recognizer. This work provides
evidence that discriminative language modeling has the potential to
deliver signiﬁcant gains for speech recognition tasks. The success of
the trigger features also shows how important discourse level information can be to transcribing spoken language.
6. REFERENCES

Table 2. Weights of bin features in ﬁnal model.

using the sign test at the conversation level.
We created several bins for the backoff trigger features with the
expectation that words with different frequencies and content levels
would have different trigger behavior. The learned weights of these
features for the ﬁnal discriminative model are listed in Table 2. From
bin0 to bin6 the learned weights increase. This conﬁrms our hypothesis that words with increasing content levels (or bin numbers) are
more inﬂuenced by triggering events. We see approximately a threefold increase in weight between bin1 and bin6 , suggesting that the
difference in behavior between the words in the two bins is quite
large, and therefore it may be worthwhile to try to create a backoff
scheme that is more sensitive to these differences. Finally, somewhat erratic weights are seen for bin7 through bin10 . One reason
for this may be that these are the rarest words in the training set, and
therefore weights for these bins are not adequately trained.
The words which have the 20 highest weights for their associated
unigram trigger features are listed in Table 3. These include content
words such as truck, as well as stylistic words such as gonna. We
posit that words such as gonna get high trigger weights because they
are more heavily used by some speakers than others. Additionally,
we see that some of the words in the list are homonyms of other
words, such as wear and where, wood and would, and weather and
whether. It seems likely that the occurrence of one of these words
earlier in a discourse should make it more likely to see it later and
help distinguish between homonyms.
Finally we see that the perceptron algorithm we present provides
additional gains over the baseline perceptron algorithm. Future work
might consider alternative ways to select the parameters τ as this
might lead to further gains.
5. CONCLUSION
In this paper we use the discriminative language model of [1] to create a reranker that includes discourse–level features. Speciﬁcally,
we introduce trigger features that help the discriminative language
model to adapt to discourse context. We use lexicalized and backoff trigger features that each show individual improvements and to-

[1] B. Roark, M. Saraclar, M. Collins, and M. Johnson, “Discriminative language modeling with conditional random ﬁelds and
the perceptron algorithm,” in Proceeding of the 42nd Annual
Meeting of the ACL, 2004, pp. 48–55.
[2] M. Collins, B. Roark, and M. Saraclar, “Discriminative Syntactic Language Modeling for Speech Recognition,” in Proceeding of the 43rd Annual Meeting of the ACL, 2005, pp. 507–514.
[3] R. Lau, R. Rosenfeld, and S. Roukos, “Trigger-based language
models: a maximum entropy approach,” in proc. ICASSP-93,
1993, vol. 2, pp. 45–58.
[4] R. Rosenfeld, Adaptive Statistical Language Modeling: A
Maximum Entropy Approach, Ph.D. thesis, Carnegie Mellon
University, 1994.
[5] G. Salton and M.J. McGill, Introduction to Modern Information Retrieval, McGraw-Hill, 1983.
[6] A. Ljolje, E. Bocchieri, M. Riley, B. Roark, M. Saraclar, and
I. Shafran, “The AT&T 1xRT CTS System,” in Rich Transcription Workshop, 2003.
[7] K. Crammer and Y. Singer, “Ultraconservative online algorithms for multiclass problems,” Journal of Machine Learning
Research, vol. 3, no. 4-5, pp. 951–991, 2003.
[8] L. Shen and A. K. Joshi, “Ranking and reranking with perceptron,” Machine Learning, vol. 60, pp. 73–96, 2005.
[9] B. Tasker, C. Guestrin, and D. Koller., “Max-margin markov
networks,” in Neural Information Processing Systems Conference, 2003.
[10] J.J. Godfrey and E. Holliman, “Switchboard-1 release 2,”
1997.
[11] D. Graff, K. Walker, and D. Miller, “Switchboard cellular,”
2001.
[12] A. Canavan, D. Graff, and G. Zipperlen, “Callhome american
english speech,” 1997.
[13] J.S. Garofolo, J. Fiscus, and A. Le, “2002 rich transcription
broadcast news and conversational telephone speech,” 2004.
[14] S. Strassel, C. Walker, and H. Lee, “Rt-03 mdetraining data
speech,” 2004.

