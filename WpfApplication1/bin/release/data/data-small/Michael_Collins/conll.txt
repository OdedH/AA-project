An SVM Approach
for Natural Language Learning

Michael Collins
MIT EECS/CSAIL
Joint work with Peter Bartlett, David McAllester, Ben Taskar

Supervised Learning in NLP
• Goal is to learn a function F : X → Y,
where X is a set of possible inputs,
Y is a set of possible outputs.
• We have a training sample (x1 , y1 ), (x2 , y2 ), . . . , (xn , yn )
where each (xi , yi ) ∈ X × Y
E.g., each xi is a sentence, each yi is a gold-standard parse

Global Linear Models

•

Three components:
GEN is a function from a string to a set of candidates
Φ maps a candidate to a feature vector
W is a parameter vector

Component 1: GEN
• GEN enumerates a set of candidates for a sentence

She announced a program to promote safety in trucks and vans

⇓ GEN

S

S

S

S

VP

NP

NP

She
NP

S

NP

VP

She

VP

NP
NP

announced
announced

S

VP

She

She

VP

NP
announced

VP

She

She

NP

NP
announced

VP

NP
a

NP

VP

NP
program

announced
to

promote

safety

VP

NP

NP
a

NP

a

program

announced

program
to

promote

NP

NP

in

in

NP

a

program

trucks
to

NP

trucks

and

to

promote

trucks

NP

and

vans
and

NP
and

NP

NP

safety

VP

NP

NP

in

program
to

NP
trucks

promote

NP
safety

program
to

promote

PP

NP
safety

in

NP
trucks

PP
in

VP

NP

vans

PP

a
a

NP
trucks

NP
vans

and

NP

vans

promote

NP
safety

vans

NP

VP

NP

safety

PP

in

PP

NP

PP

and

vans

Component 2: Φ
• Φ maps a candidate to a feature vector ∈ Rd
• Φ deﬁnes the representation of a candidate
S

NP

VP

She
announced

NP

NP
a

VP

program
to

VP

promote

NP

PP

safety

in

NP

NP

⇓Φ
1, 0, 2, 0, 0, 15, 5

trucks

and

NP
vans

Features
• A “feature” is a function on a structure, e.g.,
h(x) = Number of times
B

T1

is seen in x

A
C

T2

A
B

A

C

B

C

D

E

F

G

D

E

F

d

e

f

g

d

e

h

A
C

b

h(T1 ) = 1

B

c

h(T2 ) = 2

Feature Vectors
• A set of functions h1 . . . hd deﬁne a feature vector
Φ(x) = h1 (x), h2 (x) . . . hd (x)

T1

T2

A

A

C

B

C

B

D

E

F

G

D

E

F

d

e

f

g

d

e

h

A
C

b

Φ(T1 ) = 1, 0, 0, 3

B

c

Φ(T2 ) = 2, 0, 1, 1

Component 3: W
• W is a parameter vector ∈ Rd
• Φ and W together map a candidate to a real-valued score
S

NP

VP

She
announced

NP

NP
a

VP

program
to

VP

promote

NP

PP

safety

in

NP

NP

⇓Φ

trucks

and

NP
vans

1, 0, 2, 0, 0, 15, 5

⇓Φ·W
1, 0, 2, 0, 0, 15, 5 · 1.9, −0.3, 0.2, 1.3, 0, 1.0, −2.3 = 5.8

Putting it all Together
• X is set of sentences, Y is set of possible outputs (e.g. trees)
• Need to learn a function F : X → Y
• GEN, Φ, W deﬁne

F(x) = arg max

y∈GEN(x)

Φ(x, y) · W

Choose the highest scoring tree as the most plausible structure
• Given examples (xi , yi ), how to set W?

She announced a program to promote safety in trucks and vans

⇓ GEN
S

S

S

S

VP

NP

NP

She
NP

S

NP

VP

She

VP

NP
NP

announced
announced

S

VP

She

She

VP

NP
announced

VP

She

She

NP

NP
announced

VP

NP
a

NP

VP

NP
program

announced
to

promote

NP

NP
a

NP
safety

a

VP

announced

program

program

to

promote

NP

NP

in

in

NP

a

program

trucks
to

NP

trucks

and

to

promote

trucks

NP

and

vans

and

NP
and

NP
safety

VP

NP

a
NP

promote

PP

NP
safety

in

NP
trucks

PP
in

program
to

NP
safety

VP

NP

trucks

promote

vans

NP

NP
vans

PP
in

program
to

and

NP

vans

vans

a

promote

safety
and

NP

vans
NP

NP

VP

NP

safety

PP

in

PP

NP

PP

NP
trucks

⇓Φ
1, 1, 3, 5

⇓Φ
2, 0, 0, 5

⇓Φ

⇓Φ

1, 0, 1, 5

⇓Φ

0, 0, 3, 0

0, 1, 0, 5

⇓Φ
0, 0, 1, 5

⇓Φ·W

⇓Φ·W

⇓Φ·W

⇓Φ·W ⇓Φ·W

⇓Φ·W

13.6

12.6

12.1

3.3

11.1

9.4

⇓ arg max
S

NP

VP

She
announced

NP

VP

NP
a

program
to

VP

promote

NP

PP

safety

in

NP

NP
trucks

and

NP
vans

Examples of Global Linear Models
• Parse Reranking, e.g., [Ratnaparkhi, Reynar and Roukos, 1994],
[Johnson et. al, 1999], [Collins 2000], [Riezler et. al, 2004], [Shen, Sarkar
and Joshi, 2003], [Charniak and Johnson, 2005]

• Conditional random ﬁelds for tagging problems
[Lafferty, McCallum, and Pereira, 2001; Sha and Pereira, 2003]

• Speech recognition: estimating a discriminative n-gram model
[Roark, Saraclar and Collins, 2004]

• Dependency parsing [McDonald, Pereira, Ribarov and Hajic, 2005]
• Reranking for machine translation [Shen and Joshi, 2005; Shen,
Sarkar and Och, 2004]

• Alignments in MT [Taskar, Lacoste-Julien, and Klein, 2005]

Overview
• Margins, and the large margin solution
• An SVM algorithm
• Local feature vectors
(what to do when GEN is large...)
• Justiﬁcation for the algorithm
• Conclusions

Margins

• Given parameter values W, the margin on parse y
for i’th training example is
Mi,y = Φ(xi , yi ) · W − Φ(xi , y) · W
This is the difference in score between the correct
parse, and parse y

She announced a program to promote safety in trucks and vans

⇓ GEN
S

S

S

S

VP

NP

NP

She
NP

S

NP

VP

She

VP

NP
NP

announced
announced

S

VP

She

She

VP

NP
announced

VP

She

She

NP

NP
announced

VP

NP
a

NP

VP

NP
program

announced
to

promote

NP

NP
a

NP
safety

a

VP

program

announced

program
to

promote

NP

NP

in

in

NP

a

program

trucks
to

NP

trucks

and

to

promote

trucks

NP

and

vans

and

NP
and

NP
safety

VP

NP

a
NP

promote

PP

NP
safety

in

NP
trucks

PP
in

program
to

NP
safety

VP

NP

trucks

promote

vans

NP

NP
vans

PP
in

program
to

and

NP

vans

vans

a

promote

safety
and

NP

vans
NP

NP

VP

NP

safety

PP

in

PP

NP

PP

NP
trucks

⇓Φ

⇓Φ

⇓Φ

⇓Φ

⇓Φ

⇓Φ·W

⇓Φ·W

⇓Φ·W

⇓Φ·W ⇓Φ·W

⇓Φ·W

13.6

12.6

12.1

3.3

11.1

9.4

⇓Φ

Margins (assuming ﬁrst parse is correct):
—

1.0

1.5

10.3

4.2

2.5

She announced a program to promote safety in trucks and vans

⇓ GEN
S

S

S

S

VP

NP

NP

She
NP

S

NP

VP

She

VP

NP
NP

announced
announced

S

VP

She

She

VP

NP
announced

VP

She

She

NP

NP
announced

VP

NP
a

NP

VP

NP
program

announced
to

promote

NP

NP
a

NP
safety

a

VP

program

announced

program
to

promote

NP

NP

in

in

NP

a

program

trucks
to

NP

trucks

and

to

promote

trucks

NP

and

vans

and

NP
and

NP
safety

VP

NP

a
NP

promote

PP

NP
safety

in

NP
trucks

PP
in

program
to

NP
safety

VP

NP

trucks

promote

vans

NP

NP
vans

PP
in

program
to

and

NP

vans

vans

a

promote

safety
and

NP

vans
NP

NP

VP

NP

safety

PP

in

PP

NP

PP

NP
trucks

⇓Φ

⇓Φ

⇓Φ

⇓Φ

⇓Φ

⇓Φ·W

⇓Φ·W

⇓Φ·W

⇓Φ·W ⇓Φ·W

⇓Φ·W

13.6

14.8

12.1

3.3

11.1

9.4

⇓Φ

Margins (assuming ﬁrst parse is correct):
—

-1.2

1.5

10.3

4.2

2.5

Support Vector Machines: The Large Margin Solution

Minimize
||W||2
under the constraints
∀i, ∀y = yi , Mi,y ≥ 1

(Note: a solution doesn’t always exist)

W

2

=

W2
j
j

Support Vector Machines: The Large Margin Solution

Minimize
||W||2
under the constraints
∀i, ∀y = yi , Mi,y ≥ 1
Statistical justiﬁcation:
• Assume there is a distribution P (x, y) underlying training and test
examples
W
n

2

is small, with high probability W will have low error rate w.r.t.
• If
P (x, y)

Overview
• Margins, and the large margin solution
• An SVM algorithm
• Local feature vectors
(what to do when GEN is large...)
• Justiﬁcation for the algorithm
• Conclusions

Training an SVM: Dual Variables
• For the perceptron, SVMs, and conditional random ﬁelds, the
ﬁnal parameter values can be expressed as:
W=

αi,y [Φ(xi , yi ) − Φ(xi , y)]
i,y

where αi,y are dual variables

She announced a program to promote safety in trucks and vans

⇓ GEN
S

S

S

S

VP

NP

NP

She
NP

S

NP

VP

She

VP

NP
NP

announced
announced

S

VP

She

She

VP

NP
announced

VP

She

She

NP

NP
announced

VP

NP
a

NP

VP

NP
program

announced
to

promote

NP

NP
a

NP
safety

a

VP

program

announced

program
to

promote

NP

NP

in

in

NP

a

program

trucks
to

NP

trucks

and

to

promote

trucks

NP

and

vans

and

NP
and

NP
safety

VP

NP

a
NP

promote

PP

NP
safety

in

NP
trucks

PP
in

program
to

NP
safety

VP

NP

trucks

promote

vans

NP

NP
vans

PP
in

program
to

and

NP

vans

vans

a

promote

safety
and

NP

vans
NP

NP

VP

NP

safety

PP

in

PP

NP

PP

NP
trucks

⇓Φ
Φ1

⇓Φ
Φ2

⇓Φ
Φ3

⇓Φ
Φ4

⇓Φ
Φ5

⇓Φ
Φ6

0.05

0.04

0.01

Dual variables αi,y :
0.1

0.3

0.5

Assuming ﬁrst parse is correct, contribution to W is
0.1[Φ1 − Φ1 ] + 0.3[Φ1 − Φ2 ] + 0.6[Φ1 − Φ3 ] + . . .

Training an SVM
Inputs:

Training set (xi , yi ) for i = 1 . . . n

Initialization:

Set αi,y to initial values,
Calculate W =

Note: must have αi,y > 0,

y

αi,y = 1

i,y

αi,y [Φ(xi , yi ) − Φ(xi , y)]

Training an SVM: The Algorithm
(1) Calculate Margins:
∀i, y, Mi,y = Φ(xi , yi ) · W − Φ(xi , y) · W
(2) Update Dual Variables:

∀i, y, αi,y ← . . .
(More on this in a moment...)
(3) Update Parameters: W =

i,y

αi,y [Φ(xi , yi ) − Φ(xi , y)]

(4) If not converged, return to Step (1)

Updating the Dual Variables

∀i, y, αi,y ←

αi,y

η i,y
e

y αi,y

η i,y
e

where
i,y
i,y

=0
for y = yi
= 1 − Mi,y for y = yi

Intuition:
• if Mi,y > 1, αi,y decreases
• if Mi,y < 1, αi,y increases
• if Mi,y = 1, αi,y stays the same
• The learning rate η > 0 controls the magnitude of the updates

∀i, y, αi,y ←

αi,y eη i,y
αi,y eη i,y
y

where

Margins: Margins: ⇓ Φ
⇓Φ·W

i,y
i,y

=0
= 1 − Mi,y

for y = yi
for y = yi

⇓Φ

⇓Φ

⇓Φ

⇓Φ·W

⇓Φ·W

⇓Φ·W

13.6
Margins:

13.0

14.8

3.3

—

0.6

-1.2

10.3

αi,y eη i,y
αi,y eη i,y
y

∀i, y, αi,y ←

where

Margins: Margins: ⇓ Φ
⇓Φ·W

i,y
i,y

=0
= 1 − Mi,y

for y = yi
for y = yi

⇓Φ

⇓Φ

⇓Φ

⇓Φ·W

⇓Φ·W

⇓Φ·W

13.6

Values for
Values for eη
(with η = 1)

i,y

:

3.3

0.6

-1.2

10.3

0.0

i,y :

14.8

—

Margins:

13.0

0.4

2.2

-9.3

1.0

1.49

9.03

0.00001

αi,y eη i,y
αi,y eη i,y
y

∀i, y, αi,y ←

where

Margins: Margins: ⇓ Φ
⇓Φ·W

i,y
i,y

=0
= 1 − Mi,y

for y = yi
for y = yi

⇓Φ

⇓Φ

⇓Φ

⇓Φ·W

⇓Φ·W

⇓Φ·W

13.6

13.0

14.8

3.3

—

0.6

-1.2

10.3

0.0

0.4

2.2

-9.3

1.0

1.49

9.03

0.00001

Old dual values αi,y : 0.1

0.3

0.5

0.1

New dual values αi,y : 0.02

0.088

0.89

0.0

Margins:
Values for
Values for eη
(with η = 1)

i,y :
i,y

:

Training an SVM: The Algorithm
(1) Calculate Margins:
∀i, y, Mi,y = Φ(xi , yi ) · W − Φ(xi , y) · W
(2) Update Dual Variables:
∀i, y, αi,y ←

αi,y eη i,y
αi,y eη i,y
y

where
i,y
i,y

=0
= 1 − Mi,y

for y = yi
for y = yi

(3) Update Parameters: W =

i,y

αi,y [Φ(xi , yi ) − Φ(xi , y)]

(4) If not converged, return to Step (1)

Theory
• Algorithm converges to the minimum of

i

1
max (1 − Mi,y )+ + ||W||2
y
2

where
(1 − Mi,y )+ =

(1 − Mi,y ) if (1 − Mi,y ) > 0
0
otherwise

This is the hinge loss: penalizes values for Mi,y that are < 1
Note, as before:
Mi,y = Φ(xi , yi ) · W − Φ(xi , y) · W

She announced a program to promote safety in trucks and vans

⇓ GEN
S

S

S

S

VP

NP

NP

She
NP

S

NP

VP

She

VP

NP
NP

announced
announced

S

VP

She

She

VP

NP
announced

VP

She

She

NP

NP
announced

VP

NP
a

NP

VP

NP
program

announced
to

promote

NP

NP
a

NP
safety

a

VP

program

announced

program
to

promote

NP

NP

in

in

NP

a

program

trucks
to

NP

trucks

and

to

promote

trucks

NP

and

vans

and

NP
and

NP
safety

VP

NP

a
NP

promote

PP

NP
safety

in

NP
trucks

PP
in

program
to

NP
safety

VP

NP

trucks

promote

vans

NP

NP
vans

PP
in

program
to

and

NP

vans

vans

a

promote

safety
and

NP

vans
NP

NP

VP

NP

safety

PP

in

PP

NP

PP

NP
trucks

⇓Φ

⇓Φ

⇓Φ

⇓Φ

⇓Φ

⇓Φ·W

⇓Φ·W

⇓Φ·W

⇓Φ·W ⇓Φ·W

⇓Φ·W

13.6

12.6

12.1

3.3

11.1

9.4

⇓Φ

Margins (assuming ﬁrst parse is correct):
—

1.0

1.5

10.3

4.2

In this case maxy (1 − Mi,y )+ = 0

2.5

She announced a program to promote safety in trucks and vans

⇓ GEN
S

S

S

S

VP

NP

NP

She
NP

S

NP

VP

She

VP

NP
NP

announced
announced

S

VP

She

She

VP

NP
announced

VP

She

She

NP

NP
announced

VP

NP
a

NP

VP

NP
program

announced
to

promote

NP

NP
a

NP
safety

a

VP

program

announced

program
to

promote

NP

NP

in

in

NP

a

program

trucks
to

NP

trucks

and

to

promote

trucks

NP

and

vans

and

NP
and

NP
safety

VP

NP

a
NP

promote

PP

NP
safety

in

NP
trucks

PP
in

program
to

NP
safety

VP

NP

trucks

promote

vans

NP

NP
vans

PP
in

program
to

and

NP

vans

vans

a

promote

safety
and

NP

vans
NP

NP

VP

NP

safety

PP

in

PP

NP

PP

NP
trucks

⇓Φ

⇓Φ

⇓Φ

⇓Φ

⇓Φ

⇓Φ·W

⇓Φ·W

⇓Φ·W

⇓Φ·W ⇓Φ·W

⇓Φ·W

13.6

12.2

12.1

3.3

11.1

9.4

⇓Φ

Margins (assuming ﬁrst parse is correct):
—

1.4

1.5

10.3

4.2

In this case maxy (1 − Mi,y )+ = 0

2.5

She announced a program to promote safety in trucks and vans

⇓ GEN
S

S

S

S

VP

NP

NP

She
NP

S

NP

VP

She

VP

NP
NP

announced
announced

S

VP

She

She

VP

NP
announced

VP

She

She

NP

NP
announced

VP

NP
a

NP

VP

NP
program

announced
to

promote

NP

NP
a

NP
safety

a

VP

program

announced

program
to

promote

NP

NP

in

in

NP

a

program

trucks
to

NP

trucks

and

to

promote

trucks

NP

and

vans

and

NP
and

NP
safety

VP

NP

a
NP

promote

PP

NP
safety

in

NP
trucks

PP
in

program
to

NP
safety

VP

NP

trucks

promote

vans

NP

NP
vans

PP
in

program
to

and

NP

vans

vans

a

promote

safety
and

NP

vans
NP

NP

VP

NP

safety

PP

in

PP

NP

PP

NP
trucks

⇓Φ

⇓Φ

⇓Φ

⇓Φ

⇓Φ

⇓Φ·W

⇓Φ·W

⇓Φ·W

⇓Φ·W ⇓Φ·W

⇓Φ·W

13.6

13.0

12.1

3.3

11.1

9.4

⇓Φ

Margins (assuming ﬁrst parse is correct):
—

0.6

1.5

10.3

4.2

In this case maxy (1 − Mi,y )+ = 0.4

2.5

She announced a program to promote safety in trucks and vans

⇓ GEN
S

S

S

S

VP

NP

NP

She
NP

S

NP

VP

She

VP

NP
NP

announced
announced

S

VP

She

She

VP

NP
announced

VP

She

She

NP

NP
announced

VP

NP
a

NP

VP

NP
program

announced
to

promote

NP

NP
a

NP
safety

a

VP

program

announced

program
to

promote

NP

NP

in

in

NP

a

program

trucks
to

NP

trucks

and

to

promote

trucks

NP

and

vans

and

NP
and

NP
safety

VP

NP

NP

PP

NP
in

NP
trucks

PP
in

promote

safety

NP
safety

program
to

trucks

promote

VP

NP
a

to

vans

NP

NP
vans

PP
in

program

and

NP

vans

vans

a

promote

safety
and

NP

vans
NP

NP

VP

NP

safety

PP

in

PP

NP

PP

NP
trucks

⇓Φ

⇓Φ

⇓Φ

⇓Φ

⇓Φ

⇓Φ·W

⇓Φ·W

⇓Φ·W

⇓Φ·W ⇓Φ·W

⇓Φ·W

13.6

13.0

14.8

3.3

11.1

9.4

⇓Φ

Margins (assuming ﬁrst parse is correct):
—

0.6

-1.2

10.3

4.2

In this case maxy (1 − Mi,y )+ = 2.2

2.5

Theory
• Algorithm converges to the minimum of
max (1 − Mi,y )+
i

y

Penalizes margins less than 1

+
i

1
||W||2
2

i

Penalizes large parameter values

Theory
• Algorithm converges to the minimum of
max (1 − Mi,y )+
i

y

+
i

Penalizes margins less than 1

1
||W||2
2

Penalizes large parameter values

• Note: it’s trivial to modify the algorithm to minimize
C
i

i

1
max (1 − Mi,y )+ + ||W||2
y
2

for some C > 0
• As C → ∞ we get closer to the large margin solution

Optimizing Other Loss Functions
• Suppose for each incorrect parse tree, we have a “loss”
Li,y
E.g., Li,y is number of parsing errors in y for sentence xi
• New updates:
∀i, y, αi,y =

αi,y eη(Li,y −Mi,y )
η(Li,y −Mi,y )
y αi,y e

• Algorithm converges to the minimum of

i

1
max (Li,y − Mi,y )+ + ||W||2
y
2

(Loss function from [Taskar, Guestrin, Koller, 2003])

She announced a program to promote safety in trucks and vans

⇓ GEN
S

S

S

S

VP

NP

NP

She
NP

S

NP

VP

She

VP

NP
NP

announced
announced

S

VP

She

She

VP

NP
announced

VP

She

She

NP

NP
announced

VP

NP
a

NP

VP

NP
program

announced
to

promote

NP

NP
a

NP
safety

a

VP

program

announced

program
to

promote

NP

NP

in

in

NP

a

program

trucks
to

NP

trucks

and

to

promote

trucks

NP

and

vans

and

NP
and

NP
safety

VP

NP

a
NP

promote

PP

NP
safety

in

NP
trucks

PP
in

program
to

NP
safety

VP

NP

trucks

promote

vans

NP

NP
vans

PP
in

program
to

and

NP

vans

vans

a

promote

safety
and

NP

vans
NP

NP

VP

NP

safety

PP

in

PP

NP

PP

NP
trucks

⇓Φ

⇓Φ

⇓Φ

⇓Φ

⇓Φ

⇓Φ·W

⇓Φ·W

⇓Φ·W

⇓Φ·W ⇓Φ·W

⇓Φ·W

13.6

13.0

14.8

3.3

11.1

9.4

⇓Φ

Margins (assuming ﬁrst parse is correct):
—
0.6
-1.2
10.3
4.2

2.5

Values for Li,y :
0
5.0

2.5

1.0

2.3

1.7

In this case maxy (Li,y − Mi,y )+ = 4.4

Accuracy on a Parse Reranking Task
94.8

94.5

Accuracy

94.2

93.9
"Max-Margin"
"Boosting"
"Baseline"
93.6

93.3

93.0

92.7
0

20

40

60

80

Iterations

• ≈ 36, 000 training examples, 1 million trees total

100

Overview
• Margins, and the large margin solution
• An SVM algorithm
• Local feature vectors
(what to do when GEN is large...)
• Justiﬁcation for the algorithm
• Conclusions

Local Representations: What to do when GEN is large
• Suppose GEN(x) is all parses for x under a context-free
grammar
• We now have an exponential number of parses
• We have an exponential number of dual variables αi,y , margins
Mi,y , feature vectors Φ(xi , y), error terms Li,y etc.

Local Representations
A tree:
S
NP

VP

D

N

V

the

man

saw

NP

VP,
N,
NP,
N,

N

the

Its context-free productions:
S → NP
NP → D
VP → V
NP → D

D

dog

1, 2, 5
1, 1, 2
3, 3, 5
4, 4, 5

A part is a rule, start-point, mid-point, end-point tuple

Assumption 1: Local Feature-Vector Representations
• If x is a sentence, r is a part, then
φ(x, r)
is a local feature-vector
• For any parse tree y, we deﬁne
Φ(x, y) =

φ(x, r)
r∈y

Local Feature Vectors
(x, y) =

S
VP

NP
D

N

V

the

man

saw

NP
D

N

the

dog

Φ(x, y) =
+φ(the man saw the dog,
+φ(the man saw the dog,
+φ(the man saw the dog,
+φ(the man saw the dog,

S → NP
NP → D
VP → V
NP → D

VP, 1, 2, 5 )
N, 1, 1, 2 )
NP, 3, 3, 5 )
N, 4, 4, 5 )

Can ﬁnd arg maxy W · Φ(x, y) using CKY

Assumption 2: Local Error Functions
• For any example i, assume li,r is “cost” of proposing rule r in
parse tree for xi
• For example: li,r = 1 if rule r is not in the correct parse yi , 0
otherwise
• Deﬁne
Li,y =

li,r
r∈y

Local Error Functions
(xi , y) =

S
VP

NP
D

N

V

the

man

saw

NP

S → NP
NP → D
VP → V
NP → D

N

the

Li,y =
+l(i,
+l(i,
+l(i,
+l(i,

D

dog

VP, 1, 2, 5 )
N, 1, 1, 2 )
NP, 3, 3, 5 )
N, 4, 4, 5 )

The EG Algorithm under Local Assumptions
• The updates:
∀i, y, αi,y =

αi,y eη(Li,y −Mi,y )
η(Li,y −Mi,y )
y αi,y e

• But now, we have
Li,y − Mi,y =

(li,r + W · φi,r ) − W · Φ(xi , yi )
r∈y

• We can represent αi,y variables compactly:
αi,y =

e
ye

r∈y

θi,r

r∈y

θi,r

• The updates are implemented as θi,r ← θi,r + η(li,r + W · φi,r )

Local Dual Variables
(xi , y) =

S

Si,y

VP

NP
D

N

V

the

man

saw

αi,y

NP

S → NP
NP → D
VP → V
NP → D

N

the

Si,y =
+θ(i,
+θ(i,
+θ(i,
+θ(i,

D

e
=
Z

dog

VP, 1, 2, 5 )
N, 1, 1, 2 )
NP, 3, 3, 5 )
N, 4, 4, 5 )

There are an exponential number of αi,y variables,
but there are a polynomial number of θ(i, rule) variables

Overview
• Margins, and the large margin solution
• An SVM algorithm
• Local feature vectors
(what to do when GEN is large...)
• Justiﬁcation for the algorithm
• Conclusions

How did we Derive the Algorithm?
• We want to ﬁnd the W that minimizes:
i

max (1 − Mi,y )+
y

Penalizes margins less than 1

+
i

1
||W||2
2

i

Penalizes large parameter values

The Dual Optimization Problem for the SVM
Choose αi,y values to maximize
1
Q(¯ ) =
α
αi,y − W
2
i,y=yi

2

where
W=

αi,y [Φ(xi , yi ) − Φ(xi , y)]
i,y

Constraints:
∀i, ∀y, αi,y ≥ 0
∀i,

αi,y = 1
y

The Dual Optimization Problem for the SVM
• We want to maximize Q(¯ )
α
• It can be shown that
dQ(¯ )
α
=
dαi,y

where
i,y

i,y
i,y

=0
= 1 − Mi,y

for y = yi
for y = yi

• Gradient ascent:
αi,y ← αi,y + η

i,y

• Exponentiated Gradient:
αi,y ←

αi,y eη i,y
η i,y
y αi,y e

(Motivation: αi,y ’s remain positive,

y

αi,y = 1)

• The exponentiated gradient method is an example of
multiplicative updates:
central to AdaBoost (Freund
and Schapire), online learning algorithms such as
Winnow (Warmuth), several applications to combinatorial
optimization, linear programming, problems in game theory,
etc. etc. (survey article by Arora, Hazan and Kale)
• Analysis of the algorithm builds on work by Warmuth and
collaborators in online learning

Convergence on a Parse Reranking Task
5.5
"Dual"
"Primal"
5

4.5

Objective

4

3.5

3

2.5

2

1.5

1
0

10

20

30

40

50

60

70

80

90

Iterations

• ≈ 36, 000 training examples, 1 million trees total
• ≈ 500, 000 sparse features

100

Overview
• Margins, and the large margin solution
• An SVM algorithm
• Local feature vectors
(what to do when GEN is large...)
• Justiﬁcation for the algorithm
• Conclusions

Contributions
• A simple algorithm for ﬁnding the SVM solution
• Relies on close connections between margins, dual variables,
dual problem for the SVM
• Experiments show good performance on reranking tasks
• The algorithm has a convenient compact form for context-free
grammars with local feature–vectors

