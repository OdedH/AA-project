Spectral Learning of Latent-Variable PCFGs
Shay B. Cohen1 , Karl Stratos1 , Michael Collins1 , Dean P. Foster2 , and Lyle Ungar3
1
Dept. of Computer Science, Columbia University
2
3
Dept. of Statistics/ Dept. of Computer and Information Science, University of Pennsylvania
{scohen,stratos,mcollins}@cs.columbia.edu, foster@wharton.upenn.edu, ungar@cis.upenn.edu

Abstract
We introduce a spectral learning algorithm for
latent-variable PCFGs (Petrov et al., 2006).
Under a separability (singular value) condition, we prove that the method provides consistent parameter estimates.

1 Introduction
Statistical models with hidden or latent variables are
of great importance in natural language processing,
speech, and many other ﬁelds. The EM algorithm is
a remarkably successful method for parameter estimation within these models: it is simple, it is often
relatively efﬁcient, and it has well understood formal
properties. It does, however, have a major limitation:
it has no guarantee of ﬁnding the global optimum of
the likelihood function. From a theoretical perspective, this means that the EM algorithm is not guaranteed to give consistent parameter estimates. From
a practical perspective, problems with local optima
can be difﬁcult to deal with.
Recent work has introduced polynomial-time
learning algorithms (and consistent estimation methods) for two important cases of hidden-variable
models: Gaussian mixture models (Dasgupta, 1999;
Vempala and Wang, 2004) and hidden Markov models (Hsu et al., 2009). These algorithms use spectral methods: that is, algorithms based on eigenvector decompositions of linear systems, in particular singular value decomposition (SVD). In the general case, learning of HMMs or GMMs is intractable
(e.g., see Terwijn, 2002). Spectral methods ﬁnesse
the problem of intractibility by assuming separability conditions. For example, the algorithm of Hsu
et al. (2009) has a sample complexity that is polynomial in 1/σ, where σ is the minimum singular value
of an underlying decomposition. These methods are
not susceptible to problems with local maxima, and
give consistent parameter estimates.
In this paper we derive a spectral algorithm
for learning of latent-variable PCFGs (L-PCFGs)
(Petrov et al., 2006; Matsuzaki et al., 2005). Our

method involves a signiﬁcant extension of the techniques from Hsu et al. (2009). L-PCFGs have been
shown to be a very effective model for natural language parsing. Under a separation (singular value)
condition, our algorithm provides consistent parameter estimates; this is in contrast with previous work,
which has used the EM algorithm for parameter estimation, with the usual problems of local optima.
The parameter estimation algorithm (see ﬁgure 4)
is simple and efﬁcient. The ﬁrst step is to take
an SVD of the training examples, followed by a
projection of the training examples down to a lowdimensional space. In a second step, empirical averages are calculated on the training example, followed by standard matrix operations. On test examples, simple (tensor-based) variants of the insideoutside algorithm (ﬁgures 2 and 3) can be used to
calculate probabilities and marginals of interest.
Our method depends on the following results:
• Tensor form of the inside-outside algorithm.
Section 5 shows that the inside-outside algorithm for
L-PCFGs can be written using tensors. Theorem 1
gives conditions under which the tensor form calculates inside and outside terms correctly.
• Observable representations. Section 6 shows
that under a singular-value condition, there is an observable form for the tensors required by the insideoutside algorithm. By an observable form, we follow the terminology of Hsu et al. (2009) in referring
to quantities that can be estimated directly from data
where values for latent variables are unobserved.
Theorem 2 shows that tensors derived from the observable form satisfy the conditions of theorem 1.
• Estimating the model. Section 7 gives an algorithm for estimating parameters of the observable
representation from training data. Theorem 3 gives a
sample complexity result, showing that the estimates
√
converge to the true distribution at a rate of 1/ M
where M is the number of training examples.
The algorithm is strikingly different from the EM
algorithm for L-PCFGs, both in its basic form, and
in its consistency guarantees. The techniques de-

veloped in this paper are quite general, and should
be relevant to the development of spectral methods
for estimation in other models in NLP, for example alignment models for translation, synchronous
PCFGs, and so on. The tensor form of the insideoutside algorithm gives a new view of basic calculations in PCFGs, and may itself lead to new models.

2 Related Work
For work on L-PCFGs using the EM algorithm, see
Petrov et al. (2006), Matsuzaki et al. (2005), Pereira
and Schabes (1992). Our work builds on methods for learning of HMMs (Hsu et al., 2009; Foster et al., 2012; Jaeger, 2000), but involves several extensions: in particular in the tensor form of
the inside-outside algorithm, and observable representations for the tensor form. Balle et al. (2011)
consider spectral learning of ﬁnite-state transducers;
Lugue et al. (2012) considers spectral learning of
head automata for dependency parsing. Parikh et al.
(2011) consider spectral learning algorithms of treestructured directed bayes nets.

3 Notation

4 L-PCFGs: Basic Deﬁnitions
This section gives a deﬁnition of the L-PCFG formalism used in this paper. An L-PCFG is a 5-tuple
(N , I, P, m, n) where:
• N is the set of non-terminal symbols in the
grammar. I ⊂ N is a ﬁnite set of in-terminals.
P ⊂ N is a ﬁnite set of pre-terminals. We assume
that N = I ∪ P, and I ∩ P = ∅. Hence we have
partitioned the set of non-terminals into two subsets.
• [m] is the set of possible hidden states.
• [n] is the set of possible words.
• For all a ∈ I, b ∈ N , c ∈ N , h1 , h2 , h3 ∈ [m],
we have a context-free rule a(h1 ) → b(h2 ) c(h3 ).
• For all a ∈ P, h ∈ [m], x ∈ [n], we have a
context-free rule a(h) → x.
Hence each in-terminal a ∈ I is always the lefthand-side of a binary rule a → b c; and each preterminal a ∈ P is always the left-hand-side of a
rule a → x. Assuming that the non-terminals in
the grammar can be partitioned this way is relatively
benign, and makes the estimation problem cleaner.
We deﬁne the set of possible “skeletal rules” as
R = {a → b c : a ∈ I, b ∈ N , c ∈ N }. The
parameters of the model are as follows:
• For each a → b c ∈ R, and h ∈ [m], we have
a parameter q(a → b c|h, a). For each a ∈ P,
x ∈ [n], and h ∈ [m], we have a parameter
q(a → x|h, a). For each a → b c ∈ R, and
h, h′ ∈ [m], we have parameters s(h′ |h, a → b c)
and t(h′ |h, a → b c).
These deﬁnitions give a PCFG, with rule probabilities

Given a matrix A or a vector v, we write A⊤ or v ⊤
for the associated transpose. For any integer n ≥ 1,
we use [n] to denote the set {1, 2, . . . n}. For any
row or column vector y ∈ Rm , we use diag(y) to
refer to the (m × m) matrix with diagonal elements
equal to yh for h = 1 . . . m, and off-diagonal elements equal to 0. For any statement Γ, we use [[Γ]]
to refer to the indicator function that is 1 if Γ is true,
and 0 if Γ is false. For a random variable X, we use
E[X] to denote its expected value.
p(a(h1 ) → b(h2 ) c(h3 )|a(h1 )) =
We will make (quite limited) use of tensors:
Deﬁnition 1 A tensor C ∈ R(m×m×m) is a set of
m3 parameters Ci,j,k for i, j, k ∈ [m]. Given a tensor C, and a vector y ∈ Rm , we deﬁne C(y) to be
the (m × m) matrix with components [C(y)]i,j =
k∈[m] Ci,j,k yk . Hence C can be interpreted as a
function C : Rm → R(m×m) that maps a vector
y ∈ Rm to a matrix C(y) of dimension (m × m).
In addition, we deﬁne the tensor C∗ ∈ R(m×m×m)
for any tensor C ∈ R(m×m×m) to have values
[C∗ ]i,j,k = Ck,j,i
Finally, for vectors x, y, z ∈ Rm , xy ⊤ z ⊤ is the
tensor D ∈ Rm×m×m where Dj,k,l = xj yk zl (this
is analogous to the outer product: [xy ⊤ ]j,k = xj yk ).

q(a → b c|h1 , a) × s(h2 |h1 , a → b c) × t(h3 |h1 , a → b c)

and p(a(h) → x|a(h)) = q(a → x|h, a).
In addition, for each a ∈ I, for each h ∈ [m], we
have a parameter π(a, h) which is the probability of
non-terminal a paired with hidden variable h being
at the root of the tree.
An L-PCFG deﬁnes a distribution over parse trees
as follows. A skeletal tree (s-tree) is a sequence of
rules r1 . . . rN where each ri is either of the form
a → b c or a → x. The rule sequence forms
a top-down, left-most derivation under a CFG with
skeletal rules. See ﬁgure 1 for an example.
A full tree consists of an s-tree r1 . . . rN , together
with values h1 . . . hN . Each hi is the value for

S1
NP2

VP5

D3

N4

V6

P7

the

dog

saw

him

r1
r2
r3
r4
r5
r6
r7

= S → NP VP
= NP → D N
= D → the
= N → dog
= VP → V P
= V → saw
= P → him

Inputs: s-tree r1 . . . rN , L-PCFG (N , I, P, m, n), parameters
• C a→b c ∈ R(m×m×m) for all a → b c ∈ R
• c∞ ∈ R(1×m) for all a ∈ P, x ∈ [n]
a→x
• c1 ∈ R(m×1) for all a ∈ I.
a
Algorithm: (calculate the f i terms bottom-up in the tree)
• For all i ∈ [N ] such that ai ∈ P, f i = c∞
ri

Figure 1: An s-tree, and its sequence of rules. (For convenience we have numbered the nodes in the tree.)

the hidden variable for the left-hand-side of rule ri .
Each hi can take any value in [m].
Deﬁne ai to be the non-terminal on the left-handside of rule ri . For any i ∈ {2 . . . N } deﬁne pa(i)
to be the index of the rule above node i in the tree.
Deﬁne L ⊂ [N ] to be the set of nodes in the tree
which are the left-child of some parent, and R ⊂
[N ] to be the set of nodes which are the right-child of
some parent. The probability mass function (PMF)
over full trees is then
p(r1 . . . rN , h1 . . . hN ) = π(a1 , h1 )
N

×
×

i=1

i∈R

q(ri |hi , ai ) ×

i∈L

s(hi |hpa(i) , rpa(i) )

t(hi |hpa(i) , rpa(i) )

(1)

The PMF over s-trees is p(r1 . . . rN ) =
h1 ...hN p(r1 . . . rN , h1 . . . hN ).
In the remainder of this paper, we make use of matrix form of parameters of an L-PCFG, as follows:
• For each a → b c ∈ R, we deﬁne Qa→b c ∈
m×m to be the matrix with values q(a → b c|h, a)
R
for h = 1, 2, . . . m on its diagonal, and 0 values for
its off-diagonal elements. Similarly, for each a ∈ P,
x ∈ [n], we deﬁne Qa→x ∈ Rm×m to be the matrix
with values q(a → x|h, a) for h = 1, 2, . . . m on its
diagonal, and 0 values for its off-diagonal elements.
• For each a → b c ∈ R, we deﬁne S a→b c ∈
Rm×m where [S a→b c ]h′ ,h = s(h′ |h, a → b c).
• For each a → b c ∈ R, we deﬁne T a→b c ∈
Rm×m where [T a→b c ]h′ ,h = t(h′ |h, a → b c).
• For each a ∈ I, we deﬁne the vector π a ∈ Rm
where [π a ]h = π(a, h).

• For all i ∈ [N ] such that ai ∈ I, f i = f γ C ri (f β ) where
β is the index of the left child of node i in the tree, and γ
is the index of the right child.
Return: f 1 c1 1 = p(r1 . . . rN )
a

Figure 2: The tensor form for calculation of p(r1 . . . rN ).

1. For a given
p(r1 . . . rN ).

s-tree

r1 . . . rN ,

calculate

2. For a given input sentence x = x1 . . . xN , calculate the marginal probabilities
µ(a, i, j) =

p(τ )
τ ∈T (x):(a,i,j)∈τ

for each non-terminal a ∈ N , for each (i, j)
such that 1 ≤ i ≤ j ≤ N .
Here T (x) denotes the set of all possible s-trees for
the sentence x, and we write (a, i, j) ∈ τ if nonterminal a spans words xi . . . xj in the parse tree τ .
The marginal probabilities have a number of uses.
Perhaps most importantly, for a given sentence x =
x1 . . . xN , the parsing algorithm of Goodman (1996)
can be used to ﬁnd
arg max

τ ∈T (x)

µ(a, i, j)
(a,i,j)∈τ

This is the parsing algorithm used by Petrov et al.
(2006), for example. In addition, we can calculate the probability for an input sentence, p(x) =
τ ∈T (x) p(τ ), as p(x) =
a∈I µ(a, 1, N ).
Variants of the inside-outside algorithm can be
used for problems 1 and 2. This section introduces a
novel form of these algorithms, using tensors. This
is the ﬁrst step in deriving the spectral estimation
method.
The algorithms are shown in ﬁgures 2 and 3. Each
algorithm takes the following inputs:

5 Tensor Form of the Inside-Outside
Algorithm

1. A tensor C a→b c ∈ R(m×m×m) for each rule
a → b c.

Given an L-PCFG, two calculations are central:

2. A vector c∞ ∈ R(1×m) for each rule a → x.
a→x

3. A vector c1 ∈ R(m×1) for each a ∈ I.
a
The following theorem gives conditions under
which the algorithms are correct:
Theorem 1 Assume that we have an L-PCFG with
parameters Qa→x , Qa→b c , T a→b c , S a→b c , π a , and
that there exist matrices Ga ∈ R(m×m) for all a ∈
N such that each Ga is invertible, and such that:

1. For all rules a → b c, C a→b c (y)
=
c T a→b c diag(yGb S a→b c )Qa→b c (Ga )−1
G
2. For all rules a → x,

c∞
a→x

=

1⊤ Qa→x (Ga )−1

3. For all a ∈ I, c1 = Ga π a
a
Then: 1) The algorithm in ﬁgure 2 correctly computes p(r1 . . . rN ) under the L-PCFG. 2) The algorithm in ﬁgure 3 correctly computes the marginals
µ(a, i, j) under the L-PCFG.

Inputs: Sentence x1 . . . xN , L-PCFG (N , I, P, m, n), parameters C a→b c ∈ R(m×m×m) for all a → b c ∈ R, c∞ ∈
a→x
R(1×m) for all a ∈ P, x ∈ [n], c1 ∈ R(m×1) for all a ∈ I.
a
Data structures:
• Each αa,i,j ∈ R1×m for a ∈ N , 1 ≤ i ≤ j ≤ N is a
row vector of inside terms.
• Each β a,i,j ∈ Rm×1 for a ∈ N , 1 ≤ i ≤ j ≤ N is a
column vector of outside terms.
• Each µ(a, i, j) ∈ R for a ∈ N , 1 ≤ i ≤ j ≤ N is a
marginal probability.
Algorithm:
(Inside base case) ∀a ∈ P, i ∈ [N ], αa,i,i = c∞ i
a→x
(Inside recursion) ∀a ∈ I, 1 ≤ i < j ≤ N,
j−1

αa,i,j =
k=i a→b c

(Outside base case) ∀a ∈ I, β a,1,n = c1
a
(Outside recursion) ∀a ∈ N , 1 ≤ i ≤ j ≤ N,
i−1

k=1 b→c a

6 Estimating the Tensor Model
A crucial result is that it is possible to directly estimate parameters C a→b c , c∞ and c1 that satisfy the
a→x
a
conditions in theorem 1, from a training sample consisting of s-trees (i.e., trees where hidden variables
are unobserved). We ﬁrst describe random variables
underlying the approach, then describe observable
representations based on these random variables.
Random Variables Underlying the Approach

Each s-tree with N rules r1 . . . rN has N nodes. We
will use the s-tree in ﬁgure 1 as a running example.
Each node has an associated rule: for example,
node 2 in the tree in ﬁgure 1 has the rule NP → D N.
If the rule at a node is of the form a → b c, then there
are left and right inside trees below the left child and
right child of the rule. For example, for node 2 we
have a left inside tree rooted at node 3, and a right
inside tree rooted at node 4 (in this case the left and
right inside trees both contain only a single rule production, of the form a → x; however in the general
case they might be arbitrary subtrees).
In addition, each node has an outside tree. For
node 2, the outside tree is
S
VP

NP

C b→c a (αc,k,i−1 )β b,k,j

β a,i,j =

Proof: See section 9.1.

6.1

αc,k+1,j C a→b c (αb,i,k )

V

P

saw

him

N
b→a c
C∗
(αc,j+1,k )β b,i,k

+
k=j+1 b→a c

(Marginals) ∀a ∈ N , 1 ≤ i ≤ j ≤ N,
a,i,j
αa,i,j βh
h

µ(a, i, j) = αa,i,j β a,i,j =
h∈[m]

Figure 3: The tensor form of the inside-outside algorithm,
for calculation of marginal terms µ(a, i, j).

The outside tree contains everything in the s-tree
r1 . . . rN , excluding the subtree below node i.
Our random variables are deﬁned as follows.
First, we select a random internal node, from a random tree, as follows:
• Sample an s-tree r1 . . . rN from the PMF
p(r1 . . . rN ). Choose a node i uniformly at random from [N ].
If the rule ri for the node i is of the form a → b c,
we deﬁne random variables as follows:
• R1 is equal to the rule ri (e.g., NP → D N).
• T1 is the inside tree rooted at node i. T2 is the
inside tree rooted at the left child of node i, and T3
is the inside tree rooted at the right child of node i.
• H1 , H2 , H3 are the hidden variables associated
with node i, the left child of node i, and the right
child of node i respectively.

• A1 , A2 , A3 are the labels for node i, the left
child of node i, and the right child of node i respectively. (E.g., A1 = NP, A2 = D, A3 = N.)
• O is the outside tree at node i.
• B is equal to 1 if node i is at the root of the tree
(i.e., i = 1), 0 otherwise.
If the rule ri for the selected node i is of
the form a → x, we have random variables R1 , T1 , H1 , A1 , O, B as deﬁned above, but
H2 , H3 , T2 , T3 , A2 , and A3 are not deﬁned.
We assume a function ψ that maps outside trees o
′
to feature vectors ψ(o) ∈ Rd . For example, the feature vector might track the rule directly above the
node in question, the word following the node in
question, and so on. We also assume a function φ
that maps inside trees t to feature vectors φ(t) ∈ Rd .
As one example, the function φ might be an indicator function tracking the rule production at the root
of the inside tree. Later we give formal criteria for
what makes good deﬁnitions of ψ(o) of φ(t). One
requirement is that d′ ≥ m and d ≥ m.
In tandem with these deﬁnitions, we assume pro′
jection matices U a ∈ R(d×m) and V a ∈ R(d ×m)
for all a ∈ N . We then deﬁne additional random
variables Y1 , Y2 , Y3 , Z as
Y1 = (U a1 )⊤ φ(T1 ) Z = (V a1 )⊤ ψ(O)
Y2 = (U a2 )⊤ φ(T2 ) Y3 = (U a3 )⊤ φ(T3 )
where ai is the value of the random variable Ai .
Note that Y1 , Y2 , Y3 , Z are all in Rm .
6.2

Observable Representations

Given the deﬁnitions in the previous section, our
representation is based on the following matrix, tensor and vector quantities, deﬁned for all a ∈ N , for
all rules of the form a → b c, and for all rules of the
form a → x respectively:
Σa = E[Y1 Z ⊤ |A1 = a]

D a→b c = E [[R1 = a → b c]]Y3 Z ⊤ Y2⊤ |A1 = a
⊤
d∞
a→x = E [[R1 = a → x]]Z |A1 = a

Assuming access to functions φ and ψ, and projection matrices U a and V a , these quantities can be estimated directly from training data consisting of a
set of s-trees (see section 7).

Our observable representation then consists of:
C a→b c (y) = D a→b c (y)(Σa )−1
c∞
a→x
c1
a

=

d∞ (Σa )−1
a→x

= E [[[A1 = a]]Y1 |B = 1]

(2)
(3)
(4)

We next introduce conditions under which these
quantities satisfy the conditions in theorem 1.
The following deﬁnition will be important:
Deﬁnition 2 For all a ∈ N , we deﬁne the matrices
′
I a ∈ R(d×m) and J a ∈ R(d ×m) as
[I a ]i,h = E[φi (T1 ) | H1 = h, A1 = a]
[J a ]i,h = E[ψi (O) | H1 = h, A1 = a]

In addition, for any a ∈ N , we use γ a ∈ Rm to
a
denote the vector with γh = P (H1 = h|A1 = a).
The correctness of the representation will rely on
the following conditions being satisﬁed (these are
parallel to conditions 1 and 2 in Hsu et al. (2009)):
Condition 1 ∀a ∈ N , the matrices I a and J a are
of full rank (i.e., they have rank m). For all a ∈ N ,
a
for all h ∈ [m], γh > 0.

Condition 2 ∀a ∈ N , the matrices U a ∈ R(d×m)
′
and V a ∈ R(d ×m) are such that the matrices Ga =
(U a )⊤ I a and K a = (V a )⊤ J a are invertible.

The following lemma justiﬁes the use of an SVD
calculation as one method for ﬁnding values for U a
and V a that satisfy condition 2:
Lemma 1 Assume that condition 1 holds, and for
all a ∈ N deﬁne
Ωa = E[φ(T1 ) (ψ(O))⊤ |A1 = a]

(5)

Then if U a is a matrix of the m left singular vectors of Ωa corresponding to non-zero singular values, and V a is a matrix of the m right singular vectors of Ωa corresponding to non-zero singular values, then condition 2 is satisﬁed.
Proof sketch: It can be shown that Ωa =
I a diag(γ a )(J a )⊤ . The remainder is similar to the
proof of lemma 2 in Hsu et al. (2009).
The matrices Ωa can be estimated directly from a
training set consisting of s-trees, assuming that we
have access to the functions φ and ψ.
We can now state the following theorem:

Theorem 2 Assume conditions 1 and 2 are satisﬁed.
For all a ∈ N , deﬁne Ga = (U a )⊤ I a . Then under
the deﬁnitions in Eqs. 2-4:
1. For all rules a → b c, C a→b c (y)
=
Gc T a→b c diag(yGb S a→b c )Qa→b c (Ga )−1
2. For all rules a → x, c∞ = 1⊤ Qa→x (Ga )−1 .
a→x
3. For all a ∈ N , c1 = Ga π a
a
Proof: The following identities hold (see section 9.2):
Da→b c (y) =
c

G T

a→b c

(6)
b

diag(yG S

a→b c

)Q

a→b c

a

a

a ⊤

diag(γ )(K )

d∞ = 1⊤ Qa→x diag(γ a )(K a )⊤
a→x
a

a

a ⊤

(7)

Σ = G diag(γ )(K )

(8)

c1 = Ga π a
a

(9)

Under conditions 1 and 2, Σa is invertible, and
(Σa )−1 = ((K a )⊤ )−1 (diag(γ a ))−1 (Ga )−1 . The
identities in the theorem follow immediately.

7 Deriving Empirical Estimates
Figure 4 shows an algorithm that derives estimates of the quantities in Eqs 2, 3, and 4. As
input, the algorithm takes a sequence of tuples
(r (i,1) , t(i,1) , t(i,2) , t(i,3) , o(i) , b(i) ) for i ∈ [M ].
These tuples can be derived from a training set
consisting of s-trees τ1 . . . τM as follows:
• ∀i ∈ [M ], choose a single node ji uniformly at
random from the nodes in τi . Deﬁne r (i,1) to be the
rule at node ji . t(i,1) is the inside tree rooted at node
ji . If r (i,1) is of the form a → b c, then t(i,2) is the
inside tree under the left child of node ji , and t(i,3)
is the inside tree under the right child of node ji . If
r (i,1) is of the form a → x, then t(i,2) = t(i,3) =
NULL. o(i) is the outside tree at node ji . b(i) is 1 if
node ji is at the root of the tree, 0 otherwise.
Under this process, assuming that the s-trees
τ1 . . . τM are i.i.d. draws from the distribution
p(τ ) over s-trees under an L-PCFG, the tuples
(r (i,1) , t(i,1) , t(i,2) , t(i,3) , o(i) , b(i) ) are i.i.d. draws
from the joint distribution over the random variables
R1 , T1 , T2 , T3 , O, B deﬁned in the previous section.
The algorithm ﬁrst computes estimates of the projection matrices U a and V a : following lemma 1,
this is done by ﬁrst deriving estimates of Ωa ,
and then taking SVDs of each Ωa . The matrices
are then used to project inside and outside trees

t(i,1) , t(i,2) , t(i,3) , o(i) down to m-dimensional vectors y (i,1) , y (i,2) , y (i,3) , z (i) ; these vectors are used to
derive the estimates of C a→b c , c∞ , and c1 .
a
a→x
We now state a PAC-style theorem for the learning
algorithm. First, for a given L-PCFG, we need a
couple of deﬁnitions:
• Λ is the minimum absolute value of any element
of the vectors/matrices/tensors c1 , d∞ , D a→b c ,
a
a→x
(Σa )−1 . (Note that Λ is a function of the projection matrices U a and V a as well as the underlying
L-PCFG.)
• For each a ∈ N , σ a is the value of the m’th
largest singular value of Ωa . Deﬁne σ = mina σ a .
We then have the following theorem:
Theorem 3 Assume that the inputs to the algorithm
in ﬁgure 4 are i.i.d. draws from the joint distribution
over the random variables R1 , T1 , T2 , T3 , O, B, under an L-PCFG with distribution p(r1 . . . rN ) over
s-trees. Deﬁne m to be the number of latent states
in the L-PCFG. Assume that the algorithm in ﬁgˆ
ˆ
ure 4 has projection matrices U a and V a derived as
a , as deﬁned in
left and right singular vectors of Ω
ˆ
Eq. 5. Assume that the L-PCFG, together with U a
a , has coefﬁcients Λ > 0 and σ > 0. In addiˆ
and V
tion, assume that all elements in c1 , d∞ , D a→b c ,
a
a→x
and Σa are in [−1, +1]. For any s-tree r1 . . . rN deﬁne p(r1 . . . rN ) to be the value calculated by the
ˆ
algorithm in ﬁgure 3 with inputs c1 , c∞ , C a→b c
ˆa ˆa→x ˆ
derived from the algorithm in ﬁgure 4. Deﬁne R to
be the total number of rules in the grammar of the
form a → b c or a → x. Deﬁne Ma to be the number of training examples in the input to the algorithm
in ﬁgure 4 where r i,1 has non-terminal a on its lefthand-side. Under these assumptions, if for all a
Ma ≥

√
2N+1

128m2
1+ǫ−1

Then
1−ǫ≤

2

Λ2 σ 4

log

2mR
δ

p(r1 . . . rN )
ˆ
≤1+ǫ
p(r1 . . . rN )

A similar theorem (omitted for space) states that
ˆ
1 − ǫ ≤ µ(a,i,j) ≤ 1 + ǫ for the marginals.
µ(a,i,j)
ˆ
ˆ
The condition that U a and V a are derived from
a , as opposed to the sample estimate Ωa , follows
ˆ
Ω
Foster et al. (2012). As these authors note, similar
techniques to those of Hsu et al. (2009) should be

ˆ
applicable in deriving results for the case where Ωa
a.
is used in place of Ω
Proof sketch: The proof is similar to that of Foster
et al. (2012). The basic idea is to ﬁrst show that
under the assumptions of the theorem, the estimates
ˆ
c1 , d∞ , D a→b c , Σa are all close to the underlying
ˆa ˆa→x ˆ
values being estimated. The second step is to show
p(r ...r )
ˆ
that this ensures that p(r1 ...rN ′′ ) is close to 1.
1
N
The method described of selecting a single tuple
(r (i,1) , t(i,1) , t(i,2) , t(i,3) , o(i) , b(i) ) for each s-tree ensures that the samples are i.i.d., and simpliﬁes the
analysis underlying theorem 3. In practice, an implementation should most likely use all nodes in all
trees in training data; by Rao-Blackwellization we
know such an algorithm would be better than the
one presented, but the analysis of how much better
would be challenging. It would almost certainly lead
to a faster rate of convergence of p to p.
ˆ

8 Discussion
There are several potential applications of the
method. The most obvious is parsing with LPCFGs.1 The approach should be applicable in other
cases where EM has traditionally been used, for example in semi-supervised learning. Latent-variable
HMMs for sequence labeling can be derived as special case of our approach, by converting tagged sequences to right-branching skeletal trees.
The sample complexity of the method depends on
the minimum singular values of Ωa ; these singular
values are a measure of how well correlated ψ and
φ are with the unobserved hidden variable H1 . Experimental work is required to ﬁnd a good choice of
values for ψ and φ for parsing.

9 Proofs
This section gives proofs of theorems 1 and 2. Due
to space limitations we cannot give full proofs; instead we provide proofs of some key lemmas. A
long version of this paper will give the full proofs.
9.1

Proof of Theorem 1

First, the following lemma leads directly to the correctness of the algorithm in ﬁgure 2:
1
Parameters can be estimated using the algorithm in
ﬁgure 4; for a test sentence x1 . . . xN we can ﬁrst
use the algorithm in ﬁgure 3 to calculate marginals
µ(a, i, j), then use the algorithm of Goodman (1996) to ﬁnd
arg maxτ ∈T (x) (a,i,j)∈τ µ(a, i, j).

Inputs: Training examples (r (i,1) , t(i,1) , t(i,2) , t(i,3) , o(i) , b(i) )
for i ∈ {1 . . . M }, where r (i,1) is a context free rule; t(i,1) ,
t(i,2) and t(i,3) are inside trees; o(i) is an outside tree; and
b(i) = 1 if the rule is at the root of tree, 0 otherwise. A function
φ that maps inside trees t to feature-vectors φ(t) ∈ Rd . A func′
tion ψ that maps outside trees o to feature-vectors ψ(o) ∈ Rd .
Algorithm:
Deﬁne ai to be the non-terminal on the left-hand side of rule
r (i,1) . If r (i,1) is of the form a → b c, deﬁne bi to be the nonterminal for the left-child of r (i,1) , and ci to be the non-terminal
for the right-child.
(Step 0: Singular Value Decompositions)
ˆ
• Use the algorithm in ﬁgure 5 to calculate matrices U a ∈
′
(d×m)
ˆ a ∈ R(d ×m) for each a ∈ N .
R
and V
(Step 1: Projection)
ˆ
• For all i ∈ [M ], compute y (i,1) = (U ai )⊤ φ(t(i,1) ).
• For all i ∈ [M ] such that r (i,1) is of the form
ˆ
a → b c, compute y (i,2) = (U bi )⊤ φ(t(i,2) ) and y (i,3) =
ˆ
(U ci )⊤ φ(t(i,3) ).
ˆ
• For all i ∈ [M ], compute z (i) = (V ai )⊤ ψ(o(i) ).
(Step 2: Calculate Correlations)
• For each a ∈ N , deﬁne δa = 1/

M
i=1 [[ai

= a]]

ˆ
• For each rule a → b c, compute Da→b c = δa ×
M
[[r (i,1) = a → b c]]y (i,3) (z (i) )⊤ (y (i,2) )⊤
i=1
ˆa→x = δa ×
• For each rule a → x, compute d∞
M
[[r (i,1) = a → x]](z (i) )⊤
i=1
ˆ
• For each a ∈ N , compute Σa
M
[[ai = a]]y (i,1) (z (i) )⊤
i=1

=

δa ×

(Step 3: Compute Final Parameters)
ˆ
ˆ
ˆ
• For all a → b c, C a→b c (y) = Da→b c (y)(Σa )−1
∞
ˆ∞ (Σa )−1
ˆ
• For all a → x, ca→x = da→x
ˆ
• For all a ∈ I, c1 =
ˆa

M
b(i) =1]]y (i,1)
i=1 [[ai =a
M [[b(i) =1]]
i=1

and

Figure 4: The spectral learning algorithm.
Inputs: Identical to algorithm in ﬁgure 4.
Algorithm:
′
ˆ
• For each a ∈ N , compute Ωa ∈ R(d ×d) as
ˆ
Ωa =

M
i=1 [[ai

= a]]φ(t(i,1) )(ψ(o(i) ))⊤
M
i=1 [[ai

= a]]

ˆ
and calculate a singular value decomposition of Ωa .
ˆ
• For each a ∈ N , deﬁne U a ∈ Rm×d to be a matrix of the left
ˆ
singular vectors of Ωa corresponding to the m largest singular
′
a
ˆ
values. Deﬁne V ∈ Rm×d to be a matrix of the right singular
ˆ
vectors of Ωa corresponding to the m largest singular values.

Figure 5: Singular value decompositions.

Lemma 2 Assume that conditions 1-3 of theorem 1
are satisﬁed, and that the input to the algorithm in
ﬁgure 2 is an s-tree r1 . . . rN . Deﬁne ai for i ∈ [N ]
to be the non-terminal on the left-hand-side of rule
ri , and ti for i ∈ [N ] to be the s-tree with rule ri
at its root. Finally, for all i ∈ [N ], deﬁne the row
vector bi ∈ R(1×m) to have components
bi = P (Ti = ti |Hi = h, Ai = ai )
h

Lemma 3 Assume that conditions 1-3 of theorem 1
are satisﬁed, and that the input to the algorithm in
ﬁgure 3 is a sentence x1 . . . xN . For any a ∈ N , for
any 1 ≤ i ≤ j ≤ N , deﬁne αa,i,j ∈ R(1×m) to have
¯
a,i,j
components αh = p(xi . . . xj |h, a) for h ∈ [m].
¯
¯
In addition, deﬁne β a,i,j ∈ R(m×1) to have compoa,i,j
¯
nents βh
= p(x1 . . . xi−1 , a(h), xj+1 . . . xN ) for
h ∈ [m]. Then for all i ∈ [N ], αa,i,j = αa,i,j (Ga )−1
¯
¯
and β a,i,j = Ga β a,i,j . It follows that for all (a, i, j),
¯
¯
¯
µ(a, i, j) = αa,i,j (Ga )−1 Ga β a,i,j = αa,i,j β a,i,j
¯
=
αa,i,j βh =
¯h ¯a,i,j
p(τ )

for h ∈ [m]. Then for all i ∈ [N ], f i = bi (G(ai ) )−1 .
It follows immediately that
f 1 c11 = b1 (G(a1 ) )−1 Ga1 πa1 = p(r1 . . . rN )
a

h

This lemma shows a direct link between the vectors f i calculated in the algorithm, and the terms bi ,
h
which are terms calculated by the conventional inside algorithm: each f i is a linear transformation
(through Gai ) of the corresponding vector bi .
Proof: The proof is by induction.
First consider the base case. For any leaf—i.e., for
any i such that ai ∈ P—we have bi = q(ri |h, ai ),
h
and it is easily veriﬁed that f i = bi (G(ai ) )−1 .
The inductive case is as follows. For all i ∈ [N ]
such that ai ∈ I, by the deﬁnition in the algorithm,
f i = f γ C ri (f β )

bγ (G(aγ ) )−1

Assuming by induction that
=
β = bβ (G(aβ ) )−1 , this simpliﬁes to
f
i

r

l

ri

ai −1

f = κ diag(κ )Q (G )
κr

bγ T ri ,

κl

bβ S ri .

and

(10)

κr

where
=
and
=
is a row
γ ri
r =
vector with components κh
h′ ∈[m] bh′ Th′ ,h =
γ
′ |h, r ). Similarly, κl is a row vector
i
h′ ∈[m] bh′ t(h

with components equal to κl =
h
β
′
h′ ∈[m] bh′ s(h |h, ri ). It
κr diag(κl )Qri is a row

equal to κr κl q(ri |h, ai ).
h h
But bi = q(ri |h, ai )×
h

β
′
h′ ∈[m] bh′ s(h |h, ri )
κr diag(κl )Qri = bi and

β ri
h′ ∈[m] bh′ Sh′ ,h

9.2

Proof of the Identity in Eq. 6

We now prove the identity in Eq. 6, used in the proof
of theorem 2. For reasons of space, we do not give
the proofs of identities 7-9: the proofs are similar.
The following identities can be veriﬁed:
a→b
E [Y3,j |H1 = h, R1 = a → b c] = Ej,h c
a
E [Zk |H1 = h, R1 = a → b c] = Kk,h

a→b
E [Y2,l |H1 = h, R1 = a → b c] = Fl,h c

where E a→b c = Gc T a→b c , F a→b c = Gb S a→b c .
Y3 , Z and Y2 are independent when conditioned
on H1 , R1 (this follows from the independence assumptions in the L-PCFG), hence

=

can then be veriﬁed that
vector with components
γ
′
h′ ∈[m] bh′ t(h |h, ri )

Thus the vectors αa,i,j and β a,i,j are linearly re¯
lated to the vectors αa,i,j and β a,i,j , which are the
¯
inside and outside terms calculated by the conventional form of the inside-outside algorithm.
The proof is by induction, and is similar to the
proof of lemma 2; for reasons of space it is omitted.

P (R1 = a → b c|H1 = h, A1 = a) = q(a → b c|h, a)

= f γ Gaγ T ri diag(f β Gaβ S ri )Qri (Gai )−1
fγ

τ ∈T (x):(a,i,j)∈τ

×

= q(ri |h, ai )κr κl , hence
h h

the inductive case follows
immediately from Eq. 10.
Next, we give a similar lemma, which implies the
correctness of the algorithm in ﬁgure 3:

E [[[R1 = a → b c]]Y3,j Zk Y2,l | H1 = h, A1 = a]

a→b
a
a→b
= q(a → b c|h, a)Ej,h c Kk,h Fl,h c

a
Hence (recall that γh = P (H1 = h|A1 = a)),
a→b
Dj,k,l c = E [[[R1 = a → b c]]Y3,j Zk Y2,l | A1 = a]

=
h

=
h

a
γh E [[[R1 = a → b c]]Y3,j Zk Y2,l | H1 = h, A1 = a]

a
a→b
a
a→b
γh q(a → b c|h, a)Ej,h c Kk,h Fl,h c

from which Eq. 6 follows.

(11)

Acknowledgements: Columbia University gratefully acknowledges the support of the Defense Advanced Research Projects Agency (DARPA) Machine Reading Program under Air Force Research Laboratory (AFRL)
prime contract no. FA8750-09-C-0181. Any opinions,
ﬁndings, and conclusions or recommendations expressed
in this material are those of the author(s) and do not necessarily reﬂect the view of DARPA, AFRL, or the US
government. Shay Cohen was supported by the National
Science Foundation under Grant #1136996 to the Computing Research Association for the CIFellows Project.
Dean Foster was supported by National Science Foundation grant 1106743.

References
B. Balle, A. Quattoni, and X. Carreras. 2011. A spectral learning algorithm for ﬁnite state transducers. In
Proceedings of ECML.
S. Dasgupta. 1999. Learning mixtures of Gaussians. In
Proceedings of FOCS.
Dean P. Foster, Jordan Rodu, and Lyle H. Ungar.
2012. Spectral dimensionality reduction for hmms.
arXiv:1203.6130v1.
J. Goodman. 1996. Parsing algorithms and metrics. In
Proceedings of the 34th annual meeting on Association for Computational Linguistics, pages 177–183.
Association for Computational Linguistics.
D. Hsu, S. M. Kakade, and T. Zhang. 2009. A spectral algorithm for learning hidden Markov models. In
Proceedings of COLT.
H. Jaeger. 2000. Observable operator models for discrete
stochastic time series. Neural Computation, 12(6).
F. M. Lugue, A. Quattoni, B. Balle, and X. Carreras.
2012. Spectral learning for non-deterministic dependency parsing. In Proceedings of EACL.
T. Matsuzaki, Y. Miyao, and J. Tsujii. 2005. Probabilistic CFG with latent annotations. In Proceedings
of the 43rd Annual Meeting on Association for Computational Linguistics, pages 75–82. Association for
Computational Linguistics.
A. Parikh, L. Song, and E. P. Xing. 2011. A spectral algorithm for latent tree graphical models. In Proceedings of The 28th International Conference on Machine
Learningy (ICML 2011).
F. Pereira and Y. Schabes. 1992. Inside-outside reestimation from partially bracketed corpora. In Proceedings of the 30th Annual Meeting of the Association for
Computational Linguistics, pages 128–135, Newark,
Delaware, USA, June. Association for Computational
Linguistics.

S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning accurate, compact, and interpretable tree annotation. In Proceedings of the 21st International
Conference on Computational Linguistics and 44th
Annual Meeting of the Association for Computational
Linguistics, pages 433–440, Sydney, Australia, July.
Association for Computational Linguistics.
S. A. Terwijn. 2002. On the learnability of hidden
markov models. In Grammatical Inference: Algorithms and Applications (Amsterdam, 2002), volume
2484 of Lecture Notes in Artiﬁcial Intelligence, pages
261–268, Berlin. Springer.
S. Vempala and G. Wang. 2004. A spectral algorithm for
learning mixtures of distributions. Journal of Computer and System Sciences, 68(4):841–860.

