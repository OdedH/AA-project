Identiﬁability and Unmixing of Latent Parse Trees

Daniel Hsu
Microsoft Research

Sham M. Kakade
Microsoft Research

Percy Liang
Stanford University

Abstract
This paper explores unsupervised learning of parsing models along two directions.
First, which models are identiﬁable from inﬁnite data? We use a general technique for numerically checking identiﬁability based on the rank of a Jacobian matrix, and apply it to several standard constituency and dependency parsing models.
Second, for identiﬁable models, how do we estimate the parameters efﬁciently?
EM suffers from local optima, while recent work using spectral methods [1] cannot be directly applied since the topology of the parse tree varies across sentences.
We develop a strategy, unmixing, which deals with this additional complexity for
restricted classes of parsing models.

1

Introduction

Generative parsing models, which deﬁne joint distributions over sentences and their parse trees, are
one of the core techniques in computational linguistics. We are interested in the unsupervised learning of these models [2–6], where the goal is to estimate the model parameters given only examples
of sentences. Unsupervised learning can fail for a number of reasons [7]: model misspeciﬁcation,
non-identiﬁability, estimation error, and computation error. In this paper, we delve into two of these
issues: identiﬁability and computation. In doing so, we confront a central challenge of parsing
models—that the topology of the parse tree is unobserved and varies across sentences. This is in
contrast to standard phylogenetic models [8] and other latent tree models for which there is a single
ﬁxed global tree across all examples [9].
A model is identiﬁable if there is enough information in the data to pinpoint the parameters (up
to some trivial equivalence class); establishing the identiﬁability of a model is often a highly nontrivial task. A classic result of Kruskal [10] has been employed to prove the identiﬁability of a wide
class of latent variable models, including hidden Markov models and certain restricted mixtures of
latent tree models [11–13]. However, these techniques cannot be directly applied to parsing models
since the tree topology varies over an exponential set of possible topologies. Instead, we turn to
techniques from algebraic geometry [14–17]; we show that a simple numerical procedure can be
used to check identiﬁability for a wide class of models in NLP. Using this tool, we discover that
probabilistic context-free grammars (PCFGs) are non-identiﬁable, but that simpler PCFG variants
and dependency models are identiﬁable.
The most common way to estimate unsupervised parsing models is by using local techniques such
as EM [18] or MCMC sampling [19], but these methods can suffer from local optima and slow
mixing. Meanwhile, recent work [1,20–23] has shown that spectral methods can be used to estimate
mixture models and HMMs with provable guarantees. These techniques express low-order moments
of the observable distribution as a product of matrix parameters and use eigenvalue decomposition
to recover these matrices. However, these methods are not directly applicable to parsing models
because the tree topology again varies non-trivially. To address this, we propose a new technique,
unmixing. The main idea is to express moments of the observable distribution as a mixture over
the possible topologies. For restricted parsing models, the moments for a ﬁxed tree structure can
E-mail: dahsu@microsoft.com, skakade@microsoft.com, pliang@cs.stanford.edu

1

π

π

z03

z03

T

T

z02
T

T

z23

O

x3

T

A
A

A

x2

x3

Topology(z) = 1

z13

π

T

z01

z12

z12

O

O

O

x1

x2

x2

x1

z23

O

x3

Topology(z) = 1

x1

x1

T

z01

O

T

π

A

x2

A

x3

Topology(z) = 2

x1

x2

A

π

x2

A

x3

A

π

x3

Topology(z) = 5
x1

A

x2

A

π

x3

Topology(z) = 6
A

A

Topology(z) = 3
x1

x2

A

π

Topology(z) = 2

x1

x1

A

x2

π

x3

Topology(z) = 7

x3

Topology(z) = 4
(a) Constituency (PCFG-IE)

(b) Dependency (DEP-IE)

Figure 1: The two constituency trees and seven dependency trees over L = 3 words, x1 , x2 , x3 . (a)
A constituency tree consists of a hierarchical grouping of the words with a latent state zv for each
node v. (b) A dependency tree consists of a collection of directed edges between the words. In both
cases, we have labeled each edge from i to j with the parameters used to generate the state of node
j given i.

be “unmixed”, thereby reducing the problem to one with a ﬁxed topology, which can be tackled
using standard techniques [1]. Importantly, our unmixing technique does not require the training
sentences be annotated with the tree topologies a priori, in contrast to recent extensions of [21] to
learning PCFGs [24] and dependency trees [25, 26], which work on a ﬁxed topology.

2

Notation
def

For a positive integer n, deﬁne [n] = {1, . . . , n} and n = {e1 , . . . , en }, where ei is the vector
which is 1 in component i and 0 elsewhere. For integers a, b ∈ [n], let a⊗n b = (a−1)n+b ∈ [n2 ] be
the integer encoding of the pair (a, b). For a pair of matrices, A, B ∈ Rm×n , deﬁne the columnwise
2
tensor product A ⊗C B ∈ Rm ×n to be such that (A ⊗C B)(i1 ⊗m i2 )j = Ai1 j Bi2 j . For a matrix
A ∈ Rm×n , let A† denote the Moore-Penrose pseudoinverse.

3

Parsing models

A sentence is a sequence of L words, x = (x1 , . . . , xL ), where each word xi ∈ d is one of
d possible word types. A (generative) parsing model deﬁnes a joint distribution Pθ (x, z) over a
sentence x and its parse tree z (to be made precise later), where θ are the model parameters (a
collection of multinomials). Each parse tree z has a topology Topology(z) ∈ Topologies, which
is both unobserved and varying across sentences. The learning problem is to recover θ given only
samples of x.
Two important classes of models of natural language syntax are constituency models, which represent a hierarchical grouping and labeling of the phrases of a sentence (e.g., Figure 1(a)), and
dependency models, which represent pairwise relationships between the words of a sentence (e.g.,
Figure 1(b)).
2

3.1

Constituency models

A constituency tree z = (V, s) consists of a set of nodes V and a collection of hidden states s =
{sv }v∈V . Each state sv ∈ k represents one of k possible syntactic categories. Each node v has
the form [i : j] for 0 ≤ i < j ≤ L corresponding to the phrase between positions i and j of the
sentence. These nodes form a binary tree as follows: the root node is [0 : L] ∈ V , and for each node
[i : j] ∈ V with j − i > 1, there exists a unique m with i < m < j deﬁning the two children nodes
[i : m] ∈ V and [m : j] ∈ V . Let Topology(z) be an integer encoding of V .
PCFG. Perhaps the most well-known constituency parsing model is the probabilistic context-free
grammar (PCFG). The parameters of a PCFG are θ = (π, B, O), where π ∈ Rk speciﬁes the initial
2
state distribution, B ∈ Rk ×k speciﬁes the binary production distributions, and O ∈ Rd×k speciﬁes
the emission distributions.
A PCFG corresponds to the following generative process (see Figure 1(a) for an example): choose a
topology Topology(z) uniformly at random; generate the state of the root node using π; recursively
generate pairs of children states given their parents using B; and ﬁnally generate words xi given
their parents using O. This generative process deﬁnes a joint probability over a sentence x and a
parse tree z:
L

Pθ (x, z) = | Topologies |−1 π s[0:L]

(s[i:m] ⊗k s[m:j] ) Bs[i:j]

xi Os[i−1:i] , (1)
i=1

[i:m],[m:j]∈V

We will also consider two variants of the PCFG with additional restrictions:
PCFG-I. The left and right children states are generated independently—that is, we have the following factorization: B = T1 ⊗C T2 for some T1 , T2 ∈ Rk×k .
PCFG-IE. The left and the right productions are independent and equal: B = T ⊗C T .
3.2

Dependency tree models

In contrast to constituency trees, which posit internal nodes with latent states, dependency trees
connect the words directly. A dependency tree z is a set of directed edges (i, j), where i, j ∈ [L]
are distinct positions in the sentence. Let Root(z) denote the position of the root node of z. We
consider only projective dependency trees [27]: z is projective if for every path from i to j to k in
z, we have that j and k are on the same side of i (that is, j − i and k − i have the same sign). Let
Topology(z) be an integer encoding of z.
DEP-I. We consider the simple dependency model of [4]. The parameters of this model are θ =
(π, A , A ), where π ∈ Rd is the initial word distribution and A , A ∈ Rd×d are the left and
right argument distributions. The generative process is as follows: choose a topology Topology(z)
uniformly at random, generate the root word using π, and recursively generate argument words to
the left to the right given the parent word using A and A , respectively. The corresponding joint
probability distribution is as follows:
Pθ (x, z) = | Topologies |−1 π xRoot(z)

xj Adir(i,j) xi ,

(2)

(i,j)∈z

where dir(i, j) =

if j < i and

if j > i.

We also consider the following two variants:
DEP-IE. The left and right argument distributions are equal: A = A
DEP-IES. A = A

=A

=A .

and π is the stationary distribution of A (that is, π = Aπ).

Usually a PCFG induces a topology via a state-dependent probability of choosing a binary production
versus an emission. Our model is a restriction which corresponds to a state-independent probability.

3

4

Identiﬁability

Our goal is to estimate model parameters θ0 ∈ Θ given only access to sentences x ∼ Pθ0 . Speciﬁcally, suppose we have an observation function φ(x) ∈ Rm , which is the only lens through which an
algorithm can view the data. We ask a basic question: in the limit of inﬁnite data, is it informationdef
theoretically possible to identify θ0 from the observed moments µ(θ0 ) = Eθ0 [φ(x)]?
To be more precise, deﬁne the equivalence class of θ0 to be the set of parameters θ that yield the
same observed moments:
SΘ (θ0 ) = {θ ∈ Θ : µ(θ) = µ(θ0 )}.
(3)
It is impossible for an algorithm to distinguish among the elements of SΘ (θ0 ). Therefore, one might
want to ensure that |SΘ (θ0 )| = 1 for all θ0 ∈ Θ. However, this requirement is too strong for two reasons. First, models often have natural symmetries—e.g., the k states of any PCFG can be permuted
without changing µ(θ), so |SΘ (θ0 )| ≥ k!. Second, |SΘ (θ0 )| = ∞ for some pathological θ0 ’s—e.g.,
PCFGs where all states have the same emission distribution O are indistinguishable regardless of
the production distributions B. The following deﬁnition of identiﬁability accommodates these two
exceptional cases:
Deﬁnition 1 (Identiﬁability). A model family with parameter space Θ is (globally) identiﬁable from
φ if there exists a measure zero set E such that |SΘ (θ0 )| is ﬁnite for every θ0 ∈ Θ\E. It is locally
identiﬁable from φ if there exists a measure zero set E such that, for every θ0 ∈ Θ\E, there exists an
open neighborhood N (θ0 ) around θ0 such that SΘ (θ0 ) ∩ N (θ0 ) = {θ0 }.
Example of non-identiﬁability. Consider the DEP-IE model with L = 2 with the full observation
function φ(x) = x1 ⊗ x2 . The corresponding observed moments are µ(θ) = 0.5A diag(π) +
0.5 diag(π)A . Note that A diag(π) is an arbitrary d × d matrix whose entries sum to 1, which
has d2 − 1 degrees of freedom, whereas µ(θ) is a symmetric matrix whose entries sum to 1, which
has d+1 − 1 degrees of freedom. Therefore, SΘ (θ) has dimension d and therefore the model is
2
2
non-identiﬁable.
Parameter counting. It is important to compute the degrees of freedom correctly—simple parameter counting is insufﬁcient. For example, consider the PCFG-IE model with L = 2. The observed
moments with respect to φ(x) = x1 ⊗ x2 is a d × d matrix, which places d2 constraints on the
k 2 + (d − 1)k parameters. When d ≥ 2k, there are more constraints than parameters, but the PCFGIE model with L = 2 is actually non-identiﬁable (as we will see later). The issue here is that the
number of constraints does not reveal the fact that some of these constraints are redundant.
4.1

Observation functions

An observation function φ(x) and its associated observed moments µ(θ0 ) = Eθ0 [φ(x)] reveals
aspects of the distribution Pθ0 (x). For example, φ(x) = x1 would only reveal the marginal distribution of the ﬁrst word, whereas φ(x) = x1 ⊗ · · · ⊗ xL reveals the entire distribution of x. There is a
tradeoff: Higher-order moments provide more information, but are harder to estimate reliably given
ﬁnite data, and are also computationally more expensive. In this paper, we consider the following
intermediate moments:
def

def

φ12 (x) = x1 ⊗ x2

φ∗∗ (x) = xi ⊗ xj : i, j ∈ [L]

def

def

φ123 (x) = x1 ⊗ x2 ⊗ x3
def

φ123η (x) = (x1 ⊗ x2 )(η x3 )

φ∗∗∗ (x) = xi ⊗ xj ⊗ xk : i, j, k ∈ [L]
def

φ∗∗∗η (x) = (xi ⊗ xj )(η xk ) : i, j, k ∈ [L]

def

φall (x) = x1 ⊗ · · · ⊗ xL
Above, η ∈ Rd denotes a unit vector in Rd (e.g., e1 ) which picks out a linear combination of matrix
slices from a third-order d × d × d tensor.
4.2

Automatically checking identiﬁability

One immediate goal is to determine which models in Section 3 are identiﬁable from which of the
observed moments (Section 4.1). A powerful analytic tool that has been succesfully applied in
4

previous work is Kruskal’s theorem [10, 11], but (i) it is does not immediately apply to models with
random topologies, and (ii) only gives sufﬁcient conditions for identiﬁability, and cannot be used to
determine non-identiﬁability. Furthermore, since it is common practice to explore many different
models for a given problem in rapid succession, we would like to check identiﬁability quickly and
reliably. In this section, we develop an automatic procedure to do this.
To establish identiﬁability, let us examine the algebraic structure of SΘ (θ0 ) for θ0 ∈ Θ, where we
assume that the parameter space Θ is an open subset of [0, 1]n . Recall that SΘ (θ0 ) is deﬁned by the
moment constraints µ(θ) = µ(θ0 ). We can write these constraints as hθ0 (θ) = 0, where
def

hθ0 (θ) = µ(θ) − µ(θ0 )
is a vector of m polynomials in θ.
Let us now compute the number of degrees of freedom of hθ0 around θ0 . The key quantity is
J(θ) ∈ Rm×n , the Jacobian of hθ0 at θ (note that the Jacobian of hθ0 does not depend on θ0 ; it
is precisely the Jacobian of µ). This Jacobian criterion is well-established in algebraic geometry,
and has been adopted in the statistical literature for testing model identiﬁability and other related
properties [14–17].
Intuitively, each row of J(θ0 ) corresponds to a direction of a constraint violation, and thus the row
space of J(θ0 ) corresponds to all directions that would take us outside the equivalence class SΘ (θ0 ).
If J(θ0 ) has less than rank n, then there is a direction orthogonal to all the rows along which we
can move and still satisfy all the constraints—in other words, |SΘ (θ0 )| is inﬁnite, and therefore the
model is non-identiﬁable. This intuition leads to the following algorithm:
C HECK I DENTIFIABILITY:
˜
−1. Choose a point θ ∈ Θ uniformly at random.
˜
−2. Compute the Jacobian matrix J(θ).
˜ = n and “no” otherwise.
−3. Return “yes” if the rank of J(θ)

The following theorem asserts the correctness of C HECK I DENTIFIABILITY. It is largely based on
techniques in [16], although we have not seen it explicitly stated in this form.
Theorem 1 (Correctness of C HECK I DENTIFIABILITY). Assume the parameter space Θ is a nonempty open connected subset of [0, 1]n ; and the observed moments µ : Rn → Rm , with respect to
observation function φ, is a polynomial map. Then with probability 1, C HECK I DENTIFIABILITY
returns “yes” iff the model family is locally identiﬁable from φ. Moreover, if it returns “yes”, then
there exists E ⊂ Θ of measure zero such that the model family with parameter space Θ \ E is
identiﬁable from φ.
The proof of Theorem 1 is given in Appendix A.
4.3

Implementation of C HECK I DENTIFIABILITY

Computing the Jacobian. The rows of J correspond to ∂Eθ [φj (x)]/∂θ and can be computed efﬁciently by adapting dynamic programs used in the E-step of an EM algorithm for parsing models.
There are two main differences: (i) we must sum over possible values of x in addition to z, and (ii)
we are not computing moments, but rather gradients thereof. Speciﬁcally, we adapt the CKY algorithm for constituency models and the algorithm of [27] for dependency models. See Appendix C.1
for more details.
Numerical issues. Because we implemented C HECK I DENTIFIABILITY on a ﬁnite precision machine, the results are subject to numerical precision errors. However, we veriﬁed that our numerical
results are consistent with various analytically-derived identiﬁability results (e.g., from [11]).
While we initially deﬁned θ to be a tuple of conditional probability matrices, we will now use its nonredundant vectorized form θ ∈ Rn .

5

Model \ Observation function
PCFG
PCFG-I / PCFG-IE
DEP-I
DEP-IE / DEP-IES

φ12
No
No

φ∗∗
φ123e1 φ123 φ∗∗∗e1
No, even from φall for L ∈ {3, 4, 5}
Yes iff L ≥ 4
Yes iff L ≥ 3
Yes iff L ≥ 3
Yes iff L ≥ 3

φ∗∗∗

Figure 2:
Local identiﬁability of parsing models.
These ﬁndings are given by
C HECK I DENTIFIABILITY have the semantics from Theorem 1. These were checked for d ∈
{2, 3, . . . , 8}, k ∈ {2, . . . , d} (applies only for PCFG models), L ∈ {2, 3, . . . , 9}.
4.4

Identiﬁability of constituency and dependency tree models

We checked the identiﬁability status of various constituency and dependency tree models using our
implementation of C HECK I DENTIFIABILITY. We focus on the regime where d ≥ k for PCFGs;
additional results for d < k are given in Appendix B.
The results are reported in Figure 2. First, we found that the PCFG is not identiﬁable from φall (and
therefore not identiﬁable from any φ) for L ∈ {3, 4, 5}; we believe that the same holds for all L. This
negative result motivates exploring restricted subclasses of PCFGs, such as PCFG-I and PCFG-IE,
which factorize the binary productions. For these classes, we found that the sentence length L and
choice of observation function can inﬂuence identiﬁability: Both models are identiﬁable for large
enough L (e.g., L ≥ 3) and with a sufﬁciently rich observation function (e.g., φ123η ).
The dependency models, DEP-I and DEP-IE, were all found to be identiﬁable for L ≥ 3 from
second-order moments φ∗∗ . The conditions for identiﬁability are less stringent than their constituency counterparts (PCFG-I and PCFG-IE), which is natural since dependency models are simpler without the latent states. Note that in all identiﬁable models, second-order moments sufﬁce to
determine the distribution—this is good news because low-order moments are easier to estimate.

5

Unmixing algorithms

Having established which parsing models are identiﬁable, we now turn to parameter estimation for
these models. We will consider algorithms based on moment matching—those that try to ﬁnd a θ
satisfying µ(θ) = u for some u. Typically, u is an empirical estimate of µ(θ0 ) = Eθ0 [φ(x)] based
on samples x ∼ Pθ0 .
In general, solving µ(θ) = u corresponds to ﬁnding solutions to systems of multivariate polynomials, which is NP-hard [28]. However, µ(θ) often has additional structure which we can exploit.
For instance, for an HMM, the sliced third-order moments µ123η (θ) can be written as a product of
parameter matrices in θ, and each matrix can be recovered by decomposing the product [1].
For parsing models, the challenge is that the topology is random, so the moments is not a single product, but a mixture over products. To deal with this complication, we propose a new technique, which
we call unmixing: We “unmix” the products from the mixtures, essentially reducing the problem to
one with a ﬁxed topology.
We will ﬁrst present the general idea of unmixing (Section 5.1) and then apply it to the PCFG-IE
model (Section 5.2) and the DEP-IES model (Section 5.3).
5.1

General case

We assume the observation function φ(x) consists of a collection of observation matrices
{φo (x)}o∈O (e.g., for o = (i, j), φo (x) = xi ⊗ xj ). Given an observation matrix φo (x) and a
topology t ∈ Topologies, consider the mapping that computes the observed moment conditioned on
Note that these subclasses occupy measure zero subsets of the PCFG parameter space, which is expected
given the non-identiﬁability of the general PCFG.
We will develop our algorithms assuming true moments (u = µ(θ0 )). The empirical moments converge
1
to the true moments at Op (n− 2 ), and matrix perturbation arguments (e.g., [1]) can be used derive sample
complexity arguments for the parameter error.

6

that topology: Ψo,t (θ) = Eθ [φo (x) | Topology = t]. As we range o over O and t over Topologies,
we will enounter a ﬁnite number of such mappings. We call these mappings compound parameters,
denoted {Ψp }p∈P .
Now write the observed moments as a weighted sum:
µo (θ) =

P(Ψo,Topology = Ψp ) Ψp
p∈P

for all o ∈ O,

(4)

def

= Mop

where we have deﬁned Mop to be the probability mass over tree topologies that yield compound
parameter Ψp . We let {Mop }o∈O,p∈P be the mixing matrix. Note that (4) deﬁnes a system of
equations µ = M Ψ, where the variables are the compound parameters and the constraints are the
observed moments. In a sense, we have replaced the original system of polynomial equations (in θ)
with a system of linear equations (in Ψ).
The key to the utility of this technique is that the number of compound parameters can be polynomial
in L even when the number of possible topologies is exponential in L. Previous analytic techniques
[13] based on Kruskal’s theorem [10] cannot be applied here because the possible topologies are too
many and too varied.
Note that the mixing equation µ = M Ψ holds for each sentence length L, but many compound parameters p appear in the equations of multiple L. Therefore, we can combine the equations across all
observed sentence lengths, yielding a more constrained system than if we considered the equations
of each L separately.
The following proposition shows how we can recover θ by unmixing the observed moments µ:
Proposition 1 (Unmixing). Suppose that there exists an efﬁcient base algorithm to recover θ from
some subset of compound parameters {Ψp (θ) : p ∈ P0 }, and that ep is in the row space of M for
each p ∈ P0 . Then we can recover θ as follows:
U NMIX(µ):
−1. Compute the mixing matrix M (4).
−2. Retrieve the compound parameters Ψp (θ) = (M † µ)p for each p ∈ P0 .
−3. Call the base algorithm on {Ψp (θ) : p ∈ P0 } to obtain θ.

For all our parsing models, M can be computed efﬁciently using dynamic programming (Appendix C.2). Note that M is data-independent, so this computation can be done once in advance.
5.2

Application to the PCFG-IE model

As a concrete example, consider the PCFG-IE model over L = 3 words. Write A = OT . For
any η ∈ Rd , we can express the observed moments as a sum over the two possible topologies in
Figure 1(a):
def

def

µ123η = E[x1 ⊗ x2 (η x3 )] = 0.5Ψ1;η + 0.5Ψ2;η ,

Ψ1;η = A diag(T diag(π)A η)A ,

def

def

µ132η = E[x1 ⊗ x3 (η x2 )] = 0.5Ψ3;η + 0.5Ψ2;η ,

Ψ2;η = A diag(π)T diag(A η)A ,

def

def

µ231η = E[x2 ⊗ x3 (η x1 )] = 0.5Ψ3;η + 0.5Ψ1;η ,

Ψ3;η = A diag(A η)T diag(π)A ,

or compactly in matrix form:
µ123η
µ132η
µ231η
observed moments µη

=

0.5I
0
0.5I

0.5I
0.5I
0

0
0.5I
0.5I

mixing matrix M

Ψ1;η
Ψ2;η
Ψ3;η

.

compound parameters Ψη

Let us observe µη at two different values of η, say at η = 1 and η = τ for some random τ . Since
the mixing matrix M is invertible, we can obtain the compound parameters Ψ2;1 = (M −1 µ1 )2 and
Ψ2;τ = (M −1 µτ )2 .
7

Now we will recover θ from Ψ2;1 and Ψ2;τ by ﬁrst extracting A = OT via an eigenvalue decomposition, and then recovering π, T , and O in turn (all up to the same unknown permutation) via
elementary matrix operations.
For the ﬁrst step, we will use the following tool (adapted from Algorithm A of [1]), which allow us
to decompose two related matrix products:
Lemma 1 (Spectral decomposition). Let M1 , M2 ∈ Rd×k have full column rank and D be a diagonal matrix with distinct diagonal entries. Suppose we observe X = M1 M2 and Y = M1 DM2 .
Then D ECOMPOSE(X, Y ) recovers M1 up to a permutation and scaling of the columns.
D ECOMPOSE(X, Y ):
−1. Find U1 , U2 ∈ Rd×k such that range(U1 ) = range(X) and range(U2 ) = range(X ).
−2. Perform an eigenvalue decomposition of (U1 Y U2 )(U1 XU2 )−1 = V SV −1 .
−3. Return (U1 )† V .

First, run D ECOMPOSE(X = Ψ2;1 , Y = Ψ2;τ ) (Lemma 1), which corresponds to M1 = A and
M2 = A diag(π)T . This produces AΠS for some permutation matrix Π and diagonal scaling S.
Since we know that the columns of A sum to one, we can identify AΠ.
To recover the initial distribution π (up to permutation), take Ψ2;1 1 = Aπ and left-multiply by
(AΠ)† to get Π−1 π. For T , put the entries of π in a diagonal matrix: Π−1 diag(π)Π. Take Ψ2;1 =
AT diag(π)A and multiply by (AΠ)† on the left and ((AΠ) )† (Π−1 diag(π)Π)−1 on the right,
which yields Π−1 T Π. (Note that Π is orthogonal, so Π−1 = Π .) Finally, multiply AΠ = OT Π
and (Π−1 T Π)−1 , which yields OΠ.
The above algorithm identiﬁes the PCFG-IE from only length 3 sentences. To exploit sentences of
different lengths, we can compute a mixing matrix M which includes constraints from sentences
of length 1 ≤ L ≤ Lmax up to some upper bound Lmax . For example, Lmax = 10 results in a
990 × 2376 mixing matrix. We can retrieve the same compound parameters (Ψ2;1 and Ψ2;τ ) from
the pseudoinverse of M and as proceed as before.
5.3

Application to the DEP-IES model

We now turn to the DEP-IES model over L = 3 words. Our goal is to recover the parameters
θ = (π, A). Let D = diag(π) = diag(Aπ), where the second equality is due to stationarity of π.
def

µ1 = E[x1 ] = π,
def

µ12 = E[x1 ⊗ x2 ] = 7−1 (DA + DA + DA A + AD + ADA + AD + DA ),
def

µ13 = E[x1 ⊗ x3 ] = 7−1 (DA + DA A + DA + ADA + AD + AAD + AD),
def ˜
µ12 = E[x1 ⊗ x2 ] = 2−1 (DA + AD),
˜
˜
where E[·] is taken with respect to length 2 sentences. Having recovered π from µ1 , it remains to
recover A. By selectively combining the moments above, we can compute AA + A = [7(µ13 −
µ12 ) + 2˜12 ] diag(µ1 )−1 . Assuming A is generic position, it is diagonalizable: A = QΛQ−1 for
µ
some diagonal matrix Λ = diag(λ1 , . . . , λd ), possibly with complex entries. Therefore, we can
recover Λ2 + Λ = Q−1 (AA + A)Q. Since Λ is diagonal, we simply have d independent quadratic
equations in λi , which can be solved in closed form. After obtaining Λ, we retrieve A = QΛQ−1 .

6

Discussion

In this work, we have shed some light on the identiﬁability of standard generative parsing models using our numerical identiﬁability checker. Given the ease with which this checker can be applied, we
believe it should be a useful tool for analyzing more sophisticated models [6], as well as developing
new ones which are expressive yet identiﬁable.
There is still a large gap between showing identiﬁability and developing explicit algorithms. We
have made some progress on closing it with our unmixing technique, which can deal with models
where the tree topology varies non-trivially.
8

References
[1] A. Anandkumar, D. Hsu, and S. M. Kakade. A method of moments for mixture models and hidden
Markov models. In COLT, 2012.
[2] F. Pereira and Y. Shabes. Inside-outside reestimation from partially bracketed corpora. In ACL, 1992.
[3] G. Carroll and E. Charniak. Two experiments on learning probabilistic dependency grammars from corpora. In Workshop Notes for Statistically-Based NLP Techniques, AAAI, pages 1–13, 1992.
[4] M. A. Paskin. Grammatical bigrams. In NIPS, 2002.
[5] D. Klein and C. D. Manning. Conditional structure versus conditional estimation in NLP models. In
EMNLP, 2002.
[6] D. Klein and C. D. Manning. Corpus-based induction of syntactic structure: Models of dependency and
constituency. In ACL, 2004.
[7] P. Liang and D. Klein. Analyzing the errors of unsupervised learning. In HLT/ACL, 2008.
[8] J. T. Chang. Full reconstruction of Markov models on evolutionary trees: Identiﬁability and consistency.
Mathematical Biosciences, 137:51–73, 1996.
[9] A. Anandkumar, K. Chaudhuri, D. Hsu, S. M. Kakade, L. Song, and T. Zhang. Spectral methods for
learning multivariate latent tree structure. In NIPS, 2011.
[10] J. B. Kruskal. Three-way arrays: Rank and uniqueness of trilinear decompositions, with application to
arithmetic complexity and statistics. Linear Algebra and Applications, 18:95–138, 1977.
[11] E. S. Allman, C. Matias, and J. A. Rhodes. Identiﬁability of parameters in latent structure models with
many observed variables. Annals of Statistics, 37:3099–3132, 2009.
[12] E. S. Allman, S. Petrovi, J. A. Rhodes, and S. Sullivant. Identiﬁability of 2-tree mixtures for group-based
models. Transactions on Computational Biology and Bioinformatics, 8:710–722, 2011.
[13] J. A. Rhodes and S. Sullivant. Identiﬁability of large phylogenetic mixture models. Bulletin of Mathematical Biology, 74(1):212–231, 2012.
[14] T. J. Rothenberg. Identiﬁcation in parameteric models. Econometrica, 39:577–591, 1971.
[15] L. A. Goodman. Exploratory latent structure analysis using both identiﬁabile and unidentiﬁable models.
Biometrika, 61(2):215–231, 1974.
[16] D. Bamber and J. P. H. van Santen. How many parameters can a model have and still be testable? Journal
of Mathematical Psychology, 29:443–473, 1985.
[17] D. Geiger, D. Heckerman, H. King, and C. Meek. Stratiﬁed exponential families: graphical models and
model selection. Annals of Statistics, 29:505–529, 2001.
[18] K. Lari and S. J. Young. The estimation of stochastic context-free grammars using the inside-outside
algorithm. Computer Speech and Language, 4:35–56, 1990.
[19] M. Johnson, T. Grifﬁths, and S. Goldwater. Bayesian inference for PCFGs via Markov chain Monte Carlo.
In HLT/NAACL, 2007.
[20] E. Mossel and S. Roch. Learning nonsingular phylogenies and hidden Markov models. Annals of Applied
Probability, 16(2):583–614, 2006.
[21] D. Hsu, S. M. Kakade, and T. Zhang. A spectral algorithm for learning hidden Markov models. In COLT,
2009.
[22] S. M. Siddiqi, B. Boots, and G. J. Gordon. Reduced-rank hidden Markov models. In AISTATS, 2010.
[23] A. Parikh, L. Song, and E. P. Xing. A spectral algorithm for latent tree graphical models. In ICML, 2011.
[24] S. B. Cohen, K. Stratos, M. Collins, D. P. Foster, and L. Ungar. Spectral learning of latent-variable
PCFGs. In ACL, 2012.
[25] F. M. Luque, A. Quattoni, B. Balle, and X. Carreras. Spectral learning for non-deterministic dependency
parsing. In EACL, 2012.
[26] P. Dhillon, J. Rodue, M. Collins, D. P. Foster, and L. Ungar. Spectral dependency parsing with latent
variables. In EMNLP-CoNLL, 2012.
[27] J. Eisner. Three new probabilistic models for dependency parsing: An exploration. In COLING, 1996.
[28] S. Sahni. Computationally related problems. SIAM Journal on Computing, 3:262–279, 1974.
[29] J. Eisner. Bilexical grammars and their cubic-time parsing algorithms. In Advances in Probabilistic and
Other Parsing Technologies, pages 29–62, 2000.

9

A

Proof of Theorem 1

Theorem 1 (restated). Assume Θ is a non-empty open connected subset of [0, 1]n and µ : Rn →
Rm is a polynomial map. With probability 1, the following holds.
• C HECK I DENTIFIABILITY returns “no” ⇒ for almost all θ0 ∈ Θ and any open neighborhood N (θ0 ) around θ0 , |SΘ (θ0 ) ∩ N (θ0 )| is inﬁnite (not locally identiﬁable).
• C HECK I DENTIFIABILITY returns “yes” ⇒ (i) for almost all θ0 ∈ Θ, there exists an open
neighborhood N (θ0 ) around θ0 such that |SΘ (θ0 ) ∩ N (θ0 )| = 1 (locally identiﬁable); and
(ii) there exists a set E ⊂ Θ with measure zero such that |SΘ\E (θ0 )| is ﬁnite for every
θ0 ∈ Θ \ E (identiﬁability of Θ\E).
The proof of Theorem 1 crucially relies on the following lemma from [16] which holds even in the
case that µ is merely an analytic function (see Lemma 9 of [17] for a simpler proof in the case µ is
a polynomial map); it states that the Jacobian achieves its maximal rank almost everywhere in Θ.
def
def
To state this precisely, ﬁrst deﬁne rmax = max{rank(J(θ)) : θ ∈ Θ} and Θmax = {θ ∈ Θ :
rank(J(θ)) = rmax }.
Lemma 2. The set Θ \ Θmax has Lebesgue measure zero. That is, Θmax is almost all of Θ.
˜
Proof of Theorem 1. By Lemma 2, C HECK I DENTIFIABILITY chooses a point θ ∈ Θmax with prob˜ = rmax .
ability 1. We henceforth condition on this event, so rank(J(θ))
˜
Case 1: rank(J(θ)) < n (i.e., “no” is returned). In this case, we have rmax < n. We now employ
an argument from the proof of Proposition 20 of [16]. Fix any θ0 ∈ Θmax . Since Θ is open, Weyl’s
theorem implies that there is an open neighborhood U around θ0 in Θ on which rank(J(θ)) = rmax
for all θ ∈ U (i.e., rank(J(·)) is constant on U ). Therefore, by the constant rank theorem, there is
an open neighborhood N (θ0 ) around θ0 in Θ such that µ−1 (µ(θ0 )) ∩ N (θ0 ) is homeomorphic with
an open set in Rn−rmax . Therefore SΘ (θ0 ) ∩ N (θ0 ) is uncountably inﬁnite.
˜
Case 2: rank(J(θ)) = n (i.e., “yes” is returned). In this case, we have rmax = n. Therefore
for every θ0 ∈ Θmax , the Jacobian J(θ0 ) has full column rank, and thus by the inverse function
theorem, µ is injective on a neighborhood of θ0 . This in turn implies that for all θ0 ∈ Θmax , there
exists an open neighborhood N (θ0 ) around θ0 such that SΘ (θ0 ) ∩ N (θ0 ) = {θ0 }. This proves (i).
def

To show (ii), deﬁne E = Θ \ Θmax , and now claim that for every θ0 ∈ Θmax , the equivalence
class SΘmax (θ0 ) is ﬁnite. Observe that by (i), the set SΘmax (θ0 ) contains only geometrically isolated
solutions to the system of polynomial equations given by µ(θ) = µ(θ0 ). Therefore the claim follows immediately from B´ zout’s Theorem, which implies that the number of geometrically isolated
e
solutions is ﬁnite.
Remark. All the models considered in this paper have moments µ which correspond to a polynomial map. However, for some models (e.g., exponential families), µ will not be a polynomial map,
but rather, a general analytic function. In this case, Theorem 1 holds with one modiﬁcation to (ii).
If C HECK I DENTIFIABILITY returns “yes”, then we have the following weaker guarantee in place of
(ii): SΘmax (θ0 ) is countable (but not necessarily ﬁnite) for all θ0 ∈ Θmax . The above proof does not
require the fact that µ is a polynomial map except in the invocation of B´ zout’s Theorem. In place
e
of B´ zout’s Theorem, we use the following argument. If SΘmax (θ0 ) is uncountable, then it contains
e
a limit point θ∗ ∈ SΘmax (θ0 ); thus for any small enough neighborhood N (θ∗ ) of θ∗ , there is some
θ ∈ SΘmax (θ0 ) ∩ N (θ∗ ). This contradicts (i) as applied to θ∗ , and thus we conclude that SΘmax (θ0 )
is countable.

B

Additional results from the identiﬁability checker

PCFG models with d < k. The PCFG models that we’ve considered so far assume that the
number of words d is at least the number of hidden states k, which is a realistic assumption for
natural language. However, there are applications, e.g., computational biology, where the vocabulary
size d is relatively small. In this regime, identiﬁability becomes trickier because the data doesn’t
10

reveal as much about the hidden states, and brings us closer to the boundary between identiﬁability
and non-identiﬁability. In this section, we consider the d < k regime.
The following table gives additional identiﬁability results from C HECK I DENTIFIABILITY for values
of d, k, and L where d < k (recall that the results reported in Section 4.4 only considered values
where d ≥ k). In each cell, we show the (k, d, L) values for which C HECK I DENTIFIABILITY
returned “yes”; the values checked were k ∈ {3, 4, . . . , 8}, d ∈ {2, . . . , k − 1}, L ∈ {3, 4, . . . , 9}.
φ12

φ∗∗

φ123e1

φ123
None

PCFG

PCFG-I

PCFG-IE

None

None

(3, 2, ≥ 6)
(4, 2, ≥ 8)
(4, 3, ≥ 5)
(5, 3, ≥ 6)
(5, 4, ≥ 4)
(6, 3, ≥ 7)
(6, 4, ≥ 5)
(6, 5, ≥ 4)
(7, 3, ≥ 8)
(7, 4, ≥ 6)
(7, 5, ≥ 5)
(7, 6, ≥ 4)
(3, 2, ≥ 6)
(4, 2, ≥ 8)
(4, 3, ≥ 5)
(5, 3, ≥ 6)
(5, 4, ≥ 5)
(6, 3, ≥ 7)
(6, 4, ≥ 5)
(6, 5, ≥ 4)
(7, 3, ≥ 8)
(7, 4, ≥ 6)
(7, 5, ≥ 5)
(7, 6, ≥ 4)

(4, 3, ≥ 4)
(5, 4, ≥ 4)
(6, ≥ 4, ≥ 4)
(7, ≥ 5, ≥ 4)

φ∗∗∗

(3, 2, ≥ 5)
(4, 2, ≥ 6)
(4, 3, ≥ 4)
(5, 2, ≥ 7)
(5, ≥ 3, ≥ 4)
(6, 2, ≥ 8)
(6, 3, ≥ 5)
(6, ≥ 4, ≥ 4)
(7, 2, ≥ 9)
(7, 3, ≥ 5)
(7, ≥ 4, ≥ 4)

(5, 4, ≥ 4)
(6, 5, ≥ 4)
(7, 5, ≥ 4)
(7, 6, ≥ 4)

None

(5, 4, ≥ 4)
(6, 5, ≥ 4)
(7, 5, ≥ 5)
(7, 6, ≥ 4)

φ∗∗∗e1

(3, 2, ≥ 5)
(4, 2, ≥ 6)
(4, 3, ≥ 4)
(5, 2, ≥ 7)
(5, 3, ≥ 5)
(5, 4, ≥ 4)
(6, 2, ≥ 8)
(6, 3, ≥ 5)
(6, ≥ 4, ≥ 4)
(7, 2, ≥ 9)
(7, 3, ≥ 5)
(7, ≥ 4, ≥ 4)

(3, 2, ≥ 5)
(4, 2, ≥ 6)
(4, 3, ≥ 4)
(5, 2, ≥ 7)
(5, ≥ 3, ≥ 4)
(6, 2, ≥ 8)
(6, 3, ≥ 5)
(6, ≥ 4, ≥ 4)
(7, 2, ≥ 9)
(7, 3, ≥ 5)
(7, ≥ 4, ≥ 4)

Fixed topology models. We now present some results for latent class models (LCMs) and hidden
Markov models (HMMs). While identiﬁability for these models are more developed than for parsing
models, we show that the identiﬁability checker can reﬁne the results even for the classic models.
The parameters of an HMM are θ = (π, T, O), where π ∈ Rk speciﬁes the initial state distribution, T ∈ Rk×k speciﬁes the state transition probabilities, and O ∈ Rd×k speciﬁes the emission
distributions. The probability over a sentence x is:
Pθ (x) = 1 T diag(O xL ) · · · T diag(O x2 )T diag(O x1 )π.

(5)

The parameters of an LCM are θ = (π, O)—the same as that of an HMM except with T ≡ I. The
probability over a sentence x is also given by (5) (with T = I).
The following table summarizes some identiﬁability results obtained by C HECK I DENTIFIABILITY
(for d ≥ k); these results have all been proven analytically in previous work (e.g., [8, 10, 11, 20, 21])
except for the identiﬁability of HMMs from φ∗∗ .
φ12
LCM
HMM

No

φ∗∗
No

φ123e1

φ123 φ∗∗∗e1
Yes iff L ≥ 3
Yes iff L ≥ 3

φ∗∗∗

It is known that LCMs are not identiﬁable from φ∗∗ for any value of L [8]. However, LCMs constitute a subfamily of HMMs arising from a measure zero subset of the HMM parameter space.
Therefore the identiﬁability of HMMs from φ∗∗ (for L ≥ 3) does not contradict this result. The
result does not appear to be covered by application of Kruskal’s theorem in previous work [11], so
we prove the result rigorously below.
11

It can be checked using (5) that
Eθ [φ12 (x)] = O diag(π)T O
Eθ [φ34 (x)] = O diag(T π)T O .
def

def

def

Let M1 = O, M2 = OT diag(π), and D = diag(T π) diag(π)−1 . Provided that
1. π > 0,
2. O has full column rank,
3. T is invertible,
4. the ratios of probabilities (T π)i /πi , ranging over i ∈ [k], are distinct
(all of which are true for all but a measure zero set of parameters in Θ), the matrices M1 and M2
have full column rank and the diagonal matrix D has distinct diagonal entries. Therefore Lemma 1
can be applied with X = Eθ [φ12 (x)] = M1 M2 and Y = Eθ [φ34 (x)] = M1 DM2 to recover
M1 = O. It is easy to see that π and T can also easily be recovered.
Note that the fourth condition above, that T π be entry-wise distinct from π, is violated when a LCM
distribution is cast as an HMM distribution (by setting T = I so T π = π). However, the set of
HMM parameters satisfying this equation is a measure zero set.
Discussion. C HECK I DENTIFIABILITY tests for local identiﬁability. If it ﬁnds that a model family
is not locally identiﬁable, then it is not globally identiﬁable. However the inverse claim is not
necessarily true: if it ﬁnds that a model family is locally identiﬁable, it is not necessarily globally
identiﬁable. Theorem 1 provides the somewhat weaker guarantee that a restricted model family is
globally identiﬁable, where the equivalence classes SΘ\E (θ0 ) are only taken with respect to a subset
Θ \ E ⊆ Θ of the parameter space. However, there is a gap between this property (which is with
respect to Θ \ E) and true global identiﬁability (which is with respect to Θ).
On the other hand, having explicit estimators guarantees us proper global identiﬁability with respect
to the original model family Θ. In fact, the exceptional set E can typically be characterized explicitly.
For instance, in the case of PCFG-IE, the set Θ \ E contains those θ = (π, T, O) that satisfy full
rank conditions:
Θ \ E = {(π, T, O) : π

0, T is invertible, O has full column rank}.

(6)

Additionally, the explicit estimators also provides an explicit characterization of the elements in
the equivalence class SΘ (θ0 ) for each θ0 ∈ Θ \ E: the set SΘ (θ0 ) contains exactly k! elements
corresponding to permutation of the hidden states. Speciﬁcally,
SΘ ((π, T, O)) = {(Π−1 π, Π−1 T Π, OΠ) : Π is a permutation matrix.

(7)

Note that this is shaper than Theorem 1, which only says that the equivalence classes have to be
ﬁnite.

C

Dynamic programs

For a sentence of length L, the number of parse trees is exponential in L. Therefore, dynamic
programming is often employed to efﬁciently compute expectations over the parse trees, the core
computation in the E-step of the EM algorithm. In the case of PCFG, this dynamic program is
referred to as the CKY algorithm, which runs in O(L3 k 3 ) time, where k is the number of hidden
states. For simple dependency models, a O(L3 ) dynamic program was developed by [29]. At a
high-level, the states of the dynamic program in both cases are the spans [i : j] of the sentence (and
for the PCFG, the these states include the hidden states z[i:j] of the nodes).
In this paper, we need to compute (i) the Jacobian matrix for checking identiﬁability (Section 4.2)
and (ii) the mixing matrix for recovering compound parameters (Section 5.1). Both computations
can be performed efﬁciently with a modiﬁed version of the classic dynamic programs, which we
will describe in this section.
12

C.1

Computing the Jacobian matrix

Recall that the j-th row of the Jacobian matrix J is (the transpose of) the gradient of hj (θ) =
µj (θ) − µj (θ0 ). Speciﬁcally, entry Jji is the derivative of the j-th moment with respect to the i-th
parameter:
∂hj (θ)
Jji =
(8)
∂θi
∂Eθ [φj (x)]
=
(9)
∂θi
∂pθ (x, z)
=
φj (x).
(10)
∂θi
x,z
We can encode the sum over the exponential set of possible sentences x and parse trees z using a
directed acyclic hypergraph so that each hyperpath through the hypergraph corresponds to a (x, z)
pair. Speciﬁcally, a hypergraph consists of the following:
• a set of nodes V with a designated start node S TART ∈ V and an end node E ND ∈ V, and
• a set of hyperedges E where each hyperedge e ∈ E has a source node e.a ∈ V and a pair
of target nodes (e.b, e.c) ∈ V × V (we say that e connects e.a to e.b and e.c) and an index
e.i ∈ [n] corresponding to a component of the parameter vector θ ∈ Rn .
Deﬁne a hyperpath P to be a subset of the edges E such that:
• (S TART, a, b) ∈ P for some a, b ∈ V;
• if (a, b, c) ∈ P and b = E ND, then (b, d, e) ∈ P for some d, e ∈ V; and
• if (a, b, c) ∈ P and c = E ND, then (c, d, e) ∈ P for some d, e ∈ V.
Each hyperpath P , encoding (x, z), is associated with a probability equal to the product of all of the
parameters on that hyperpath:
pθ (x, z) = pθ (P ) =

θe.i .

(11)

e∈P

In this way, the hypergraph compactly deﬁnes a distribution over exponentially many hyperpaths.
Now, we assume that each moment φj (x) corresponds to a function fj : E → R mapping each
hyperedge e to a real number so that the moment is equal to the product over function values:
φj (x) =

fj (e),

(12)

e∈P

where P is any hyperpath that encodes the sentence x and some parse tree z (we assume that the
product is the same no matter what z is).
Now, let us write out the Jacobian matrix entries in terms of hyperpaths:
∂θe0 .i
θe.i fj (e).
Jji =
∂θi
P e0 ∈P

(13)

e∈P,e=e0

The sum over hyperpaths P can be computed efﬁciently as follows. For each hypergraph node a,
we compute an inside score α(a), which sums over all possible partial hyperpaths terminating at
the target node, and an outside score β(a), which sums over all possible partial hyperpaths from the
source node:
def
α(a) =
θe.i α(e.b)α(e.c),
(14)
e∈E:e.a=a
def

β(a) =

θe.i α(e.c)β(e.a)

θe.i α(e.b)β(e.a).

(15)

e∈E:e.c=a

e∈E:e.b=a

The Jacobian entry Jji can be computed as follows:
Jji =

β(e.a)α(e.b)α(e.c)I[i = e.i].
e∈E

13

(16)

π

π

z03

z03

T

z02
T

z01

T

T

z23

T

O

z12

T

z01
O

x3

z13

x1

T

z12

T

z23

O

O

O

O

x1

x2

x2

x3

Topology(z) = 1

Topology(z) = 2

Figure 3: An example of a backbone structure in blue corresponding to the compound parameter
OT diag(T π)T O , which appears in two different topologies, for two observation matrices, φ12
and φ23 , respectively.
Example: PCFG. For a PCFG, nodes V have the form (i, j, s) ∈ [L] × [L] × [k], corresponding to
a hidden state s over span [i : j]. For each hidden state s, we have a hyperedge e connecting e.a =
S TART to e.b = (s, 0, L) and e.c = E ND; this hyperedge has parameter index e.i corresponding to
πs . For each span [i : j] with j − i > 1, split point i < m < j, and hidden states s1 , s2 , s3 ∈ [k],
E contains a hyperedge e connecting e.a = (i, j, s1 ) to e.b = (i, m, s2 ) and e.c = (m, j, s3 ); the
parameter index e.i corresponds to the binary production B(s2 ⊗k s3 )s1 . For each span [i − 1 : i],
hidden state s ∈ [k] and word x ∈ [d], we have a hyperedge e connecting e.a = (i − 1, i, s) to
e.b = E ND and e.c = E ND with parameter index e.i corresponding to the emission Oxs .
The moments can be encoded as follows: For example, if φj (x) = I[xi = t], then we deﬁne fj (e)
to be 0 if the source node corresponds to position i (e.a = (i − 1, i, s)) and the parameter index e.i
does not correspond to Ots for some s ∈ [k], and 1 otherwise. In this way, e∈P fj (e) is zero if P
encodes a sentence with xi = t.
Higher-order moments simply correspond to hyperedge-wise multiplication of these ﬁrst-order moments. For example, if φj1 (x) = I[xi1 = t1 ] and φj2 (x) = I[xi2 = t2 ], then the second-order
moment φj (x) = I[xi1 = t1 , xi2 = t2 ] corresponds to fj (e) = fj1 (e)fj2 (e).
C.2

Computing the mixing matrix

Recall that the mixing matrix M includes a row for each observation matrix o ∈ O and a column for
each compound parameter p ∈ P. Assuming a uniform distribution over topologies, computing each
entry of M reduces to counting the number of topologies t consistent with a particular compound
parameter Ψp :
Mop = P(Ψo,Topology = Ψp )

(17)

= | Topologies |−1

(18)

I[Ψo,t = Ψp ].
t

First, we will characterize the set of compound parameters graphically in terms of backbone structures. As an example, consider the PCFG-IE model and the observation matrix φ12 (o = 12) corresponding to the marginal distribution over the ﬁrst two words of the sentence. Given a topology
t, consider starting at the root, descending to the lowest common ancestor of x1 and x2 , and then
following both paths down to x1 and x2 , respectively. We refer to this traversal as the backbone
structure with respect to topology t and observation matrix φ12 . See Figure 3 for an example of the
backbone structure, outlined in blue.
Note that the compound parameter Ψ12,t (θ) = Eθ [φ12 (x) | Topology = t] can be written as a
product over the parameter matrices, one for each edge of the backbone structure. For Figure 3, this
would yield
Ψ12,1 (θ) = OT diag(T π)T O .
14

(19)

For general trees, we would have
Ψ12,t (θ) = OT n1 diag(T n3 π)(T )n2 O .

(20)

for some positive integers n1 , n2 , n3 corresponding to the number of edges (in t) from the common
node to the preterminal node z01 , the preterminal node z12 , and the root z0L , respectively.
Note that the compound parameter does not depend on the structure of t outside the backbone—
that part of the topology is effectively marginalized out—so the compound parameter Ψ12,t (θ) will
be identical for all topologies sharing that same backbone structure. Therefore, there are only a
polynomial number of compound parameters despite an exponential number of topologies t.
We deﬁne a dynamic program that recursively computes Mop for the PCFG-IE model under a ﬁxed
second-order observation matrix φi0 j0 . Speciﬁcally, for each span [i : j] deﬁne H(i, j) to be the set
of pairs t, n where t is a partial backbone structure t and n is the number of partial topologies over
span [i : j] which are consistent with t.
In the base case H(i − 1, i), if i is either of the designated leaf positions deﬁned by the observation
matrix (i0 or j0 ), then we return the single-node backbone structure •; otherwise, we return the null
backbone structure ø:
H(i − 1, i) =

{ ø, 1 }
{ •, 1 }

if i = i0 or i = j0
otherwise.

(21)

In the recursive case H(i, j), we consider all split points m, partial backbones t1 and t2 from
H(i, m) and H(m, j), respectively, and create a new tree with t1 and/or t2 as the subtrees if they
are not null:
+

+

+

C OMBINE(t1 , t2 ), n1 n2 ,

H(i, j) =

(22)

i<m<j t1 ,n1 ∈H(i,m) t2 ,n2 ∈H(m,j)


(T : t1 , T : t2 ) if t1 = ø and t2 = ø,


T : t1
if t1 = ø,
C OMBINE(t1 , t2 ) =
T : t 2
if t2 = ø,


ø
otherwise.

(23)

+

Here, we use the notation
to denote a multi-set union: { t, n1 } ∪+ { t, n2 } = { t, n1 + n2 }.
In this notation, the backbone structure in Figure 3 would be represented as T : (T : O : •, T : O :
•), which can be easily converted to the compound parameter OT diag(T π)T O .
For third-order observation matrices (e.g., φi0 j0 k0 η ), we add an additional case to H(i−1, i) to return
◦, 1 if i = k0 ; note that k0 is represented by a special node ◦ because that observation is projected
using η. The ﬁrst case of C OMBINE(t1 , t2 ) undergoes one change: if t2 is a chain ending in ◦, then
we return (T : t2 , T : t1 ). The reason for this is best demonstrated by an example: consider topology
1 in Figure 3, and the two observation matrices φ132η and φ231η . Without the reordering, we would
have the backbone structure: (T : (T : •, T : ◦), T : •) and (T : (T : ◦, T : •), T : •). However,
they have the same compound parameter OT diag(T O η)T diag(π)T O. This is because the
contribution of a subtree ending in ◦ is simply a diagonal matrix (diag(T O η) in this case) which
is applied on the hidden state regardless of whether it came from the left or right side.

One might also see why the unmixing technique does not directly apply to the PCFG-I model, where T is
replaced with T1 for left edges and T2 for right edges. In that case, there are many backbone structures (and
thus more compound parameters) due to the different interleavings of left and right edges.

15

