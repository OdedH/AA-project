Learning Minimal Abstractions
POPL - Austin, TX

January 26, 2011

Percy Liang

Omer Tripp

Mayur Naik

UC Berkeley

Tel-Aviv Univ.

Intel Labs Berkeley

The Minimal Abstraction Problem

2

The Minimal Abstraction Problem
Given a family of abstractions A

2

The Minimal Abstraction Problem
Given a family of abstractions A
ﬁnest/most precise

A

coarsest/least precise

2

The Minimal Abstraction Problem
Given a family of abstractions A and a client query q...
ﬁnest/most precise

A

coarsest/least precise

2

The Minimal Abstraction Problem
Given a family of abstractions A and a client query q...
ﬁnest/most precise

proves q
A

¬ proves q
coarsest/least precise

2

The Minimal Abstraction Problem
Given a family of abstractions A and a client query q...
ﬁnest/most precise

proves q
A

¬ proves q
coarsest/least precise

What is the coarsest abstraction a ∈ A that proves the query q?
2

Motivating application: race detection
getnew() {
h1:
z1 = new C
h2:
z2 = new C
return z2
}

// Thread 1
x = getnew()
x.f = ...

// Thread 2
y = getnew()
y.f = ...

3

Motivating application: race detection
getnew() {
h1:
z1 = new C
h2:
z2 = new C
return z2
}

// Thread 1
x = getnew()
x.f = ...

// Thread 2
y = getnew()
y.f = ...

Query: is there a data race between x.f = ... and y.f = ...? no

3

Motivating application: race detection
getnew() {
h1:
z1 = new C
h2:
z2 = new C
return z2
}

// Thread 1
x = getnew()
x.f = ...

// Thread 2
y = getnew()
y.f = ...

Query: is there a data race between x.f = ... and y.f = ...? no
Heap abstractions: for each allocation site, context sensitive (1) or not (0)

3

Motivating application: race detection
getnew() {
h1:
z1 = new C
h2:
z2 = new C
return z2
}

// Thread 1
x = getnew()
x.f = ...

// Thread 2
y = getnew()
y.f = ...

Query: is there a data race between x.f = ... and y.f = ...? no
Heap abstractions: for each allocation site, context sensitive (1) or not (0)

h1 h2
0 0
¬ proves q
3

Motivating application: race detection
getnew() {
h1:
z1 = new C
h2:
z2 = new C
return z2
}

// Thread 1
x = getnew()
x.f = ...

// Thread 2
y = getnew()
y.f = ...

Query: is there a data race between x.f = ... and y.f = ...? no
Heap abstractions: for each allocation site, context sensitive (1) or not (0)
h1 h2
1 1
proves q

h1 h2
0 0
¬ proves q
3

Motivating application: race detection
getnew() {
h1:
z1 = new C
h2:
z2 = new C
return z2
}

// Thread 1
x = getnew()
x.f = ...

// Thread 2
y = getnew()
y.f = ...

Query: is there a data race between x.f = ... and y.f = ...? no
Heap abstractions: for each allocation site, context sensitive (1) or not (0)
h1 h2
1 1
proves q

h1 h2
1 0

h1 h2
0 1

¬ proves q

proves q

h1 h2
0 0
¬ proves q
3

Motivating application: race detection
getnew() {
h1:
z1 = new C
h2:
z2 = new C
return z2
}

// Thread 1
x = getnew()
x.f = ...

// Thread 2
y = getnew()
y.f = ...

Query: is there a data race between x.f = ... and y.f = ...? no
Heap abstractions: for each allocation site, context sensitive (1) or not (0)
h1 h2
1 1

best

proves q

h1 h2
1 0

h1 h2
0 1

¬ proves q

proves q

h1 h2
0 0
¬ proves q
3

The landscape
Motivating problem:
Given a query, try to prove it
as cheaply as possible

4

The landscape
Motivating problem:
Given a query, try to prove it
as cheaply as possible
Existing solutions:
Abstraction reﬁnement [Guyer & Lin 2003]
[Heintze & Tardieu 2001]
[Sridharan et al. 2005]
[Zheng & Rugina 2008] ...

4

The landscape
ﬁnest/most precise

Motivating problem:
Given a query, try to prove it
as cheaply as possible
Existing solutions:

A

Abstraction reﬁnement [Guyer & Lin 2003]
[Heintze & Tardieu 2001]
[Sridharan et al. 2005]
[Zheng & Rugina 2008] ...
coarsest/least precise

4

The landscape
ﬁnest/most precise

Motivating problem:
Given a query, try to prove it
as cheaply as possible
Existing solutions:

A

Abstraction reﬁnement [Guyer & Lin 2003]
[Heintze & Tardieu 2001]
[Sridharan et al. 2005]
[Zheng & Rugina 2008] ...
coarsest/least precise
ﬁnest/most precise

Our problem (scientiﬁc question):
Given that we’ve proved a query,
cheapest abstraction in hindsight?

proves q
A

¬ proves q
coarsest/least precise

4

The landscape
ﬁnest/most precise

Motivating problem:
Given a query, try to prove it
as cheaply as possible
Existing solutions:

A

Abstraction reﬁnement [Guyer & Lin 2003]
[Heintze & Tardieu 2001]
[Sridharan et al. 2005]
[Zheng & Rugina 2008] ...
coarsest/least precise
ﬁnest/most precise

Our problem (scientiﬁc question):
Given that we’ve proved a query,
cheapest abstraction in hindsight?
Suﬃcient/necessary conditions:
what aspects of program to model?

proves q
A

¬ proves q
coarsest/least precise

4

Binary representation
Abstraction a ∈ A is a binary vector (subset of components):
a = 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 (most precise)

A

a = 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 (least precise)

5

Binary representation
Abstraction a ∈ A is a binary vector (subset of components):
a = 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 (most precise)

A

a = 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 (least precise)

Examples:
k-limited [Milanova et al. 2002]: treat site context-sensitively?
Predicate abstraction [Ball et al. 2001]: include predicate?
Shape analysis [Sagiv et al. 2002]: treat as abstraction predicate?
5

Finding a minimal abstraction
Given a static analysis F:
001000111111010010111011001110
F
0 (proven) OR 1 (not proven)

6

Finding a minimal abstraction
Given a static analysis F:
001000111111010010111011001110
F
0 (proven) OR 1 (not proven)
Goal: ﬁnd a minimal abstraction a (not necessarily unique):
(i) F(a) = 0 (proves the query)
(ii) For a
a, F(a ) = 1 (can’t coarsen locally)

6

Finding a minimal abstraction
Given a static analysis F:
001000111111010010111011001110
F
0 (proven) OR 1 (not proven)
Goal: ﬁnd a minimal abstraction a (not necessarily unique):
(i) F(a) = 0 (proves the query)
(ii) For a
a, F(a ) = 1 (can’t coarsen locally)
Challenge: |A| = 2# components abstractions to consider

6

Finding a minimal abstraction
Given a static analysis F:
001000111111010010111011001110
F
0 (proven) OR 1 (not proven)
Goal: ﬁnd a minimal abstraction a (not necessarily unique):
(i) F(a) = 0 (proves the query)
(ii) For a
a, F(a ) = 1 (can’t coarsen locally)
Challenge: |A| = 2# components abstractions to consider
Approach: machine learning algorithms that exploit randomization

6

Key theme: sparsity
Sparsity hypothesis:
Only a small fraction of components of a need to be reﬁned
a=0100000000000000000101010

7

Key theme: sparsity
Sparsity hypothesis:
Only a small fraction of components of a need to be reﬁned
a=0100000000000000000101010

Main results:

7

Key theme: sparsity
Sparsity hypothesis:
Only a small fraction of components of a need to be reﬁned
a=0100000000000000000101010

Main results:
Theoretical: machine learning algorithms are eﬃcient under sparsity

7

Key theme: sparsity
Sparsity hypothesis:
Only a small fraction of components of a need to be reﬁned
a=0100000000000000000101010

Main results:
Theoretical: machine learning algorithms are eﬃcient under sparsity
Empirical: for k-limited race detection,
only 0.4%–2.3% components need to be 1!

7

Key theme: sparsity
Sparsity hypothesis:
Only a small fraction of components of a need to be reﬁned
a=0100000000000000000101010

Main results:
Theoretical: machine learning algorithms are eﬃcient under sparsity
Empirical: for k-limited race detection,
only 0.4%–2.3% components need to be 1!
(eﬀectively “0.004-CFA”–“0.023-CFA”)

7

Algorithms
ﬁnest/most precise

A

coarsest/least precise

8

Algorithms
ﬁnest/most precise

Reﬁne
BasicRefine
StatRefine

A

coarsest/least precise

8

Algorithms
ﬁnest/most precise

Reﬁne
BasicRefine
StatRefine

Coarsen
ScanCoarsen
ActiveCoarsen

A

coarsest/least precise

8

BasicRefine
Idea: start with imprecise a, incrementally reﬁne “relevant” components

9

BasicRefine
Idea: start with imprecise a, incrementally reﬁne “relevant” components
a ← (0, . . . , 0)
Loop:
Run analysis F(a)
Find relevant components by cause-eﬀect analysis
Add these components to a

9

BasicRefine
Idea: start with imprecise a, incrementally reﬁne “relevant” components
a ← (0, . . . , 0)
Loop:
Run analysis F(a)
Find relevant components by cause-eﬀect analysis
Add these components to a
Reasonable iterative reﬁnement baseline

9

BasicRefine
Idea: start with imprecise a, incrementally reﬁne “relevant” components
a ← (0, . . . , 0)
Loop:
Run analysis F(a)
Find relevant components by cause-eﬀect analysis
Add these components to a
Reasonable iterative reﬁnement baseline
Solves the motivating problem of proving a new query cheaply

9

BasicRefine
Idea: start with imprecise a, incrementally reﬁne “relevant” components
a ← (0, . . . , 0)
Loop:
Run analysis F(a)
Find relevant components by cause-eﬀect analysis
Add these components to a
Reasonable iterative reﬁnement baseline
Solves the motivating problem of proving a new query cheaply
Does not solve the minimal abstraction problem (it reﬁnes too much)

9

ScanCoarsen
Idea: start with most precise a, incrementally discard components

10

ScanCoarsen
Idea: start with most precise a, incrementally discard components
a ← (1, . . . , 1)
Loop:
Remove a component from a
Run analysis F(a)
If F(a) = 1: add component back permanently

10

ScanCoarsen
Idea: start with most precise a, incrementally discard components
a ← (1, . . . , 1)
Loop:
Remove a component from a
Run analysis F(a)
If F(a) = 1: add component back permanently
Exploits monotonicity of F:
Component whose removal causes F(a) = 1 must exist in min. abstraction
⇒ never visit a component more than once

10

ScanCoarsen
Idea: start with most precise a, incrementally discard components
a ← (1, . . . , 1)
Loop:
Remove a component from a
Run analysis F(a)
If F(a) = 1: add component back permanently
Exploits monotonicity of F:
Component whose removal causes F(a) = 1 must exist in min. abstraction
⇒ never visit a component more than once
Problem: takes O(# components) time (can be > 10, 000 ⇒ > 30 days)

10

StatRefine
Idea: run F on random a, learn correlations between components and F(a)

11

StatRefine
Idea: run F on random a, learn correlations between components and F(a)
Loop:
Gather n training examples (a, F(a)) where p(aj = 1) = α
Add component j with largest # of a with aj = 1 and F(a) = 0

11

StatRefine
Idea: run F on random a, learn correlations between components and F(a)
Loop:
Gather n training examples (a, F(a)) where p(aj = 1) = α
Add component j with largest # of a with aj = 1 and F(a) = 0
Example: F(a) = ¬(a4 ∧ a9 ∧ a11 )
1
1
1
1
0
0
1
1
1
0
1
0
1
1
0
1
1
1

0
1
1
1
1
0
1
1
1
1
1
1
1
0
0
1
1
1

0
0
0
1
1
1
1
0
0
0
1
1
1
1
1
1
1
1

1
1
1
1
1
1
0
1
0
1
1
1
1
1
0
1
1
1

1
1
1
1
1
1
1
1
1
1
1
1
1
0
0
0
1
0

1
1
1
0
0
0
1
1
1
1
1
1
1
0
1
1
0
0

1
0
1
1
0
0
1
1
0
1
1
1
0
1
1
1
0
1

1
0
1
1
1
1
1
1
0
1
1
1
0
1
1
1
1
1

1
1
1
1
1
1
0
1
1
1
1
1
1
0
0
1
1
1

1
0
1
1
0
1
1
0
1
0
1
1
1
1
1
1
0
1

1
0
1
1
1
1
0
1
1
1
0
0
1
1
1
1
1
1

1
1
1
1
1
1
1
0
1
0
1
1
1
1
1
1
1
1

1
1
1
1
0
0
1
0
1
1
1
1
0
1
1
1
1
1

1
1
1
1
1
1
1
0
0
1
1
0
1
1
1
1
1
1

1
1
1
1
1
1
0
1
1
1
1
1
0
1
1
1
1
1

1
1
1
1
1
1
1
1
0
0
1
1
1
1
1
1
1
1

1
1
1
1
1
1
1
1
1
1
1
1
0
1
0
1
1
0

1
1
1
1
1
1
0
0
0
1
1
1
1
1
0
0
1
0

1
1
1
1
1
1
1
0
0
1
1
1
1
0
1
1
0
1

1
1
1
1
0
1
1
1
1
1
1
1
1
1
1
0
1
1

⇒
⇒
⇒
⇒
⇒
⇒
⇒
⇒
⇒
⇒
⇒
⇒
⇒
⇒
⇒
⇒
⇒
⇒

0
1
0
0
0
0
1
0
1
0
1
1
0
1
1
0
0
0
11

StatRefine
Idea: run F on random a, learn correlations between components and F(a)
Loop:
Gather n training examples (a, F(a)) where p(aj = 1) = α
Add component j with largest # of a with aj = 1 and F(a) = 0
Example: F(a) = ¬(a4 ∧ a9 ∧ a11 )
1
1
1
1
0
0
1
1
1
0
1
0
1
1
0
1
1
1

0
1
1
1
1
0
1
1
1
1
1
1
1
0
0
1
1
1

0
0
0
1
1
1
1
0
0
0
1
1
1
1
1
1
1
1

1
1
1
1
1
1
0
1
0
1
1
1
1
1
0
1
1
1

1
1
1
1
1
1
1
1
1
1
1
1
1
0
0
0
1
0

1
1
1
0
0
0
1
1
1
1
1
1
1
0
1
1
0
0

1
0
1
1
0
0
1
1
0
1
1
1
0
1
1
1
0
1

1
0
1
1
1
1
1
1
0
1
1
1
0
1
1
1
1
1

1
1
1
1
1
1
0
1
1
1
1
1
1
0
0
1
1
1

1
0
1
1
0
1
1
0
1
0
1
1
1
1
1
1
0
1

1
0
1
1
1
1
0
1
1
1
0
0
1
1
1
1
1
1

1
1
1
1
1
1
1
0
1
0
1
1
1
1
1
1
1
1

1
1
1
1
0
0
1
0
1
1
1
1
0
1
1
1
1
1

1
1
1
1
1
1
1
0
0
1
1
0
1
1
1
1
1
1

1
1
1
1
1
1
0
1
1
1
1
1
0
1
1
1
1
1

1
1
1
1
1
1
1
1
0
0
1
1
1
1
1
1
1
1

1
1
1
1
1
1
1
1
1
1
1
1
0
1
0
1
1
0

1
1
1
1
1
1
0
0
0
1
1
1
1
1
0
0
1
0

1
1
1
1
1
1
1
0
0
1
1
1
1
0
1
1
0
1

1
1
1
1
0
1
1
1
1
1
1
1
1
1
1
0
1
1

⇒
⇒
⇒
⇒
⇒
⇒
⇒
⇒
⇒
⇒
⇒
⇒
⇒
⇒
⇒
⇒
⇒
⇒

0
1
0
0
0
0
1
0
1
0
1
1
0
1
1
0
0
0
11

StatRefine
Idea: run F on random a, learn correlations between components and F(a)
Loop:
Gather n training examples (a, F(a)) where p(aj = 1) = α
Add component j with largest # of a with aj = 1 and F(a) = 0
Example: F(a) = ¬(a4 ∧ a9 ∧ a11 )
1
1
1
1
0
0
1
1
1
0
1
0
1
1
0
1
1
1
8

0
1
1
1
1
0
1
1
1
1
1
1
1
0
0
1
1
1
9

0 1
0 1
0 1
1 1
1 1
1 1
1 0
0 1
0 0
0 1
1 1
1 1
1 1
1 1
1 0
1 1
1 1
1 1
7 11

1
1
1
1
1
1
1
1
1
1
1
1
1
0
0
0
1
0
9

1
1
1
0
0
0
1
1
1
1
1
1
1
0
1
1
0
0
6

1 1
0 0
1 1
1 1
0 1
0 1
1 1
1 1
0 0
1 1
1 1
1 1
0 0
1 1
1 1
1 1
0 1
1 1
7 10

1
1
1
1
1
1
0
1
1
1
1
1
1
0
0
1
1
1
11

1 1
0 0
1 1
1 1
0 1
1 1
1 0
0 1
1 1
0 1
1 0
1 0
1 1
1 1
1 1
1 1
0 1
1 1
7 11

1
1
1
1
1
1
1
0
1
0
1
1
1
1
1
1
1
1
9

1 1
1 1
1 1
1 1
0 1
0 1
1 1
0 0
1 0
1 1
1 1
1 0
0 1
1 1
1 1
1 1
1 1
1 1
7 10

1
1
1
1
1
1
0
1
1
1
1
1
0
1
1
1
1
1
10

1
1
1
1
1
1
1
1
0
0
1
1
1
1
1
1
1
1
10

1
1
1
1
1
1
1
1
1
1
1
1
0
1
0
1
1
0
9

1
1
1
1
1
1
0
0
0
1
1
1
1
1
0
0
1
0
8

1
1
1
1
1
1
1
0
0
1
1
1
1
0
1
1
0
1
9

1
1
1
1
0
1
1
1
1
1
1
1
1
1
1
0
1
1
9

⇒
⇒
⇒
⇒
⇒
⇒
⇒
⇒
⇒
⇒
⇒
⇒
⇒
⇒
⇒
⇒
⇒
⇒

0
1
0
0
0
0
1
0
1
0
1
1
0
1
1
0
0
0
11

StatRefine
Theorem:
s = # components in the largest minimal abstraction
d = # components in any minimal abstraction
|J| = total # components

12

StatRefine
Theorem:
s = # components in the largest minimal abstraction
d = # components in any minimal abstraction
|J| = total # components
If:
Set reﬁnement probability α =

s
s+1

12

StatRefine
Theorem:
s = # components in the largest minimal abstraction
d = # components in any minimal abstraction
|J| = total # components
If:
Set reﬁnement probability α =

s
s+1

Obtain n = Ω(d2 log |J|) examples

12

StatRefine
Theorem:
s = # components in the largest minimal abstraction
d = # components in any minimal abstraction
|J| = total # components
If:
Set reﬁnement probability α =

s
s+1

Obtain n = Ω(d2 log |J|) examples
Then:
StatRefine outputs a minimal abstraction with high probability
in O(sd2 log |J|) time
Signiﬁcance: s, d are small,
only logarithmic dependence on total # components

12

StatRefine
Theorem:
s = # components in the largest minimal abstraction
d = # components in any minimal abstraction
|J| = total # components
If:
Set reﬁnement probability α =

s
s+1

Obtain n = Ω(d2 log |J|) examples
Then:
StatRefine outputs a minimal abstraction with high probability
in O(sd2 log |J|) time
Signiﬁcance: s, d are small,
only logarithmic dependence on total # components
Proof sketch: large deviation bounds + optimization over α
12

ActiveCoarsen algorithm
Idea: try to remove a constant fraction of components in each step

13

ActiveCoarsen algorithm
Idea: try to remove a constant fraction of components in each step
a ← (1, . . . , 1)
Loop:
Try removing each component with probability 1 − α
Run analysis F(a)
If F(a) = 1: add components back
Else: remove components permanently

13

ActiveCoarsen algorithm
Idea: try to remove a constant fraction of components in each step
a ← (1, . . . , 1)
Loop:
Try removing each component with probability 1 − α
Run analysis F(a)
If F(a) = 1: add components back
Else: remove components permanently
Example: F(a) = ¬(a4 ∧ a9 ∧ a11 )
1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ⇒ 0

13

ActiveCoarsen algorithm
Idea: try to remove a constant fraction of components in each step
a ← (1, . . . , 1)
Loop:
Try removing each component with probability 1 − α
Run analysis F(a)
If F(a) = 1: add components back
Else: remove components permanently
Example: F(a) = ¬(a4 ∧ a9 ∧ a11 )
1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ⇒ 0
1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 1 1 1 1 1 1 1 1 1 1 1 ⇒ 1

13

ActiveCoarsen algorithm
Idea: try to remove a constant fraction of components in each step
a ← (1, . . . , 1)
Loop:
Try removing each component with probability 1 − α
Run analysis F(a)
If F(a) = 1: add components back
Else: remove components permanently
Example: F(a) = ¬(a4 ∧ a9 ∧ a11 )
1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ⇒ 0
1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 1 1 1 1 1 1 1 1 1 1 1 ⇒ 1
1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1 0 1 0 1 1 1 0 1 1 1 1 0 0 ⇒ 0

13

ActiveCoarsen algorithm
Idea: try to remove a constant fraction of components in each step
a ← (1, . . . , 1)
Loop:
Try removing each component with probability 1 − α
Run analysis F(a)
If F(a) = 1: add components back
Else: remove components permanently
Example: F(a) = ¬(a4 ∧ a9 ∧ a11 )
1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ⇒ 0
1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 1 1 1 1 1 1 1 1 1 1 1 ⇒ 1
1 1 1 1 1 1
1 1 1
1
1 1 1 1
1
1 1 1
1 1 1 1
⇒ 0

13

ActiveCoarsen algorithm
Idea: try to remove a constant fraction of components in each step
a ← (1, . . . , 1)
Loop:
Try removing each component with probability 1 − α
Run analysis F(a)
If F(a) = 1: add components back
Else: remove components permanently
Example: F(a) = ¬(a4 ∧ a9 ∧ a11 )
1
1
1
0

1
1
1
1

1
1
1
0

1
1
1
1

1
1
1
1

1 1 1 1 1 1 1 1
1 1 1 1 1 1 1 1
1
1 1 1
1
1 0 1 1 0 0 1 0

1
1
1
1

1
1
1
1

1
0
1
1

1 1 1 1
0 1 0 1
1
1
1 0 0 0

1
1
1
0

1
1
1
1

1 1 1 1 1
1 1 1 1 1
1
1 1 1
1 0 0 1 1

1 1 1 ⇒ 0
1 1 1 ⇒ 1
1
⇒ 0
1 0 0 ⇒ 0

13

ActiveCoarsen algorithm
Idea: try to remove a constant fraction of components in each step
a ← (1, . . . , 1)
Loop:
Try removing each component with probability 1 − α
Run analysis F(a)
If F(a) = 1: add components back
Else: remove components permanently
Example: F(a) = ¬(a4 ∧ a9 ∧ a11 )
1 1 1 1
1 1 1 1
1 1 1 1
1
1

1
1
1
1

1 1 1 1 1 1 1 1 1 1 1
1 1 1 1 1 1 1 1 1 1 0
1
1 1 1
1
1 1 1
1
1 1
1
1 1 1

1 1 1 1 1 1 1 1 1 1 1
0 1 0 1 1 1 1 1 1 1 1
1
1
1 1 1
1 1 1
1
1 1
1 1

1 1 1 ⇒ 0
1 1 1 ⇒ 1
1
⇒ 0
1
⇒ 0

13

ActiveCoarsen algorithm
Idea: try to remove a constant fraction of components in each step
a ← (1, . . . , 1)
Loop:
Try removing each component with probability 1 − α
Run analysis F(a)
If F(a) = 1: add components back
Else: remove components permanently
Example: F(a) = ¬(a4 ∧ a9 ∧ a11 )
1 1 1 1
1 1 1 1
1 1 1 1
1
1
0 1 0 1

1
1
1
1
1

1 1 1 1 1 1 1 1
1 1 1 1 1 1 1 1
1
1 1 1
1
1
1 1
1
1 0 1 1 0 0 0 0

1
1
1
1
1

1
1
1
1
1

1
0
1
1
1

1 1 1 1
0 1 0 1
1
1
1
0 0 0 0

1 1 1 1 1 1 1 1 1 1 ⇒ 0
1 1 1 1 1 1 1 1 1 1 ⇒ 1
1 1 1
1 1 1 1
⇒ 0
1 1
1 1 1
⇒ 0
0 1 1 0 0 1 1 1 0 0 ⇒ 0

13

ActiveCoarsen algorithm
Idea: try to remove a constant fraction of components in each step
a ← (1, . . . , 1)
Loop:
Try removing each component with probability 1 − α
Run analysis F(a)
If F(a) = 1: add components back
Else: remove components permanently
Example: F(a) = ¬(a4 ∧ a9 ∧ a11 )
1 1 1 1
1 1 1 1
1 1 1 1
1
1
1
1

1
1
1
1
1

1 1 1 1 1 1 1 1 1 1 1
1 1 1 1 1 1 1 1 1 1 0
1
1 1 1
1
1 1 1
1
1 1
1
1 1 1
1
1 1
1 1 1

1 1 1 1 1 1 1 1 1 1 1 1 1 1 ⇒ 0
0 1 0 1 1 1 1 1 1 1 1 1 1 1 ⇒ 1
1
1
1 1 1
1 1 1 1
⇒ 0
1
1 1
1 1 1
⇒ 0
1 1
1 1 1
⇒ 0

13

ActiveCoarsen algorithm
Idea: try to remove a constant fraction of components in each step
a ← (1, . . . , 1)
Loop:
Try removing each component with probability 1 − α
Run analysis F(a)
If F(a) = 1: add components back
Else: remove components permanently
Example: F(a) = ¬(a4 ∧ a9 ∧ a11 )
1 1 1 1
1 1 1 1
1 1 1 1
1
1
1
1
0 1 0 0

1
1
1
1
1
0

1 1 1 1 1 1 1 1
1 1 1 1 1 1 1 1
1
1 1 1
1
1
1 1
1
1
1 1
1 0 0 0 0 0 0 0

1
1
1
1
1
0

1
1
1
1
1
1

1
0
1
1
1
1

1 1 1 1 1 1 1 1 1 1 1 1 1 1 ⇒ 0
0 1 0 1 1 1 1 1 1 1 1 1 1 1 ⇒ 1
1
1
1 1 1
1 1 1 1
⇒ 0
1
1 1
1 1 1
⇒ 0
1 1
1 1 1
⇒ 0
0 0 0 0 0 0 1 0 0 1 1 0 0 0 ⇒ 1

13

ActiveCoarsen algorithm
Idea: try to remove a constant fraction of components in each step
a ← (1, . . . , 1)
Loop:
Try removing each component with probability 1 − α
Run analysis F(a)
If F(a) = 1: add components back
Else: remove components permanently
Example: F(a) = ¬(a4 ∧ a9 ∧ a11 )
1 1 1 1
1 1 1 1
1 1 1 1
1
1
1
1
0 1 0 0
0 1 0 1

1
1
1
1
1
0
1

1
1
1
1
1
1
0

1 1 1
1 1 1
1 1
1 1
1 1
0 0 0
0 1 1

1 1 1 1 1 1 1
1 1 1 1 1 1 0
1
1
1 1 1
1
1 1 1
1 1 1
0 0 0 0 0 1 1
0 0 0 0 1 1 1

1 1 1 1 1 1 1
0 1 0 1 1 1 1
1
1
1 1 1
1
1 1
1 1
0 0 0 0 0 0 1
0 0 0 0 0 1 1

1 1 1
1 1 1
1 1
1
1
0 0 1
0 0 1

1
1
1
1
1
1
1

1
1
1
1
1
0
1

1 1 ⇒
1 1 ⇒
⇒
⇒
⇒
0 0 ⇒
0 0 ⇒

0
1
0
0
0
1
0

13

ActiveCoarsen algorithm
Idea: try to remove a constant fraction of components in each step
a ← (1, . . . , 1)
Loop:
Try removing each component with probability 1 − α
Run analysis F(a)
If F(a) = 1: add components back
Else: remove components permanently
Example: F(a) = ¬(a4 ∧ a9 ∧ a11 )
1 1 1 1
1 1 1 1
1 1 1 1
1
1
1
1
0 1 0 0
1
1

1
1
1
1
1
0
1

1 1 1 1 1 1 1 1 1
1 1 1 1 1 1 1 1 1
1
1 1 1
1
1
1
1 1
1
1
1
1 1
1
1 0 0 0 0 0 0 0 0
1
1 1

1
1
1
1
1
1
1

1
0
1
1
1
1
1

1 1 1 1 1 1 1 1 1 1 1 1 1 1 ⇒ 0
0 1 0 1 1 1 1 1 1 1 1 1 1 1 ⇒ 1
1
1
1 1 1
1 1 1 1
⇒ 0
1
1 1
1 1 1
⇒ 0
1 1
1 1 1
⇒ 0
0 0 0 0 0 0 1 0 0 1 1 0 0 0 ⇒ 1
1 1
1 1 1
⇒ 0

13

ActiveCoarsen algorithm
Idea: try to remove a constant fraction of components in each step
a ← (1, . . . , 1)
Loop:
Try removing each component with probability 1 − α
Run analysis F(a)
If F(a) = 1: add components back
Else: remove components permanently
Example: F(a) = ¬(a4 ∧ a9 ∧ a11 )
1 1 1 1
1 1 1 1
1 1 1 1
1
1
1
1
0 1 0 0
1
1
0 1 0 1

1
1
1
1
1
0
1
1

1 1 1 1 1
1 1 1 1 1
1
1 1 1
1
1 1
1
1 1
1 0 0 0 0
1 1
0 0 1 1 0

1 1 1
1 1 1
1
1

1
1
1
1
1
0 0 0 0
1
0 0 0 1

1
1
1
1
1
1
1
1

1
0
1
1
1
1
1
1

1 1 1 1 1 1 1
0 1 0 1 1 1 1
1
1
1 1 1
1
1 1
1 1
0 0 0 0 0 0 1
1 1
0 0 0 0 0 1 1

1 1 1
1 1 1
1 1
1
1
0 0 1
1
0 0 0

1
1
1
1
1
1
1
0

1
1
1
1
1
0
1
1

1 1 ⇒
1 1 ⇒
⇒
⇒
⇒
0 0 ⇒
⇒
0 0 ⇒

0
1
0
0
0
1
0
0

13

ActiveCoarsen algorithm
Idea: try to remove a constant fraction of components in each step
a ← (1, . . . , 1)
Loop:
Try removing each component with probability 1 − α
Run analysis F(a)
If F(a) = 1: add components back
Else: remove components permanently
Example: F(a) = ¬(a4 ∧ a9 ∧ a11 )
1 1 1 1
1 1 1 1
1 1 1 1
1
1
1
1
0 1 0 0
1
1
1
1

1
1
1
1
1
0
1
1

1 1 1 1 1 1 1 1 1 1
1 1 1 1 1 1 1 1 1 1
1
1 1 1
1
1 1
1
1 1
1
1 1
1
1 1
1 1
1 0 0 0 0 0 0 0 0 1
1 1
1 1
1 1
1 1

1
0
1
1
1
1
1
1

1 1 1 1 1 1 1 1 1 1 1 1 1 1 ⇒ 0
0 1 0 1 1 1 1 1 1 1 1 1 1 1 ⇒ 1
1
1
1 1 1
1 1 1 1
⇒ 0
1
1 1
1 1 1
⇒ 0
1 1
1 1 1
⇒ 0
0 0 0 0 0 0 1 0 0 1 1 0 0 0 ⇒ 1
1 1
1 1 1
⇒ 0
1 1
1
⇒ 0

13

ActiveCoarsen algorithm
Idea: try to remove a constant fraction of components in each step
a ← (1, . . . , 1)
Loop:
Try removing each component with probability 1 − α
Run analysis F(a)
If F(a) = 1: add components back
Else: remove components permanently
Example: F(a) = ¬(a4 ∧ a9 ∧ a11 )
1 1 1 1
1 1 1 1
1 1 1 1
1
1
1
1
0 1 0 0
1
1
1
1
0 1 0 1

1
1
1
1
1
0
1
1
1

1 1 1 1 1
1 1 1 1 1
1
1 1 1
1
1 1
1
1 1
1 0 0 0 0
1 1
1 1
0 0 0 1 0

1 1 1
1 1 1
1
1

1
1
1
1
1
0 0 0 0
1
1
0 0 0 0

1
1
1
1
1
1
1
1
1

1
0
1
1
1
1
1
1
0

1 1 1 1 1 1 1
0 1 0 1 1 1 1
1
1
1 1 1
1
1 1
1 1
0 0 0 0 0 0 1
1 1
1 1
0 0 0 0 0 1 1

1 1 1
1 1 1
1 1
1
1
0 0 1
1

1
1
1
1
1
1
1

1
1
1
1
1
0
1
1
0 0 0 0 1

1 1 ⇒
1 1 ⇒
⇒
⇒
⇒
0 0 ⇒
⇒
⇒
0 0 ⇒

0
1
0
0
0
1
0
0
1
13

ActiveCoarsen algorithm
Idea: try to remove a constant fraction of components in each step
a ← (1, . . . , 1)
Loop:
Try removing each component with probability 1 − α
Run analysis F(a)
If F(a) = 1: add components back
Else: remove components permanently
Example: F(a) = ¬(a4 ∧ a9 ∧ a11 )
1 1 1 1
1 1 1 1
1 1 1 1
1
1
1
1
0 1 0 0
1
1
1
1
0 1 0 1
0 1 0 1

1
1
1
1
1
0
1
1
1
1

1 1 1 1 1
1 1 1 1 1
1
1 1 1
1
1 1
1
1 1
1 0 0 0 0
1 1
1 1
0 0 0 1 0
0 0 1 1 0

1 1 1
1 1 1
1
1

1
1
1
1
1
0 0 0 0
1
1
0 0 0 0
0 0 0 1

1
1
1
1
1
1
1
1
1
1

1
0
1
1
1
1
1
1
0
1

1 1 1 1 1 1 1
0 1 0 1 1 1 1
1
1
1 1 1
1
1 1
1 1
0 0 0 0 0 0 1
1 1
1 1
0 0 0 0 0 1 1
0 0 0 0 0 1 1

1 1 1
1 1 1
1 1
1
1
0 0 1
1

1
1
1
1
1
1
1

1
1
1
1
1
0
1
1
0 0 0 0 1
0 0 0 0 0

1 1 ⇒
1 1 ⇒
⇒
⇒
⇒
0 0 ⇒
⇒
⇒
0 0 ⇒
0 0 ⇒

0
1
0
0
0
1
0
0
1
0
13

ActiveCoarsen algorithm
Idea: try to remove a constant fraction of components in each step
a ← (1, . . . , 1)
Loop:
Try removing each component with probability 1 − α
Run analysis F(a)
If F(a) = 1: add components back
Else: remove components permanently
Example: F(a) = ¬(a4 ∧ a9 ∧ a11 )
1 1 1 1
1 1 1 1
1 1 1 1
1
1
1
1
0 1 0 0
1
1
1
1
0 1 0 1
1
1

1
1
1
1
1
0
1
1
1
1

1 1 1 1 1
1 1 1 1 1
1
1 1 1
1
1 1
1
1 1
1 0 0 0 0
1 1
1 1
0 0 0 1 0
1 1

1 1 1
1 1 1
1
1

1
1
1
1
1
0 0 0 0
1
1
0 0 0 0
1

1
1
1
1
1
1
1
1
1
1

1
0
1
1
1
1
1
1
0
1

1 1 1 1 1 1 1
0 1 0 1 1 1 1
1
1
1 1 1
1
1 1
1 1
0 0 0 0 0 0 1
1 1
1 1
0 0 0 0 0 1 1
1 1

1 1 1
1 1 1
1 1
1
1
0 0 1
1

1
1
1
1
1
1
1

1
1
1
1
1
0
1
1
0 0 0 0 1

1 1 ⇒
1 1 ⇒
⇒
⇒
⇒
0 0 ⇒
⇒
⇒
0 0 ⇒
⇒

0
1
0
0
0
1
0
0
1
0
13

ActiveCoarsen algorithm
Idea: try to remove a constant fraction of components in each step
a ← (1, . . . , 1)
Loop:
Try removing each component with probability 1 − α
Run analysis F(a)
If F(a) = 1: add components back
Else: remove components permanently
Example: F(a) = ¬(a4 ∧ a9 ∧ a11 )
1 1 1 1
1 1 1 1
1 1 1 1
1
1
1
1
0 1 0 0
1
1
1
1
0 1 0 1
1
1
0 1 0 1

1
1
1
1
1
0
1
1
1
1
1

1 1 1 1 1
1 1 1 1 1
1
1 1 1
1
1 1
1
1 1
1 0 0 0 0
1 1
1 1
0 0 0 1 0
1 1
0 0 1 0 0

1 1 1
1 1 1
1
1

1
1
1
1
1
0 0 0 0
1
1
0 0 0 0
1
0 0 0 1

1
1
1
1
1
1
1
1
1
1
1

1
0
1
1
1
1
1
1
0
1
1

1 1 1 1 1 1 1
0 1 0 1 1 1 1
1
1
1 1 1
1
1 1
1 1
0 0 0 0 0 0 1
1 1
1 1
0 0 0 0 0 1 1
1 1
0 0 0 0 0 1 1

1 1 ⇒
1 1 ⇒
⇒
⇒
⇒
0 0 ⇒
⇒
⇒
0 0 ⇒
⇒
0 0 0 0 0 0 0 ⇒
1 1 1
1 1 1
1 1
1
1
0 0 1
1

1
1
1
1
1
1
1

1
1
1
1
1
0
1
1
0 0 0 0 1

0
1
0
0
0
1
0
0
1
0
1
13

ActiveCoarsen analysis
Theorem:
s = # components in largest minimal abstraction
|J| = total # components

14

ActiveCoarsen analysis
Theorem:
s = # components in largest minimal abstraction
|J| = total # components
If:
Set reﬁnement probability α = e−1/s

14

ActiveCoarsen analysis
Theorem:
s = # components in largest minimal abstraction
|J| = total # components
If:
Set reﬁnement probability α = e−1/s
Then:
ActiveCoarsen outputs a minimal abstraction
in O(s log |J|) expected time
Proof sketch: solve recurrence + optimization over α

14

ActiveCoarsen analysis
Theorem:
s = # components in largest minimal abstraction
|J| = total # components
If:
Set reﬁnement probability α = e−1/s
Then:
ActiveCoarsen outputs a minimal abstraction
in O(s log |J|) expected time
Proof sketch: solve recurrence + optimization over α
Signiﬁcance: s is small,
only logarithmic dependence on total # components

14

Summary of algorithms
Algorithm
BasicRefine
ScanCoarsen
StatRefine
ActiveCoarsen

Minimal
no
yes
high prob.
yes

Correct
yes
yes
high prob.
yes

# calls to F
O(1)
O(|J|)
O(sd2 log |J|)
O(slog |J|)

15

Summary of algorithms
Algorithm
BasicRefine
ScanCoarsen
StatRefine
ActiveCoarsen

Minimal
no
yes
high prob.
yes

Correct
yes
yes
high prob.
yes

# calls to F
O(1)
O(|J|)
O(sd2 log |J|)
O(slog |J|)

ActiveCoarsen: best asymptotic running time
StatRefine: parallelizes more easily, better when s, d very small

15

Summary of algorithms
Algorithm
BasicRefine
ScanCoarsen
StatRefine
ActiveCoarsen

Minimal
no
yes
high prob.
yes

Correct
yes
yes
high prob.
yes

# calls to F
O(1)
O(|J|)
O(sd2 log |J|)
O(slog |J|)

ActiveCoarsen: best asymptotic running time
StatRefine: parallelizes more easily, better when s, d very small
Extensions:
• Adapatively reﬁnement probability α
• Sharing computation across multiple queries

15

Experimental setup
Application: static race detector of [Naik et al. 2006]
Pointer analysis using k-object-sensitivity or k-CFA with heap cloning
Combination of call graph, may alias, thread escape, may happen in parallel

16

Experimental setup
Application: static race detector of [Naik et al. 2006]
Pointer analysis using k-object-sensitivity or k-CFA with heap cloning
Combination of call graph, may alias, thread escape, may happen in parallel
Benchmark statistics (determines # components in abstraction):
hedc
weblech
lusearch

# alloc sites
1,580
2,584
2,873

# call sites
7,195
12,405
13,928

16

Experimental setup
Application: static race detector of [Naik et al. 2006]
Pointer analysis using k-object-sensitivity or k-CFA with heap cloning
Combination of call graph, may alias, thread escape, may happen in parallel
Benchmark statistics (determines # components in abstraction):
hedc
weblech
lusearch

# alloc sites
1,580
2,584
2,873

# call sites
7,195
12,405
13,928

Number of races:
0-cfa
1-cfa
diﬀ. (queries)
1-obj
2-obj
diﬀ. (queries)

hedc
21,335
17,837
3,498
17,137
16,124
1,013

weblech
27,941
8,208
19,733
8,063
5,523
2,540

lusearch

37,632
31,866
5,766
31,428
20,929
10,499

16

Experimental results (all queries)
Setting: ﬁnd one abstraction to prove all queries
How large is abstraction produced by
BasicRefine (non-minimal, deterministic) and
ActiveCoarsen (minimal, randomized)?

17

Experimental results (all queries)
Setting: ﬁnd one abstraction to prove all queries
How large is abstraction produced by
BasicRefine (non-minimal, deterministic) and
ActiveCoarsen (minimal, randomized)?
k-CFA:
hedc
weblech
lusearch

total # components
8,775
14,989
16,801

BasicRefine
7,270 (83%)
12,737 (85%)
14,864 (88%)

ActiveCoarsen (minimal)
90 (1.0%)
157 (1.0%)
250 (1.5%)

17

Experimental results (all queries)
Setting: ﬁnd one abstraction to prove all queries
How large is abstraction produced by
BasicRefine (non-minimal, deterministic) and
ActiveCoarsen (minimal, randomized)?
k-CFA:
hedc
weblech
lusearch

total # components
8,775
14,989
16,801

BasicRefine
7,270 (83%)
12,737 (85%)
14,864 (88%)

ActiveCoarsen (minimal)
90 (1.0%)
157 (1.0%)
250 (1.5%)

BasicRefine
906 (57%)
1,768 (68%)
2,085 (73%)

ActiveCoarsen (minimal)
37 (2.3%)
48 (1.9%)
56 (1.9%)

k-object-sensitivity:
hedc
weblech
lusearch

total # components
1,580
2,584
2,873

17

Experimental results (breakdown by query)
Setting: ﬁnd one abstraction to prove one query
How large are the per-query minimal abstractions?

18

Experimental results (breakdown by query)
Setting: ﬁnd one abstraction to prove one query

# queries

How large are the per-query minimal abstractions?

hedc (cfa)

3000
2400
1800
1200
600

1 2 3 4 5 6 7 8 9

|a|

18

Experimental results (breakdown by query)
Setting: ﬁnd one abstraction to prove one query

hedc (cfa)

3000
2400
1800
1200
600

# queries

# queries

How large are the per-query minimal abstractions?
hedc (obj)

900
720
540
360
180

1 2 3 4 5 6 7 8 9

|a|

|a|
weblech (cfa)

25000
20000
15000
10000
5000

# queries

# queries

1 2 3 4 5 6 7 8 9

1500
1200
900
600
300

weblech (obj)

1 2 3 4 5 6 7

|a|

|a|
lusearch (cfa)

4500
3600
2700
1800
900

# queries

# queries

1 2 3 4 5 6 7 8 9 10 11 12 13 14

7000
5600
4200
2800
1400

lusearch (obj)

1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25

1 2 3 4 5 6 7 8 9 10 11

|a|

|a|

18

Conclusion
• Motivating problem: to scale static analyses, need cheap abstractions
• Scientiﬁc question: what’s the minimal abstraction to prove a query?

19

Conclusion
• Motivating problem: to scale static analyses, need cheap abstractions
• Scientiﬁc question: what’s the minimal abstraction to prove a query?
• Sparsity: very few components are needed
– Theoretical result: leads to eﬃcient machine learning algorithms
– Empirical result: leads to cheap abstractions

19

Conclusion
• Motivating problem: to scale static analyses, need cheap abstractions
• Scientiﬁc question: what’s the minimal abstraction to prove a query?
• Sparsity: very few components are needed
– Theoretical result: leads to eﬃcient machine learning algorithms
– Empirical result: leads to cheap abstractions
• Future work: tackle motivating problem with information
gathered from minimal abstractions

19

