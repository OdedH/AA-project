Asymptotically Optimal Regularization
in Smooth Parametric Models
Percy Liang
University of California, Berkeley
Berkeley, CA 94720

Francis Bach
´
INRIA - Ecole Normale Sup´rieure, France
e
75214 Paris, France

pliang@cs.berkeley.edu

francis.bach@ens.fr

Guillaume Bouchard
Xerox Research Centre Europe, France
38240 Meylan, France

Michael I. Jordan
University of California, Berkeley
Berkeley, CA 94720

Guillaume.Bouchard@xrce.xerox.com

jordan@cs.berkeley.edu

June 3, 2010
Abstract
Many regularization schemes have been employed in statistical learning, where each is motivated by
some assumption about the problem domain. In this paper, we focus on regularizers in smooth parametric
models and present an asymptotic analysis that allows us to see how the validity of these assumptions
aﬀects the risk of a particular regularized estimator. In addition, our analysis motivates an algorithm for
optimizing regularization parameters, which in turn can be analyzed within our framework. We apply our
analysis to several examples, including hybrid generative-discriminative learning and multi-task learning.

1

Introduction

Many problems in machine learning and statistics involve the estimation of parameters from ﬁnite data.
Although empirical risk minimization has favorable limiting properties, it is well known that empirical risk
minimization can overﬁt on ﬁnite data. Hence, various forms of regularization (a canonical example being a
penalty on the norm of the parameters) have been employed to control this overﬁtting.
Regularizers are usually chosen based on assumptions about the problem domain at hand. For example,
in classiﬁcation, we might use quadratic regularization if we expect the data to be separable with a large
margin. We might regularize with a generative model if we think the generative model is roughly wellspeciﬁed [10, 30, 23, 25]. In multi-task learning, we might penalize deviation between parameters across
tasks if we believe the tasks are similar [3, 17, 2, 19].
In each of these scenarios, we would like (1) a procedure for choosing the parameters of the regularizer
(for example, its strength) and (2) an analysis that shows the amount by which regularization reduces
expected risk, expressed as a function of the compatibility between the regularizer and the problem domain.
In this paper, we address these two points by developing an asymptotic analysis of smooth regularizers
for parametric problems. The key idea is to derive a second-order Taylor approximation of the expected
risk, yielding a simple and interpretable quadratic form which can be directly minimized with respect to
the regularization parameters. We ﬁrst develop the general theory (Section 2) and then apply it to some
examples of common regularizers used in practice (Section 3).

1

2

General theory

2.1

Notation

We use uppercase letters (e.g., L, R, Z) to denote random variables and script letters (e.g., L, R, I) to denote
constant limits of random variables.
...
¨
For a λ-parametrized diﬀerentiable function θ → f (λ; θ), let f˙, f , and f denote the ﬁrst, second and
third derivatives of f with respect to θ, and let f (λ; θ) denote the derivative with respect to λ. For a vector
v, let v ⊗ = vv , and v ⊗3 be the rank-3 tensor formed.
Let v be a vector, A and B be symmetric matrices, and T and U be symmetric rank-3 tensors. Then:
• T [v] is the matrix with entries T [v]ij =

k

Tijk vk .

• T [A] is the vector with components T [A]i =
• T [U ] =

i,j,k

j,k

Tijk Ajk .

Tijk Uijk .

• A[B] = tr{AB}.
The Dirac-delta function is expressed as δx (y) = 1 if x = y, 0 otherwise.
Let Xn = Op (1) denote a sequence of random variables which is bounded in probability, that is, for every
P

> 0, there exists M < ∞ such that supn P (Xn > M ) ≤ . Let Xn − X denote convergence in probability,
→
n
that is, for every > 0, limn→∞ P (|Xn − X| > ) = 0. We write Xn = Op (Yn ) to mean Xn = Op (1).
Y
Expectation and variance operators are denoted as E[·] and V[·], respectively.

2.2

Setup

We are given a loss function (·; θ) parametrized by θ ∈ Rd (e.g., for least squares linear regression,
((x, y); θ) = 1 (y − x θ)2 ). Our goal is to minimize the expected risk L(θ), deﬁned as follows:
2
def

def

θ∞ = argmin L(θ),

where L(θ) = EZ∼p∗ [ (Z; θ)],

(1)

θ∈Rd

which averages the loss over some true data generating distribution p∗ (Z). We do not have access to p∗ ,
but instead receive a sample of n i.i.d. data points Z1 , . . . , Zn drawn from p∗ . The standard unregularized
estimator minimizes the empirical risk:
ˆ0 def
θn = argmin Ln (θ),

def

where Ln (θ) =

θ∈Rd

1
n

n

(Zi , θ).

(2)

i=1

ˆ0
Although θn is consistent (that is, it converges in probability to θ∞ ) under relatively weak conditions, it is
well known that regularization can improve performance substantially for ﬁnite n.
Let Rn (λ, θ) be a (possibly data-dependent) regularization function, where λ ∈ Rb are the regularization
parameters. For linear regression, we might regularize the squared norm of the parameter vector by setting
λ
Rn (λ, θ) = 2n θ 2 , where λ ∈ R determines the amount of regularization. Note that in general, (i) the
regularizer is a random function which can depend on the training data, and (ii) there can be more than one
regularization parameter (λ is a vector).
Deﬁne the regularized estimator as follows:
ˆλ def
θn = argmin Ln (θ) + Rn (λ, θ).

(3)

θ∈Rd

P

Assume Rn (0, θ) ≡ 0 (λ = 0 corresponds to no regularization) and Rn (λ, θ) − 0 as n → ∞ (the regulariza→
tion vanishes with the amount of training data tending to inﬁnity).
2

The goal of this paper is to choose good values of λ and analyze the subsequent impact on performance.
Speciﬁcally, we wish to minimize the relative risk:
def
ˆλ
ˆ0
Ln (λ) = EZ1 ,...,Zn ∼p∗ [L(θn ) − L(θn )],

(4)

which is the diﬀerence in risk (averaged over the training data) between the regularized and unregularized
estimators; Ln (λ) < 0 is desirable. Clearly, argminλ Ln (λ) is the asymptotically optimal data-independent
regularization parameter. However, it is diﬃcult to get a handle on Ln (λ), let alone optimize it. Therefore,
the main focus of this work is on deriving an asymptotic expansion for Ln (λ) that is tractable. This will
enable us to analyze and compare diﬀerent regularizers.
In this paper, we make the following assumptions:
Assumption 1 (Compact support). The true distribution p∗ (Z) has compact support.
Assumption 2 (Smooth loss). The loss function (z, θ) is thrice-diﬀerentiable with respect to θ. Further¨
more, assume the expected Hessian of the loss function is positive deﬁnite (L(θ∞ ) 0).1
Assumption 3 (Smooth regularizer). The regularizer Rn (λ, θ) is thrice-diﬀerentiable with respect to θ and
diﬀerentiable with respect to λ.
While we do not explicitly assume convexity of and Rn , the local nature of our subsequent asymptotic
analysis will mean that we are essentially working in a strongly convex setting.
In general, we assume nothing about the relationship between the data generating distribution p∗ and
the loss function . At certain points in this paper, we will be able to obtain stronger results by considering
a specialization of our results to the setting where the model is well-speciﬁed:
Deﬁnition 1 (Well-speciﬁed model). The loss function (model) is well-speciﬁed with respect to the data
generating distribution p∗ (z) if we can write (z; θ) = − log p(z2 | z1 ; θ) for some decomposition z = (z1 , z2 )
such that p∗ (z2 | z1 ) ≡ pθ∞ (z2 | z1 ).
The typical setting is when z1 is the input and z2 is the output, in which case we call p(z2 | z1 ; θ) a
discriminative model. We also allow z1 to be degenerate, in which case we call p(z2 | z1 ; θ) a generative
model.

2.3

Rate of regularization strength

Before tackling the question of how to choose the best regularization parameter λ, let us establish some
basic properties that the regularizer Rn (λ, θ) ought to satisfy. First, a desirable property is consistency
ˆλ P
→
(θn − θ∞ ), i.e., convergence of the estimated parameters to the parameters that achieve the minimum
possible risk in our hypothesis class. To achieve this, it suﬃces (and in general also necessitates) that (1) the
loss class satisﬁes standard uniform convergence properties [33] and (2) the regularizer has vanishing strength
P
→
in the limit of inﬁnite data (Rn (λ, θ) − 0). These two properties can be veriﬁed given Assumptions 1 and
2.
The next question is at what rate Rn (λ, θ) should converge to 0? The following theorem establishes that
1
Op ( n ) is the optimal rate:
Theorem 1. In general, Rn (λ, θ) = Op (n−1 ) is the rate that minimizes the asymptotic relative risk L(λ).
See Appendix A for the proof. Here is the rough sketch: Suppose that Rn = Op (an ) for some constant
sequence an . In Appendix A, we will derive following expansion Ln (λ) = Op (n−1 )Op (an ) + Op (1)Op (a2 ) +
n
Op (a3 ). In order to minimize Ln (λ), we need the ﬁrst two terms to be on the same order (since the two
n
will typically be of opposite signs), which can be achieved with an = n−1 . Of course if the regularizer is
degenerate, e.g., Rn (θ) = an (1 − δθ∞ (θ)), then letting an = ∞ is optimal; we ignore these degenerate cases.
1 This assumption can be weakened. If L
¨ 0, the parameters can only be estimated up to the row space of L. But since we
¨
are interested in the parameters θ only in terms of L(θ), this type of non-identiﬁability of the parameters is irrelevant.

3

With this rate, it is natural to regard the regularizer as a prior p(θ | λ) ∝ exp{−Rn (λ, θ)} if Rn is nonrandom (does not depend on the training data) with λ serving as the hyperparameters of the model. If the
loss is the negative log-likelihood of some (not necessarily well-speciﬁed) model ( (z, θ) = − log p(z2 | z1 ; θ)),
ˆλ
then θn is the maximum a posteriori (MAP) estimate. In this light, our work can be viewed as a method for
setting priors in order to optimize predictive performance. It would be interesting to compare our method
with methods for setting priors based on objective Bayesian principles, for example, reference priors [7].

2.4

Asymptotic expansion of the relative risk

Basic deﬁnitions:
θ ∈ Rd
z∈Z
p∗ (z)
(z; θ)
Z1 , . . . , Z n ∼ p ∗
def 1
n

n
i=1

Ln (θ) =
Rn (λ; θ)

parameter vector
data point (e.g., z = (x, y) for prediction problems)
true data generating distribution
loss function
training data points
(Zi ; θ)

empirical risk
regularizer

def

L(θ) = E[ (Z; θ)]
ˆ0
θn = argminθ∈Rd Ln (θ)
ˆλ
θn = argminθ∈Rd Ln (θ) + Rn (λ, θ)
θ∞ = argminθ∈Rd L(θ)

empirical risk
unregularized estimator
regularized estimator with regularization parameter λ
ˆ P
→
optimal parameters (also, θλ − θ∞ )
n

Population limits (note all evaluations are at θ∞ ):
L
˙
L

def

¨
L
...
L
I

def

I

def

= E[ (Z; θ∞ )] ∈ R
= E[ ˙(Z; θ∞ )] = 0 ∈ Rd

Bayes risk

def

Expected gradient of loss (score function)

= E[ ¨(Z; θ∞ )] ∈ Rd×d
...
def
= E[ (Z; θ∞ )] ∈ Rd×d×d
def
= E[ ˙(Z; θ∞ )⊗ ] ∈ Rd×d
2

(λ)

Expected Hessian of loss
Expected third-derivative of loss
Fisher information (variance of score)

= E[ ¨(Z; θ∞ ) ⊗ ˙(θ∞ )] ∈ Rd×d×d

Expected loss Hessian-gradient product

def

R(λ)
˙
R(λ)

= limn→∞ n · E[Rn (λ, θ∞ )] ∈ R
def
˙
= limn→∞ n · E[Rn (λ, θ∞ )] ∈ Rd

¨
R(λ)

def

I r (λ)
B

Expected regularizer
Expected gradient of regularizer

¨
= limn→∞ n · E[Rn (λ, θ∞ )] ∈ Rd×d
def
2
˙
˙
= limn→∞ n · E[Ln (θ∞ )Rn (λ, θ∞ ) ] ∈ Rd×d
def
ˆ0 − θ∞ ] ∈ Rd
= limn→∞ n · E[θ
n

Expected Hessian of regularizer
Random loss-regularizer alignment
Asymptotic bias of unregularized estimator

Table 1: Expectations are taken with respect to the training set, with Z ∼ p∗ denoting a generic sample
from the true distribution. Derivatives are taken with respect to the parameter θ evaluated at the limiting
parameters θ∞ (around which we perform the asymptotic expansion). We will start to omit θ∞ (e.g., writing
˙
˙
¨
¨
Ln for Ln (θ∞ ) and Rn (λ) for and Rn (λ, θ∞ )) when there is no confusion.
Our main result is the following theorem, which provides a simple interpretable asymptotic expression
for the relative risk (see Appendix A for the proof):
Theorem 2. Assume Rn (λ, θ∞ ) = Op (n−1 ). The relative risk admits the following asymptotic expansion:
5

Ln (λ) = L(λ) · n−2 + Op (n− 2 ),

(5)

where L(λ) is the asymptotic relative risk:
def

L(λ) =

1
˙
¨
¨ ¨
¨
˙
¨
tr{R(λ)⊗ L−1 } − tr{I L−1 R(λ)L−1 } − 2B R(λ) + tr{I r (λ)L−1 },
2
4

(6)

where the various quantities are deﬁned in Table 1.
The signiﬁcance of Theorem 2 is writing the relative risk Ln (λ) (the quantity we want to minimize but
diﬃcult to analyze directly) in terms of the asymptotic relative risk L(λ) (6), which contains terms which
we will be able to interpret naturally and optimize. We emphasize that (6) is the most important equation
in this paper.
Now we proceed to interpret the asymptotic relative risk L(λ). First, note that the asymptotic relative
risk operates at the O(n−2 ) scale. This is natural, because maximum likelihood estimators have expected
excess risk O(n−1 ) and are asymptotically optimal up to this ﬁrst order; our asymptotic improvements
therefore come at the next order.
The asymptotic relative risk L(λ) consists of three parts. We will interpret each one in turn:
˙
¨
˙
• Squared bias of the regularizer tr{R(λ)⊗ L−1 }: R(λ) is the gradient of the regularizer at the limiting
parameters θ∞ . If we were sitting at θ∞ , this would be the direction that the regularizer would push
us away. Therefore, regularizers with small values are desirable.
¨
L is the expected Hessian of the loss function at θ∞ , which intuitively deﬁnes an Mahalanobis metric
¨
¨
on the parameter space. Directions of large magnitude according to L (e.g., v ∈ Rd such that Lv is
large) are easier to estimate.
¨ ˙
˙
The squared bias of the regularizer, which can also be written as R(λ) L−1 R(λ), is the squared norm
˙
¨
of R(λ) with respect to the Mahalanobis metric given by L. Note that the squared regularizer bias
is always positive; thus it always increases the risk by an amount which depends on how “wrong” the
regularizer is.
¨
¨
¨ ¨
• Variance reduction provided by the regularizer tr{I L−1 R(λ)L−1 }: The key quantity is R(λ), the
Hessian of the regularizer, which is the amount of stability provided by the regularizer (larger is
¨
better). For convex regularizers, R(λ) 0.
¨
¨
¨
The impact of R(λ) is channeled through L−1 and I . If the model is well-speciﬁed, then I = L
by the ﬁrst Bartlett identity [4] (see Proposition 2), and the variance reduction term simpliﬁes to
¨
¨
tr{R(λ)L−1 }.
˙
¨
• Alignment between regularizer bias and unregularized estimator bias 2B R(λ) − tr{I r (λ)L−1 }: The
alignment has two parts.
The ﬁrst part consists of the dot product between B, the asymptotic bias (the expected parameter
ˆ0
˙
error θn − θ∞ of the unregularized estimator scaled up by n), and R(λ), the gradient of the regularizer.
˙
A positive dot product means that the direction the regularizer is pushing away from (R(λ)) is aligned
with the direction that the unregularized estimator consistently errs in (B), which is good. Note that in
some cases, such as linear regression, the unregularized estimator (least squares) is unbiased (B = 0).
¨
The second part consists of I r L−1 , which measures the dot product (in the space deﬁned by the metric
¨
˙
L) between the direction of increasing loss (Ln ) and the direction the regularizer is pushing away from
˙
(Rn ). A negative dot product means that the regularizer compensates for the loss, which is good. Note
that I r = 0 when the regularizer Rn is non-random (i.e., does not depend on the training data).
In summary, the main equation (6) is largely about an asymptotic bias-variance tradeoﬀ, as governed by
the ﬁrst two terms of (6). However, second-order asymptotics generates many many dependencies via cross
terms of Taylor expansions, which are responsible for the additional alignment terms.
Theorem 2 provides us with an expression for the asymptotic relative risk in terms of various limiting
¨ ˙
quantities deﬁned in Table 1 (e.g., L, R, B, etc.). When we specialize to examples in Section 3, we will need
to compute these quantities in terms of the loss function and regularizer Rn . The most complicated part
of the asymptotic risk in (6) is the asymptotic bias B of the unregularized estimator. Therefore, we express
B in terms of various derivatives:

5

ˆ0
Proposition 1 (Asymptotic bias of unregularized estimator). The asymptotic bias B = E[θn − θ∞ ] is
¨
B = L−1 I

2

1 ¨ ... ¨
¨
¨
[L−1 ] − L−1 L [L−1 I L−1 ].
2

(7)

Proof: see Appendix B.
2.4.1

Simpliﬁcations

If the regularizer has the simpliﬁed form Rn (λ, θ) =
tives involving the regularizer simplify accordingly:
˙
R = λr,
˙

λ
n r(θ)

for some function r(θ), then the expected deriva-

¨
R = λ¨,
r

I

r

= 0.

(8)

If the model is well-speciﬁed (Deﬁnition 1), then we have:
Proposition 2 (First Bartlett identity). If the loss function
¨
generating distribution p∗ , then I = L.

is well-speciﬁed with respect to the data

Proof: see Appendix C. The consequence of Proposition 2 is that the second term of (6) simpliﬁes to
¨
¨
−tr{R(λ)L−1 }.

2.5

Oracle regularizer

The principal advantage of having a simple expression for the asymptotic relative risk L(λ) given by Theorem 2 is that we can minimize it with respect to λ. In particular, let
def

λ∗ = argmin L(λ)

(9)

λ

be the asymptotically optimal data-independent regularization parameter, which we call the oracle regularˆλ∗
ization parameter. Let θn be the corresponding oracle estimator that uses that optimal value λ∗ .
We have a closed form for λ∗ in the important special case that the regularization parameter λ is a scalar
denoting the strength of the regularizer:
Corollary 1 (Oracle regularization strength). If Rn (λ, θ) =
λ∗ = argmin L(λ) =
λ

λ
n r(θ)

for some r(θ), then

¨ ¨¨
tr{I L−1 rL−1 } + 2B r def C1
˙
,
=
¨−1 r
C2
r L ˙
˙

L(λ∗ ) = −

2
C1
.
2C2

(10)

λ
For simplicity, we have assumed a non-random regularizer of the form Rn (λ, θ) = n r(θ). We use C1 and
C2 to denote the numerator (variance plus alignment) and the denominator (squared bias), respectively.

Proof. Note that (6) is a quadratic function of λ. Diﬀerentiate with respect to λ, set to zero and solve.
Compute L(λ∗ ) by substituting λ∗ from (10) back into (6).
In general, λ∗ will depend on θ∞ ; hence it is not computable from data. Section 2.6 will remedy this.
Nevertheless, the oracle regularizer provides an upper bound on asymptotic performance and sheds some
insight into the relevant quantities that make a regularizer useful.
Note L(λ∗ ) ≤ 0, since optimizing λ∗ must be no worse than not regularizing since L(0) = 0. What might
be surprising at ﬁrst is that the oracle regularization strength λ∗ can be negative (corresponding to “antiregularization”). If ∂L(λ) = −C1 < 0, however, then some amount of (positive) regularization is guaranteed
∂λ
to help. In this case, λ∗ > 0, and L(λ) < 0 for 0 < λ < 2λ∗ .

6

2.6

Plugin regularizer

While the oracle regularizer Rn (λ∗ , θ) given by (10) is asymptotically optimal, λ∗ depends on the unknown
ˆλ∗
θ∞ , so θn is actually correspond to an estimator that is not implementable. In this section, we develop the
plugin regularizer as a way to avoid the dependence on θ∞ . The key idea is to substitute λ∗ with a noisy
ˆ
estimate λn . We deﬁne the plugin regularization parameter:
ˆ def
λn = λ∗ + εn ,

(11)

1

where εn = Op (n− 2 ) is some noise. We will describe how to obtain a plugin regularization parameter shortly,
but ﬁrst let us see the consequences of having one: Deﬁne the plugin estimator as follows:
ˆ
ˆλ def
ˆ
θnn = argmin Ln (θ) + Rn (λn , θ).

(12)

θ

ˆ

ˆλ
ˆ0
How well does this plugin estimator work—that is, what is its relative risk E[L(θnn ) − L(θn )]? Unfortuˆ n ) and apply Theorem 2 because the relative risk function L(·) can only
nately, we cannot simply write Ln (λ
ˆ
be applied to non-random regularization parameters; in contrast, λn depends on the data.
A new regularizer However, we can still leverage existing machinery by deﬁning a new plugin regularizer:
def
•
ˆ
Rn (λ• , θ) = λ• Rn (λn , θ)

(13)

with regularization parameter λ• ∈ R, which is deﬁned in terms of the original regularizer Rn . Note that
•
ˆ
Rn depends on the data through λn , and this dependence is one of the main reasons we needed to support
the generality of a random regularizer. Henceforth, the superscript • will denote quantities deﬁned in terms
•
of the plugin regularizer Rn rather than the original regularizer Rn .
The estimator that uses the plugin regularizer with regularization parameter λ• is deﬁned as follows:
•
ˆ•λ• def
θn = argmin Ln (θ) + Rn (λ• , θ)

(14)

θ

•
ˆ•λ•
The purpose of introducing the plugin regularizer Rn and its associated estimator θn is the following key
identity:
ˆ
ˆ•1
ˆλ
(15)
θn n = θn ,

where we relate the original plugin estimator we wished to study on the left-hand side to an estimator that
has a non-random regularization parameter (namely, λ• = 1) on the right-hand side. Notably, we know how
to compute the relative risk for estimators involving non-random regularization parameters.
Deﬁne the relative risk of estimators using the plugin regularizer (note the analogy with (4)):
def
ˆ•λ•
ˆ•0
L• (λ• ) = E[L(θn ) − L(θn )].
n

(16)

In particular, we are interested in λ• = 1, since the relative risk of L• (1) is exactly the relative risk of the
n
ˆ
ˆλ
plugin estimator θnn by (15).
Further improvements Instead of settling for λ• = 1, we could try to squeeze more out of the plugin
regularizer by further optimizing λ• as follows:
def

λ•∗ = argmin L• (λ• ),
λ•

ˆ•λ•∗
which leads to the oracle plugin estimator θn .
7

(17)

In general, this is not useful since λ•∗ might depend on θ∞ , and the whole point of plugin is to remove
this dependence. However, in a fortuitous turn of events, for some models as we will see in Sections 3.1
ˆ•λ•∗ actually is implementable. Table 2
and 3.5, λ•∗ is in fact independent of θ∞ . When this is the case, θn
summarizes all the estimators we have discussed.
Having deﬁned all the relevant plugin-related estimators, we turn to computing and comparing their
ˆ
ˆλ
ˆ•1
relative risks. The main question is whether the implementable procedure (the plugin estimator θnn = θn )
∗
ˆλ
can work “almost as well” as the unimplementable procedure (the oracle estimator θn ). The following
theorem answers exactly that:
Theorem 3 (Relative risk of plugin). The relative risk of the plugin estimator is
L• (1) = L(λ∗ ) + E,
def
˙
˙
¨
E = lim n2 E[tr{Ln ( Rn (λ∗ )εn ) L−1 }],

where

(18)
(19)

n→∞

˙
where Rn (λ∗ ) ∈ Rd×b . Furthermore, if Rn (λ) is linear in λ, then the relative risk of the oracle plugin
estimator is
E2
,
(20)
L• (λ•∗ ) = L• (1) +
4L(λ∗ )
with the oracle plugin regularization parameter
λ•∗ = 1 +

E
.
2L(λ∗ )

(21)

Proof: see Appendix D.
Note that the sign of E depends on the nature of the error εn , so Plugin could be either better or worse
than Oracle. On the other hand, OraclePlugin is always better than Plugin.
ˆ
Obtaining a plugin regularization parameter λn So far, we have analyzed the plugin estimator
ˆ n = λ∗ + εn . Now, let us consider a useful and general recipe for obtaining
assuming we have an estimate λ
ˆ
λn . Note that (10) provides us an expression for λ∗ in terms of θ∞ . Let f denote this function. We can
ˆ
ˆ0
ˆ0
plugin the unregularized estimator θn (or any other consistent estimator of θ∗ ) and set λn = f (θn ).
Figure 1 summarizes the resulting algorithm. The algorithm can be viewed as performing adaptive
regularization, where a preliminary estimate is used to determine the appropriate amount of regularization,
and then a regularized optimization problem is solved to obtain the ﬁnal estimate.
Plugin algorithm:
ˆ0
1. θn = argminθ∈Rd Ln (θ)
ˆ
ˆ0
2. λn = f (θn )

[compute unregularized estimate]

[compute plugin regularization parameter]

ˆ
ˆλ
ˆ
3. θnn = argminθ∈Rd Ln (θ) + Rn (λn , θ)

[compute regularized estimate]

Figure 1: Our proposed two-stage algorithm for setting the regularization parameter. First, we use obtain
ˆ0
crude preliminary estimate θn without using regularization. Then we estimate an appropriate regularization
ˆ n by using a function f such that λ∗ = f (θ∞ ). Finally, we re-solve the optimization problem
parameter λ
ˆ
ˆλ
with regularization to get θnn , which is our ﬁnal answer.
In this case, we can specialize E (19):
8

Estimator
Unregularized
Oracle
Plugin
OraclePlugin

Description
No regularization
Regularization with optimal reg. parameter λ∗
ˆ
Regularization with plugin reg. parameter λn
Optimize strength of the random reg.

Notation
ˆ0
θn
ˆλ∗
θn
ˆ
ˆλ
ˆ•1
θnn = θn
•λ
ˆn •∗
θ

Relative risk
0
L(λ∗ )
L• (1)
L• (λ•∗ )

Table 2: Notation for the various estimators and their relative risks.
ˆ
ˆ0
Theorem 4. Suppose λ∗ = f (θ∞ ) for some diﬀerentiable function f : Rd → Rb . If λn = f (θn ), then the
results of Theorem 3 hold with
¨
˙
¨
E = −tr{I L−1 R(λ∗ )f˙L−1 }.
(22)
Proof: see Appendix E.
Table 2 summarizes the estimators that we have deﬁned: the unregularized estimator, Unregularized
ˆ0 ); the oracle regularized estimator, Oracle (θλ∗ ); the plugin regularized estimator, Plugin (θλn ; equivˆ
ˆˆ
(θn
n
n
•∗
ˆ•1
ˆ•λ
alently, θn ); and the oracle plugin regularized estimator, OraclePlugin (θn ).

3

Some applications of the theory

In this section, we apply our results from Section 2 to speciﬁc problems. Having made all the asymptotic
derivations in the general setting, we now only need to make a few straightforward calculations to obtain the
asymptotic relative risks and regularization parameters for a given problem. To get some intuition for the
theory, we ﬁrst explore two classical examples from statistics, Gaussian mean estimation (Section 3.1) and
binomial estimation (Section 3.2). Then we consider two important examples in machine learning, hybrid
generative-discriminative learning (Section 3.4) and multi-task learning (Section 3.5).

3.1

Gaussian mean estimation

Setup A classical problem in statistics is estimating the mean of a distribution from samples. In this work,
we assume data are generated from a multivariate Gaussian distribution with d independent components
(p∗ = N (θ∞ , I)), where the mean θ∞ ∈ Rd is unknown.
1
We use the negative log-likelihood of the Gaussian distribution as the loss function, (x; θ) = 2 (x−θ)2 , so
we are working in the well-speciﬁed setting. In this case, the unregularized (maximum likelihood) estimator
is simply the empirical mean:
n
ˆ0
¯ def 1
Xi
(23)
θn = X =
n i=1
Consider a regularizer which penalizes the squared norm, heretofore known as the quadratic regularizer:
Rn (λ, θ) =

λ
r(θ),
n

r(θ) =

1
θ 2.
2

(24)

Oracle regularization To compute the oracle regularizer (10), we need to ﬁrst compute the various
˙
¯ ¨
derivatives of the loss and regularizer: Ln = θ∞ − X, L = I, B = 0, r = θ∞ , and r = I. Note that since the
˙
¨
¨
model is well-speciﬁed, we also have I = L.
By (10), we can compute the oracle regularization strength and its associated asymptotic relative risk:
λ∗ =

d
θ∞

,
2

L(λ∗ ) = −

d2
2 θ∞

2

.

The form of λ∗ is intuitive: the closer θ∞ is to zero, the more we should regularize towards zero.

9

(25)

Plugin regularization However, λ∗ depends on θ∞ , so let us use the plugin estimator Plugin from
Section 2.6. By (10), we have
−2dθ
d
and f˙(θ) =
,
(26)
f (θ) =
θ 2
θ 4
so the plugin estimator, written out explicitly, is deﬁned as follows:
ˆ
ˆλ
θnn

=

argmin
θ∈Rd

n

1
2n

1 ¯
(Xi − θ)2 + f (X) θ 2 ,
2
i=1

(27)
(28)

¯
where f (X) is the estimated regularization strength.
ˆ
ˆλ
ˆ•1
Now let us analyze the relative risk of the plugin estimator θnn = θn . From Theorem 4, we compute
E

=

2dθ∞
θ
||θ∞ ||4 ∞
2d
.
||θ∞ ||2

tr

=

(29)
(30)

Plugging E into Theorem 3 yields the relative risk for the plugin estimator:
L• (1)

= L(λ∗ ) + E
d2
2d
= −
+
2||θ∞ ||2
||θ∞ ||2
d(d − 4)
= −
.
2 θ∞ 2

(31)
(32)
(33)

Note that since E > 0, Plugin is always (asymptotically) worse than Oracle but better than Unregularized
if d > 4.
Oracle plugin regularization The implied plugin regularizer is:
•
Rn (θ) =

1 d||θ||2
¯ .
2 ||X||2

(34)

We can optimize over its regularization parameter λ• to get OraclePlugin. By Theorem 3, the optimum
is attained at
λ•∗

=
=
=

E
2L(λ∗ )
2d θ∞ 2
1−
θ∞ 2 d2
2
1− .
d
1−

(35)
(36)
(37)

It is important that λ•∗ does not depend on θ∞ , for otherwise, the oracle plugin estimator would not be
implementable. The relative risk, by Theorem 3, is
L• (λ•∗ ) = −

(d − 2)2
.
2 θ∞ 2

(38)

Note that OraclePlugin oﬀers a small improvement over Plugin. It is superior to Unregularized when
d > 2 (by comparison, Plugin is superior only when d > 4).
10

The ﬁnal estimator, written out explicitly, is as follows:
ˆ•∗
θn

=

•
argmin Ln (θ) + λ•∗ Rn (θ)
n

(39)

θ

=

argmin
θ

1
2n

n

(Xi − θ)2 +
i=1

1−
n

2
d

d θ 2
¯ .
2||X||2

(40)

We can solve this problem in closed form. Diﬀerentiating and setting to zero:
d − 2 ˆ•∗
ˆ•∗
¯
θn − X +
¯ θ = 0.
n||X||2 n

(41)

Rearranging yields:
ˆ•∗
¯
θn = X 1 −

d−2
¯
n||X||2 + d − 2

.

(42)

Comparison with James-Stein In his seminal 1961 paper [20], Stein showed the rather counterintuitive
ˆ0
result that the unregularized estimator θn is not admissible for d > 2, despite the fact that the d means
being estimated are independent.
Speciﬁcally, the James-Stein estimator (JamesStein) is deﬁned as follows:
d−2
ˆJS def ¯
θn = X 1 −
¯
n X 2

(43)

It was shown in [20] that JamesStein achieves strictly lower risk than Unregularized—that is,
ˆJS
ˆ0
E[L(θn )] < E[L(θn )]

(44)

for all n and θ∞ provided the dimensionality d > 2.
If we compare OraclePlugin in (42) with JamesStein in (43), we ﬁnd that the two are essentially the
same. Formally, it can be veriﬁed that
5
ˆ•λ•∗ ˆJS
θn − θn = Op (n− 2 ).

(45)

Both have the intuition we shrink the means towards each other by some factor that reduces variance at
expense of only a small increase in bias, which is exactly our intuition behind using regularization.
The only diﬀerence between the JamesStein and OraclePlugin is that OraclePlugin contains an
additional d − 2 term in the denominator, which gives OraclePlugin the extra property that it always
¯
returns an estimate between 0 and X, whereas JamesStein can overshoot zero. Empirically, we found that
OraclePlugin generally had a lower expected risk than JamesStein when θ∞ is large (several values
for θ∞ , d and n were tested), but JamesStein was better when θ∞ ≤ 1. For noisy settings, the standard
James-Stein estimator is generally better.
One advantage of the analysis of the James-Stein estimator is that the exact expected risk for all n can
computed. Having this explicit form relies on being able to work with closed-form expressions for various
expectations of Gaussian variables. By using asymptotics, we are able to side-step these speciﬁc computations
and generalize to a much larger class of models, obtaining Stein’s result asymptotically as a special case.
Also note that we cannot obtain the James-Stein estimator exactly with a quadratic regularizer because
λ
λ
the estimator resulting from the latter will have the form n||X||2 +λ , not n||X||2 , as in James-Stein. James¯
¯
Stein is only optimal over estimators of the form

λ
.
¯
n||X||2

11

3.2

Binomial estimation

Setup We consider the estimation of the log-odds θ ∈ R of a coin coming up heads given n i.i.d. coin ﬂips.
Then the negative log-likelihood loss corresponding to this probability model is
(x; θ) = −xθ + log(1 + eθ ),

(46)
θ

e
where x ∈ {0, 1} is the outcome of the coin. One can verify that the probability of heads is e− (1;θ) = 1+eθ
1
and the probability of tails is e− (0;θ) = 1+eθ .
The main technical novelty in binomial estimation is that it requires reasoning about the asymptotic bias
of the unregularized estimator (B) appearing in (6), which is typically ignored in ﬁrst-order asymptotics or
is zero, as we saw in Gaussian mean estimation. Also note that in binomial estimation, the model is always
¨
well-speciﬁed, so we can assume I = L without loss of generality.
We consider the conjugate regularizer deﬁned by:

Rn (λ, θ) =

λ
r(θ),
n

1
r(θ) = − θ + log(1 + eθ ),
2

(47)

which corresponds to the negative log prior of a Beta( λ + 1, λ + 1) distribution. Note that r(θ) has the same
2
2
form as (·, θ) because the Beta distribution is conjugate to the binomial. Choosing λ (the hyperparameter
of the Beta prior) has been studied extensively in statistics. Some common choices are the Haldane prior
(λ = −2), the reference (Jeﬀreys) prior (λ = −1), the uniform prior (λ = 0), and Laplace smoothing (λ = 2).
Our approach allows λ to be chosen to minimize expected risk adaptively based on data.
Oracle regularization To compute the oracle regularizer (10), we need to compute the various derivatives
of the loss and regularizer. Because the binomial distribution is an exponential family, these derivatives
correspond to the moments of the distribution, which we compute as follows:
def

µ = Eθ∞ [X] =

1
,
1 + e−θ∞

def

v = Vθ∞ [X] = µ(1 − µ),

1
def
b = µ− ,
2

(48)

where µ is the mean, v is the variance, and b is the oﬀset from 1 . Remember that these quantities depend
2
on θ∞ , though we will suppress the dependence from the notation.
...
¨
Now, the appropriate quantities in (10) can be computed: L = v, L = −2vb, r = b, r = v. For the
˙
¨
˙
¨
asymptotic bias, we have I 2 = v L = 0 and I = L, so B = v −1 b. Note that B tends to inﬁnity as µ tends
to 0 or 1. Indeed, maximum likelihood for binomial problems tends to underestimate the probability of rare
events.
Plugging these quantities into (10), we get that the oracle regularization strength is
λ∗ =

1 + 2v −1 b2
= vb−2 + 2.
v −1 b2

(49)

Note that λ∗ is always positive, so (positive) regularization is always helpful. In particular, the minimum
regularization strength is λ∗ = 2 when v = 0 (corresponding to when the probability of heads µ is 0 or 1);
1
λ∗ = 2 corresponds to using Laplace smoothing. When µ → 2 , the oracle regularization strength tends to
inﬁnity.
We can calculate the corresponding relative risk:
L(λ∗ ) =

−(1 + 2v −1 b2 )2
=−
2v −1 b2

1 −2
vb + 2 + 2v −1 b2 .
2

(50)

Note that there are two cases when regularization helps immensely (L(λ∗ ) → −∞): (1) when µ → 1 , we
2
regularize heavily to stabilize the estimates; and (2) when µ tends to 0 or 1, we regularize with the minimum
value of 2, but this also helps enormously in guarding against highly skewed estimates.
12

Plugin regularization
ization strengths:

Recall that Plugin depends on a function linking parameters to oracle regularf (θ∞ ) = 2 + vb−2 .

(51)

f˙(θ∞ ) = (−2vb)b−2 + v(−2b−3 )v = −2vb−1 − 2v 2 b−3 .

(52)

Take its derivative:
Now we can compute diﬀerence in relative risk between Plugin and Oracle:
L• (1) − L(λ∗ ) = E = −bf˙v −1 = 2 + 2vb−2 ,

(53)

˙
where we used the fact that R(λ∗ ) = r = b. Since E ≥ 2, suboptimality of Plugin relative to Oracle
˙
1
costs us asymptotically at least 2, with larger relative risk if µ is close to 2 .
Combining (50) and (53), we get:
L• (1) =

3 −2
vb − 2v −1 b2 .
2

(54)

Therefore, Plugin is better than Unregularized when L• (1) < 0, which happens when 3v 2 < 4b4 ; in
terms of the heads probability, µ ≥ 0.84 or µ ≤ 0.16. Intuitively, we need the variance to be small enough
1
so that the plugin estimates are reliable enough. When µ is close to 2 , regularization is not as critical in the
∗
∗
ﬁrst place, unless we can pinpoint λ , but estimating λ is diﬃcult in this regime because the variance v is
large.

3.3

Entropy regularization

Setup In prediction tasks, we wish to learn a mapping from some input x ∈ X to an output y ∈ Y.
A common approach is to use conditional exponential families, which are deﬁned by a vector of suﬃcient
statistics (features) φ(x, y) ∈ Rd and an accompanying vector of parameters θ ∈ Rd in the following way:
pθ (y | x)

=

exp{φ(x, y) θ − A(θ; x)},

A(θ; x) = log

exp{φ(x, y) θ}dy.

(55)

Y

To map this setup onto our general notation, let z = (x, y) and (z; θ) = − log pθ (y | x). Conditional
exponential families contain logistic regression and conditional random ﬁelds [22] as special cases.
Suppose in addition to our n labeled examples, we have m unlabeled examples Xn+1 , . . . , Xn+m . We
would like to exploit this unlabeled data to perform semi-supervised learning. One method of using the
unlabeled data is to use entropy regularization [18], which minimizes the entropy of a learned distribution
pθ (y | x) as measured on unlabeled points. The motivation behind this criteria is that often in classiﬁcation
problems, the classes are well separated, and separation corresponds to having low entropy pθ (y | x).
Formally, the entropy regularizer is deﬁned as follows:
λ
Rn (λ, θ) = r(θ),
n
where H(p(y | x)) = −
of the labeled data.

1
r(θ) =
m

m

H(pθ (Y | Xn+i )),

(56)

i=1

p(y | x) log p(y | x)dy. Note that while Rn is a random quantity, it is independent

Asymptotic analysis Now we study the asymptotic eﬀects of entropy regularization. We assume that
m → ∞ as n → ∞. First, we can compute the Hessian of the loss using standard moment-generating
properties of the log-partition function A(θ; x):
¨
L =

Ep∗ (X) [Vpθ∞ (Y |X) [φ(X, Y ) | X]],

13

(57)

Now we turn to the regularizer. Since m → ∞, we have
R(λ, θ) = λ · Ep∗ (X) [H(pθ∞ (Y | X))].

(58)

First, let us compute the derivative of the entropy function:
| x))]

= −

θ [pθ (y

| x) log pθ (y | x)]dy

= −

θ [pθ (y

| x)] log pθ (y | x) + pθ (y | x)

= −

θ [H(pθ (Y

θ [pθ (y

| x)](log pθ (y | x) + 1)dy

(59)
| x)]
dy
pθ (y | x)

θ [pθ (y

(60)
(61)

= −

pθ (y | x)[φ(x, y) − Epθ (Y |x) [φ(x, Y )]](φ(x, y) θ − A(θ; x) + 1)dy

(62)

= −

pθ (y | x)[φ(x, y) − Epθ (Y |x) [φ(x, Y )]]φ(x, y) θdy

(63)

= −Vpθ (Y |x) [φ(x, Y )]θ

(64)

Deﬁne the expected conditional variance of the exponential family:
def

Vx = Ep∗ (X) [Vpθ∞ (Y |X) [φ(X, Y )]].

(65)

From the derivative, it is clear that we reduce the entropy by increasing the magnitude of θ, especially
along directions with high variance. Using this calculation, we obtain the derivatives for the loss and
regularizer:
¨
L = Vx ,
˙
R = −Vx θ∞ ,
¨ = −Vx [θ∞ ] − Vx .
˙
R

(66)
(67)
(68)

The oracle regularization strength can be computed:
λ∗ =

−1
−1 ˙
−tr{I Vx (Vx [θ∞ ] + Vx )Vx } − 2B Vx θ∞
.
θ∞ Vx θ∞

(69)

The sign and magnitude of λ∗ provide some indication of the regimes in which entropy regularization
should be helpful (large positive values indicate that regularization is useful). Unlike the regularizers we
have considered so far, the entropy regularizer is non-convex, the variance reduction term is not necessarily
positive. Therefore a small positive λ∗ is not guaranteed to reduce the relative risk.
To gain some intuition, consider the case where X is a singleton and Y = {0, 1}, which brings us back to
the task of binomial estimation. In this case, the oracle regularization strength simpliﬁes to
λ∗ =

−(−2vbθ∞ + v)v −1 − 2(v −1 b)vθ∞
1
=− 2 .
2
θ∞ v
θ∞

(70)

Note that this quantity is negative, which means that (positive) entropy regularization always hurts performance. This is not surprising given that entropy regularization is in some sense “anti-regularization”,
pushing the parameter estimate µ towards 0 and 1 rather than shrinking towards 1 . For the case of an
2
non-singleton input space X (e.g., in logistic regression), we do not have such a simple and interpretable
formula that captures the oracle regularization, but we suspect that the qualitative conclusion is similar to
that of binomial estimation.
14

Does this analysis show that entropy regularization is not helpful? This is not necessarily the case, for
our analysis comes with two caveats. First, this analysis is only an asymptotic one where the problem size
is ﬁxed and the sample size grows. Therefore, the regime in which the analysis applies is one in which we
are trying to stabilize the estimator; however, in practice, we might be in the regime where the entropy
regularizer is playing more of a global structural role in reducing the space of possible θ rather than reﬁning
the estimate of θ locally.
Second, we have been assuming that log-loss is the desired loss. Therefore, one pays a heavy penalty if
one does not faithfully represent very low probability outcomes. As a result, regularization to place suﬃcient
support on those outcomes is very important. On the other hand, in classiﬁcation tasks in practice, 0-1 loss
is often the preferred loss, in which case low probability events can be essentially ignored without sacriﬁcing
much performance. Entropy regularization seems to be tailored more to the coarser 0-1 loss.

3.4

Hybrid generative-discriminative learning

In this section, we are again concerned in prediction tasks, in which we would like to learn a mapping
from an input x ∈ X to an output y ∈ Y. Although one could solve this problem directly by learning a
discriminative predictor of y given x, both theory and practice have demonstrated that one can perform
better on prediction, especially for smaller training sets, by exploiting a generative model pθ (x, y). In recent
years, there has been interest in combining generative and discriminative learning [10, 30, 23, 27, 25].
Setup We consider generative and discriminative models deﬁned by exponential families, where we let
φ(x, y) ∈ Rd denote the vector of suﬃcient statistics (features) of the exponential family and let θ ∈ Rd
be the parameters. These features and parameters can be used to deﬁne a generative model (71) or a
discriminative model (72):
pθ (x, y)

=

exp{φ(x, y) θ − A(θ)},

A(θ) = log

exp{φ(x, y) θ}dydx,
X

pθ (y | x)

=

exp{φ(x, y) θ − A(θ; x)},

(71)

exp{φ(x, y) θ}dy.

(72)

Y

A(θ; x) = log
Y

ˆgen
Maximum (conditional) likelihood in these models leads to a generative estimator θn or a discriminative
ˆdis :
estimator θn
ˆgen
θn
ˆdis
θn

def

=

argmin Gn (θ),

Gn (θ) = −

θ
def

=

argmin Dn (θ),

Dn (θ) = −

θ

1
n
1
n

n

log pθ (x, y),

(73)

log pθ (y | x).

(74)

i=1
n

i=1

Since we are interested in prediction, our loss function is the negative log likelihood of the discriminative
model, that is, (x, y) = − log pθ (y | x).
The work of [25] showed that if the generative model is well-speciﬁed (p∗ (x, y) = pθ∞ (x, y)), then the
generative estimator is better in the sense that
3
c
ˆgen
ˆdis
L(θn ) ≤ L(θn ) − + Op (n− 2 )
n

(75)

for some c ≥ 0. This is because the generative model, by virtual of modeling x, has higher Fisher information
than the discriminative model. On the other hand, if the model is misspeciﬁed, the discriminative estimator is asymptotically better, because the generative model does not even converge to the optimal limiting
parameters θ∞ .
We would like to reﬁne the result of [25] by creating a hybrid estimator that interpolates between the
generative and the discriminative estimators, so that if the model is “extremely mis-speciﬁed,” we can favor
15

the discriminative model, and if the model is “almost well-speciﬁed,” we give more weight to the generative
model.
To formalize these intuitions, deﬁne a hybrid estimator in our regularization framework by treating the
discriminative and generative objectives as the empirical risk and the regularizer, respectively:
((x, y); θ)

=

− log pθ (y | x),

(76)

Ln (θ)

=

(77)

Rn (λ, θ)

=

Dn (θ),
λ
Gn (θ).
n

(78)

Note that the regularizer is random as it depends on the training data. Our hybrid estimator is then just
the standard regularized estimator:
λ
ˆλ
θn = argmin Dn (θ) + Gn (θ).
n
θ∈Rd

(79)

Note that as n → ∞, the discriminative objective Dn (θ) dominates as desired.
Asymptotic analysis Now we analyze the relative risk of these estimators and try to ﬁnd the best value
of λ. Our approach generalizes the analysis of [9], which applies only to unbiased estimators for conditionally
well-speciﬁed models.
We need to compute the derivatives of the loss and regularizer. First, deﬁne the following moments of
the generative and discriminative models:
µxy
µx
µ

def

=

def

=

def

=

Vx

def

V

def

=

=

Ep∗ (X,Y ) [φ(X, Y )],

(80)

Ep∗ (X)pθ∞ (Y |X) [φ(X, Y )],

(81)

Epθ∞ (X,Y ) [φ(X, Y )],

(82)

Ep∗ (X) [Vpθ∞ (Y |X) [φ(X, Y )]],

(83)

Vpθ∞ (X,Y ) [φ(X, Y )]].

(84)

By moment-generating properties of the exponential family, we have:
¨
L = Vx ,
˙
R(λ) = λr,
˙
¨
R(λ) = λ¨,
r

(85)
r = µ − µxy ,
˙

(86)

r = V.
¨

(87)

The oracle regularization strength is then
λ∗ =

−1
−1
−1
tr{I Vx VVx } + 2B (µ − µxy ) − tr{I r Vx }
.
−1
tr{(µ − µxy )⊗ Vx }

(88)

Well-speciﬁed discriminative model To gain more insight into (88), let us consider the simpliﬁed
setting where the discriminative model is well-speciﬁed, that is, p∗ (y | x) = pθ∞ (y | x). Note that this is a
much weaker assumption than assuming the generative model is well-speciﬁed.
¨
In this setting, we have I = L by the ﬁrst Bartlett identity (Proposition 2). Next, we compute:
I

r

= E[(φ(X, Y ) − Ep∗ (X)pθ∞ (Y |X) [φ(X, Y )])(φ(X, Y ) − Epθ∞ (X,Y ) [φ(X, Y )]) ]

(89)

= E[(φ(X, Y ) − Ep∗ (X)pθ∞ (Y |X) [φ(X, Y )])φ(X, Y ) ]

(90)

= E[(φ(X, Y ) − Ep∗ (X)pθ∞ (Y |X) [φ(X, Y )])(φ(X, Y ) − Ep∗ (X)pθ∞ (Y |X) [φ(X, Y )]) ]

(91)

= Vx .

(92)
16

Misspeciﬁcation
0%
5%
50%

−1
−1
tr{I Vx VVx }
5
5.38
13.8

2B (µ − µxy )
0
−0.073
−1.0

−1
tr{(µ − µxy )⊗ Vx }
0
0.00098
0.034

λ∗
∞
310
230

L(λ∗ )
−0.65
−48
−808

Table 3: The oracle regularizer for the hybrid generative-discriminative estimator. As misspeciﬁcation
increases, we regularize less, but the relative risk is reduced more (due to more variance reduction).
Replacing I

and I

r

with our new expressions in (88), we get that the oracle regularization strength is
λ∗ =

−1
tr{(V − Vx )Vx } + 2B (µ − µxy )
.
−1
tr{(µ − µxy )⊗ Vx }

(93)

Now we interpret λ∗ . Recall that larger positive values of λ∗ means that we ought to leverage the generative model more. First, observe that V Vx —that is, the generative model has larger Fisher information
than the discriminative model; this was the key fact used in [25]. This identity means that the ﬁrst term
of the numerator is always non-negative with its magnitude equal to the fraction of missing information
provided by the generative model. The second term 2B (µ − µxy ), is opaque at this level of generality; it is
unclear whether this is term is positive or negative.
Finally, the denominator, which is always positive, aﬀects the magnitude of the regularization. Recall
our intuition that how much we leverage the generative model depends on how well-speciﬁed it is. The
−1
denominator formalizes an asymptotic notion of misspeciﬁcation, namely tr{(µ − µxy )⊗ Vx }, which is the
Mahalanobis distance between the moments of the generative model (µ) and the moments under the true
distribution (µxy ). In the extreme case when the generative model is well-speciﬁed, we have µ = µxy .
An empirical example To provide some concrete intuition, we investigated the oracle regularizer for a
synthetic binary classiﬁcation problem of predicting y ∈ {0, 1} from x ∈ {0, 1}k . We use features φ(x, y) =
(I[y = 0]x , I[y = 1]x ) ∈ R2k to deﬁne the generative (Naive Bayes) and discriminative (logistic regression)
models.
1
3
3
1
We use k = 5 and θ∞ = ( 10 , · · · , 10 , 10 , · · · , 10 ) . The data generating distribution is a mixture between
a well speciﬁed generative model and a distribution that enforces x1 = · · · = xk (which clearly violates the
Naive Bayes assumption):
p∗ (x, y) = (1 − ε)pθ∞ (x, y) + εpθ∞ (y)pθ∞ (x1 | y)I[x1 = · · · = xk ].

(94)

Note that The amount of misspeciﬁcation is controlled by 0 ≤ ε ≤ 1, the fraction of examples whose features
are perfectly correlated.
Table 3 shows how the oracle regularizer changes with ε. As ε increases, λ∗ decreases (we regularize less)
as expected. But perhaps surprisingly, the relative risk is reduced with more misspeciﬁcation; this is due to
the fact that the variance reduction term increases and has a quadratic eﬀect on L(λ∗ ). Note that this eﬀect
happens because we are changing the learning problem; if we were holding the learning problem ﬁxed (p∗ )
but only changing the generative model, we would not have this eﬀect.
Figure 2 shows the relative risk Ln (λ) for various values of λ. The vertical line corresponds to λ∗ , which
was computed numerically by sampling. Note that the minimum of the curves (argminλ Ln (λ)), the desired
quantity, is quite close to λ∗ and approaches λ∗ as n increases, which empirically justiﬁes our asymptotic
approximations.
Unlabeled data One of the key advantages of having a generative model is that we can leverage unlabeled
examples by treating the outputs as latent variables which are marginalized out. Speciﬁcally, suppose we
have m i.i.d. unlabeled examples Xn+1 , . . . , Xn+m ∼ p∗ (x), with m → ∞ as n → ∞. Deﬁne the unlabeled

17

0

relative risk

−0.005
−0.01
−0.015
−0.02

n= 75
n=100
n=150
minimum
oracle reg.

−0.025

0

10

2

4

10
regularization

10

Figure 2: Relative risk Ln (λ) of the hybrid generative/discriminative estimator for various regularization
strengths λ on a simple artiﬁcial example. Note that as the number of training points n increases, the λ
attaining the minimum of Ln (λ) approaches the oracle λ∗ (the dotted vertical line).
regularizer as
Rn (λ, θ) =

λ
r(θ),
n

r(θ) =

1
m

m

− log pθ (Xn+i ),

(95)

i=1

in a manner similar to entropy regularization (Section 3.3).
Using the fact that log pθ (x) = A(θ; x) − A(θ), we can diﬀerentiate the regularizer to obtain:
˙
R
¨
R

=
=

µ − µx ,
V − Vx .

(96)
(97)

Note that I r = 0, since the regularizer doesn’t depend on the labeled data.
If the discriminative model is well-speciﬁed, then
λ∗ =

−1
tr{(V − Vx )Vx } + 2B (µ − µxy )
.
−1
tr{(µ − µx )⊗ Vx }

(98)

˙
Since L(θ∞ ) = 0, we have µx = µxy , so that our expression for λ∗ is identical to the one from (93).
In summary, we see that regularizing the marginal likelihood on unlabeled data is asymptotically equivalent to regularizing the joint likelihood on labeled data if the discriminative model is well-speciﬁed. Although
marginal likelihood has lower Fisher information than joint likelihood, the regularizer based on marginal likelihood uses unlabeled data independent of the labeled data; the joint likelihood has higher Fisher information,
but is deﬁned on the labeled data. These two factors balance out perfectly.
This equivalence suggests that the dominant asymptotic concern in hybrid generative-discriminative
learning is developing a good generative model that is as well-speciﬁed as possible; the exact manner in
which it is used in learning is less important. In practice, if one has a very small number of training points,
regularizing on a large amount of unlabeled data is more advantageous that regularizing on the existing
labeled data, but this diﬀerence is outside the scope of our asymptotic analysis.

3.5

Multi-task linear regression

Suppose that we want to solve K related tasks given n training examples per task. The idea of multi-task
learning is to leverage the fact that the tasks are related and estimate the parameters for the tasks jointly,
thereby sharing statistical strength between tasks. There have been a fair amount of work in the literature
on multi-task learning (e.g., see [12, 3, 17, 2, 19] and reference therein).
18

Setup In this section, we focus on linear regression in the multi-task setting. We assume the following
generative process for our data:
For each task k = 1, . . . , K:
−For each data point i = 1, . . . , n:
k⊗
k
k
−−Generate the input Xi ∼ p∗ (Xi ) where E[Xi ] = Id
k
k
k
−−Generate the output Yi ∼ N (Xi θ∞ , 1)

By following this setup, we are assuming that the discriminative models is well-speciﬁed. The constraint
k⊗
E[Xi ] = Id on the true input distribution is mostly for convenience, as it leads to simpler expressions.
We can cast the multi-task problem as an ordinary single task problem by concatenating the vectors for
all the tasks:
1
K
Xi = (Xi , . . . , Xi ) ∈ RKd ,

Y = (Y 1 , . . . , Y K ) ∈ RK ,

θ = (θ1 , . . . , θK ) ∈ RKd .

(99)

It will also be useful to represent θ ∈ RKd by the matrix Θ = (θ1 , . . . , θK ) ∈ Rd×K .
We use the standard squared loss function for each task. The loss function, deﬁned over data points
(x, y) ∈ RKd × RK , one from each task, is thus:
((x, y), θ) =

1
2

K

(y k − xk θk )2 .

(100)

k=1

The purpose of the regularizer is to shrink the parameters of the K tasks towards each other. In general,
since some tasks are more related than others, it will be useful to allow the regularizer to shrink the tasks
towards each other by varying amounts. To this end, let us deﬁne a positive deﬁnite matrix Λ ∈ RK×K of
inter-task aﬃnities. Deﬁne the quadratic regularizer as follows:
r(Λ, θ) =

1
θ (Λ ⊗ Id )θ = tr{ΛΘ Θ}.
2

(101)

Note that there are O(K 2 ) regularization parameters contained in the matrix Λ to be set.
Oracle regularization The derivation of the oracle regularizer closely parallels that of Gaussian mean
estimation Section 3.1, but now extended to matrices. Let us ﬁrst compute the various derivatives of the
loss and regularizer:
¨
L = Id
˙ = (Λ ⊗ Id )θ
R
¨
R = Λ ⊗ Id

(102)
(103)
(104)

Substituting these quantities into (6), we get the following expression for the relative risk:
L(Λ)

=
=

1
⊗
tr{(Λ ⊗ Id )2 θ∞ } − tr{Λ ⊗ Id }
2
1
tr{Λ2 Θ∞ Θ∞ } − d · tr{Λ},
2

(105)
(106)

¨
where we used the fact that I = L because the discriminative model is well-speciﬁed.
Optimizing the relative risk L(λ) with respect to Λ produces the oracle regularization parameter:
Λ∗ = d(Θ∞ Θ∞ )−1 .
The associated relative risk is

(107)

1
L(Λ∗ ) = − d2 · tr{(Θ∞ Θ∞ )−1 }.
2

(108)

19

Plugin regularization Now we analyze Plugin. Letting f (Θ) = d(Θ Θ)−1 , we compute its derivative:
f˙(Θ) = −d(Θ Θ)−1 (2Θ (·))(Θ Θ)−1 .

(109)

Plugging this expression into (22), we get
E = 2d · tr{(Θ∞ Θ∞ )−1 }.

(110)

Since L• (1) − L(λ∗ ) = E, the asymptotic relative risk of Plugin is greater than that of Oracle by this
amount. Combining (108) and (110), we get that the asymptotic relative risk of Plugin is
1
L• (1) = − d(d − 4)tr{(Θ∞ Θ∞ )−1 }.
2

(111)

When this quantity is negative, Plugin is better than Unregularized; this happens for d > 4.
Oracle plugin regularization Now let us derive OraclePlugin. By Theorem 3, we get that the plugin
regularizer just needs to be reweighted by:
λ•∗ = 1 +

2d · tr{(Θ∞ Θ∞ )−1 }
2
E
=
=1− .
∗)
2 · tr{(Θ Θ )−1 }
2L(λ
−d
d
∞ ∞

(112)

The asymptotic relative risk of OraclePlugin is
L• (λ•∗ ) = L• (1) +

E2
1
= − (d − 2)2 tr{(Θ∞ Θ∞ )−1 }.
4L(λ∗ )
2

(113)

When this quantity is negative, OraclePlugin is better than Unregularized; this happens for d > 2.
Joint versus independent regularization A principal question is how much we improve performance
by using multi-task learning rather than learning the parameters of each task independently. If we solve the
K regression tasks independently with K independent regularization parameters (which are set according to
OraclePlugin), the asymptotic relative risk would be
1
•∗
2
L•
indep (λ ) = − (d − 2)
2

K
k
θ∞

−2

,

(114)

k=1

since the risks are simply additive over the tasks.
Now let us compare the relative risks of OraclePlugin using joint versus independent regularization.
Let A = Θ∞ Θ∞ with eigendecomposition A = U DU . Then the asymptotic relative risk of joint regularization can be written as
K
1
−1
L• (λ•∗ ) = − (d − 2)2
Dkk ,
(115)
2
k=1

since tr{A

−1

} = tr{D

−1

}. The asymptotic relative risk of independent regularization can be written as
•∗
L•
indep (λ )

1
= − (d − 2)2
2

K

A−1 .
kk

(116)

k=1

The gap between joint and independent regularization is large when the tasks are non-trivial but similar
k
k
(θ∞ s are close, but θ∞ is fairly large). In this case, the ﬁrst eigenvalue of A is large (corresponding to
−1
k
the principal direction along which the θ∞ s point) and the rest are small. Therefore, Dkk is small for k = 1
−1
• •∗
k 2
but quite large for k > 1. Therefore, L (λ ) is favorable. In contrast, Akk = θ∞ is small for all k, and
•∗
therefore, L•
indep (λ ) is relatively small in magnitude.
20

17
"unregularized"
"diag CV"
"uniform CV"
"plugin CV"

test risk

16
15
14
13
200

300
500
800 1000
number of training points (n)

1500

Figure 3: On the MHC-I binding prediction task, test risk for the four multi-task estimators. See text
for the description of the estimators. PluginCV (estimating all pairwise task aﬃnities using Plugin and
cross-validating the strength) works best.
MHC-I binding prediction We evaluated our multi-task regularization method on the IEDB MHC-I
peptide binding dataset created by [29] and used by [19]. The goal here is to predict the binding aﬃnity (represented by log IC50 ) of a MHC-I molecule given its amino-acid sequence (represented by a vector of binary
features, reduced to a 20-dimensional real vector using SVD). We created ﬁve regression tasks corresponding
to the ﬁve most common MHC-I molecules.
We compared four estimators:
• Unregularized: no regularization was used;
• DiagCV (Λ = cI): solve each regression task independently;
• UniformCV (Λ = c(1⊗ + 10−5 IK )): use a multi-task regularizer that shrinks the tasks towards each
other, but using the same task-aﬃnity for all pairs of tasks; and
ˆ ˆ
• PluginCV (Λ = cd(Θn Θn )−1 ): using the Plugin estimator with regularization strength c.
In all cases, the parameter where c was chosen by three-fold cross-validation from 21 candidates in [10−5 , 105 ].
Figure 3 shows the results averaged over 30 independent train/test splits. First, note that if we assume all
tasks are equally related (UniformCV), multi-task regularization actually performs worse than independent
learning (DiagCV). By learning the full matrix of task aﬃnities (PluginCV), we obtain the best results.
We also experimented with setting c directly via OraclePlugin rather than cross-validation; this did
not work very well, presumably to the inexactness of the asymptotics. Nonetheless, our asymptotic analysis
is useful for producing the form of the regularizer (determined by Λ) by setting the O(K 2 ) regularization
parameters. This would have not been computational feasible to do via cross-validation. Note that there
are alternative approaches for optimizing multiple hyperparameters for multi-task learning [19].

4

Related work and discussion

The problem of choosing regularizers has received much attention in both the machine learning literature
and the statistics literature [8]. In this section, we attempt to place our asymptotic analysis of regularization
in relation to existing work.

21

Relationship to learning theory bounds In machine learning, much of the learning theory literature
ˆλ
focuses on risk bounds, which approximate the expected risk (L(θn )) with upper bounds. Our asymptotic
analysis provides a diﬀerent type of approximation—one that is exact in the ﬁrst few terms of the expansion
of the risk, but also one that makes no precise statement about the risk for any ﬁxed n.
The two approaches also deal with fundamentally diﬀerent aspects of the problem: Risk bounds are
generally based on the complexity of the hypothesis class, whereas asymptotic expansions are based on the
variance of the estimator. [26] shows that these analyses are complementary and actually capture diﬀerent
regimes of the learning curve.
Vanilla uniform convergence bounds yield worst-case analyses, whereas our asymptotic analysis is tailored
to a particular problem (p∗ and θ∞ ) and algorithm (estimator). Localization techniques [5], regret analyses
[13], and stability-based bounds [11] all allow for some degree of problem- and algorithm-dependence. As
bounds, however, they necessarily have some looseness, whereas our analysis provides exact constants, at
least the ones associated with the lowest-order terms.
One of the principal advantages of using asymptotic expansions rather than bounds is that exact control
over the ﬁrst few terms allows us to, at least asymptotically, compare the quality of diﬀerent estimators and
choose the best one.
Relationship to asymptotic analyses in statistics Asymptotics has a rich tradition in statistics, in
particular for model selection. One of the classic analyses is the Akaike information criterion (AIC) [1], which
approximates the risk of the maximum likelihood estimate by subtracting oﬀ the asymptotic bias. There
have been many extensions to AIC [32, 31, 28, 21, 16] which apply in the misspeciﬁed setting and work for
estimators other than maximum likelihood.
Note that though our goal is diﬀerent—choosing regularization parameters rather than performing model
selection. However, we share the general idea of performing an asymptotic expansion of the risk. An
additional diﬀerence is that we are working with a single parametric model, for which we know that maximum
likelihood is asymptotically eﬃcient—that is, obtains the minimum asymptotic risk. Therefore, we need to
consider second-order asymptotics to elicit the diﬀerences in the relative risk across diﬀerent regularization
parameters. Higher-order asymptotics has been used in many statistical settings, for example, in the context
of estimating means [6, 24] or estimating shift parameters in semiparametric models [15].
Last remarks Another method for choosing regularization parameters, which is perhaps the most common
in practice, is cross-validation [14]. However, note that cross-validation is feasible only when the number of
regularization parameters is very small, as the optimization of the regularization parameters often can only be
approached in a black-box manner. In contrast, our approach can optimize many hyperparameters at once.
Perhaps the most eﬀective solution is to combine the two approaches by optimizing the form of the regularizer
using our asymptotic analyses and further calibrating the regularization weight using cross-validation. This
is a useful technique that we exploited in multi-task learning (Section 3.5).
To conclude, we have developed a general asymptotic framework for analyzing regularization, along with
an eﬃcient procedure for choosing regularization parameters based on asymptotic criteria. We believe that
the tools we have developed provide a complementary perspective on analyzing learning algorithms to that
of risk bounds, thus deepening our understanding of regularization. An important direction for future work
is to develop analyses that (1) work for non-smooth losses and regularizers and (2) work in a non-parametric
setting.

References
[1] H. Akaike. A new look at the statistical model identiﬁcation. IEEE Transactions on Automatic Control,
19:716–723, 1974.
[2] A. Argyriou, T. Evgeniou, and M. Pontil. Multi-task feature learning. In Advances in Neural Information
Processing Systems (NIPS), pages 41–48, 2007.
22

[3] B. Bakker and T. Heskes. Task clustering and gating for Bayesian multitask learning. Journal of
Machine Learning Research, 4:83–99, 2003.
[4] M. S. Bartlett. Approximate conﬁdence intervals. II. More than one unknown parameter. Biometrika,
40:306–317, 1953.
[5] P. L. Bartlett, O. Bousquet, and S. Mendelson. Local Rademacher complexities. Annals of Statistics,
33(4):1497–1537, 2005.
[6] P. E. Berkhin and B. Y. Levit. Second-order asymptotically minimax estimates for the mean of a normal
population. Problemy Peredachi Informatsii, 16:60–79, 1980.
[7] J. M. Bernardo. Reference posterior distributions for Bayesian inference. Journal of the Royal Statistics
Society: Series B (Statistical Methodology), 41:113–147, 1979.
[8] P. Bickel and B. Li. Regularization in statistics. Sociedad de Estad´
ıstica e Investigaci´n Operativa Test,
o
15:271–344, 2006.
[9] G. Bouchard. Bias-variance tradeoﬀ in hybrid generative-discriminative models. In Sixth International
Conference on Machine Learning and Applications (ICMLA), pages 124–129, 2007.
[10] G. Bouchard and B. Triggs. The trade-oﬀ between generative and discriminative classiﬁers. In International Conference on Computational Statistics, pages 721–728, 2004.
[11] O. Bousquet and A. Elisseeﬀ. Stability and generalization. Journal of Machine Learning Research,
2:499–526, 2002.
[12] R. Caruana. Multitask learning. Machine Learning, 28:41–75, 1997.
[13] N. Cesa-Bianchi and G. Lugosi. Prediction, learning, and games. Cambridge University Press, 2006.
[14] P. Craven and G. Wahba. Smoothing noisy data with spline functions. estimating the correct degree
of smoothing by the method of generalized cross-validation. Numerische Mathematik, 31(4):377–403,
1978.
[15] A. S. Dalalyan, G. K. Golubev, and A. B. Tsybakov. Penalized maximum likelihood and semiparametric
second-order eﬃciency. Annals of Statistics, 34(1):169–201, 2006.
[16] Y. C. Eldar. Generalized SURE for exponential families: Applications to regularization. IEEE Transactions on Signal Processing, 57(2):471–481, 2009.
[17] T. Evgeniou, C. Micchelli, and M. Pontil. Learning multiple tasks with kernel methods. Journal of
Machine Learning Research, 6:615–637, 2005.
[18] Y. Grandvalet and Y. Bengio. Entropy regularization. In Semi-Supervised Learning, 2005.
[19] L. Jacob, F. Bach, and J. Vert. Clustered multi-task learning: A convex formulation. In Advances in
Neural Information Processing Systems (NIPS), pages 745–752, 2009.
[20] W. James and C. Stein. Estimation with quadratic loss. In Fourth Berkeley Symposium in Mathematics,
Statistics, and Probability, pages 361–380, 1961.
[21] S. Konishi and G. Kitagawa. Generalized information criteria in model selection. Biometrika, 83(4):875–
890, 1996.
[22] J. Laﬀerty, A. McCallum, and F. Pereira. Conditional random ﬁelds: Probabilistic models for segmenting
and labeling data. In International Conference on Machine Learning (ICML), pages 282–289, 2001.

23

[23] J. A. Lasserre, C. M. Bishop, and T. P. Minka. Principled hybrids of generative and discriminative
models. In Computer Vision and Pattern Recognition (CVPR), pages 87–94, 2006.
[24] B. Y. Levit. Second-order asymptotic optimality and positive solutions of the schr¨dinger equation.
o
Theory of Probability and its Applications, 30:333–363, 1985.
[25] P. Liang and M. I. Jordan. An asymptotic analysis of generative, discriminative, and pseudolikelihood
estimators. In International Conference on Machine Learning (ICML), 2008.
[26] P. Liang and N. Srebro. On the interaction between norm and dimensionality: Multiple regimes in
learning. In International Conference on Machine Learning (ICML), 2010.
[27] A. McCallum, C. Pal, G. Druck, and X. Wang. Multi-conditional learning: Generative/discriminative
training for clustering and classiﬁcation. In Association for the Advancement of Artiﬁcial Intelligence
(AAAI), 2006.
[28] N. Murata, S. Yoshizawa, and S. Amari. Network information criterion—determining the number of
hidden units for an artiﬁcial neural network model. IEEE Transactions on Neural Networks, 5(6):865–
872, 1994.
[29] B. Peters, H. Bui, S. Frankild, M. Nielson, C. Lundegaard, E. Kostem, D. Basch, K. Lamberth, M. Harndahl, W. Fleri, S. S. Wilson, J. Sidney, O. Lund, S. Buus, and A. Sette. A community resource benchmarking predictions of peptide binding to MHC-I molecules. PLoS Compututational Biology, 2, 2006.
[30] R. Raina, Y. Shen, A. Ng, and A. McCallum. Classiﬁcation with hybrid generative/discriminative
models. In Advances in Neural Information Processing Systems (NIPS), 2004.
[31] R. Shibata. Statistical aspects of model selection. In From Data to Model, pages 215–240. 1989.
[32] C. M. Stein. Estimation of the mean of a multivariate normal distribution. Annals of Statistics,
9(6):1135–1151, 1981.
[33] A. W. van der Vaart. Asymptotic Statistics. Cambridge University Press, 1998.

A

Proof of Theorem 1 and Theorem 2

In the following, we hold λ ﬁxed, so we will omit it from the equations that follow. First deﬁne the training
objective:
def
Mn (θ) = Ln (θ) + Rn (θ).
(117)
Let

def ˆ
δn = θn − θ∞

(118)

denote the parameter error of the regularized estimator with the ﬁxed λ.
P
It is clear that the regularizer should go to zero in order to obtain consistency (δn − 0). In other words,
→
if Rn (θ) = Op (an ), then we require that an → 0. But what rate should this convergence happen? Let
1
1
us assume that an decays fast enough so that δn = Op (n− 2 ). This is always possible because Op (n− 2 ) is
achievable without regularization, and we really shouldn’t make things worse with regularization. Also, we
˙
¨
assume that Rn and Rn are also Op (an ).
The plan is as follows: We ﬁrst derive an asymptotic expression for the parameter error δn . Then we
plug this result into a Taylor expansion of the risk L to get the ﬁnal expression. From this, we will see that
the optimal rate is an = O(n−1 ).

24

A.1

Asymptotic parameter error

ˆ0 P
We assume that unregularized estimator is consistent, that is, θn − θ∞ . This licenses us to perform a
→
Taylor expansion. Taylor-expand the derivative of the training objective around θ∞ to the second-order (by
1
assumption, δn = Op (n− 2 )):
3
1 ... ⊗
˙ ˆ
˙
¨
0 = Mn (θn ) = Mn + Mn δn + M n [δn ] + Op (n− 2 ).
2

(119)

Perform some algebra on (119) to get:
1 ...
¨
˙
δn = −(Mn + M n [δn ] + Op (n−1 ))−1 Mn .
2

(120)

¨
Taylor-expand the inverse function around Mn :
¨ −1 ˙
¨ −1
δ n = −M n M n + M n

1 ...
−1
¨ −1 ˙
M n δn + Op (n ) Mn Mn .
2

(121)

Note that this is a recursive deﬁnition of δn . To remove the recursion, note that the ﬁrst-order expansion
¨ −1 ˙
(ﬁrst two terms of (119)) yields δn = −Mn Mn + Op (n−1 ). Plugging this in for δn yields:
¨ −1 ˙
¨ −1
δn = −Mn Mn − Mn

1 ... ¨ −1 ˙
−1
¨ −1 ˙
M n [Mn Mn ] + Op (n ) Mn Mn .
2

(122)

Simplifying and rearranging terms:
3
1 ¨ −1 ...
¨ −1 ˙
¨ −1 ˙
δn = −Mn Mn − Mn M n [(Mn Mn )⊗ ] + Op (n− 2 ).
2

(123)

We have thus obtained the asymptotic expression for δn in terms of derivatives of the training objective
Mn (θ). The next step will be to rewrite this expression explicitly in terms of (the derivatives of) Ln and
Rn . The expansions of the relevant quantities are as follows:
˙
Mn
−1
¨
Mn
...
Mn

˙
˙
= Ln + Rn ,
¨
¨
¨n
¨n ¨ ¨n
= (Ln + Rn )−1 = L−1 − L−1 Rn L−1 + Op (a2 ),
n
...
...
= L n + Rn,

(124)
(125)
(126)

where the ﬁrst and third equations follow from linearity and the second follows by Taylor expanding the
¨
¨
inverse function around Ln , leveraging the fact that Rn = Op (an ).
To simplify notation, deﬁne
... def ¨ −1 ...
˙ def ¨ n ˙
Un = L−1 Ln ,
(127)
U n = Ln L n ,
˙ def ¨ n ˙
Vn = L−1 Rn ,

¨ def ¨ n ¨
Vn = L−1 Rn .

(128)

We split the parameter error δn (123) into two parts, one that does not depend on the regularizer (Bn ) and
one that does (Cn ):
δn = Bn + Cn ,

(129)

where
Bn
Cn

def

=

def

=

3
1 ... ˙ ⊗
˙
−Un − U n [Un ] + Op (n− 2 ),
2
... ˙ ˙
˙
¨ ˙
−Vn + Vn Un − U n [Un Vn ] + Op (an n−1 ) + Op (a2 ).
n

(130)

(131)
...
˙
¨ −1
These equations are obtained by expanding Mn , Mn , and M n in the context of (123) and separating out
the cross terms, and only keeping the ones which are large enough.
25

A.2

Asymptotic risk

Having a handle on the parameters, we turn to the risk. Expand the risk:
1¨ ⊗
1 ... ⊗3
ˆ
˙
L(θn ) = L + L[δn ] + L[δn ] + L [δn ] + Op (n−2 ).
2
6

(132)

=0

ˆ
ˆ0
˙
Note that L(θ∞ ) = 0 because θ∞ is the minimizer of L. Now we will compute Ln = E[L(θn ) − L(θn )]
⊗
⊗3
from (132), which requires expanding δn and δn using (129). Before doing this computation, note the
following:
• All terms that don’t depend on the regularizer (that is, those terms that don’t involve Cn ) cancel in
the relative risk Ln , so we do not consider them.
• We absorb all terms of order o(an n−1 ) or o(a2 ) into · · · .
n
...
1
˙
˙
¨
• Recall that Un = Op (n− 2 ), U n = Op (1), Vn = Op (an ), and Vn = Op (an ).
˙ ˙
˙ P ˙
˙
¨ P ¨
¨
→
→
• We have a−1 Rn − R for some R; an Rn − R for some R; and a−1 nE[Ln Rn ] → I r .
n
n
P ¨
¨
˙ P
• We have L−1 − L−1 , and nL⊗ − I .
→
n
n →

Using these points, we compute and keep the following terms of the relative risk Ln :
(a)

1 ¨
2L

times the expectation of twice2 the outer product of the ﬁrst term in Bn and the ﬁrst term in Cn :
¨ ˙ ˙
E[L[Un Vn ]]

¨ ¨n ˙ ˙ ¨n
= E[L[L−1 Ln Rn L−1 ]].

(133)

¨
˙
¨n
We expand this last expression by taking a multivariate expansion of L−1 around L−1 , of Ln around
˙ n around E[Rn ]. Note that E[Ln ] = 0, so all cross-terms that do not include the product of
˙
˙
0, and of R
˙
Ln and another random variable vanish. Here are the terms which are left in the expansion:
¨ ¨n
¨
˙
˙ ¨
E tr{L(L−1 − L−1 )Ln E[Rn ]L−1 } +
¨¨ ˙ ˙
˙
¨
E tr{LL−1 Ln (Rn − E[Rn ]) L−1 } +
¨¨ ˙
˙ ¨n
¨
E tr{LL−1 Ln ERn (L−1 − L−1 )} + o(an n−1 )

(134)
(135)
(136)

¨n ˙
˙
˙ ˙ ¨
= 2E[L−1 Ln ] ERn + tr{E[Ln Rn ]L−1 } + o(an n−1 )
˙
˙
¨
= 2an E[Un ] R + an n−1 tr{I r L−1 } + o(an n−1 ).
(b)

1 ¨
2L

(137)
(138)

times the expectation of twice the outer product of the ﬁrst term of Bn and the second term of Cn :
¨¨ ˙ ⊗
¨ ¨¨
−E tr{LVn Un } = −an n−1 tr{I L−1 RL−1 } + o(an n−1 ).

(c)

1 ¨
2L

times the expectation of twice the outer product of the ﬁrst term of Bn and the third term of Cn :
... ˙ ⊗
... ˙ ⊗
¨... ˙ ˙
˙
˙
E tr{LU n [Un Vn ]Un } = E L n [Un ⊗ Vn ] + o(an n−1 ) = an E U n [Un ]

(d)

1 ¨
2L

times the expectation of twice the outer product of the second term of Bn and the ﬁrst term of Cn :
... ˙ ⊗
1 ... ˙ ⊗
1
¨ 1 ... ˙ ⊗ ˙
˙
E tr{L U n [Un ]Vn } = E L n [Un ⊗ Vn ] + o(an n−1 ) = an E U n [Un ]
2
2
2

2 Once

˙
R + o(an n−1 ).

for Bn Cn and once for Cn Bn .

26

˙
R + o(an n−1 ).

(e)

1 ¨
2L

times the expectation of twice the square of the ﬁrst term of Cn :
1
1
¨˙⊗
˙ ¨
E tr{LVn } = a2 tr{R⊗ L−1 } + o(a2 ).
n
2
2 n

(f) The

...

1
6L

times thrice the square of the ﬁrst term of Bn times the ﬁrst term of Cn :
... ˙ ⊗
1 ... ˙ ⊗
1
˙
− L [Un ⊗ Vn ] = − an E U n [Un ]
2
2

˙
R + o(an n−1 ).

... ˙ ⊗
˙
Note that the ﬁrst term in (a) contains E[Un ] and the sum of (c),(d), and (f) contains E[U n [Un ]], which
come together to create twice the expected parameter error 2E[Bn ]. Note that Bn is also referred to as the
unregularizer estimator bias, which has a limit according to nEBn → B, Using this information, we have:

Ln =

1 2
˙ ¨
˙
¨ ¨¨
¨
a tr{R⊗ L−1 } − an n−1 tr{I L−1 RL−1 } − 2an n−1 B R + an n−1 tr{I r L−1 } + · · ·
2 n
(e)

(a),(c),(d),(f )

(b)

(139)

(a)

To minimize this quantity, we want the O(an n−1 ) terms to balance the O(a2 ) terms, implying that
n
an = n−1 is in general the optimal rate. Substituting n−1 for an yields (6), thus completing the proof.

B

Proof of Proposition 1

Expanding the asymptotic expansion of the parameter error (130) and taking expectations, we get that
3
1 ¨ ... ¨ ˙ ¨
ˆ0
¨n ˙
E[θn − θ∞ ] = −E[L−1 Ln ] − E[L−1 L n [L−1 L⊗ L−1 ]] + Op (n− 2 ).
n
n
n n
2

(140)

¨n
Let’s tackle the ﬁrst term of the right-hand side of (140). Taylor expand L−1 :
¨
¨
¨
¨
¨ ¨
L−1 = L−1 − L−1 (Ln − L)L−1 + Op (n−1 ).
n

(141)

Plugging this into the ﬁrst term of (140) yields:
¨n ˙
−E[L−1 Ln ]

3
¨ ˙
¨
¨
¨ ¨ ˙
= −E[L−1 Ln ] + E[L−1 (Ln − L)L−1 Ln ] + Op (n− 2 )
3
¨ ¨ ¨ ˙
= E[L−1 Ln L−1 Ln ] + Op (n− 2 ),

(142)
(143)

˙
where the second inequality uses the fact that E[Ln ] = 0. Rearrange to put the random quantities next to
each other, creating a random rank-3 tensor:
¨ ¨ ¨ ˙
¨
¨
˙
¨
E[L−1 Ln L−1 Ln ] = E[L−1 (Ln ⊗ Ln )[L−1 ]].

(144)

¨
¨
Letting n → ∞ and scaling by n yields L−1 I 2 L−1 .
The second term of the right-hand side of (140) is simpler because all its factors have nonzero limiting
˙
¨ ... ¨
¨
expectations. Scaling by n (for L⊗ to converge to a non-trivial limit), we get L−1 L [L−1 I L−1 ].
n

C

Proof of Proposition 2

In the well-speciﬁed setting, we have (z; θ∞ ) = − log p∗ (z2 | z1 ). Start with the fact that probabilities
integrate to 1:
e−

(z;θ∞ ) ∗

p (z1 )dz = E[1] = 1.
27

(145)

With suﬃcient regularity conditions, diﬀerentiate the integrand of (145) with respect to θ:
e−

(z;θ∞ )

[− ˙(z; θ∞ )]p∗ (z1 )dz = 0.

(146)

Using the fact that e− (z;θ∞ ) p∗ (z1 ) = p∗ (z1 , z2 ), the left-hand side is equivalent to E[− ˙(Z; θ∞ )]. Therefore,
˙
˙
˙
we get the ﬁrst Bartlett identity: L = 0. Note that L = L(θ∞ ) = 0 actually holds regardless of whether the
model is well-speciﬁed because θ∞ is always chosen to minimize the expected loss L(θ), which is attained
˙
when L(θ) = 0.
Now diﬀerentiate (146) with respect to θ:
e−

(z;θ∞ )

[ ˙(z; θ∞ )⊗ − ¨(z; θ∞ )]p∗ (z1 )dz = 0.

(147)

˙
Again, note the left-hand side is E[ ˙(Z; θ∞ )⊗ − ¨(Z; θ∞ )], which is exactly I − L = 0.
Note that we can continue diﬀerentiating to obtain higher-order Bartlett identities, but these will not be
useful to us.

D

Proof of Theorem 3

ˆ P
˙
˙
¨
¨
Proof. Since λn − λ∗ , R• (λ• ) = λ• R(λ∗ ) and R• (λ• ) = λ• R(λ∗ ). We can evaluate L• (λ• ) using (6) as a
→
n
• 2
reference. In particular, the ﬁrst term is multiplied by (λ ) , the second and third terms are multiplied by
λ• , and the last term changes in the following way:
¨
˙
˙
¨
¨
tr{I •r L−1 } = lim n2 E[tr{Ln λ• Rn (λ∗ + εn ) L−1 }] = λ• (tr{I r L−1 } + E),
n→∞

˙•
˙
where the ﬁrst equality used the fact that Rn (λ• ) = λ• Rn (λ∗ + εn ), and the second equality follows from
˙ n around λ∗ . As a result,
Taylor expanding R
L• (λ• ) =

1 • 2
˙
¨
¨ ¨
¨
˙
¨
(λ ) tr{R(λ∗ )⊗ L−1 } − λ• (tr{I L−1 R(λ∗ )L−1 } + 2B R(λ∗ ) − tr{I r (λ∗ )L−1 } − E).
2

When λ• = 1, then the expression reduces to L(λ∗ ) + E, proving the ﬁrst part of the theorem.
Now, we optimize λ• . Deﬁne
A1
A2

def

=

¨ ¨
¨
˙
¨
tr{I L−1 R(λ∗ )L−1 } + 2B R(λ∗ ) − tr{I r (λ∗ )L−1 },

def

˙
tr{R(λ ) L

=

∗ ⊗

¨−1

}.

(148)
(149)

Note that

1
A2 − (A1 − E).
2
Now optimizing L• (λ• ) with respect to λ• yields
L• (1) =

λ•∗ =

A1 − E
,
A2

L• (λ•∗ ) =

−(A1 − E)2
.
2A2

(150)

(151)

Subtracting, we have
L• (λ•∗ ) − L• (1) = −

(A1 − E − A2 )2
.
2A2

(152)

If Rn (λ) is linear in λ, the linear and quadratic terms of the oracle relative risk must be balanced, so
A1 = A2 = −2L(λ∗ ); as a special case, Corollary 1 applies when λ is a scalar. The second part of the
theorem follows from algebra.
28

E

Proof of Theorem 4

Proof. The theorem essentially follows by Taylor expanding f around θ∞ . First, we have
ˆ0
¨n ˙
θn − θ∞ = −L−1 Ln + Op (n−1 ).

(153)

ˆ
¨n ˙
εn = λn − λ∗ = −f˙L−1 Ln + Op (n−1 ),

(154)

Since f is diﬀerentiable,
where f˙ ∈ Rb×d is the Jacobian of f . Plugging into the deﬁnition of E (19), we get
E

=
=

5
˙
˙
¨ ˙
¨
lim n2 E[tr{Ln ( Rn (λ∗ )(−f˙L−1 Ln )) L−1 } + Op (n− 2 )]
n

(155)

5
˙ ¨
˙
¨
lim −n2 E[tr{L⊗ L−1 ( Rn (λ∗ )f˙) L−1 } + Op (n− 2 )].
n n

(156)

n→∞
n→∞

Taking limits and observing that I

¨
and L are symmetric completes the proof.

29

