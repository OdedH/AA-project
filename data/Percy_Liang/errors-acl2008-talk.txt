Analyzing Errors of Unsupervised Learning
ACL 2008

Columbus, Ohio

June 18, 2008

Percy Liang

Dan Klein

Unsupervised grammar induction
Goal: induce hidden syntax

The man

ate

a

tasty sandwich

2

Unsupervised grammar induction
Goal: induce hidden syntax

DT

NN VBD DT

The man

ate

a

JJ

NN

tasty sandwich

POS tagging

2

Unsupervised grammar induction
Goal: induce hidden syntax

DT

NN VBD DT

The man

ate

a

JJ

NN

tasty sandwich

DT NN VBD
DT
JJ NN

POS tagging

2

Unsupervised grammar induction
Goal: induce hidden syntax
S
NP
DT

NN VBD DT

The man

ate

a

JJ

NN

tasty sandwich

VP

DT NN VBD

NP
DT

POS tagging

NN

JJ NN
Constituency parsing

2

Unsupervised grammar induction
Goal: induce hidden syntax
S
NP
DT

NN VBD DT

The man

ate

a

JJ

NN

tasty sandwich

VP

DT NN VBD

NP
DT

POS tagging

NN

JJ NN
Constituency parsing

For example, on POS tagging using HMMs:
Unsupervised using EM 60%

2

Unsupervised grammar induction
Goal: induce hidden syntax
S
NP
DT

NN VBD DT

The man

ate

a

JJ

NN

tasty sandwich

VP

DT NN VBD

NP
DT

POS tagging

NN

JJ NN
Constituency parsing

For example, on POS tagging using HMMs:
Unsupervised using EM 60%
Supervised ≥ 90%
2

Unsupervised grammar induction
Goal: induce hidden syntax
S
NP
DT

NN VBD DT

The man

ate

a

JJ

NN

tasty sandwich

VP

DT NN VBD

NP
DT

POS tagging

NN

JJ NN
Constituency parsing

For example, on POS tagging using HMMs:
Unsupervised using EM 60%
Supervised ≥ 90%
Why does EM fail?
2

Four types of errors:

3

Four types of errors:
Optimization error
Local optima

3

Four types of errors:
Optimization error
Local optima

Estimation error
Limited data

3

Four types of errors:
Optimization error
Local optima

Estimation error
Limited data

Approximation error
Likelihood objective ⇔ accuracy

3

Four types of errors:
Optimization error
Local optima

Estimation error
Limited data

Approximation error
Likelihood objective ⇔ accuracy

Identiﬁability error
Diﬀerent parameter settings → same objective

3

Approximation error
Problem: model likelihood ⇔ prediction accuracy

4

Approximation error
Problem: model likelihood ⇔ prediction accuracy
PCFG (EM starting from supervised parameter estimate):
1.0

-17.2

0.8

Labeled F1

log-likelihood

-16.7

-17.6
-18.0
-18.4
20

40

60

iteration

80

100

0.6
0.4
0.2
20

40

60

80

100

iteration

4

Approximation error
Problem: model likelihood ⇔ prediction accuracy
PCFG (EM starting from supervised parameter estimate):
1.0

-17.2

0.8

Labeled F1

log-likelihood

-16.7

-17.6
-18.0
-18.4
20

40

60

iteration

80

100

0.6
0.4
0.2
20

40

60

80

100

iteration

What qualitative changes is EM making?
4

Migrations
For the HMM:
Truth

DT NN
NN
RB VBD
NNS
The chief executive allegedly made contributions

Iteration 1

DT JJ
NN
RB VBN
NNS
The chief executive allegedly made contributions

5

Migrations
For the HMM:
Truth

DT NN
NN
RB VBD
NNS
The chief executive allegedly made contributions

Iteration 1

DT JJ
NN
RB VBN
NNS
The chief executive allegedly made contributions

Summarize changes by a set of migrations:
NN

VBD

JJ

VBN

5

Migrations
For the HMM:
Truth

DT NN
NN
RB VBD
NNS
The chief executive allegedly made contributions

Iteration 1

DT JJ
NN
RB VBN
NNS
The chief executive allegedly made contributions

Summarize changes by a set of migrations:
NN→NN

VBD→made

JJ→NN

VBN→made

5

Migrations
For the HMM:
Truth

DT NN
NN
RB VBD
NNS
The chief executive allegedly made contributions

Iteration 1

DT JJ
NN
RB VBN
NNS
The chief executive allegedly made contributions

Summarize changes by a set of migrations:
NN→NN

VBD→made

JJ→NN

VBN→made

What are the prominent migrations over the entire corpora?
5

Top HMM migrations
Iteration 1
START

NN
NNP

Sentence-initial nouns are often proper
START Revenue/NN/NNP rose

6

Top HMM migrations
Iteration 1
START
NN
JJ

NN
NNP

Sentence-initial nouns are often proper
START Revenue/NN/NNP rose

NN

Noun adjuncts → adjectives (inconsistent gold tags)
chief/NN/JJ executive/NN oﬃcer

6

Top HMM migrations
Iteration 1
NN
NNP

Sentence-initial nouns are often proper
START Revenue/NN/NNP rose

NN
JJ

NN

Noun adjuncts → adjectives (inconsistent gold tags)
chief/NN/JJ executive/NN oﬃcer

NNP

NNP
NNPS

Inconsistent gold tags
UBS Securities/NNP/NNPS

START

6

Top HMM migrations
Iteration 1
NN
NNP

Sentence-initial nouns are often proper
START Revenue/NN/NNP rose

NN
JJ

NN

Noun adjuncts → adjectives (inconsistent gold tags)
chief/NN/JJ executive/NN oﬃcer

NNP

NNP
NNPS

Inconsistent gold tags
UBS Securities/NNP/NNPS

START

Iteration 2
NN
NN
JJ
START
JJ
RB

(same as above)

NN
NNP

(same as above)

TO

Inconsistent gold tags
contribute much/JJ/RB to
6

Meta-modeling for PCFGs
S
EX
Truth

VP

There VBZ
is

NP
NP

PP

an element of make-work
S
VP

EX
VP

Iteration 1 There
VBZ

NP

is

PP
of make-work

an element

7

Meta-modeling for PCFGs
S
EX
Truth

VP

There VBZ
is

NP
NP

NP→NP PP

VP→VBZ NP

VP→VBZ NP

VP→VP PP

PP

an element of make-work
S
VP

EX
VP

Iteration 1 There
VBZ

NP

is

PP
of make-work

an element

7

Meta-modeling for PCFGs
S
EX
Truth

VP

There VBZ
is

NP
NP

NP→NP PP

VP→VBZ NP

VP→VBZ NP

VP→VP PP

PP

an element of make-work
S
VP

EX
VP

Iteration 1 There
VBZ

NP

is

PP
of make-work

an element

Migrations less clear due to uncertainty in tree structure...

7

Meta-modeling for PCFGs
S
EX
Truth

VP

There VBZ
is

NP
NP

NP→NP PP

VP→VBZ NP

VP→VBZ NP

VP→VP PP

PP

an element of make-work
S
VP

EX
VP

Iteration 1 There
VBZ

NP

is

PP
of make-work

an element

Migrations less clear due to uncertainty in tree structure...

7

Meta-modeling for PCFGs
S
EX
Truth

VP

There VBZ
is

NP
NP

NP→NP PP

VP→VBZ NP

VP→VBZ NP

VP→VP PP

PP

an element of make-work
S
VP

EX
VP

Iteration 1 There
VBZ

NP

is

PP
of make-work

an element

Migrations less clear due to uncertainty in tree structure...
Our approach: use a meta-model
• Migrations are hidden alignments to be learned
• Fit using EM
7

Meta-modeling for PCFGs
S
EX
Truth

VP

There VBZ
is

NP
NP

NP→NP PP

VP→VBZ NP

VP→VBZ NP

VP→VP PP

PP

an element of make-work
S
VP

EX
VP

Iteration 1 There
VBZ

NP

is

PP
of make-work

an element

Migrations less clear due to uncertainty in tree structure...
Our approach: use a meta-model
• Migrations are hidden alignments to be learned
• Fit using EM (convex, similar to IBM model 1)
7

Top PCFG migrations learned by meta-model
Iteration 1
S
RB

VP

RB

VP

Sentential adverbs → VP adverbs

VP

8

Top PCFG migrations learned by meta-model
Iteration 1
S
RB

VP

RB

VP

Sentential adverbs → VP adverbs

VP
NP
NP

PP

VP

PPs raised from NPs to verbal level

PP
VP

8

Top PCFG migrations learned by meta-model
Iteration 1
S
RB

VP

RB

VP

Sentential adverbs → VP adverbs

VP
NP
NP

PP

VP

PPs raised from NPs to verbal level

PP
VP

Iteration 2
NP
NNP

NP

Right-branching → left-branching structures

NNP NNP
NP

8

Top PCFG migrations learned by meta-model
Iteration 1
S
RB

VP

RB

VP

Sentential adverbs → VP adverbs

VP
NP
NP

PP

VP

PPs raised from NPs to verbal level

PP
VP

Iteration 2
NP
NNP

NP

Right-branching → left-branching structures

NNP NNP
NP
VP
VBN PP
VP

PP raised to higher VP

PP
VP
8

Meta-modeling summary
• Meta-model: a diagnostic tool to analyze errors systematically

9

Meta-modeling summary
• Meta-model: a diagnostic tool to analyze errors systematically
• General phenomenon: regularization of syntactic structure

9

Meta-modeling summary
• Meta-model: a diagnostic tool to analyze errors systematically
• General phenomenon: regularization of syntactic structure
Approximation error
Identiﬁability error
Estimation error
Optimization error

9

Identiﬁability error
x: input sentence
y: hidden output
pθ (x, y): joint distribution with parameters θ

10

Identiﬁability error
x: input sentence
y: hidden output
pθ (x, y): joint distribution with parameters θ
Non-identiﬁability:

θ1

?

θ2

10

Identiﬁability error
x: input sentence
y: hidden output
pθ (x, y): joint distribution with parameters θ
Non-identiﬁability:
Learning is indiﬀerent...

pθ1 (x)

=

pθ2 (x)

θ1

?

θ2

10

Identiﬁability error
x: input sentence
y: hidden output
pθ (x, y): joint distribution with parameters θ
Non-identiﬁability:

but matters to prediction (bad!)

pθ1 (x)

=

pθ2 (x)

θ1

Learning is indiﬀerent...

?

θ2

pθ1 (y | x) = pθ2 (y | x)

10

Examples of non-identiﬁability
• Label symmetries
1

2

a

b

and

2

1

a

both generate abababab...

b

11

Examples of non-identiﬁability
• Label symmetries
1

2

a

and

b

2

1

a

both generate abababab...

b

• K-state HMM (if true distribution is < K-state HMM)
1

2

a

b

and

1

2

3

a

b

both generate abababab...

a

11

Examples of non-identiﬁability
• Label symmetries
1

2

a

and

b

2

1

a

both generate abababab...

b

• K-state HMM (if true distribution is < K-state HMM)
1

2

a

and

b

1

2

3

a

b

both generate abababab...

a

• PCFG (if true distribution is HMM)
and

can both simulate the HMM

11

Examples of non-identiﬁability
• Label symmetries
1

2

a

and

b

2

1

a

both generate abababab...

b

• K-state HMM (if true distribution is < K-state HMM)
1

2

a

and

b

1

2

3

a

b

both generate abababab...

a

• PCFG (if true distribution is HMM)
and

can both simulate the HMM

Real data is complex, so last two are not an issue
11

Identiﬁability and distance
Given θ1 and θ2, how to measure distance between them?
Want distance

(

1

2

a

b

,

2

1

a

b

)

=0

12

Identiﬁability and distance
Given θ1 and θ2, how to measure distance between them?
Want distance

(

1

2

a

b

,

2

1

a

b

)

=0

• Computing label-permutation invariant distance is NP-hard
• We use bipartite matching to ﬁnd lower and upper bounds

12

Identiﬁability and distance
Given θ1 and θ2, how to measure distance between them?
Want distance

(

1

2

a

b

,

2

1

a

b

)

=0

• Computing label-permutation invariant distance is NP-hard
• We use bipartite matching to ﬁnd lower and upper bounds
Approximation error
Identiﬁability error
Estimation error
Optimization error
12

Estimation and optimization errors
Experiment setup:
• Take some parameters θ∗ (say, supervised estimate on real data)
• Use θ∗ to generate synthetic data
• Can we recover θ∗ using EM?

13

Estimation and optimization errors
Experiment setup:
• Take some parameters θ∗ (say, supervised estimate on real data)
• Use θ∗ to generate synthetic data
• Can we recover θ∗ using EM?
θ∗

estimation
error

global optimum

optimization
error

EM solution

13

Estimation and optimization errors
Experiment setup:
• Take some parameters θ∗ (say, supervised estimate on real data)
• Use θ∗ to generate synthetic data
• Can we recover θ∗ using EM? No?
θ∗

estimation
error

optimization
error

global optimum

EM solution

HMM on 5K examples:
Accuracy

1.0
0.8
0.6
0.4

Sup. init.
Unif. init.

0.2
20

40

60

80

100

iteration

13

Estimation and optimization errors
Experiment setup:
• Take some parameters θ∗ (say, supervised estimate on real data)
• Use θ∗ to generate synthetic data
• Can we recover θ∗ using EM? Yes!
θ∗

estimation
error

optimization
error

global optimum

EM solution

HMM on 500K examples:
Accuracy

1.0
0.8
0.6
0.4

Sup. init.
Unif. init.

0.2
20

40

60

80

100

iteration

13

Optimization error decreases with more data
On HMM model (similar for PCFG and a dependency model):
Sup. init
Unif. init
Accuracy

1.0
0.9
0.8
0.7
0.6
500

5K

50K

500K

# examples

Synthetic data

14

Optimization error decreases with more data
On HMM model (similar for PCFG and a dependency model):
Sup. init
Unif. init
0.8

0.9

0.7

Accuracy

Accuracy

1.0

0.8
0.7
0.6
500

5K

50K

500K

# examples

Synthetic data

0.6
0.4
0.3
1K

3K

10K

40K

# examples

Real data

14

Optimization error decreases with more data
On HMM model (similar for PCFG and a dependency model):
Sup. init
Unif. init
0.8

0.9

0.7

Accuracy

Accuracy

1.0

0.8
0.7
0.6
500

5K

50K

500K

# examples

Synthetic data

0.6
0.4
0.3
1K

3K

10K

40K

# examples

Real data

Why does this phenomenon happen?
• Intuition: with more data, EM can pick up the
salient patterns more easily
• Was also shown for mixture of Gaussians [Srebro, 2006]
14

Summary
Approximation error
Meta-model: tool for systematic error analysis

15

Summary
Approximation error
Meta-model: tool for systematic error analysis

Identiﬁability error
Distance robust to label symmetries

15

Summary
Approximation error
Meta-model: tool for systematic error analysis

Identiﬁability error
Distance robust to label symmetries

Estimation error
Decreases with more data

15

Summary
Approximation error
Meta-model: tool for systematic error analysis

Identiﬁability error
Distance robust to label symmetries

Estimation error
Decreases with more data

Optimization error
Decreases with more data!

15

