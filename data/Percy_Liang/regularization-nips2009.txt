Asymptotically Optimal Regularization
in Smooth Parametric Models
Percy Liang
University of California, Berkeley

Francis Bach
´
INRIA - Ecole Normale Sup´ rieure, France
e

pliang@cs.berkeley.edu

francis.bach@ens.fr

Guillaume Bouchard
Xerox Research Centre Europe, France

Michael I. Jordan
University of California, Berkeley

Guillaume.Bouchard@xrce.xerox.com

jordan@cs.berkeley.edu

Abstract
Many types of regularization schemes have been employed in statistical learning,
each motivated by some assumption about the problem domain. In this paper,
we present a uniﬁed asymptotic analysis of smooth regularizers, which allows us
to see how the validity of these assumptions impacts the success of a particular
regularizer. In addition, our analysis motivates an algorithm for optimizing regularization parameters, which in turn can be analyzed within our framework. We
apply our analysis to several examples, including hybrid generative-discriminative
learning and multi-task learning.

1

Introduction

Many problems in machine learning and statistics involve the estimation of parameters from ﬁnite
data. Although empirical risk minimization has favorable limiting properties, it is well known that
this procedure can overﬁt on ﬁnite data. Hence, various forms of regularization have been employed
to control this overﬁtting. Regularizers are usually chosen based on assumptions about the problem
domain at hand. For example, in classiﬁcation, we might use L2 regularization if we expect the data
to be separable with a large margin. We might regularize with a generative model if we think it is
roughly well-speciﬁed [7, 20, 15, 17]. In multi-task learning, we might penalize deviation between
parameters across tasks if we believe the tasks to be similar [3, 12, 2, 13].
In each case, we would like (1) a procedure for choosing the parameters of the regularizer (for example, its strength) and (2) an analysis that shows the amount by which regularization reduces expected
risk, expressed as a function of the compatibility between the regularizer and the problem domain.
In this paper, we address these two points by developing an asymptotic analysis of smooth regularizers for parametric problems. The key idea is to derive a second-order Taylor approximation of the
expected risk, yielding a simple and interpretable quadratic form which can be directly minimized
with respect to the regularization parameters. We ﬁrst develop the general theory (Section 2) and
then apply it to some examples of common regularizers used in practice (Section 3).

2

General theory

We use uppercase letters (e.g., L, R, Z) to denote random variables and script letters (e.g., L, R, I)
to denote constant limits of random variables. For a λ-parametrized differentiable function θ →
...
¨
f (λ; θ), let f˙, f , and f denote the ﬁrst, second and third derivatives of f with respect to θ, and
let f (λ; θ) denote the derivative with respect to λ. Let Xn = Op (n−α ) denote a sequence of
1

P

random variables for which nα Xn is bounded in probability. Let Xn − X denote convergence in
→
probability. For a vector v, let v ⊗ = vv . Expectation and variance operators are denoted as E[·]
and V[·], respectively.
2.1

Setup

1
We are given a loss function (·; θ) parametrized by θ ∈ Rd (e.g., ((x, y); θ) = 2 (y − x θ)2 for
linear regression). Our goal is to minimize the expected risk,
def

θ∞ = argmin L(θ),

def

L(θ) = EZ∼p∗ [ (Z; θ)],

(1)

θ∈Rd

which averages the loss over some true data generating distribution p∗ (Z). We do not have access
to p∗ , but instead receive a sample of n i.i.d. data points Z1 , . . . , Zn drawn from p∗ . The standard
unregularized estimator minimizes the empirical risk:
ˆ0 def
θn = argmin Ln (θ),

def

Ln (θ) =

θ∈Rd

1
n

n

(Zi , θ).

(2)

i=1

ˆ0
Although θn is consistent (that is, it converges in probability to θ∞ ) under relatively weak conditions, it is well known that regularization can improve performance substantially for ﬁnite n. Let
Rn (λ, θ) be a (possibly data-dependent) regularization function, where λ ∈ Rb are the regularizaλ
tion parameters. For linear regression, we might use squared regularization (Rn (λ, θ) = 2n θ 2 ),
where λ ∈ R determines the strength. Deﬁne the regularized estimator as follows:
ˆ def
θλ = argmin Ln (θ) + Rn (λ, θ).
(3)
n

θ∈Rd

The goal of this paper is to choose good values of λ and analyze the subsequent impact on performance. Speciﬁcally, we wish to minimize the relative risk:
def
ˆ
ˆ
Ln (λ) = EZ ,...,Z ∼p∗ [L(θλ ) − L(θ0 )],
(4)
1

n

n

n

which is the difference in risk (averaged over the training data) between the regularized and unregularized estimators; Ln (λ) < 0 is desirable. Clearly, argminλ Ln (λ) is the optimal regularization
parameter. However, it is difﬁcult to get a handle on Ln (λ). Therefore, the main focus of this work is
on deriving an asymptotic expansion for Ln (λ). In this paper, we make the following assumptions:1
Assumption 1 (Compact support). The true distribution p∗ (Z) has compact support.
Assumption 2 (Smooth loss). The loss function (z, θ) is thrice-differentiable with respect to θ.
¨
Furthermore, assume the expected Hessian of the loss function is positive deﬁnite (L(θ∞ ) 0).2
Assumption 3 (Smooth regularizer). The regularizer Rn (λ, θ) is thrice-differentiable with respect
P
to θ and differentiable with respect to λ. Assume Rn (0, θ) ≡ 0 and Rn (λ, θ) − 0 as n → ∞.
→
2.2

Rate of regularization strength

Let us establish some basic properties that the regularizer Rn (λ, θ) should satisfy. First, a desirable
ˆλ P
property is consistency (θn − θ∞ ), i.e., convergence to the parameters that achieve the minimum
→
possible risk in our hypothesis class. To achieve this, it sufﬁces (and in general also necessitates)
that (1) the loss class satisﬁes standard uniform convergence properties [22] and (2) the regularizer
P
has a vanishing impact in the limit of inﬁnite data (Rn (λ, θ) − 0). These two properties can be
→
veriﬁed given our assumptions.
The next question is at what rate Rn (λ, θ) should converge to 0? As we show in [16], Rn (λ, θ) =
Op (n−1 ) is the rate that minimizes the relative risk Ln . With this rate, it is natural to consider the
regularizer as a prior p(θ | λ) ∝ exp{−Rn (λ, θ)} (and − (z, θ) as the log-likelihood), in which
ˆλ
case θn is the maximum a posteriori (MAP) estimate.
1

While we do not explicitly assume convexity of and Rn , the local nature of our analysis means that we
are essentially working under strong convexity.
2
¨
This assumption can be weakened. If L 0, the parameters can only be estimated up to the row space of
¨
L. But since we are interested in the parameters θ only in terms of L(θ), this particular non-identiﬁability of
the parameters is irrelevant.

2

2.3

Asymptotic expansion

Our main result is the following theorem, which provides a simple interpretable asymptotic expression for the relative risk, characterizing the impact of regularization (see [16] for proof):
Theorem 1. Assume Rn (λ, θ∞ ) = Op (n−1 ). The relative risk admits the following asymptotic
expansion:
5
Ln (λ) = L(λ) · n−2 + Op (n− 2 )
(5)
in terms of the asymptotic relative risk:
def 1
˙
¨
¨ ¨
¨
˙
¨
L(λ) = tr{R(λ)⊗ L−1 } − tr{I L−1 R(λ)L−1 } − 2B R(λ) + tr{I r (λ)L−1 },
(6)
2
def
¨ def
where L = E[ ¨(Z; θ∞ )], R(λ) = limn→∞ nRn (λ, θ∞ ) (derivatives thereof are deﬁned analodef
def
def
ˆ
˙ ˙
gously), I = E[ ˙(Z; θ∞ )⊗ ], I r (λ) = limn→∞ nE[Ln Rn (λ) ], B = limn→∞ nE[θ0 − θ∞ ].
n

The most important equation of this paper is (6), which captures the lowest-order terms of the relative
risk deﬁned in (4).
Interpretation The signiﬁcance of Theorem 1 is in identifying the three problem-dependent contributions to the asymptotic relative risk:
˙
¨
˙
Squared bias of the regularizer tr{R(λ)⊗ L−1 }: R(λ) is the gradient of the regularizer at the lim˙
iting parameters θ∞ ; the squared regularizer bias is the squared norm of R(λ) with respect to the
¨ Note that the squared regularizer bias is always positive: it always
Mahalanobis metric given by L.
increases the risk by an amount which depends on how “wrong” the regularizer is.
¨ ¨
¨
¨
Variance reduction provided by the regularizer tr{I L−1 R(λ)L−1 }: The key quantity is R(λ),
¨
the Hessian of the regularizer, whose impact on the relative risk is channeled through L−1 and
¨
I . For convex regularizers, R(λ)
0, so we always improve the stability of the estimate by
regularizing. Furthermore, if the loss is the negative log-likelihood and our model is well-speciﬁed
¨
(that is, p∗ (z) = exp{− (z; θ∞ )}), then I = L by the ﬁrst Bartlett identity [4], and the variance
¨ L−1 }.
¨
reduction term simpliﬁes to tr{R(λ)
˙
¨
Alignment between regularizer bias and unregularized estimator bias 2B R(λ) − tr{I r (λ)L−1 }:
The alignment has two parts, the ﬁrst of which is nonzero only for non-linear models and the second
of which is nonzero only when the regularizer depends on the training data. The unregularized
˙
estimator errs in direction B; we can reduce the risk if the regularizer bias R(λ) helps correct for the
˙
estimator bias (B R(λ) > 0). The second part carries the same intuition: the risk is reduced when
¨
the random regularizer compensates for the loss (tr{I r (λ)L−1 } < 0).
2.4

Oracle regularizer

The principal advantage of having a simple expression for L(λ) is that we can minimize it with
def
ˆλ∗
respect to λ. Let λ∗ = argminλ L(λ) and call θn the oracle estimator. We have a closed form for
λ∗ in the important special case that the regularization parameter λ is the strength of the regularizer:
λ
Corollary 1 (Oracle regularization strength). If Rn (λ, θ) = n r(θ) for some r(θ), then
λ∗ = argmin L(λ) =
λ

¨ ¨¨
tr{I L−1 rL−1 } + 2B r def C1
˙
=
,
¨−1 r
C2
r L ˙
˙

L(λ∗ ) = −

2
C1
.
2C2

(7)

Proof. (6) is a quadratic in λ; solve by differentiation. Compute L(λ∗ ) by substitution.
In general, λ∗ will depend on θ∞ and hence is not computable from data; Section 2.5 will remedy
this. Nevertheless, the oracle regularizer provides an upper bound on performance and some insight
into the relevant quantities that make a regularizer useful.
Note L(λ∗ ) ≤ 0, since optimizing λ∗ must be no worse than not regularizing since L(0) = 0.
But what might be surprising at ﬁrst is that the oracle regularization parameter λ∗ can be negative
3

Estimator
Notation
Relative risk

U NREGULARIZED
ˆ0
θn
0

O RACLE
ˆλ∗
θn
L(λ∗ )

P LUGIN
ˆ
ˆλ
ˆ•1
θn n = θn
•
L (1)

O RACLE P LUGIN
ˆ•λ•∗
θn
L• (λ•∗ )

Table 1: Notation for the various estimators and their relative risks.
(corresponding to “anti-regularization”). But if
helps (λ∗ > 0 and L(λ) < 0 for 0 < λ < 2λ∗ ).
2.5

∂L(λ)
∂λ

= −C1 < 0, then (positive) regularization

Plugin regularizer

While the oracle regularizer Rn (λ∗ , θ) given by (7) is asymptotically optimal, λ∗ depends on the
ˆλ∗
unknown θ∞ , so θn is actually not implementable. In this section, we develop the plugin regularizer
ˆ def
as a way to avoid this dependence. The key idea is to substitute λ∗ with an estimate λn = λ∗ + εn
1
ˆˆ def
ˆ
where εn = Op (n− 2 ). We then use the plugin estimator θλn = argmin Ln (θ) + Rn (λn , θ).
n

θ

ˆ
ˆλ
ˆ0
How well does this plugin estimator work, that is, what is its relative risk E[L(θnn ) − L(θn )]?
ˆ n ) and apply Theorem 1 because L(·) can only be applied to nonWe cannot simply write Ln (λ
random arguments. However, we can still leverage existing machinery by deﬁning a new plugin
def
•
ˆ
regularizer Rn (λ• , θ) = λ• Rn (λn , θ) with regularization parameter λ• ∈ R. Henceforth, the
superscript • will denote quantities concerning the plugin regularizer. The corresponding estimator
•
ˆ•0
ˆ•λ•
ˆ•λ• def
θn = argminθ Ln (θ) + Rn (λ• , θ) has relative risk L• (λ• ) = E[L(θn ) − L(θn )]. The key
n
ˆ
ˆˆ
ˆ
ˆλn = θ•1 , which means the asymptotic risk of the plugin estimator θλn is simply L• (1).
identity is θn
n
n

We could try to squeeze more out of the plugin regularizer by further optimizing λ• according to
def
ˆ•λ•∗ rather than just using λ• =
λ•∗ = argminλ• L• (λ• ) and use the oracle plugin estimator θn
1. In general, this is not useful since λ•∗ might depend on θ∞ , and the whole point of plugin
is to remove this dependence. However, in a fortuitous turn of events, for some linear models
ˆ•λ•∗ is actually implementable.
(Sections 3.1 and 3.4), λ•∗ is in fact independent of θ∞ , and so θn
Table 1 summarizes all the estimators we have discussed.
The following theorem relates the risks of all estimators we have considered (see [16] for the proof):
Theorem 2 (Relative risk of plugin). The relative risk of the plugin estimator is L• (1) = L(λ∗ )+E,
def
¨
˙
˙
where E = limn→∞ nE[tr{Ln ( Rn (λ∗ )εn ) L−1 }]. If Rn (λ) is linear in λ, then the relative risk
of the oracle plugin estimator is L• (λ•∗ ) = L• (1) +

E2
4L(λ∗ )

with λ•∗ = 1 +

E
2L(λ∗ ) .

Note that the sign of E depends on the nature of the error εn , so P LUGIN could be either better or
worse than O RACLE. On the other hand, O RACLE P LUGIN is always better than P LUGIN. We can
get a simpler expression for E if we know more about εn (see [16] for the proof):
ˆ
ˆ0
Theorem 3. Suppose λ∗ = f (θ∞ ) for some differentiable f : Rd → Rb . If λn = f (θn ), then the
−1
∗ ˙ ¨−1
¨
˙
results of Theorem 2 hold with E = −tr{I L
R(λ )f L }.

3

Examples

In this section, we apply our results from Section 2 to speciﬁc problems. Having made all the
asymptotic derivations in the general setting, we now only need to make a few straightforward
calculations to obtain the asymptotic relative risks and regularization parameters for a given problem.
We ﬁrst explore two classical examples from statistics (Sections 3.1 and 3.2) to get some intuition
for the theory. Then we consider two important examples in machine learning (Sections 3.3 and 3.4).
3.1

Estimation of normal means

Assume that data are generated from a multivariate normal distribution with d independent compo1
nents (p∗ = N (θ∞ , I)). We use the negative log-likelihood as the loss function: (x; θ) = 2 (x−θ)2 ,
so the model is well-speciﬁed.
4

In his seminal 1961 paper [14], Stein showed that, surprisingly, the standard empirical risk minimizer
ˆ0
ˆJS def ¯
¯ def 1 n
θn = X = n i=1 Xi is beaten by the James-Stein estimator θn = X 1 − nd−2 2 in the sense
¯
X
ˆJS
ˆ0
that E[L(θn )] < E[L(θn )] for all n and θ∞ if d > 2. We will show that the James-Stein estimator
1
is essentially equivalent to O RACLE P LUGIN with quadratic regularization (r(θ) = 2 θ 2 ).
˙
¯ ¨
First compute Ln = θ∞ − X, L = I, B = 0, r = θ∞ , and r = I. By (7), the oracle regularization
˙
¨
d2
d
∗
weight is λ = θ∞ 2 , which yields a relative risk of L(λ∗ ) = − 2 θ∞ 2 .
Now let us derive P LUGIN (Section 2.5). We have f (θ) =
•

2d
θ∞

d
θ 2

and f˙(θ) =

−2dθ
θ 4 .

By Theorems 2

d(d−4)
− 2 θ∞ 2 .

Note that since E > 0, P LUGIN is always (asymptotiand 3, E =
2 and L (1) =
cally) worse than O RACLE but better than U NREGULARIZED if d > 4.
2
To get O RACLE P LUGIN, compute λ•∗ = 1 − d (note that this doesn’t depend on θ∞ ), which results
1− 2

2

(d−2)
•
in Rn (θ) = 1 X d2 θ 2 . By Theorem 2, its relative risk is L• (λ•∗ ) = − 2 θ∞ 2 , which offers a
2 ¯
small improvement over P LUGIN (and is superior to U NREGULARIZED when d > 2).

Note that the O RACLE P LUGIN and P LUGIN are adaptive: We regularize more or less depend¯
ing on whether our preliminary estimate of X is small or large, respectively. By simple al•λ•∗
ˆ
¯
= X 1 − n Xd−2
gebra, O RACLE P LUGIN has a closed form θn
¯ 2 +d−2 , which differs from
•∗
5
ˆ•λ − θJS = Op (n− 2 ). O RACLE P LUGIN has the added
ˆ
JAMES S TEIN by a very small amount: θn
n
beneﬁt that it always shrinks towards zero by an amount between 0 and 1, whereas JAMES S TEIN can
overshoot. Empirically, we found that O RACLE P LUGIN generally had a lower expected risk than
JAMES S TEIN when θ∞ is large, but JAMES S TEIN was better when θ∞ ≤ 1.
3.2

Binomial estimation

Consider the estimation of θ, the log-odds of a coin coming up heads. We use the negative loglikelihood loss (x; θ) = −xθ + log(1 + eθ ), where x ∈ {0, 1} is the outcome of the coin. This
example serves to provide intuition for the bias B appearing in (6), which is typically ignored in
ﬁrst-order asymptotics or is zero (for linear models).
1
Consider a regularizer r(θ) = 2 (θ + 2 log(1 + e−θ )), which corresponds to a Beta( λ , λ ) prior.
2 2
Choosing λ has been studied extensively in statistics. Some common choices are the Haldane prior
(λ = 0), the reference (Jeffreys) prior (λ = 1), the uniform prior (λ = 2), and Laplace smoothing
(λ = 4). We will choose λ to minimize expected risk adaptively based on data.
...
def
def
def
¨
˙
Deﬁne µ = 1+e1 ∞ , v = µ(1 − µ), and b = µ − 1 . Then compute L = v, L = −2vb, r = b,
−θ
2
v
−1
∗
∗
r = v, B = −v b. O RACLE corresponds to λ = 2 + b2 . Note that λ > 0, so again (positive)
¨
regularization always helps.
√

We can compute the difference between O RACLE and P LUGIN: E = 2 − 2v . If |b| > 42 , E > 0,
b2
which means that P LUGIN is worse; otherwise P LUGIN is actually better. Even when P LUGIN
is worse than O RACLE, P LUGIN is still better than U NREGULARIZED, which can be veriﬁed by
5
checking that L• (1) = − 2 vb−2 − 2v −1 b2 < 0 for all θ∞ .
3.3

Hybrid generative-discriminative learning

In prediction tasks, we wish to learn a mapping from some input x ∈ X to an output y ∈ Y. A
common approach is to use probabilistic models deﬁned by exponential families, which is deﬁned
by a vector of sufﬁcient statistics (features) φ(x, y) ∈ Rd and an accompanying vector of parameters
θ ∈ Rd . These features can be used to deﬁne a generative model (8) or a discriminative model (9):
pθ (x, y)

=

exp{φ(x, y) θ − A(θ)},

A(θ) = log

exp{φ(x, y) θ}dydx,
X

pθ (y | x)

=

exp{φ(x, y) θ − A(θ; x)},

(8)

exp{φ(x, y) θ}dy.

(9)

Y

A(θ; x) = log
Y

5

Misspeciﬁcation
0%
5%
50%

−1
−1
tr{I vx vvx }
5
5.38
13.8

2B (µ − µxy )
0
-0.073
-1.0

−1
tr{(µ − µxy )⊗ vx }
0
0.00098
0.034

λ∗
∞
310
230

L(λ∗ )
-0.65
-48
-808

Table 2: The oracle regularizer for the hybrid generative-discriminative estimator. As misspeciﬁcation increases, we regularize less, but the relative risk is reduced more (due to more variance
reduction).
ˆgen def
Given these deﬁnitions, we can either use a generative estimator θn = argminθ Gn (θ), where
n
1
ˆdis def
Gn (θ) = − n i=1 log pθ (x, y) or a discriminative estimator θn = argminθ Dn (θ), where
n
1
Dn (θ) = − n i=1 log pθ (y | x).
There has been a ﬂurry of work on combining generative and discriminative learning [7, 20, 15,
18, 17]. [17] showed that if the generative model is well-speciﬁed (p∗ (x, y) = pθ∞ (x, y)), then
3
c
ˆgen
ˆdis
the generative estimator is better in the sense that L(θn ) ≤ L(θn ) − n + Op (n− 2 ) for some
c ≥ 0; if the model is misspeciﬁed, the discriminative estimator is asymptotically better. To create a
hybrid estimator, let us treat the discriminative and generative objectives as the empirical risk and the
λ
regularizer, respectively, so ((x, y); θ) = − log pθ (y | x), so Ln = Dn and Rn (λ, θ) = n Gn (θ).
As n → ∞, the discriminative objective dominates as desired. Our approach generalizes the analysis
of [6], which applies only to unbiased estimators for conditionally well-speciﬁed models.
By moment-generating properties of the exponential family, we arrive at the following quantidef
def
¨
˙
ties (write φ for φ(X, Y )): L = vx = Ep∗ (X) [Vpθ∞ (Y |X) [φ|X]], R(λ) = λ(µ − µxy ) =
def
¨
λ(Epθ∞ (X,Y ) [φ] − Ep∗ (X,Y ) [φ]), and R(λ) = λv = λVpθ∞ (X,Y ) [φ]. The oracle regularization
parameter is then
λ∗ =

−1
−1
−1
tr{I vx vvx } + 2B (µ − µxy ) − tr{I r vx }
.
⊗ v −1 }
tr{(µ − µxy ) x

(10)

The sign and magnitude of λ∗ provides insight into how generative regularization improves prediction as a function of the model and problem: Speciﬁcally, a large positive λ∗ suggests regularization is helpful. To simplify, assume that the discriminative model is well-speciﬁed, that is,
p∗ (y | x) = pθ∞ (y | x) (note that the generative model could still be misspeciﬁed). In this case,
−1
¨
I = L, I r = vx , and so the numerator reduces to tr{(v − vx )vx } + 2B (µ − µxy ).
Since v
vx (the key fact used in [17]), the variance reduction (plus the random alignment term
from I r ) is always non-negative with magnitude equal to the fraction of missing information provided by the generative model. There is still the non-random alignment term 2B (µ − µxy ), whose
sign depends on the problem. Finally, the denominator (always positive) affects the optimal magnitude of the regularization. If the generative model is almost well-speciﬁed, µ will be close to µxy ,
and the regularizer should be trusted more (large λ∗ ). Since our analysis is local, misspeciﬁcation
(how much pθ∞ (x, y) deviates from p∗ (x, y)) is measured by a Mahalanobis distance between µ
and µxy , rather than something more stringent and global like KL-divergence.
An empirical example To provide some concrete intuition, we investigated the oracle regularizer
for a synthetic binary classiﬁcation problem of predicting y ∈ {0, 1} from x ∈ {0, 1}k . Using
features φ(x, y) = (I[y = 0]x , I[y = 1]x ) deﬁnes the corresponding generative (Naive Bayes)
1
3
3
1
and discriminative (logistic regression) estimators. We set k = 5, θ∞ = ( 10 , · · · , 10 , 10 , · · · , 10 ) ,
∗
and p (x, y) = (1 − ε)pθ∞ (x, y) + εpθ∞ (y)pθ∞ (x1 | y)I[x1 = · · · = xk ]. The amount of misspeciﬁcation is controlled by 0 ≤ ε ≤ 1, the fraction of examples whose features are perfectly
correlated.
Table 2 shows how the oracle regularizer changes with ε. As ε increases, λ∗ decreases (we regularize
less) as expected. But perhaps surprisingly, the relative risk is reduced with more misspeciﬁcation;
this is due to the fact that the variance reduction term increases and has a quadratic effect on L(λ∗ ).
Figure 1(a) shows the relative risk Ln (λ) for various values of λ. The vertical line corresponds
to λ∗ , which was computed numerically by sampling. Note that the minimum of the curves
6

(argminλ Ln (λ)), the desired quantity, is quite close to λ∗ and approaches λ∗ as n increases, which
empirically justiﬁes our asymptotic approximations.
Unlabeled data One of the main advantages of having a generative model is that we can leverage unlabeled examples by marginalizing out their hidden outputs. Speciﬁcally, suppose we have
m i.i.d. unlabeled examples Xn+1 , . . . , Xn+m ∼ p∗ (x), with m → ∞ as n → ∞. Deﬁne the
m
λ
unlabeled regularizer as Rn (λ, θ) = − nm i=1 log pθ (Xn+i ).
˙
We can compute R = µ − µxy using the stationary conditions of the loss function at θ∞ . Also,
¨ = v − vx , and I r = 0 (the regularizer doesn’t depend on the labeled data). If the model is
R
conditionally well-speciﬁed, we can verify that the oracle regularization parameter λ∗ is the same as
if we had regularized with Gn . This equivalence suggests that the dominant concern asymptotically
is developing an adequate generative model with small bias and not exactly how it is used in learning.
3.4

Multi-task regression

The intuition behind multi-task learning is to share statistical strength between tasks [3, 12, 2, 13].
Suppose we have K regression tasks. For each task k = 1, . . . , K, we generate each data point
k
k
k
k
i = 1, . . . , n independently as follows: Xi ∼ p∗ (Xi ) and Yik ∼ N (Xi θ∞ , 1). We can treat this
1
K
as a single task problem by concatenating the vectors for all the tasks: Xi = (Xi , . . . , Xi ) ∈
Kd
1
K
K
1
K
Kd
R , Y = (Y , . . . , Y ) ∈ R , and θ = (θ , . . . , θ ) ∈ R . It will also be useful to
represent θ ∈ RKd by the matrix Θ = (θ1 , . . . , θK ) ∈ Rd×K . The loss function is ((x, y), θ) =
K
1
k
k
θk )2 . Assume the model is conditionally well-speciﬁed.
k=1 (y − x
2
We would like to be ﬂexible in case some tasks are more related than others, so let us deﬁne a positive
deﬁnite matrix Λ ∈ RK×K of inter-task afﬁnities and use the quadratic regularizer: r(Λ, θ) =
k⊗
1
¨
= Id , which implies that I = L = IKd .
2 θ (Λ ⊗ Id )θ. For simplicity, assume EXi
Most of the computations that follow parallel those of Section 3.1, only extended to matrices. Substituting the relevant quantities into (6) yields the relative risk: L(Λ) = 1 tr{Λ2 Θ∞ Θ∞ } − dtr{Λ}.
2
Optimizing with respect to Λ produces the oracle regularization parameter Λ∗ = d(Θ∞ Θ∞ )−1 and
its associated relative risk L(Λ∗ ) = − 1 d2 tr{(Θ∞ Θ∞ )−1 }.
2
To analyze P LUGIN, ﬁrst compute f˙ = −d(Θ∞ Θ∞ )−1 (2Θ∞ (·))(Θ∞ Θ∞ )−1 ; we ﬁnd that P LUGIN
increases the asymptotic risk by E = 2dtr{(Θ∞ Θ∞ )−1 }. However, the relative risk of P LUGIN is
1
still favorable when d > 4, as L• (1) = − 2 d(d − 4)tr{(Θ∞ Θ∞ )−1 } < 0 for d > 4.
2
We can do slightly better using O RACLE P LUGIN (λ•∗ = 1 − d ), which results in a relative risk of
1
• •∗
2
−1
L (λ ) = − 2 (d − 2) tr{(Θ∞ Θ∞ ) }. For comparison, if we had solved the K regression tasks
completely independently with K independent regularization parameters, our relative risk would
K
k
have been − 1 (d − 2)2 ( k=1 θ∞ −2 ) (following similar but simpler computations).
2

We now compare joint versus independent regularization. Let A = Θ∞ Θ∞ with eigendecomposition A = U DU . The difference in relative risks between joint and independent regularization
−1
is ∆ = − 1 (d − 2)2 ( k Dkk − k A−1 ) (∆ < 0 means joint regularization is better). The gap
kk
2
k
between joint and independent regularization is large when the tasks are non-trivial but similar (θ∞ s
−1
−1
k
are close, but θ∞ is large). In that case, Dkk is quite large for k > 1, but all the Akk s are small.
MHC-I binding prediction We evaluated our multitask regularization method on the IEDB
MHC-I peptide binding dataset created by [19] and used by [13]. The goal here is to predict the
binding afﬁnity (represented by log IC50 ) of a MHC-I molecule given its amino-acid sequence (represented by a vector of binary features, reduced to a 20-dimensional real vector using SVD). We
created ﬁve regression tasks corresponding to the ﬁve most common MHC-I molecules.
We compared four estimators: U NREGULARIZED, D IAG CV (Λ = cI), U NIFORM CV (using
the same task-afﬁnity for all pairs of tasks with Λ = c(1⊗ + 10−5 I)), and P LUGIN CV (Λ =
ˆ ˆ
cd(Θn Θn )−1 ), where c was chosen by cross-validation.3 Figure 1 shows the results averaged over
3

We performed three-fold cross-validation to select c from 21 candidates in [10−5 , 105 ].

7

0
17
16
−0.01
−0.015
−0.02
−0.025

test risk

relative risk

−0.005

n= 75
n=100
n=150
minimum
oracle reg.

0

10

15

"unregularized"
"diag CV"
"uniform CV"
"plugin CV"

14
13
2

10
regularization

4

200

10

(a)

300
500
800 1000
number of training points (n)

1500

(b)

Figure 1: (a) Relative risk (Ln (λ)) of the hybrid generative/discriminative estimator for various λ;
the λ attaining the minimum of Ln (λ) is close to the oracle λ∗ (the vertical line). (b) On the MHCI binding prediction task, test risk for the four multi-task estimators; P LUGIN CV (estimating all
pairwise task afﬁnities using P LUGIN and cross-validating the strength) works best.

30 independent train/test splits. Multi-task regularization actually performs worse than independent
learning (D IAG CV) if we assume all tasks are equally related (U NIFORM CV). By learning the full
matrix of task afﬁnities (P LUGIN CV), we obtain the best results. Note that setting the O(K 2 ) entries
of Λ via cross-validation is not computationally feasible, though other approaches are possible [13].

4

Related work and discussion

The subject of choosing regularization parameters has received much attention. Much of the learning
ˆλ
theory literature focuses on risk bounds, which approximate the expected risk (L(θn )) with upper
bounds. Our analysis provides a different type of approximation—one that is exact in the ﬁrst few
terms of the expansion. Though we cannot make a precise statement about the risk for any given n,
exact control over the ﬁrst few terms offers other advantages, e.g., the ability to compare estimators.
To elaborate further, risk bounds are generally based on the complexity of the hypothesis class,
whereas our analysis is based on the variance of the estimator. Vanilla uniform convergence bounds
yield worst-case analyses, whereas our asymptotic analysis is tailored to a particular problem (p∗
and θ∞ ) and algorithm (estimator). Localization techniques [5], regret analyses [9], and stabilitybased bounds [8] all allow for some degree of problem- and algorithm-dependence. As bounds,
however, they necessarily have some looseness, whereas our analysis provides exact constants, at
least the ones associated with the lowest-order terms.
Asymptotics has a rich tradition in statistics. In fact, our methodology of performing a Taylor
expansion of the risk is reminiscent of AIC [1]. However, our aim is different: AIC is intended
for model selection, whereas we are interested in optimizing regularization parameters. The Stein
unbiased risk estimate (SURE) is another method of estimating the expected risk for linear models
[21], with generalizations to non-linear models [11].
In practice, cross-validation procedures [10] are quite effective. However, they are only feasible
when the number of hyperparameters is very small, whereas our approach can optimize many hyperparameters. Section 3.4 showed that combining the two approaches can be effective.
To conclude, we have developed a general asymptotic framework for analyzing regularization, along
with an efﬁcient procedure for choosing regularization parameters. Although we are so far restricted
to parametric problems with smooth losses and regularizers, we think that these tools provide a
complementary perspective on analyzing learning algorithms to that of risk bounds, deepening our
understanding of regularization.
8

References
[1] H. Akaike. A new look at the statistical model identiﬁcation. IEEE Transactions on Automatic
Control, 19:716–723, 1974.
[2] A. Argyriou, T. Evgeniou, and M. Pontil. Multi-task feature learning. In Advances in Neural
Information Processing Systems (NIPS), pages 41–48, 2007.
[3] B. Bakker and T. Heskes. Task clustering and gating for Bayesian multitask learning. Journal
of Machine Learning Research, 4:83–99, 2003.
[4] M. S. Bartlett. Approximate conﬁdence intervals. II. More than one unknown parameter.
Biometrika, 40:306–317, 1953.
[5] P. L. Bartlett, O. Bousquet, and S. Mendelson. Local Rademacher complexities. Annals of
Statistics, 33(4):1497–1537, 2005.
[6] G. Bouchard. Bias-variance tradeoff in hybrid generative-discriminative models. In Sixth
International Conference on Machine Learning and Applications (ICMLA), pages 124–129,
2007.
[7] G. Bouchard and B. Triggs. The trade-off between generative and discriminative classiﬁers. In
International Conference on Computational Statistics, pages 721–728, 2004.
[8] O. Bousquet and A. Elisseeff. Stability and generalization. Journal of Machine Learning
Research, 2:499–526, 2002.
[9] N. Cesa-Bianchi and G. Lugosi. Prediction, learning, and games. Cambridge University Press,
2006.
[10] P. Craven and G. Wahba. Smoothing noisy data with spline functions. estimating the correct
degree of smoothing by the method of generalized cross-validation. Numerische Mathematik,
31(4):377–403, 1978.
[11] Y. C. Eldar. Generalized SURE for exponential families: Applications to regularization. IEEE
Transactions on Signal Processing, 57(2):471–481, 2009.
[12] T. Evgeniou, C. Micchelli, and M. Pontil. Learning multiple tasks with kernel methods. Journal of Machine Learning Research, 6:615–637, 2005.
[13] L. Jacob, F. Bach, and J. Vert. Clustered multi-task learning: A convex formulation. In Advances in Neural Information Processing Systems (NIPS), pages 745–752, 2009.
[14] W. James and C. Stein. Estimation with quadratic loss. In Fourth Berkeley Symposium in
Mathematics, Statistics, and Probability, pages 361–380, 1961.
[15] J. A. Lasserre, C. M. Bishop, and T. P. Minka. Principled hybrids of generative and discriminative models. In Computer Vision and Pattern Recognition (CVPR), pages 87–94, 2006.
[16] P. Liang, F. Bach, G. Bouchard, and M. I. Jordan. Asymptotically optimal regularization in
smooth parametric models. Technical report, ArXiv, 2010.
[17] P. Liang and M. I. Jordan. An asymptotic analysis of generative, discriminative, and pseudolikelihood estimators. In International Conference on Machine Learning (ICML), 2008.
[18] A. McCallum, C. Pal, G. Druck, and X. Wang. Multi-conditional learning: Generative/discriminative training for clustering and classiﬁcation. In Association for the Advancement of Artiﬁcial Intelligence (AAAI), 2006.
[19] B. Peters, H. Bui, S. Frankild, M. Nielson, C. Lundegaard, E. Kostem, D. Basch, K. Lamberth, M. Harndahl, W. Fleri, S. S. Wilson, J. Sidney, O. Lund, S. Buus, and A. Sette. A
community resource benchmarking predictions of peptide binding to MHC-I molecules. PLoS
Compututational Biology, 2, 2006.
[20] R. Raina, Y. Shen, A. Ng, and A. McCallum.
Classiﬁcation with hybrid generative/discriminative models. In Advances in Neural Information Processing Systems (NIPS),
2004.
[21] C. M. Stein. Estimation of the mean of a multivariate normal distribution. Annals of Statistics,
9(6):1135–1151, 1981.
[22] A. W. van der Vaart. Asymptotic Statistics. Cambridge University Press, 1998.

9

