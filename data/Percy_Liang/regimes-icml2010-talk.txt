N o r m ∼ Dimensionality
Multiple Regimes in Learning
ICML - Haifa, Israel

June 24, 2010

Percy Liang

Nati Srebro

UC Berkeley

TTI Chicago

A classic question

2

A classic question
Linear regression:
X ∼ N (0, Id)

Y = X β ∗ + N (0, 1)

2

A classic question
Linear regression:
X ∼ N (0, Id)
Norm: β ∗

2

=C

Y = X β ∗ + N (0, 1)
Dimensionality: d

2

A classic question
Linear regression:
X ∼ N (0, Id)
Norm: β ∗

2

=C

Y = X β ∗ + N (0, 1)
Dimensionality: d

What is the excess risk
ˆ
En = E[(Y − X βERM on n samples)2] − E[(Y − X β ∗)2]?
(equivalently, what is sample complexity n to achieve risk ?)

2

A classic question
Linear regression:
X ∼ N (0, Id)
Norm: β ∗

2

Y = X β ∗ + N (0, 1)

=C

Dimensionality: d

What is the excess risk
ˆ
En = E[(Y − X βERM on n samples)2] − E[(Y − X β ∗)2]?
(equivalently, what is sample complexity n to achieve risk ?)

En = O

C
√
n

?

2

A classic question
Linear regression:
X ∼ N (0, Id)
Norm: β ∗

2

Y = X β ∗ + N (0, 1)

=C

Dimensionality: d

What is the excess risk
ˆ
En = E[(Y − X βERM on n samples)2] − E[(Y − X β ∗)2]?
(equivalently, what is sample complexity n to achieve risk ?)

En = O

C
√
n

?

En = O

d
n

?

2

Some answers
Finite sample complexity bounds (via uniform convergence)
En = O

C
√
n

independent of d
+ Works for any n

⇒ n = O(C 2)
− Bound can be loose

3

Some answers
Finite sample complexity bounds (via uniform convergence)
En = O

C
√
n

⇒ n = O(C 2)

independent of d
+ Works for any n

− Bound can be loose

Classical asymptotics (e.g., [van der Vaart, 1998; Liang & Jordan, 2008])
En =

d
n
2

+ op

1
n

independent of C
+ Exact up to ﬁrst order

⇒ n = Θ(d)
− Works for large n

3

Some answers
Finite sample complexity bounds (via uniform convergence)
En = O

C
√
n

⇒ n = O(C 2)

independent of d
+ Works for any n

− Bound can be loose

Classical asymptotics (e.g., [van der Vaart, 1998; Liang & Jordan, 2008])
En =

d
n
2

+ op

1
n

independent of C
+ Exact up to ﬁrst order

⇒ n = Θ(d)
− Works for large n

What’s the true behavior of En?
3

A heuristic answer

Trivial

En

n

4

A heuristic answer

Trivial

Bounds

En

n

4

A heuristic answer

Trivial

Bounds

Classical asymptotics

En

n

4

A heuristic answer

Trivial

Bounds

Classical asymptotics

En

n
Suggests multiple regimes

4

A heuristic answer

Trivial

Bounds

Classical asymptotics

En

?
n
Suggests multiple regimes
Are these the actual regimes?
Where are the regime transitions and how do they behave?
4

The learning curve
Goal: precisely characterize the full learning curve at all n

En

n

5

Overview of approach
Excess risk:
En(Ψ) (Ψ is problem complexity, e.g., Ψ = (C 2, d))

6

Overview of approach
Excess risk:
En(Ψ) (Ψ is problem complexity, e.g., Ψ = (C 2, d))
Asymptotics: n → ∞, exploit concentration to simplify

6

Overview of approach
Excess risk:
En(Ψ) (Ψ is problem complexity, e.g., Ψ = (C 2, d))
Asymptotics: n → ∞, exploit concentration to simplify
Classical:
Ψ ﬁxed
En → 0

6

Overview of approach
Excess risk:
En(Ψ) (Ψ is problem complexity, e.g., Ψ = (C 2, d))
Asymptotics: n → ∞, exploit concentration to simplify
Classical:
Ψ ﬁxed
En → 0
High-dimensional:
Ψ→∞
En > 0

6

Overview of approach
Excess risk:
En(Ψ) (Ψ is problem complexity, e.g., Ψ = (C 2, d))
Asymptotics: n → ∞, exploit concentration to simplify
Classical:
Ψ ﬁxed
En → 0
High-dimensional:
Ψ→∞
En > 0
Important: preserve ratio between sample size and complexity
d
˜
˜
Ψ→Ψ
(e.g., n → d)

6

Overview of approach
Excess risk:
En(Ψ) (Ψ is problem complexity, e.g., Ψ = (C 2, d))
Asymptotics: n → ∞, exploit concentration to simplify
Classical:
Ψ ﬁxed
En → 0
High-dimensional:
Ψ→∞
En > 0
Important: preserve ratio between sample size and complexity
d
˜
˜
Ψ→Ψ
(e.g., n → d)
Asymptotic excess risk:
P

En(Ψ) − E(Ψ) non-degenerate
→ ˜
6

Overview of approach
Excess risk:
En(Ψ) (Ψ is problem complexity, e.g., Ψ = (C 2, d))
Asymptotics: n → ∞, exploit concentration to simplify
Classical:
Ψ ﬁxed
En → 0
High-dimensional:
Ψ→∞
En > 0
Important: preserve ratio between sample size and complexity
d
˜
˜
Ψ→Ψ
(e.g., n → d)
Asymptotic excess risk:
P

˜
En(Ψ) − E(Ψ) non-degenerate
→
6

Two examples

Mean estimation

Linear regression

6

Mean estimation: setup
Problem:
Data: X (1), · · · , X (n) ∼ N (µ∗, I)
µ∗

2

= B2

µ ∗ ∈ Rd

Goal: estimate µ∗

7

Mean estimation: setup
Problem:
Data: X (1), · · · , X (n) ∼ N (µ∗, I)
µ∗

2

µ ∗ ∈ Rd

= B2

Goal: estimate µ∗
Estimator:
µ=
ˆ

¯
BX
¯ ,
X

¯
X=

1
n

n
X (i)
i=1

7

Mean estimation: setup
Problem:
Data: X (1), · · · , X (n) ∼ N (µ∗, I)
µ∗

2

µ ∗ ∈ Rd

= B2

Goal: estimate µ∗
Estimator:
µ=
ˆ

¯
BX
¯ ,
X

¯
X=

1
n

n
X (i)
i=1

Excess risk:
En = (ˆ − µ∗)2
µ

7

Mean estimation: theorem
If:
˜
B2 → B2

d
n

˜
→d

9

Mean estimation: theorem
If:
˜
B2 → B2
Then:
P

En − E,
→

˜
E = 4B 2 sin2

d
n

˜
→d
1
2

arctan

˜
d
˜
B2

9

Mean estimation: theorem
If:
˜
B2 → B2
Then:
P

En − E,
→

˜
E = 4B 2 sin2

d
n

˜
→d
1
2

arctan

˜
d
˜
B2

excess risk

1.28
0.64
0.32
0.16
0.08
0.04

actual (En)

101

102

103

104

n
B 2 = 1, d = 1000
9

Mean estimation: theorem
If:
˜
B2 → B2
Then:
P

En − E,
→

˜
E = 4B 2 sin2

d
n

˜
→d
1
2

arctan

˜
d
˜
B2

excess risk

1.28
0.64
0.32
0.16
0.08
0.04

actual (En)
asymptotic (E)

101

102

103

104

n
B 2 = 1, d = 1000
9

Mean estimation: theorem
If:
˜
B2 → B2

d
n

Then:
P

En − E,
→

˜
E = 4B 2 sin2

1
2

arctan

˜
d
˜
B2

˜ ˜
E B = min 2B 2, d

1.28

excess risk

˜
→d

0.64
0.32
0.16

actual (En)

0.08

asymptotic (E)

0.04

upper bound (E B)

101

102

103

104

n
B 2 = 1, d = 1000
9

Mean estimation: theorem
If:
˜
B2 → B2

d
n

Then:
P

En − E,
→

˜
E = 4B 2 sin2

1
2

arctan

˜
d
˜
B2

˜ ˜
E B = min 2B 2, d

1.28

excess risk

˜
→d

0.64

˜
Random regime (B 2

0.32
0.16

actual (En)

0.08

asymptotic (E)

0.04

µ is a random guess on sphere
ˆ
˜
Norm B dominates

upper bound (E B)

101

102

103

˜
d):

104

n
B 2 = 1, d = 1000
9

Mean estimation: theorem
If:
˜
B2 → B2

d
n

Then:
P

En − E,
→

˜
E = 4B 2 sin2

1
2

arctan

˜
d
˜
B2

˜ ˜
E B = min 2B 2, d

1.28

excess risk

˜
→d

0.64

˜
Random regime (B 2

0.32
0.16

actual (En)

0.08

asymptotic (E)

0.04

µ is a random guess on sphere
ˆ
˜
Norm B dominates

upper bound (E B)

101

102

103

n
B 2 = 1, d = 1000

˜
d):

104

˜
Unregularized regime (d

˜
B 2):

µ − µ∗ ∼ Gaussian
ˆ
˜
Dimensionality d dominates
9

Linear regression: setup
Data:
X ∼ N (0, Σ)

Y ∼ N (X β ∗, σ 2)

10

Linear regression: setup
Data:
X ∼ N (0, Σ)

Y ∼ N (X β ∗, σ 2)

Regularized least-squares estimator:
n

1
β = argmin
Y (i) − X (i) β
β∈Rd n i=1
ˆλ def

2

+λ β

2

10

Linear regression: setup
Data:
X ∼ N (0, Σ)

Y ∼ N (X β ∗, σ 2)

Regularized least-squares estimator:
n

1
β = argmin
Y (i) − X (i) β
β∈Rd n i=1
ˆλ def

2

+λ β

2

Excess risk (prediction squared loss):
λ
ˆ
En = E[(Y − X β λ)2] − E[(Y − X β ∗)2]

10

Linear regression: setup
Data:
X ∼ N (0, Σ)

Y ∼ N (X β ∗, σ 2)

Regularized least-squares estimator:
n

1
β = argmin
Y (i) − X (i) β
β∈Rd n i=1
ˆλ def

2

+λ β

2

Excess risk (prediction squared loss):
λ
ˆ
En = E[(Y − X β λ)2] − E[(Y − X β ∗)2]
Oracle excess risk:
∗
λ
En = inf λ≥0 En
10

Linear regression: special structure
β ∗ = (1, 0, . . . , 0)
2

Σ = diag(B ,

C2
C2
d−1 , . . . , d−1 )

Intuition:
B: norm of signal in data
C: norm of sum of irrelevant components

11

Linear regression: simpliﬁcation
Componentwise estimator:
For j = 1, . . . , d:
ˆλ = least-squares solution using {X (i)}n
βj
i=1
j

12

Linear regression: simpliﬁcation
Componentwise estimator:
For j = 1, . . . , d:
ˆλ = least-squares solution using {X (i)}n
βj
i=1
j
Note: oracle selection of λ still couples components

12

Componentwise linear regression: theorem
If:
˜
B2 → B2

C 2σ2
n

˜
→ C2

dσ 2
n

˜
→d

13

Componentwise linear regression: theorem
If:
˜
B2 → B2

C 2σ2
n

˜
→ C2

dσ 2
n

˜
→d

Then:
∗
En=

λ
inf λ≥0 En

P

− inf λ≥0 E λ =E ∗
→

13

Componentwise linear regression: theorem
If:
˜
B2 → B2

C 2σ2
n

˜
→ C2

dσ 2
n

˜
→d

Then:
∗
En

=

λ
inf λ≥0 En

P

− inf λ≥0 E λ =E ∗
→

13

Componentwise linear regression: theorem
If:
˜
B2 → B2

C 2σ2
n

˜
→ C2

dσ 2
n

˜
→d

Then:
∗
En

=

λ
inf λ≥0 En

P

− inf λ≥0 E λ = E ∗
→

13

Componentwise linear regression: theorem
If:
˜
B2 → B2

C 2σ2
n

dσ 2
n

˜
→ C2

˜
→d

Then:
∗
En

=

λ
inf λ≥0 En

P

− inf λ≥0 E λ = E ∗
→
˜
C4
˜
d

˜
B 2 λ2
Eλ =
+ ˜2
2 + λ)2
˜
(B
( C˜ + λ)2
d
squared bias

variance

13

Componentwise linear regression: intuition
˜2 2

˜
C4
˜
d

squared bias

variance

B λ
E = inf
+ ˜2
2 + λ)2
˜
λ≥0 (B
( C˜ + λ)2
d
∗

13

Componentwise linear regression: intuition
˜2 2

˜
C4
˜
d

squared bias

variance

B λ
E = inf
+ ˜2
2 + λ)2
˜
λ≥0 (B
( C˜ + λ)2
d
∗

7.8e-1

excess risk

1.6e-1
3.1e-2
6.3e-3
1.3e-3
2.5e-4

actual (En)

102

104

106

n
B 2 = 1, C 2 = 10, d = 100, σ 2 = 100

13

Componentwise linear regression: intuition
˜2 2

˜
C4
˜
d

squared bias

variance

B λ
E = inf
+ ˜2
2 + λ)2
˜
λ≥0 (B
( C˜ + λ)2
d
∗

7.8e-1

excess risk

1.6e-1
3.1e-2
6.3e-3
1.3e-3
2.5e-4

actual (En)
asymptotic (E ∗)

102

104

106

n
B 2 = 1, C 2 = 10, d = 100, σ 2 = 100

13

Componentwise linear regression: intuition
˜2 2

˜
C4
˜
d

squared bias

variance

B λ
E = inf
+ ˜2
2 + λ)2
˜
λ≥0 (B
( C˜ + λ)2
d
∗

E

7.8e-1

excess risk

1.6e-1

˜
C
˜
˜ 2, 2√2 , d
= min B

B def

˜
B

˜
d

3.1e-2
6.3e-3

actual (En)

1.3e-3

asymptotic (E ∗)

2.5e-4

1
lower bound ( 4 E B)

upper bound (E B)

102

104

106

n
B 2 = 1, C 2 = 10, d = 100, σ 2 = 100

13

Componentwise linear regression: intuition
˜2 2

˜
C4
˜
d

squared bias

variance

B λ
E = inf
+ ˜2
2 + λ)2
˜
λ≥0 (B
( C˜ + λ)2
d
∗

E

7.8e-1

excess risk

1.6e-1
3.1e-2
6.3e-3
1.3e-3
2.5e-4

˜
C
˜
˜ 2, 2√2 , d
= min B

B def

˜
B

˜
d

Random (λ → ∞):
Bias dominates; E ∗ ∼ 1

actual (En)
∗

asymptotic (E )
1
lower bound ( 4 E B)

upper bound (E B)

102

104

106

n
B 2 = 1, C 2 = 10, d = 100, σ 2 = 100

13

Componentwise linear regression: intuition
˜2 2

˜
C4
˜
d

squared bias

variance

B λ
E = inf
+ ˜2
2 + λ)2
˜
λ≥0 (B
( C˜ + λ)2
d
∗

E

7.8e-1

excess risk

1.6e-1
3.1e-2
6.3e-3
1.3e-3
2.5e-4

˜
C
˜
˜ 2, 2√2 , d
= min B

B def

˜
B

˜
d

Random (λ → ∞):
Bias dominates; E ∗ ∼ 1

actual (En)
∗

asymptotic (E )
1
lower bound ( 4 E B)

Regularized (λ non-trivial):

upper bound (E B)

102

104

106

Balance bias/variance; E ∗ ∼

1
√
n

n
B 2 = 1, C 2 = 10, d = 100, σ 2 = 100

13

Componentwise linear regression: intuition
˜2 2

˜
C4
˜
d

squared bias

variance

B λ
E = inf
+ ˜2
2 + λ)2
˜
λ≥0 (B
( C˜ + λ)2
d
∗

E

7.8e-1

excess risk

1.6e-1
3.1e-2

˜
B

˜
d

Random (λ → ∞):

6.3e-3

Bias dominates; E ∗ ∼ 1

actual (En)
∗

asymptotic (E )

1.3e-3

1
lower bound ( 4 E B)

2.5e-4

Regularized (λ non-trivial):

upper bound (E B)

102

104

106

n
2

˜
C
˜
˜ 2, 2√2 , d
= min B

B def

2

2

B = 1, C = 10, d = 100, σ = 100

Balance bias/variance; E ∗ ∼

1
√
n

Unregularized (λ → 0):
Variance dominates; E ∗ ∼

1
n
13

Full linear regression: speculation
Stitch together results:
E∗

˜
min{ B 2 ,
trivial

15

Full linear regression: speculation
Stitch together results:
E

∗

˜2

min{ B

trivial

,

O

˜
C2
σ2
˜

˜
+C

,

bounds from
[Srebro et al., 2010]

15

Full linear regression: speculation
Stitch together results:
E

∗

˜2

min{ B

trivial

,

O

˜
C2
σ2
˜

˜
+C

bounds from
[Srebro et al., 2010]

,

˜
d

}

classical
asymptotics

15

Full linear regression: speculation
Stitch together results:
E

∗

˜2

min{ B

trivial

,

O

˜
C2
σ2
˜

˜
+C

bounds from
[Srebro et al., 2010]

,

˜
d

}

classical
asymptotics

Four regimes:

15

Full linear regression: speculation
Stitch together results:
E

∗

˜2

min{ B

trivial

,

O

˜
C2
σ2
˜

˜
+C

bounds from
[Srebro et al., 2010]

,

˜
d

}

classical
asymptotics

Four regimes:
B2

Random
E∗

B2

En

C2
B2

n

15

Full linear regression: speculation
Stitch together results:
E

∗

˜2

,

min{ B

O

˜
C2
σ2
˜

˜
+C

bounds from
[Srebro et al., 2010]

trivial

,

˜
d

}

classical
asymptotics

Four regimes:
B2

Random
En

σ

2

E∗

Low noise

B2

E

C2
B2

∗

C2
n

C2
σ2

n

15

Full linear regression: speculation
Stitch together results:
E

∗

˜2

,

min{ B

O

˜
C2
σ2
˜

˜
+C

bounds from
[Srebro et al., 2010]

trivial

˜
d

,

}

classical
asymptotics

Four regimes:
B2

Random
En

σ

2

E∗

Low noise

B2

E

∗

Regularized

C2
n

E∗

C 2σ2
n

C2
d

C2
B2

C2
σ2

d2 σ 2
C2

n

15

Full linear regression: speculation
Stitch together results:
E

∗

˜2

,

min{ B

O

˜
C2
σ2
˜

˜
+C

bounds from
[Srebro et al., 2010]

trivial

˜
d

,

}

classical
asymptotics

Four regimes:
B2

Random
En

σ

2

E∗

Low noise

B2

E

∗

Regularized Unregularized

C2
n

E

∗

C 2σ2
n

E

∗

dσ 2
n

C2
d

C2
B2

C2
σ2

d2 σ 2
C2

n

15

Full linear regression: speculation
Stitch together results:
E

∗

˜2

,

min{ B

O

˜
C2
σ2
˜

˜
+C

bounds from
[Srebro et al., 2010]

trivial

˜
d

,

}

classical
asymptotics

Four regimes:
B2

Random
En

σ

2

E∗

Low noise

B2

E

C2
d

∗

Regularized Unregularized

C2
n

E

∗

C 2σ2
n

E

∗

dσ 2
n

?
C2
B2

C2
σ2

d2 σ 2
C2

n

15

Conclusions
Summary: studied two simple examples
(mean estimation and componentwise linear regression)

16

Conclusions
Summary: studied two simple examples
(mean estimation and componentwise linear regression)
Observations:
Complexities (norm, dimensionality) active in diﬀerent regimes
Transitions between regimes are smooth

16

Conclusions
Summary: studied two simple examples
(mean estimation and componentwise linear regression)
Observations:
Complexities (norm, dimensionality) active in diﬀerent regimes
Transitions between regimes are smooth
Broad goal: understand the learning curve

16

Conclusions
Summary: studied two simple examples
(mean estimation and componentwise linear regression)
Observations:
Complexities (norm, dimensionality) active in diﬀerent regimes
Transitions between regimes are smooth
Broad goal: understand the learning curve
Finite sample bounds, classical asymptotics: partial picture
High-dimensional asymptotics (statistics, statistical physics)
Key: concentration in high dimensions

16

Conclusions
Summary: studied two simple examples
(mean estimation and componentwise linear regression)
Observations:
Complexities (norm, dimensionality) active in diﬀerent regimes
Transitions between regimes are smooth
Broad goal: understand the learning curve
Finite sample bounds, classical asymptotics: partial picture
High-dimensional asymptotics (statistics, statistical physics)
Key: concentration in high dimensions
Future work: analyze more complex settings
16

