Structure Compilation:
Trading Structure for Features
ICML 2008

Helsinki, Finland

July 8, 2008

Percy Liang

Hal Daum´
e

Dan Klein

UC Berkeley

U of Utah

UC Berkeley

Structured prediction:
y: DT

NNP

NNP

VBD

x: The European Commision agreed

Part-of-speech tagging (POS)

2

Structured prediction:
y: DT

NNP

NNP

VBD

y:

O

B-ORG

I-ORG

O

x: The European Commision agreed

x: The European Commission agreed

Part-of-speech tagging (POS)

Named-entity recognition (NER)

2

Structured prediction:
y: DT

NNP

NNP

VBD

y:

O

B-ORG

I-ORG

O

x: The European Commision agreed

x: The European Commission agreed

Part-of-speech tagging (POS)

Named-entity recognition (NER)

Current methods:
Structured models: accurate but slow
conditional random ﬁelds (CRFs) with loopy graphs, large tag sets

2

Structured prediction:
y: DT

NNP

NNP

VBD

y:

O

B-ORG

I-ORG

O

x: The European Commision agreed

x: The European Commission agreed

Part-of-speech tagging (POS)

Named-entity recognition (NER)

Current methods:
Structured models: accurate but slow
conditional random ﬁelds (CRFs) with loopy graphs, large tag sets

Independent models: less accurate but fast
independent logistic regressions (ILRs)

2

Structured prediction:
y: DT

NNP

NNP

VBD

y:

O

B-ORG

I-ORG

O

x: The European Commision agreed

x: The European Commission agreed

Part-of-speech tagging (POS)

Named-entity recognition (NER)

Current methods:
Structured models: accurate but slow
conditional random ﬁelds (CRFs) with loopy graphs, large tag sets

Independent models: less accurate but fast
independent logistic regressions (ILRs)

Our goal:

transfer

predictive power

accurate and fast at test time

2

Structured prediction:
y: DT

NNP

NNP

VBD

y:

O

B-ORG

I-ORG

O

x: The European Commision agreed

x: The European Commission agreed

Part-of-speech tagging (POS)

Named-entity recognition (NER)

Current methods:
Structured models: accurate but slow
conditional random ﬁelds (CRFs) with loopy graphs, large tag sets

Independent models: less accurate but fast
independent logistic regressions (ILRs)

Our goal:

transfer

predictive power

accurate and fast at test time

Questions: are independent models...
• ...expressive enough (approximation error)?
2

Structured prediction:
y: DT

NNP

NNP

VBD

y:

O

B-ORG

I-ORG

O

x: The European Commision agreed

x: The European Commission agreed

Part-of-speech tagging (POS)

Named-entity recognition (NER)

Current methods:
Structured models: accurate but slow
conditional random ﬁelds (CRFs) with loopy graphs, large tag sets

Independent models: less accurate but fast
independent logistic regressions (ILRs)

Our goal:

transfer

predictive power

accurate and fast at test time

Questions: are independent models...
• ...expressive enough (approximation error)?
• ...easy to learn (estimation error)?
2

Some empirical motivation
labeled
labeled
labeled
data
data
data

CRF(f1)

POS: 95.0%
NER: 75.3%

f1: words/preﬁxes/suﬃxes/forms

3

Some empirical motivation
labeled
labeled
labeled
data
data
data

CRF(f1)

POS: 95.0%
NER: 75.3%

ILR(f1)

POS: 91.7%
NER: 69.1%

f1: words/preﬁxes/suﬃxes/forms

3

Some empirical motivation
labeled
labeled
labeled
data
data
data

CRF(f1)

POS: 95.0%
NER: 75.3%

f1: words/preﬁxes/suﬃxes/forms
f2: f1 applied to larger radius

ILR(f1)
ILR(f2)

POS: 91.7%
NER: 69.1%
POS: 94.4%
NER: 66.2%

3

Some empirical motivation
labeled
labeled
labeled
data
data
data

unlabeled
unlabeled
unlabeled
unlabeled
unlabeled
unlabeled
unlabeled
data
unlabeled
data
unlabeled
data
unlabeled
data
data
data
data
data
data
data

CRF(f1)

POS: 95.0%
NER: 75.3%

f1: words/preﬁxes/suﬃxes/forms
f2: f1 applied to larger radius

ILR(f1)
ILR(f2)

POS: 91.7%
NER: 69.1%
POS: 94.4%
NER: 66.2%

3

Some empirical motivation
labeled
labeled
labeled
data
data
data

auto-labeled
auto-labeled
auto-labeled
auto-labeled
auto-labeled
auto-labeled
auto-labeled
data
auto-labeled
data
auto-labeled
data
auto-labeled
data
data
data
data
data
data
data

CRF(f1)

POS: 95.0%
NER: 75.3%

f1: words/preﬁxes/suﬃxes/forms
f2: f1 applied to larger radius

ILR(f1)
ILR(f2)

POS: 91.7%
NER: 69.1%
POS: 94.4%
NER: 66.2%

ILR(f2) [compiled]

POS: 95.0%
NER: 72.7%

3

Some empirical motivation
labeled
labeled
labeled
data
data
data

auto-labeled
auto-labeled
auto-labeled
auto-labeled
auto-labeled
auto-labeled
auto-labeled
data
auto-labeled
data
auto-labeled
data
auto-labeled
data
data
data
data
data
data
data

CRF(f1)

POS: 95.0%
NER: 75.3%

f1: words/preﬁxes/suﬃxes/forms
f2: f1 applied to larger radius

ILR(f1)
ILR(f2)

POS: 91.7%
NER: 69.1%
POS: 94.4%
NER: 66.2%

ILR(f2) [compiled]

POS: 95.0%
NER: 72.7%

Structure compilation: reduces the gap between the ILR and CRF
3

Analysis of structure compilation
labeled
labeled
labeled
data
data
data

auto-labeled
auto-labeled
auto-labeled
auto-labeled
auto-labeled
auto-labeled
auto-labeled
data
auto-labeled
data
auto-labeled
data
auto-labeled
data
data
data
data
data
data
data

CRF(f1)

ILR(f2)

4

Analysis of structure compilation
labeled
labeled
labeled
data
data
data

auto-labeled
auto-labeled
auto-labeled
auto-labeled
auto-labeled
auto-labeled
auto-labeled
data
auto-labeled
data
auto-labeled
data
auto-labeled
data
data
data
data
data
data
data

CRF(f1)

ILR(f2)

Goal: analyze risk of ﬁnal compiled ILR(f2)

4

Analysis of structure compilation
labeled
labeled
labeled
data
data
data

auto-labeled
auto-labeled
auto-labeled
auto-labeled
auto-labeled
auto-labeled
auto-labeled
data
auto-labeled
data
auto-labeled
data
auto-labeled
data
data
data
data
data
data
data

CRF(f1)

ILR(f2)

Goal: analyze risk of ﬁnal compiled ILR(f2)
Decomposition of errors:
Approximation error: best loss of model (with inﬁnite data)

4

Analysis of structure compilation
labeled
labeled
labeled
data
data
data

auto-labeled
auto-labeled
auto-labeled
auto-labeled
auto-labeled
auto-labeled
auto-labeled
data
auto-labeled
data
auto-labeled
data
auto-labeled
data
data
data
data
data
data
data

CRF(f1)

ILR(f2)

Goal: analyze risk of ﬁnal compiled ILR(f2)
Decomposition of errors:
Approximation error: best loss of model (with inﬁnite data)
Estimation error: suboptimality due to ﬁnite data
4

Approximation/estimation errors for ILR
auto-labeled
auto-labeled
auto-labeled
auto-labeled
auto-labeled
auto-labeled
auto-labeled
data
auto-labeled
data
auto-labeled
data
auto-labeled
data
data
data
data
data
data
data

CRF(f1)

ILR(f2)

5

Approximation/estimation errors for ILR
auto-labeled
auto-labeled
auto-labeled
auto-labeled
auto-labeled
auto-labeled
auto-labeled
data
auto-labeled
data
auto-labeled
data
auto-labeled
data
data
data
data
data
data
data

CRF(f1)

ILR(f2)

pc: CRF(f1) trained on labeled data

5

Approximation/estimation errors for ILR
auto-labeled
auto-labeled
auto-labeled
auto-labeled
auto-labeled
auto-labeled
auto-labeled
data
auto-labeled
data
auto-labeled
data
auto-labeled
data
data
data
data
data
data
data

CRF(f1)

ILR(f2)

pc: CRF(f1) trained on labeled data
pi: ILR(f2) trained on m auto-labeled examples

5

Approximation/estimation errors for ILR
auto-labeled
auto-labeled
auto-labeled
auto-labeled
auto-labeled
auto-labeled
auto-labeled
data
auto-labeled
data
auto-labeled
data
auto-labeled
data
data
data
data
data
data
data

CRF(f1)

ILR(f2)

pc: CRF(f1) trained on labeled data
pi: ILR(f2) trained on m auto-labeled examples
pi∗ : ILR(f2) trained on inﬁnite auto-labeled data

5

Approximation/estimation errors for ILR
auto-labeled
auto-labeled
auto-labeled
auto-labeled
auto-labeled
auto-labeled
auto-labeled
data
auto-labeled
data
auto-labeled
data
auto-labeled
data
data
data
data
data
data
data

CRF(f1)

ILR(f2)

pc: CRF(f1) trained on labeled data
pi: ILR(f2) trained on m auto-labeled examples
pi∗ : ILR(f2) trained on inﬁnite auto-labeled data

KL (pc || pi) = KL (pc || pi∗ ) + KL (pc || pi) − KL (pc || pi∗ )
approx. error

estimation error

5

Approximation/estimation errors for ILR
auto-labeled
auto-labeled
auto-labeled
auto-labeled
auto-labeled
auto-labeled
auto-labeled
data
auto-labeled
data
auto-labeled
data
auto-labeled
data
data
data
data
data
data
data

CRF(f1)

ILR(f2)

pc: CRF(f1) trained on labeled data
pi: ILR(f2) trained on m auto-labeled examples
pi∗ : ILR(f2) trained on inﬁnite auto-labeled data

KL (pc || pi) = KL (pc || pi∗ ) + KL (pc || pi) − KL (pc || pi∗ )
approx. error

estimation error

Estimation error:
Expected value =

# features
m

+o

1
m

→ 0 [Liang & Jordan, 2008]

5

Approximation/estimation errors for ILR
auto-labeled
auto-labeled
auto-labeled
auto-labeled
auto-labeled
auto-labeled
auto-labeled
data
auto-labeled
data
auto-labeled
data
auto-labeled
data
data
data
data
data
data
data

CRF(f1)

ILR(f2)

pc: CRF(f1) trained on labeled data
pi: ILR(f2) trained on m auto-labeled examples
pi∗ : ILR(f2) trained on inﬁnite auto-labeled data

KL (pc || pi) = KL (pc || pi∗ ) + KL (pc || pi) − KL (pc || pi∗ )
approx. error

estimation error

Estimation error:
1
Expected value = # features + o m → 0 [Liang & Jordan, 2008]
m
Structured compilation can eliminate this error
5

Approximation/estimation errors for ILR
auto-labeled
auto-labeled
auto-labeled
auto-labeled
auto-labeled
auto-labeled
auto-labeled
data
auto-labeled
data
auto-labeled
data
auto-labeled
data
data
data
data
data
data
data

CRF(f1)

ILR(f2)

pc: CRF(f1) trained on labeled data
pi: ILR(f2) trained on m auto-labeled examples
pi∗ : ILR(f2) trained on inﬁnite auto-labeled data

KL (pc || pi) = KL (pc || pi∗ ) + KL (pc || pi) − KL (pc || pi∗ )
approx. error

estimation error

Estimation error:
1
Expected value = # features + o m → 0 [Liang & Jordan, 2008]
m
Structured compilation can eliminate this error

Approximation error: next...
5

Decomposition of approximation error
CRF(f1): pc

ILR(f2): pi∗

6

Decomposition of approximation error
CRF(f1): pc
coherence
marginalized CRF(f1): pmc

Σ

Σ

Σ

ILR(f2): pi∗

6

Decomposition of approximation error
CRF(f1): pc
coherence
marginalized CRF(f1): pmc

Σ

Σ

Σ

ILR(f2): pi∗

6

Decomposition of approximation error
CRF(f1): pc
coherence
marginalized CRF(f1): pmc

Σ

Σ

Σ

nonlinearities
ILR(f∞): pa∗
ILR(f2): pi∗

6

Decomposition of approximation error
CRF(f1): pc
coherence
marginalized CRF(f1): pmc

Σ

Σ

Σ

nonlinearities
ILR(f∞): pa∗
global information
ILR(f2): pi∗

6

Decomposition of approximation error
CRF(f1): pc
coherence
marginalized CRF(f1): pmc

Σ

Σ

Σ

nonlinearities
ILR(f∞): pa∗
global information
ILR(f2): pi∗
Theorem:
KL (pc || pi∗ ) = KL (pc || pmc) + KL (pmc || pa∗ ) + KL (pa∗ || pi∗ )

6

Decomposition of approximation error
CRF(f1): pc
coherence
marginalized CRF(f1): pmc

Σ

Σ

Σ

nonlinearities
ILR(f∞): pa∗
global information
ILR(f2): pi∗
Theorem:
KL (pc || pi∗ ) = KL (pc || pmc) + KL (pmc || pa∗ ) + KL (pa∗ || pi∗ )

Proof:
Generalized Pythagorean identity for KL-divergence
6

Approximation error: coherence
CRF: pc
marginalized CRF: pmc

Σ

Σ

Σ

Coherence = KL (pc || pmc):
importance of making joint predictions

7

Approximation error: coherence
CRF: pc
marginalized CRF: pmc

Σ

Σ

Σ

Coherence = KL (pc || pmc):
importance of making joint predictions
For a chain CRF:
coherence = sum of mutual information along the edges

7

Approximation error: coherence
CRF: pc
marginalized CRF: pmc

Σ

Σ

Σ

Coherence = KL (pc || pmc):
importance of making joint predictions
For a chain CRF:
coherence = sum of mutual information along the edges
Coherence

POS
0.003

NER
0.009

7

Approximation error: coherence
CRF: pc
marginalized CRF: pmc

Σ

Σ

Σ

Coherence = KL (pc || pmc):
importance of making joint predictions
For a chain CRF:
coherence = sum of mutual information along the edges
Coherence
Change in accuracy

POS
0.003
95.0% ⇒ 95.0%

NER
0.009
76.3% ⇒ 76.0%
7

Approximation error: coherence
CRF: pc
marginalized CRF: pmc

Σ

Σ

Σ

Coherence = KL (pc || pmc):
importance of making joint predictions
For a chain CRF:
coherence = sum of mutual information along the edges
Coherence
Change in accuracy

POS
0.003
95.0% ⇒ 95.0%

NER
0.009
76.3% ⇒ 76.0%

Coherence is not a huge concern (for these applications)
7

Approximation error: nonlinearities
marginalized CRF: pmc

Σ

Σ

Σ

ILR(f∞): pa∗
Nonlinearities = KL (pmc || pa∗ ):
importance of combining features in a nonlinear way

8

Approximation error: nonlinearities
marginalized CRF: pmc

Σ

Σ

Σ

ILR(f∞): pa∗
Nonlinearities = KL (pmc || pa∗ ):
importance of combining features in a nonlinear way
NER experiment:
Train a truncated CRF, so that both the truncated CRF
(nonlinear) and the ILR (linear) use the same features

8

Approximation error: nonlinearities
marginalized CRF: pmc

Σ

Σ

Σ

ILR(f∞): pa∗
Nonlinearities = KL (pmc || pa∗ ):
importance of combining features in a nonlinear way
NER experiment:
Train a truncated CRF, so that both the truncated CRF
(nonlinear) and the ILR (linear) use the same features
Truncated CRF
ILR(f2)
Accuracy
76.0%
72.7%

8

Approximation error: nonlinearities
marginalized CRF: pmc

Σ

Σ

Σ

ILR(f∞): pa∗
Nonlinearities = KL (pmc || pa∗ ):
importance of combining features in a nonlinear way
NER experiment:
Train a truncated CRF, so that both the truncated CRF
(nonlinear) and the ILR (linear) use the same features
Truncated CRF
ILR(f2)
Accuracy
76.0%
72.7%
Nonlinearities play an important role
8

Approximation error: global information
ILR(f∞): pa∗
ILR(f2): pi∗
Global information = KL (pa∗ || pi∗ ):
importance of using features on distant parts of the input

9

Approximation error: global information
ILR(f∞): pa∗
ILR(f2): pi∗
Global information = KL (pa∗ || pi∗ ):
importance of using features on distant parts of the input
NER experiment:
Compare truncated CRF with marginalized CRF (they diﬀer
only in the features used)

9

Approximation error: global information
ILR(f∞): pa∗
ILR(f2): pi∗
Global information = KL (pa∗ || pi∗ ):
importance of using features on distant parts of the input
NER experiment:
Compare truncated CRF with marginalized CRF (they diﬀer
only in the features used)
Accuracy

Marginalized CRF
76.0%

Truncated CRF
76.0%
9

Approximation error: global information
ILR(f∞): pa∗
ILR(f2): pi∗
Global information = KL (pa∗ || pi∗ ):
importance of using features on distant parts of the input
NER experiment:
Compare truncated CRF with marginalized CRF (they diﬀer
only in the features used)
Accuracy

Marginalized CRF
76.0%

Truncated CRF
76.0%

Distant information is not essential (for these applications)
9

Structure compilation for parsing
S
NP

VP

DT NN VBD

NP

The cat ate DT
a

NN
JJ

NN

tasty ﬁsh

10

Structure compilation for parsing
S
NP

VP

DT NN VBD

NP

The cat ate DT
a

NN
JJ

Sentence length:
Number of grammar symbols: K
Number of grammar rules: G
,K

NN

tasty ﬁsh

10

Structure compilation for parsing
S
NP

VP

DT NN VBD

NP

The cat ate DT
a

NN
JJ

Sentence length:
Number of grammar symbols: K
Number of grammar rules: G
,K

NN

tasty ﬁsh

Parse time/sentence
Structured model:
O( 3G)
Standard dynamic program for context-free grammars

10

Structure compilation for parsing
S
NP

VP

DT NN VBD

NP

The cat ate DT
a

NN
JJ

Sentence length:
Number of grammar symbols: K
Number of grammar rules: G
,K

NN

tasty ﬁsh

Parse time/sentence
Structured model:
O( 3G)
Standard dynamic program for context-free grammars
Independent model:
O( 3 + K 2)
For each of O( 2) spans:
Make a soft prediction of whether it’s a constituent
(features: words/tags/preﬁxes/suﬃxes on entire span)
10

Structure compilation for parsing
S
NP

VP

DT NN VBD

NP

The cat ate DT
a

NN
JJ

Sentence length:
Number of grammar symbols: K
Number of grammar rules: G
,K

NN

tasty ﬁsh

Parse time/sentence
Structured model:
O( 3G)
Standard dynamic program for context-free grammars
Independent model:
O( 3 + K 2)
For each of O( 2) spans:
Make a soft prediction of whether it’s a constituent
(features: words/tags/preﬁxes/suﬃxes on entire span)

Run a dynamic program to choose the best tree
10

Parsing results
Unlabeled F1

87
85
82
80

Structured
Independent

77
4

14

24

44

84 164

m (thousands)

4K labeled sentences

11

Parsing results
85

Unlabeled F1

92

Unlabeled F1

87

90

82

89

80

Structured
Independent

77
4

14

24

44

84 164

m (thousands)

4K labeled sentences

87

Structured
Independent

86
40 50 60 80 120 200 360

m (thousands)

40K labeled sentences

11

Parsing results
85

Unlabeled F1

92

Unlabeled F1

87

90

82

89

80

Structured
Independent

77
4

14

24

44

84 164

m (thousands)

4K labeled sentences

87

Structured
Independent

86
40 50 60 80 120 200 360

m (thousands)

40K labeled sentences

• Structure is important in parsing
• Need richer features or nonlinearities for the independent
model to catch up

11

Summary of structure compilation
Motivation: want fast CRF-level accuracy at test time

12

Summary of structure compilation
Motivation: want fast CRF-level accuracy at test time
labeled
labeled
labeled
data
data
data

auto-labeled
auto-labeled
auto-labeled
auto-labeled
auto-labeled
auto-labeled
auto-labeled
data
auto-labeled
data
auto-labeled
data
auto-labeled
data
data
data
data
data
data
data

CRF(f1)

ILR(f2)

12

Summary of structure compilation
Motivation: want fast CRF-level accuracy at test time
labeled
labeled
labeled
data
data
data

auto-labeled
auto-labeled
auto-labeled
auto-labeled
auto-labeled
auto-labeled
auto-labeled
data
auto-labeled
data
auto-labeled
data
auto-labeled
data
data
data
data
data
data
data

CRF(f1)
computationally complex

ILR(f2)
computationally simple

12

Summary of structure compilation
Motivation: want fast CRF-level accuracy at test time
labeled
labeled
labeled
data
data
data

auto-labeled
auto-labeled
auto-labeled
auto-labeled
auto-labeled
auto-labeled
auto-labeled
data
auto-labeled
data
auto-labeled
data
auto-labeled
data
data
data
data
data
data
data

CRF(f1)
computationally complex
statistically simple

ILR(f2)
computationally simple
statistically complex

Estimation error: structure compilation can easily drive it to 0

12

Summary of structure compilation
Motivation: want fast CRF-level accuracy at test time
labeled
labeled
labeled
data
data
data

auto-labeled
auto-labeled
auto-labeled
auto-labeled
auto-labeled
auto-labeled
auto-labeled
data
auto-labeled
data
auto-labeled
data
auto-labeled
data
data
data
data
data
data
data

CRF(f1)
computationally complex
statistically simple
very expressive

ILR(f2)
computationally simple
statistically complex
not as expressive

Estimation error: structure compilation can easily drive it to 0
Approximation error: advantages of CRF over ILR
12

Summary of structure compilation
Motivation: want fast CRF-level accuracy at test time
labeled
labeled
labeled
data
data
data

auto-labeled
auto-labeled
auto-labeled
auto-labeled
auto-labeled
auto-labeled
auto-labeled
data
auto-labeled
data
auto-labeled
data
auto-labeled
data
data
data
data
data
data
data

CRF(f1)
computationally complex
statistically simple
very expressive

ILR(f2)
computationally simple
statistically complex
not as expressive

Estimation error: structure compilation can easily drive it to 0
Approximation error: advantages of CRF over ILR
• ILR needs rich features to compensate
12

Summary of structure compilation
Motivation: want fast CRF-level accuracy at test time
labeled
labeled
labeled
data
data
data

auto-labeled
auto-labeled
auto-labeled
auto-labeled
auto-labeled
auto-labeled
auto-labeled
data
auto-labeled
data
auto-labeled
data
auto-labeled
data
data
data
data
data
data
data

CRF(f1)
computationally complex
statistically simple
very expressive

ILR(f2)
computationally simple
statistically complex
not as expressive

Estimation error: structure compilation can easily drive it to 0
Approximation error: advantages of CRF over ILR
• ILR needs rich features to compensate
• CRF’s nonlinearities are important
12

