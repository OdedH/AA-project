S → NP VP
NP → PRP
VP → VBD NP
NP → DT NN
PRP → she

S → NP VP
NP → PRP
VP → VBD NP
NP → DT NN
PRP → she

VBD → heard
DT → the
NN → noise
S → NP VP

VBD → heard
DT → the
NN → noise
S → NP VP

NP → PRP
VP → VBD NP
NP → DT NN
PRP → she

NP → PRP
VP → VBD NP
NP → DT NN
PRP → she

VBD → heard
DT → the
NN → noise
S → NP VP
NP → PRP
VP → VBD NP
NP → DT NN
PRP → she
VBD → heard
DT → the
NN → noise
S → NP VP
NP → PRP
VP → VBD NP
NP → DT NN
PRP → she
VBD → heard
DT → the
NN → noise
S → NP VP
NP → PRP

The Inﬁnite PCFG using
Hierarchical Dirichlet Processes
EMNLP 2007

Prague, Czech Republic
June 29, 2007

VBD → heard
DT → the
NN → noise
S → NP VP
NP → PRP
VP → VBD NP
NP → DT NN
PRP → she
VBD → heard
DT → the
NN → noise
S → NP VP
NP → PRP
VP → VBD NP
NP → DT NN
PRP → she
VBD → heard
DT → the
NN → noise
S → NP VP
NP → PRP

VP → VBD NP
NP → DT NN
PRP → she

VP → VBD NP
NP → DT NN
PRP → she

VBD → heard
DT → the
NN → noise

VBD → heard
DT → the
NN → noise

S → NP VP
NP → PRP
VP → VBD NP
NP → DT NN
PRP → she
VBD → heard
DT → the
NN → noise
S → NP VP
NP → PRP
VP → VBD NP
NP → DT NN

Percy Liang
Michael I. Jordan

Slav Petrov
Dan Klein

S → NP VP
NP → PRP
VP → VBD NP
NP → DT NN
PRP → she
VBD → heard
DT → the
NN → noise
S → NP VP
NP → PRP
VP → VBD NP
NP → DT NN

PRP → she
VBD → heard

PRP → she
VBD → heard

DT → the

DT → the

NN → noise

NN → noise

S → NP VP

S → NP VP

NP → PRP

NP → PRP

VP → VBD NP

VP → VBD NP

NP → DT NN

NP → DT NN

PRP → she

PRP → she

VBD → heard

VBD → heard

DT → the

DT → the

NN → noise

NN → noise

How do we choose the grammar complexity?
Grammar induction:
How many grammar symbols (NP, VP, etc.)?

?
She heard the noise

2

How do we choose the grammar complexity?
Grammar induction:
How many grammar symbols (NP, VP, etc.)?
Grammar reﬁnement:
How many grammar subsymbols (NP-loc, NP-subj, etc.)?
S-?
NP-?

VP-?

PRP-?

VBD-?

She

heard

NP-?
DT-?

NN-?

the

noise

2

How do we choose the grammar complexity?
Grammar induction:
How many grammar symbols (NP, VP, etc.)?
Grammar reﬁnement:
How many grammar subsymbols (NP-loc, NP-subj, etc.)?
S-?
NP-?

VP-?

PRP-?

VBD-?

She

heard

NP-?
DT-?

NN-?

the

noise

Our solution: the HDP-PCFG allows number of (sub)symbols
to adapt to data
2

A motivating example
True grammar:
S → AA | BB | CC | DD
A → a1 | a2 | a3
B → b1 | b2 | b3
C → c1 | c2 | c3
D → d1 | d2 | d3

3

A motivating example
True grammar:
S → AA | BB | CC | DD
A → a1 | a2 | a3
B → b1 | b2 | b3
C → c1 | c2 | c3
D → d1 | d2 | d3

Generate examples:
S

S

S

S

A A

B B

A A

C C

a2 a3

b1 b3

a1 a1

c1 c1

3

A motivating example
True grammar:
S → AA | BB | CC | DD
A → a1 | a2 | a3
B → b1 | b2 | b3
C → c1 | c2 | c3
D → d1 | d2 | d3

Collapse A,B,C,D ⇒ X:
S

S

S

S

X X

X X

X X

X X

a2 a3

b1 b3

a1 a1

c1 c1

3

A motivating example
Collapse A,B,C,D ⇒ X:

True grammar:
S → AA | BB | CC | DD
A → a1 | a2 | a3
B → b1 | b2 | b3
C → c1 | c2 | c3
D → d1 | d2 | d3

S

S

S

S

X X

X X

X X

X X

a2 a3

b1 b3

a1 a1

c1 c1

Results:
posterior

0.25

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

subsymbol

standard PCFG
3

A motivating example
Collapse A,B,C,D ⇒ X:

True grammar:
S → AA | BB | CC | DD
A → a1 | a2 | a3
B → b1 | b2 | b3
C → c1 | c2 | c3
D → d1 | d2 | d3

S

S

S

S

X X

X X

X X

X X

a2 a3

b1 b3

a1 a1

c1 c1

Results:
posterior

0.25

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

subsymbol

HDP-PCFG
3

The meeting of two ﬁelds
Grammar learning
Lexicalized
[Charniak, 1996]
[Collins, 1999]

Manual reﬁnement
[Johnson, 1998]
[Klein, Manning, 2003]

Automatic reﬁnement
[Matsuzaki, et al., 2005]
[Petrov, et al., 2006]

4

The meeting of two ﬁelds
Grammar learning
Lexicalized
[Charniak, 1996]
[Collins, 1999]

Manual reﬁnement
[Johnson, 1998]
[Klein, Manning, 2003]

Automatic reﬁnement
[Matsuzaki, et al., 2005]
[Petrov, et al., 2006]

Bayesian nonparametrics
Basic theory

...

[Ferguson, 1973]
[Antoniak, 1974]
[Sethuraman, 1994]
[Escobar, West, 1995]
[Neal, 2000]

More complex models
[Teh, et al., 2006]
[Beal, et al., 2002]
[Goldwater, et al., 2006]
[Sohn, Xing, 2007]

4

The meeting of two ﬁelds
Grammar learning
Lexicalized

Bayesian nonparametrics
Basic theory
[Ferguson, 1973]
[Antoniak, 1974]
[Sethuraman, 1994]
[Escobar, West, 1995]
[Neal, 2000]

[Charniak, 1996]
[Collins, 1999]

Manual reﬁnement
[Johnson, 1998]
[Klein, Manning, 2003]

Automatic reﬁnement
[Matsuzaki, et al., 2005]
[Petrov, et al., 2006]

...

More complex models
[Teh, et al., 2006]
[Beal, et al., 2002]
[Goldwater, et al., 2006]
[Sohn, Xing, 2007]

Nonparametric grammars
[Johnson, et al., 2006]
[Finkel, et al., 2007]
[Liang, et al., 2007]

4

The meeting of two ﬁelds
Grammar learning
Lexicalized

Bayesian nonparametrics
Basic theory

...

[Ferguson, 1973]
[Antoniak, 1974]
[Sethuraman, 1994]
[Escobar, West, 1995]
[Neal, 2000]

[Charniak, 1996]
[Collins, 1999]

Manual reﬁnement Our contribution
[Johnson, 1998]
• Deﬁnition
[Klein, Manning, 2003] of the HDP-PCFG
More complex models
• Simple and eﬃcient variational inference2006]
algorithm
Automatic reﬁnement
[Teh, et al.,
[Matsuzaki, et al., 2005]
• Empirical comparison with ﬁnite [Beal, et al., 2002]
models
[Petrov, et al., 2006]
[Goldwater, et al., 2006]
on a full-scale parsing task
[Sohn, Xing, 2007]
Nonparametric grammars
[Johnson, et al., 2006]
[Finkel, et al., 2007]
[Liang, et al., 2007]

4

Bayesian paradigm
Generative model:
α

HDP

hyperparameters
.

θ

PCFG

z: parse tree
x: sentence

grammar
.

5

Bayesian paradigm
Generative model:
α

HDP

hyperparameters
.

θ

PCFG

z: parse tree
x: sentence

grammar
.

Bayesian posterior inference:
Observe x.
What’s θ and z?

5

HDP probabilistic context-free grammars
z1

β
φB
z

z2

z3
HDP-PCFG

φE
z
z

∞

x2

x3

β ∼ GEM(α) [generate distribution over symbols]
For each symbol z ∈ {1, 2, . . . }:
φE ∼ Dirichlet(αE ) [generate emission probs]
z
φB ∼ DP(αB , ββ T ) [binary production probs]
z
For each nonterminal node i:
(zL(i), zR(i)) ∼ Multinomial(φB ) [child symbols]
zi
For each preterminal node i:
xi ∼ Multinomial(φE ) [terminal symbol]
zi
6

HDP probabilistic context-free grammars
z1

β
φB
z

z2

z3
HDP-PCFG

φE
z
z

∞

x2

x3

β ∼ GEM(α) [generate distribution over symbols]
For each symbol z ∈ {1, 2, . . . }:
φE ∼ Dirichlet(αE ) [generate emission probs]
z
φB ∼ DP(αB , ββ T ) [binary production probs]
z
For each nonterminal node i:
(zL(i), zR(i)) ∼ Multinomial(φB ) [child symbols]
zi
For each preterminal node i:
xi ∼ Multinomial(φE ) [terminal symbol]
zi
7

HDP-PCFG: prior over symbols
β ∼ GEM(α)

α=1

GEM

···

8

HDP-PCFG: prior over symbols
β ∼ GEM(α)

α=1

GEM

···

8

HDP-PCFG: prior over symbols
β ∼ GEM(α)

α=1

GEM

···

8

HDP-PCFG: prior over symbols
β ∼ GEM(α)

α=1

GEM

···

8

HDP-PCFG: prior over symbols
β ∼ GEM(α)

α = 0.5

GEM

α=1

GEM

···
···

8

HDP-PCFG: prior over symbols
β ∼ GEM(α)
α = 0.2

GEM

α = 0.5

GEM

α=1

GEM

···
···
···

8

HDP-PCFG: prior over symbols
β ∼ GEM(α)
α = 0.2

GEM

α = 0.5

GEM

α=1

GEM

α=5

GEM

α = 10

GEM

···
···
···
···
···
8

HDP probabilistic context-free grammars
z1

β
φB
z

z2

z3
HDP-PCFG

φE
z
z

∞

x2

x3

β ∼ GEM(α) [generate distribution over symbols]
For each symbol z ∈ {1, 2, . . . }:
φE ∼ Dirichlet(αE ) [generate emission probs]
z
φB ∼ DP(αB , ββ T ) [binary production probs]
z
For each nonterminal node i:
(zL(i), zR(i)) ∼ Multinomial(φB ) [child symbols]
zi
For each preterminal node i:
xi ∼ Multinomial(φE ) [terminal symbol]
zi
9

HDP-PCFG: binary productions
Distribution over symbols (top-level):
β ∼ GEM(α)

···
1

2

3

4

5

6

10

HDP-PCFG: binary productions
Distribution over symbols (top-level):
β ∼ GEM(α)

···
1

2

3

4

5

6

Mean distribution over child symbols:

ββ

T

right child

left child

10

HDP-PCFG: binary productions
Distribution over symbols (top-level):
β ∼ GEM(α)

···
1

2

3

4

5

6

Mean distribution over child symbols:

ββ

T

right child

left child

Distribution over child symbols (per-state):

φB
z

B

T

∼ DP(α , ββ )

right child

left child

10

HDP-PCFG: binary productions
Distribution over symbols (top-level):
β ∼ GEM(α)

···
1

2

3

4

5

6

Mean distribution over child symbols:

ββ

T

right child

left child

Distribution over child symbols (per-state):

φB
z

B

T

∼ DP(α , ββ )

right child

left child

10

HDP-PCFG: binary productions
Distribution over symbols (top-level):
β ∼ GEM(α)

···
1

2

3

4

5

6

Mean distribution over child symbols:

ββ

T

right child

left child

Distribution over child symbols (per-state):

φB
z

B

T

∼ DP(α , ββ )

right child

left child

10

HDP-PCFG: binary productions
Distribution over symbols (top-level):
β ∼ GEM(α)

···
1

2

3

4

5

6

Mean distribution over child symbols:

ββ

T

right child

left child

Distribution over child symbols (per-state):

φB
z

B

T

∼ DP(α , ββ )

right child

left child

10

HDP probabilistic context-free grammars
z1

β
φB
z

z2

z3
HDP-PCFG

φE
z
z

∞

x2

x3

β ∼ GEM(α) [generate distribution over symbols]
For each symbol z ∈ {1, 2, . . . }:
φE ∼ Dirichlet(αE ) [generate emission probs]
z
φB ∼ DP(αB , ββ T ) [binary production probs]
z
For each nonterminal node i:
(zL(i), zR(i)) ∼ Multinomial(φB ) [child symbols]
zi
For each preterminal node i:
xi ∼ Multinomial(φE ) [terminal symbol]
zi
11

HDP probabilistic context-free grammars
z1

β
φB
z

z2

z3
HDP-PCFG

φE
z
z

∞

x2

x3

β ∼ GEM(α) [generate distribution over symbols]
For each symbol z ∈ {1, 2, . . . }:
φE ∼ Dirichlet(αE ) [generate emission probs]
z
φB ∼ DP(αB , ββ T ) [binary production probs]
z
For each nonterminal node i:
(zL(i), zR(i)) ∼ Multinomial(φB ) [child symbols]
zi
For each preterminal node i:
xi ∼ Multinomial(φE ) [terminal symbol]
zi
12

Variational Bayesian inference
α

HDP

hyperparameters
.

θ

PCFG

z: parse tree
x: sentence

grammar
.

Goal: compute posterior p(θ, z | x)

13

Variational Bayesian inference
α

HDP

θ

hyperparameters
.

PCFG

z: parse tree
x: sentence

grammar
.

Goal: compute posterior p(θ, z | x)
Variational inference:
approximate posterior with
best from a set of tractable
distributions Q:
q ∗ = argmin KL(q || p)
q∈Q

Q
q∗

p

13

Variational Bayesian inference
α

HDP

θ

hyperparameters
.

z: parse tree
x: sentence

PCFG

grammar
.

Goal: compute posterior p(θ, z | x)
Variational inference:
approximate posterior with
best from a set of tractable
distributions Q:

Mean-ﬁeld approximation:
Q = q : q = q(z)q(β)q(φ)
β
z1

q ∗ = argmin KL(q || p)
q∈Q

φB
z

Q
q∗

p

z2

φE
z
z

z3

∞

13

Coordinate-wise descent algorithm
Goal: argmin KL(q || p)
q∈Q

q = q(z)q(φ)q(β)
z = parse tree
φ = rule probabilities
β = inventory of symbols
β
z1
φB
z
z2

φE
z
z

z3

∞

14

Coordinate-wise descent algorithm
Goal: argmin KL(q || p)
q∈Q

Iterate:
• Optimize q(z) (E-step):

q = q(z)q(φ)q(β)
z = parse tree
φ = rule probabilities
β = inventory of symbols

• Optimize q(φ) (M-step):

β
z1
• Optimize q(β) (no equivalent in EM):

φB
z
z2

φE
z
z

z3

∞

14

Coordinate-wise descent algorithm
Goal: argmin KL(q || p)
q∈Q

q = q(z)q(φ)q(β)
z = parse tree
φ = rule probabilities
β = inventory of symbols

Iterate:
• Optimize q(z) (E-step):
– Inside-outside with rule weights W (r)
– Gather expected rule counts C(r)
• Optimize q(φ) (M-step):

β
z1
• Optimize q(β) (no equivalent in EM):

φB
z
z2

φE
z
z

z3

∞

14

Coordinate-wise descent algorithm
Goal: argmin KL(q || p)
q∈Q

q = q(z)q(φ)q(β)
z = parse tree
φ = rule probabilities
β = inventory of symbols
β
z1
φB
z
z2

φE
z
z

Iterate:
• Optimize q(z) (E-step):
– Inside-outside with rule weights W (r)
– Gather expected rule counts C(r)
• Optimize q(φ) (M-step):
– Update Dirichlet posteriors
(expected rule counts + pseudocounts)
– Compute rule weights W (r)
• Optimize q(β) (no equivalent in EM):

z3

∞

14

Coordinate-wise descent algorithm
Goal: argmin KL(q || p)
q∈Q

q = q(z)q(φ)q(β)
z = parse tree
φ = rule probabilities
β = inventory of symbols
β
z1
φB
z
z2

φE
z
z

∞

z3

Iterate:
• Optimize q(z) (E-step):
– Inside-outside with rule weights W (r)
– Gather expected rule counts C(r)
• Optimize q(φ) (M-step):
– Update Dirichlet posteriors
(expected rule counts + pseudocounts)
– Compute rule weights W (r)
• Optimize q(β) (no equivalent in EM):
– Truncate at level K
(set the maximum number of symbols)
– Use projected gradient to
adapt number of symbols
14

Rule weights
• Weight W (r) of rule r similar to probability p(r)
• W (r) unnormalized ⇒ extra degree of freedom

15

Rule weights
• Weight W (r) of rule r similar to probability p(r)
• W (r) unnormalized ⇒ extra degree of freedom
EM (maximum likelihood):
P C(r)
W (r) =
C(r )
r

15

Rule weights
• Weight W (r) of rule r similar to probability p(r)
• W (r) unnormalized ⇒ extra degree of freedom
EM (maximum likelihood):
P C(r)
W (r) =
C(r )
r

EM (maximum a posteriori):
P prior(r)−1+C(r)
W (r) =
prior(r )−1+C(r )
r

15

Rule weights
• Weight W (r) of rule r similar to probability p(r)
• W (r) unnormalized ⇒ extra degree of freedom
EM (maximum likelihood):
P C(r)
W (r) =
C(r )
r

EM (maximum a posteriori):
P prior(r)−1+C(r)
W (r) =
prior(r )−1+C(r )
r

Mean-ﬁeld (with DP prior):
exp Ψ(prior(r)+C(r))
W (r) = exp Ψ( P prior(r )+C(r ))
r

15

Rule weights
• Weight W (r) of rule r similar to probability p(r)
• W (r) unnormalized ⇒ extra degree of freedom
2.0
EM (maximum likelihood):
x
C(r)
1.6
W (r) = P C(r )
exp(Ψ(·))
r

EM (maximum a posteriori):
P prior(r)−1+C(r)
W (r) =
prior(r )−1+C(r )
r

1.2
0.8

Mean-ﬁeld (with DP prior):
0.4
exp Ψ(prior(r)+C(r))
W (r) = exp Ψ( P prior(r )+C(r ))
r

0.4

0.8

1.2

1.6

2.0

x

15

Rule weights
• Weight W (r) of rule r similar to probability p(r)
• W (r) unnormalized ⇒ extra degree of freedom
2.0
EM (maximum likelihood):
x
C(r)
1.6
W (r) = P C(r )
exp(Ψ(·))
r

EM (maximum a posteriori):
P prior(r)−1+C(r)
W (r) =
prior(r )−1+C(r )
r

1.2
0.8

Mean-ﬁeld (with DP prior):
0.4
exp Ψ(prior(r)+C(r))
W (r) = exp Ψ( P prior(r )+C(r ))
r

P C(r)−0.5
r C(r )−0.5

0.4

0.8

1.2

1.6

2.0

x

15

Rule weights
• Weight W (r) of rule r similar to probability p(r)
• W (r) unnormalized ⇒ extra degree of freedom
2.0
EM (maximum likelihood):
x
C(r)
1.6
W (r) = P C(r )
exp(Ψ(·))
r

EM (maximum a posteriori):
P prior(r)−1+C(r)
W (r) =
prior(r )−1+C(r )
r

1.2
0.8

Mean-ﬁeld (with DP prior):
0.4
exp Ψ(prior(r)+C(r))
W (r) = exp Ψ( P prior(r )+C(r ))
r

P C(r)−0.5
r C(r )−0.5

0.4

0.8

1.2

1.6

2.0

x

Subtract 0.5 ⇒ small counts hurt more than large counts
⇒ rich gets richer ⇒ controls number of symbols
15

Parsing the WSJ Penn Treebank
Setup: grammar reﬁnement (split symbols into subsymbols)
Training on one section:
100.0

Standard PCFG

92.0
84.0

F1
76.0
68.0

5

9

12

16

20

maximum number of subsymbols (truncation K)

16

Parsing the WSJ Penn Treebank
Setup: grammar reﬁnement (split symbols into subsymbols)
Training on one section:
100.0

HDP-PCFG
Standard PCFG

92.0
84.0

F1
76.0
68.0

5

9

12

16

20

maximum number of subsymbols (truncation K)

16

Parsing the WSJ Penn Treebank
Setup: grammar reﬁnement (split symbols into subsymbols)
Training on one section:
100.0

HDP-PCFG (K = 20)
Standard PCFG

92.0
84.0

F1
76.0
68.0

5

9

12

16

20

maximum number of subsymbols (truncation K)

16

Parsing the WSJ Penn Treebank
Setup: grammar reﬁnement (split symbols into subsymbols)
Training on one section:
100.0

HDP-PCFG (K = 20)
Standard PCFG

92.0
84.0

F1
76.0
68.0

5

9

12

16

20

maximum number of subsymbols (truncation K)

Training on 20 sections: (K = 16)
Standard PCFG: 86.23

HDP-PCFG: 87.08

16

Parsing the WSJ Penn Treebank
Setup: grammar reﬁnement (split symbols into subsymbols)
Training on one section:
100.0

HDP-PCFG (K = 20)
Standard PCFG

92.0
84.0

F1
76.0
68.0

5

9

12

16

20

maximum number of subsymbols (truncation K)

Training on 20 sections: (K = 16)
Standard PCFG: 86.23
HDP-PCFG: 87.08
Results:
• HDP-PCFG overﬁts less than standard PCFG
• If have large amounts of data, HDP-PCFG standard PCFG
16

Conclusions
• What? HDP-PCFG model
allows number of grammar symbols to adapt to data

• How? Mean-ﬁeld algorithm (variational inference)
simple, eﬃcient, similar to EM

S → NP VP
NP → PRP
VP → VBD NP
NP → DT NN
PRP → she
VBD → heard
DT → the
NN → noise
S → NP VP
NP → PRP
VP → VBD NP
NP → DT NN
PRP → she
VBD → heard
DT → the
NN → noise
S → NP VP
NP → PRP
VP → VBD NP
NP → DT NN
PRP → she
VBD → heard
DT → the
NN → noise
S → NP VP
NP → PRP
VP → VBD NP
NP → DT NN
PRP → she
VBD → heard
DT → the

• When? Have small amounts of data
overﬁts less than standard PCFG

NN → noise
S → NP VP
NP → PRP
VP → VBD NP
NP → DT NN
PRP → she
VBD → heard
DT → the
NN → noise
S → NP VP
NP → PRP
VP → VBD NP
NP → DT NN
PRP → she
VBD → heard
DT → the
NN → noise

• Why? Declarative framework

S → NP VP
NP → PRP
VP → VBD NP
NP → DT NN
PRP → she
VBD → heard
DT → the
NN → noise

Grammar complexity speciﬁed declaratively in model
rather than in learning procedure

S → NP VP
NP → PRP
VP → VBD NP
NP → DT NN
PRP → she
VBD → heard
DT → the
NN → noise

17

