Feature Noising for Log-linear Structured Prediction
Sida I. Wang∗ Mengqiu Wang∗ Stefan Wager† ,
,
,
Percy Liang, Christopher D. Manning
Department of Computer Science, † Department of Statistics
Stanford University, Stanford, CA 94305, USA
{sidaw, mengqiu, pliang, manning}@cs.stanford.edu
swager@stanford.edu

Abstract
NLP models have many and sparse features,
and regularization is key for balancing model
overﬁtting versus underﬁtting. A recently repopularized form of regularization is to generate fake training data by repeatedly adding
noise to real data. We reinterpret this noising
as an explicit regularizer, and approximate it
with a second-order formula that can be used
during training without actually generating
fake data. We show how to apply this method
to structured prediction using multinomial logistic regression and linear-chain CRFs. We
tackle the key challenge of developing a dynamic program to compute the gradient of the
regularizer efﬁciently. The regularizer is a
sum over inputs, so we can estimate it more
accurately via a semi-supervised or transductive extension. Applied to text classiﬁcation
and NER, our method provides a >1% absolute performance gain over use of standard L2
regularization.

1

Introduction

NLP models often have millions of mainly sparsely
attested features. As a result, balancing overﬁtting
versus underﬁtting through good weight regularization remains a key issue for achieving optimal performance. Traditionally, L2 or L1 regularization is
employed, but these simple types of regularization
penalize all features in a uniform way without taking into account the properties of the actual model.
An alternative approach to regularization is to
generate fake training data by adding random noise
to the input features of the original training data. Intuitively, this can be thought of as simulating miss∗

Both authors contributed equally to the paper

ing features, whether due to typos or use of a previously unseen synonym. The effectiveness of this
technique is well-known in machine learning (AbuMostafa, 1990; Burges and Sch¨ lkopf, 1997; Simard
o
et al., 2000; Rifai et al., 2011a; van der Maaten
et al., 2013), but working directly with many corrupted copies of a dataset can be computationally
prohibitive. Fortunately, feature noising ideas often
lead to tractable deterministic objectives that can be
optimized directly. Sometimes, training with corrupted features reduces to a special form of regularization (Matsuoka, 1992; Bishop, 1995; Rifai
et al., 2011b; Wager et al., 2013). For example,
Bishop (1995) showed that training with features
that have been corrupted with additive Gaussian
noise is equivalent to a form of L2 regularization in
the low noise limit. In other cases it is possible to
develop a new objective function by marginalizing
over the artiﬁcial noise (Wang and Manning, 2013;
van der Maaten et al., 2013).
The central contribution of this paper is to show
how to efﬁciently simulate training with artiﬁcially
noised features in the context of log-linear structured prediction, without actually having to generate noised data. We focus on dropout noise (Hinton
et al., 2012), a recently popularized form of artiﬁcial feature noise where a random subset of features
is omitted independently for each training example.
Dropout and its variants have been shown to outperform L2 regularization on various tasks (Hinton
et al., 2012; Wang and Manning, 2013; Wan et al.,
2013). Dropout is is similar in spirit to feature bagging in the deliberate removal of features, but performs the removal in a preset way rather than randomly (Bryll et al., 2003; Sutton et al., 2005; Smith
et al., 2005).

Our approach is based on a second-order approximation to feature noising developed among others
by Bishop (1995) and Wager et al. (2013), which allows us to convert dropout noise into a form of adaptive regularization. This method is suitable for structured prediction in log-linear models where second
derivatives are computable. In particular, it can be
used for multiclass classiﬁcation with maximum entropy models (a.k.a., softmax or multinomial logistic regression) and for the sequence models that are
ubiquitous in NLP, via linear chain Conditional Random Fields (CRFs).
For linear chain CRFs, we additionally show how
we can use a noising scheme that takes advantage
of the clique structure so that the resulting noising
regularizer can be computed in terms of the pairwise marginals. A simple forward-backward-type
dynamic program can then be used to compute the
gradient tractably. For ease of implementation and
scalability to semi-supervised learning, we also outline an even faster approximation to the regularizer.
The general approach also works in other clique
structures in addition to the linear chain when the
clique marginals can be computed efﬁciently.
Finally, we extend feature noising for structured
prediction to a transductive or semi-supervised setting. The regularizer induced by feature noising
is label-independent for log-linear models, and so
we can use unlabeled data to learn a better regularizer. NLP sequence labeling tasks are especially
well suited to a semi-supervised approach, as input
features are numerous but sparse, and labeled data
is expensive to obtain but unlabeled data is abundant
(Li and McCallum, 2005; Jiao et al., 2006).
Wager et al. (2013) showed that semi-supervised
dropout training for logistic regression captures a
similar intuition to techniques such as entropy regularization (Grandvalet and Bengio, 2005) and transductive SVMs (Joachims, 1999), which encourage
conﬁdent predictions on the unlabeled data. Semisupervised dropout has the advantage of only using the predicted label probabilities on the unlabeled
data to modulate an L2 regularizer, rather than requiring more heavy-handed modeling of the unlabeled data as in entropy regularization or expectation regularization (Mann and McCallum, 2007).
In experimental results, we show that simulated
feature noising gives more than a 1% absolute boost

f (yt−1, yt )
yt−1

f (yt , yt+1 )
yt

yt+1

f (yt , xt )

f (yt−1, yt )
yt−1

f (yt , yt+1 )
yt

yt+1

f (yt , xt )
Figure 1: An illustration of dropout feature noising
in linear-chain CRFs with only transition features
and node features. The green squares are node features f (yt , xt ), and the orange squares are edge features f (yt−1 , yt ). Conceptually, given a training example, we sample some features to ignore (generate
fake data) and make a parameter update. Our goal is
to train with a roughly equivalent objective, without
actually sampling.
in performance over L2 regularization, on both text
classiﬁcation and an NER sequence labeling task.

2

Feature Noising Log-linear Models

Consider the standard structured prediction problem
of mapping some input x ∈ X (e.g., a sentence)
to an output y ∈ Y (e.g., a tag sequence). Let
f (y, x) ∈ Rd be the feature vector, θ ∈ Rd be the
weight vector, and s = (s1 , . . . , s|Y| ) be a vector of
scores for each output, with sy = f (y, x) · θ. Now
deﬁne a log-linear model:
p(y | x; θ) = exp{sy − A(s)},

(1)

where A(s) = log y exp{sy } is the log-partition
function. Given an example (x, y), parameter estimation corresponds to choosing θ to maximize p(y |
x; θ).
The key idea behind feature noising is to artiﬁcially corrupt the feature vector f (y, x) randomly

˜
into some f (y, x) and then maximize the average
log-likelihood of y given these corrupted features—
the motivation is to choose predictors θ that are robust to noise (missing words for example). Let ˜,
s
p(y | x; θ) be the randomly perturbed versions cor˜
˜
responding to f (y, x). We will also assume the
˜
feature noising preserves the mean: E[f (y, x)] =
f (y, x), so that E[˜] = s. This can always be done
s
by scaling the noised features as described in the list
of noising schemes.
It is useful to view feature noising as a form of
regularization. Since feature noising preserves the
mean, the feature noising objective can be written as
the sum of the original log-likelihood plus the difference in log-normalization constants:

This expression still has two sources of potential intractability, a sum over an exponential number of
noised score vectors ˜ and a sum over the |Y| coms
ponents of ˜.
s
Multiclass classiﬁcation If we assume that the
components of ˜ are independent, then Cov(˜) ∈
s
s
R|Y|×|Y| is diagonal, and we have
Rq (θ, x) =

1
2

µy (1 − µy ) Var[˜y ],
s
def

where the mean µy = pθ (y | x) is the model probability, the variance µy (1 − µy ) measures model uncertainty, and
˜
Var[˜y ] = θ Cov[f (y, x)]θ
s

E[log p(y | x; θ)] = E[˜y − A(˜)]
˜
s
s

(2)

= log p(y | x; θ) − R(θ, x), (3)
def

R(θ, x) = E[A(˜)] − A(s).
s

(4)

Since A(·) is convex, R(θ, x) is always positive by
Jensen’s inequality and can therefore be interpreted
as a regularizer. Note that R(θ, x) is in general nonconvex.
Computing the regularizer (4) requires summing
over all possible noised feature vectors, which can
imply exponential effort in the number of features.
This is intractable even for ﬂat classiﬁcation. Following Bishop (1995) and Wager et al. (2013), we
approximate R(θ, x) using a second-order Taylor
expansion, which will allow us to work with only
means and covariances of the noised features. We
take a quadratic approximation of the log-partition
function A(·) of the noised score vector ˜ around
s
the the unnoised score vector s:

(8)

y∈Y

(9)

measures the uncertainty caused by feature noising.1
The regularizer Rq (θ, x) involves the product of two
variance terms, the ﬁrst is non-convex in θ and the
second is quadratic in θ. Note that to reduce the regularization, we will favor models that (i) predict conﬁdently and (ii) have stable scores in the presence of
feature noise.
For multiclass classiﬁcation, we can explicitly
sum over each y ∈ Y to compute the regularizer,
but this will be intractable for structured prediction.
To specialize to multiclass classiﬁcation for the
moment, let us assume that we have a separate
weight vector for each output y applied to the same
feature vector g(x); that is, the score sy = θy · g(x).
Further, assume that the components of the noised
feature vector g (x) are independent. Then we can
˜
simplify (9) to the following:
2
Var[gj (x)]θyj .

Var[˜y ] =
s

(10)

j

A(˜)
s

A(s) + A(s) (˜ − s)
s
1
+ (˜ − s) 2 A(s)(˜ − s).
s
s
2

(5)

Plugging (5) into (4), we obtain a new regularizer
Rq (θ, x), which we will use as an approximation to
R(θ, x):
1
Rq (θ, x) = E[(˜ − s) 2 A(s)(˜ − s)]
s
s
2
1
= tr( 2 A(s) Cov(˜)).
s
2

(6)
(7)

Noising schemes We now give some examples of
˜
possible noise schemes for generating f (y, x) given
the original features f (y, x). This distribution affects the regularization through the variance term
Var[˜y ].
s
• Additive Gaussian:
˜
f (y, x) = f (y, x) + ε, where ε
N (0, σ 2 Id×d ).
1

∼

Here, we are using the fact that ﬁrst and second derivatives
of the log-partition function are the mean and variance.

In this case, the contribution to the regularizer
2
from noising is Var[˜y ] = j σ 2 θyj .
s
• Dropout:
˜
f (y, x) = f (y, x) z, where takes the elementwise product of two vectors. Here, z is
a vector with independent components which
1
has zi = 0 with probability δ, zi = 1−δ with
probability 1 − δ. In this case, Var[˜y ] =
s
j

gj (x)2 δ 2
1−δ θyj .

• Multiplicative Gaussian:
˜
f (y, x) = f (y, x)

(1 + ε), where
∼ N (0, σ 2 Id×d ).
Here, Var[˜y ] =
s
2 σ 2 θ 2 . Note that under our secondyj
j gj (x)
order approximation Rq (θ, x), the multiplicative Gaussian and dropout schemes are equivalent, but they differ under the original regularizer R(θ, x).
ε

2.1

A key observation (Wager et al., 2013) is that
the noising regularizer R (8), while involving a
sum over examples, is independent of the output
y. This suggests estimating R using unlabeled
data. Speciﬁcally, if we have n labeled examples
D = {x1 , x2 , . . . , xn } and m unlabeled examples
Dunlabeled = {u1 , u2 , . . . , un }, then we can deﬁne a
regularizer that is a linear combination the regularizer estimated on both datasets, with α tuning the
tradeoff between the two:
R∗ (θ, D, Dunlabeled )
def

=

3

n
n + αm

(11)
m

R(θ, xi ) + α
i=1

T

f (y, x) =

R(θ, ui ) .
i=1

Feature Noising in Linear-Chain CRFs

So far, we have developed a regularizer that works
for all log-linear models, but—in its current form—
is only practical for multiclass classiﬁcation. We
now exploit the decomposable structure in CRFs to
deﬁne a new noising scheme which does not require
us to explicitly sum over all possible outputs y ∈ Y.
The key idea will be to noise each local feature vector (which implicitly affects many y) rather than
noise each y independently.

gt (yt−1 , yt , x),

(12)

t=1

where gt (a, b, x) is deﬁned on a pair of consecutive
tags a, b for positions t − 1 and t.
Rather than working with a score sy for each
y ∈ Y, we deﬁne a collection of local scores
s = {sa,b,t }, for each tag pair (a, b) and position t = 1, . . . , T . We consider noising schemes
which independently set gt (a, b, x) for each a, b, t.
˜
˜ = {˜a,b,t } be the corresponding collection of
Let s
s
noised scores.
We can write the log-partition function of these
local scores as follows:
T

A(s) = log

exp
y∈Y

Semi-supervised learning

n

Assume that the output y = (y1 , . . . , yT ) is a sequence of T tags. In linear chain CRFs, the feature
vector f decomposes into a sum of local feature vectors gt :

syt−1 ,yt ,t

.

(13)

t=1

The ﬁrst derivative yields the edge marginals under
the model, µa,b,t = pθ (yt−1 = a, yt = b | x), and
the diagonal elements of the Hessian 2 A(s) yield
the marginal variances.
Now, following (7) and (8), we obtain the following regularizer:
1
Rq (θ, x) =
µa,b,t (1 − µa,b,t ) Var[˜a,b,t ],
s
2
a,b,t

(14)
where µa,b,t (1 − µa,b,t ) measures model uncertainty
about edge marginals, and Var[˜a,b,t ] is simply the
s
uncertainty due to noising. Again, minimizing the
regularizer means making conﬁdent predictions and
having stable scores under feature noise.
Computing partial derivatives So far, we have
deﬁned the regularizer Rq (θ, x) based on feature
noising. In order to minimize Rq (θ, x), we need to
take its derivative.
First, note that log µa,b,t is the difference of a restricted log-partition function and the log-partition
function. So again by properties of its ﬁrst derivative, we have:
log µa,b,t = Epθ (y|x,yt−1 =a,yt =b) [f (y, x)] (15)
− Epθ (y|x) [f (y, x)].

Using the fact that µa,b,t = µa,b,t log µa,b,t and
the fact that Var[˜a,b,t ] is a quadratic function in θ,
s
we can simply apply the product rule to derive the
ﬁnal gradient Rq (θ, x).

We can compute these messages recursively in the
standard way. The forward recurrence is
Fta =

pθ (yt−2 = b | yt−1 = a)
b

3.1

A Dynamic Program for the Conditional
Expectation

A naive computation of the gradient Rq (θ, x) requires a full forward-backward pass to compute
Epθ (y|yt−1 =a,yt =b,x) [f (y, x)] for each tag pair (a, b)
and position t, resulting in a O(K 4 T 2 ) time algorithm.
In this section, we reduce the running time to
O(K 2 T ) using a more intricate dynamic program.
By the Markov property of the CRF, y1:t−2 only depends on (yt−1 , yt ) through yt−1 and yt+1:T only
depends on (yt−1 , yt ) through yt .
First, it will be convenient to deﬁne the partial
sum of the local feature vector from positions i to
j as follows:
j

Gi:j =

gt (yt−1 , yt , x).

(16)

t=i

Consider the task of computing the feature expectation Epθ (y|yt−1 =a,yt =b) [f (y, x)] for a ﬁxed (a, b, t).
We can expand this quantity into
pθ (y−(t−1:t) | yt−1 = a, yt = b)G1:T .
y:yt−1 =a,yt =b

Conditioning on yt−1 , yt decomposes the sum into
three pieces:
b
[gt (yt−1 = a, yt = b, x) + Fta + Bt ],
y:yt−1 =a,yt =b

where
Fta =

pθ (y1:t−2 | yt−1 = a)G1:t−1 ,

(17)

pθ (yt+1:T | yt = b)Gt+1:T ,

(18)

y1:t−2
b
Bt =
yt+1:T

are the expected feature vectors summed over the
preﬁx and sufﬁx of the tag sequence, respectively.
b
Note that Fta and Bt are analogous to the forward
and backward messages of standard CRF inference,
with the exception that they are vectors rather than
scalars.

b
gt (yt−2 = b, yt−1 = a, x) + Ft−1 ,

and a similar recurrence holds for the backward mesb
sages Bt .
Running the resulting dynamic program takes
O(K 2 T q) time and requires O(KT q) storage,
where K is the number of tags, T is the sequence
length and q is the number of active features. Note
that this is the same order of dependence as normal
CRF training, but there is an additional dependence
on the number of active features q, which makes
training slower.

4

Fast Gradient Computations

In this section, we provide two ways to further improve the efﬁciency of the gradient calculation based
on ignoring long-range interactions and based on exploiting feature sparsity.
4.1

Exploiting Feature Sparsity and
Co-occurrence

In each forward-backward pass over a training example, we need to compute the conditional expectations for all features active in that example.
Naively applying the dynamic program in Section 3
is O(K 2 T ) for each active feature. The total complexity has to factor in the number of active features, q. Although q only scales linearly with sentence length, in practice this number could get large
pretty quickly. For example, in the NER tagging experiments (cf. Section 5), the average number of
active features per token is about 20, which means
q
20T ; this term quickly dominates the computational costs. Fortunately, in sequence tagging and
other NLP tasks, the majority of features are sparse
and they often co-occur. That is, some of the active features would ﬁre and only ﬁre at the same locations in a given sequence. This happens when a
particular token triggers multiple rare features.
We observe that all indicator features that only
ﬁred once at position t have the same conditional expectations (and model expectations). As a result, we
can collapse such a group of features into a single

feature as a preprocessing step to avoid computing
identical expectations for each of the features. Doing so on the same NER tagging experiments cuts
down q/T from 20 to less than 5, and gives us a 4
times speed up at no loss of accuracy. The exact
same trick is applicable to the general CRF gradient
computation as well and gives similar speedup.
4.2

Short-range interactions

It is also possible to speed up the method by resorting to approximate gradients. In our case, the
dynamic program from Section 3 together with the
trick described above ran in a manageable amount
of time. The techniques developed here, however,
could prove to be useful on larger tasks.
Let us rewrite the quantity we want to compute
slightly differently (again, for all a, b, t):
T

Epθ (y|x,yt−1 =a,yt =b) [gi (yi−1 , yi , x)].

(19)

i=1

The intuition is that conditioned on yt−1 , yt , the
terms gi (yi−1 , yi , x) where i is far from t will be
close to Epθ (y|x) [gi (yi−1 , yi , x)].
This motivates replacing the former with the latter
whenever |i − k| ≥ r where r is some window size.
This approximation results in an expression which
only has to consider the sum of the local feature vectors from i−r to i+r, which is captured by Gi−r:i+r :

Dataset
CoNLL
SANCL
20news
RCV14
R21578
TDT2

q
20
5
81
76
47
130

d
437906
679959
62061
29992
18933
36771

K
5
12
20
4
65
30

Ntrain
204567
761738
15935
9625/2
5946
9394/2

Ntest
46666
82405
3993
9625/2
2347
9394/2

Table 1: Description of datasets. q: average number
of non-zero features per example, d: total number
of features, K: number of classes to predict, Ntrain :
number of training examples, Ntest : number of test
examples.

5

Experiments

We show experimental results on the CoNLL-2003
Named Entity Recognition (NER) task, the SANCL
Part-of-speech (POS) tagging task, and several document classiﬁcation tasks.2 The datasets used are
described in Table 1. We used standard splits whenever available; otherwise we split the data at random into a test set and a train set of equal sizes
(RCV14 , TDT2). CoNLL has a development set
of size 51578, which we used to tune regularization parameters. The SANCL test set is divided into
3 genres, namely answers, newsgroups, and
reviews, each of which has a corresponding development set.3

Epθ (y|yt−1 =a,yt =b,x) [f (y, x)] − Epθ (y|x) [f (y, x)]
≈ Epθ (y|yt−1 =a,yt =b,x) [Gt−r:t+r ]

(20)

− Epθ (y|x) [Gt−r:t+r ].
We can further approximate this last expression by
letting r = 0, obtaining:
gt (a, b, x) − Epθ (y|x) [gt (yt−1 , yt , x)].

(21)

The second expectation can be computed from the
edge marginals.
The accuracy of this approximation hinges on the
lack of long range dependencies. Equation (21)
shows the case of r = 0; this takes almost no additional effort to compute. However, for some of our
experiments, we observed a 20% difference with the
real derivative. For r > 0, the computational savings
are more limited, but the bounded-window method
is easier to implement.

5.1

Multiclass Classiﬁcation

We begin by testing our regularizer in the simple
case of classiﬁcation where Y = {1, 2, . . . , K} for
K classes. We examine the performance of the noising regularizer in both the fully supervised setting as
well as the transductive learning setting.
In the transductive learning setting, the learner
is allowed to inspect the test features at train time
(without the labels). We used the method described
in Section 2.1 for transductive dropout.
2

The document classiﬁcation data are available
at
http://www.csie.ntu.edu.tw/˜cjlin/
libsvmtools/datasets and http://www.cad.
zju.edu.cn/home/dengcai/Data/TextData.html
3
The SANCL dataset has two additional genres—emails and
weblogs—that we did not use, as we did not have access to
development sets for these genres.

Dataset
CoNLL
20news
RCV14
R21578
TDT2

K
5
20
4
65
30

None
78.03
81.44
95.76
92.24
97.74

L2
80.12
82.19
95.90
92.24
97.91

Drop
80.90
83.37
96.03
92.24
98.00

Dataset
CoNLL
20news
RCV14
R21578
TDT2

+Test
81.66
84.71
96.11
92.58
98.12

Table 2: Classiﬁcation performance and transductive learning results on some standard datasets.
None: use no regularization, Drop: quadratic approximation to the dropout noise (8), +Test: also use
the test set to estimate the noising regularizer (11).

K
5
20
4
65
30

L2
91.46
76.55
94.76
90.67
97.34

Drop
91.81
79.07
94.79
91.24
97.54

+Unlabeled
92.02
80.47
95.16
90.30
97.89

Table 3: Semisupervised learning results on some
standard datasets. A third (33%) of the full dataset
was used for training, a third for testing, and the rest
as unlabeled.
0.9

Semi-supervised Learning with Feature
Noising

In the transductive setting, we used test data
(without labels) to learn a better regularizer. As an
alternative, we could also use unlabeled data in place
of the test data to accomplish a similar goal; this
leads to a semi-supervised setting.
To test the semi-supervised idea, we use the same
datasets as above. We split each dataset evenly into
3 thirds that we use as a training set, a test set and an
unlabeled dataset. Results are given in Table 3.
In most cases, our semi-supervised accuracies are
lower than the transductive accuracies given in Table
2; this is normal in our setup, because we used less
labeled data to train the semi-supervised classiﬁer
than the transductive one.4
5.1.2

The Second-Order Approximation

The results reported above all rely on the approximate dropout regularizer (8) that is based on a
second-order Taylor expansion. To test the validity
of this approximation we compare it to the Gaussian
method developed by Wang and Manning (2013) on
a two-class classiﬁcation task.
We use the 20-newsgroups alt.atheism vs
soc.religion.christian classiﬁcation task;
results are shown in Figure 2. There are 1427 exam4

The CoNNL results look somewhat surprising, as the semisupervised results are better than the transductive ones. The
reason for this is that the original CoNLL test set came from a
different distributions than the training set, and this made the
task more difﬁcult. Meanwhile, in our semi-supervised experiment, the test and train sets are drawn from the same distribution and so our semi-supervised task is actually easier than the
original one.

0.88
0.86
Accuracy

5.1.1

0.84
0.82
0.8
0.78
−6
10

L2 only
L2+Gaussian dropout
L2+Quadratic dropout
−4

−2

0

10
10
10
L2 regularization strength (λ)

2

10

Figure 2: Effect of λ in λ θ 2 on the testset perfor2
mance. Plotted is the test set accuracy with logistic regression as a function of λ for the L2 regularizer, Gaussian dropout (Wang and Manning, 2013)
+ additional L2 , and quadratic dropout (8) + L2 described in this paper. The default noising regularizer
is quite good, and additional L2 does not help. Notice that no choice of λ in L2 can help us combat
overﬁtting as effectively as (8) without underﬁtting.
ples with 22178 features, split evenly and randomly
into a training set and a test set.
Over a broad range of λ values, we ﬁnd that
dropout plus L2 regularization performs far better
than using just L2 regularization for any value of
λ. We see that Gaussian dropout appears to perform slightly better than the quadratic approximation discussed in this paper. However, our quadratic
approximation extends easily to the multiclass case
and to structured prediction in general, while Gaussian dropout does not. Thus, it appears that our approximation presents a reasonable trade-off between

Fβ=1
Dev
Test

computational efﬁciency and prediction accuracy.
5.2

CRF Experiments

We evaluate the quadratic dropout regularizer in
linear-chain CRFs on two sequence tagging tasks:
the CoNLL 2003 NER shared task (Tjong Kim Sang
and De Meulder, 2003) and the SANCL 2012 POS
tagging task (Petrov and McDonald, 2012) .
The standard CoNLL-2003 English shared task
benchmark dataset (Tjong Kim Sang and De Meulder, 2003) is a collection of documents from
Reuters newswire articles, annotated with four entity types: Person, Location, Organization, and
Miscellaneous. We predicted the label sequence
Y = {LOC, MISC, ORG, PER, O}T without considering the BIO tags.
For training the CRF model, we used a comprehensive set of features from Finkel et al. (2005) that
gives state-of-the-art results on this task. A total
number of 437906 features were generated on the
CoNLL-2003 training dataset. The most important
features are:
• The word, word shape, and letter n-grams (up to
6gram) at current position
• The prediction, word, and word shape of the previous and next position
• Previous word shape in conjunction with current
word shape
• Disjunctive word set of the previous and next 4
positions
• Capitalization pattern in a 3 word window
• Previous two words in conjunction with the word
shape of the previous word
• The current word matched against a list of name
titles (e.g., Mr., Mrs.)
The Fβ=1 results are summarized in Table 4. We
obtain a 1.6% and 1.1% absolute gain on the test
and dev set, respectively. Detailed results are broken down by precision and recall for each tag and are
shown in Table 6. These improvements are signiﬁcant at the 0.1% level according to the paired bootstrap resampling method of 2000 iterations (Efron
and Tibshirani, 1993).
For the SANCL (Petrov and McDonald, 2012)
POS tagging task, we used the same CRF framework
with a much simpler set of features
• word unigrams: w−1 , w0 , w1
• word bigram: (w−1 , w0 ) and (w0 , w1 )

None
89.40
84.67

L2
90.73
85.82

Drop
91.86
87.42

Table 4: CoNLL summary of results. None: no regularization, Drop: quadratic dropout regularization
(14) described in this paper.
Fβ=1
Dev
Test
Dev
Test
Dev
Test

None
L2
Drop
newsgroups
91.34 91.34 91.47
91.44 91.44 91.81
reviews
91.97 91.95 92.10
90.70 90.67 91.07
answers
90.78 90.79 90.70
91.00 90.99 91.09

Table 5: SANCL POS tagging Fβ=1 scores for the 3
ofﬁcial evaluation sets.
We obtained a small but consistent improvement
using the quadratic dropout regularizer in (14) over
the L2 -regularized CRFs baseline.
Although the difference on SANCL is small,
the performance differences on the test sets of
reviews and newsgroups are statistically signiﬁcant at the 0.1% level. This is also interesting
because here is a situation where the features are extremely sparse, L2 regularization gave no improvement, and where regularization overall matters less.

6

Conclusion

We have presented a new regularizer for learning
log-linear models such as multiclass logistic regression and conditional random ﬁelds. This regularizer
is based on a second-order approximation of feature noising schemes, and attempts to favor models that predict conﬁdently and are robust to noise
in the data. In order to apply our method to CRFs,
we tackle the key challenge of dealing with feature
correlations that arise in the structured prediction
setting in several ways. In addition, we show that
the regularizer can be applied naturally in the semisupervised setting. Finally, we applied our method
to a range of different datasets and demonstrate consistent gains over standard L2 regularization. Inves-

LOC
MISC
ORG
PER
Overall

Precision
91.47%
88.77%
85.22%
92.12%
89.84%

Recall
91.12%
81.07%
84.08%
93.97%
88.97%

Fβ=1
91.29
84.75
84.65
93.04
89.40

(a) CoNLL dev. set with no regularization

Tag
LOC
MISC
ORG
PER
Overall

Precision
87.33%
78.93%
78.70%
88.82%
84.28%

Recall
84.47%
77.12%
79.49%
93.11%
85.06%

Fβ=1
85.87
78.02
79.09
90.92
84.67

(d) CoNLL test set with no regularization

Precision
92.05%
90.51%
88.35%
93.12%
91.36%

Recall
92.84%
83.52%
85.23%
94.19%
90.11%

Fβ=1
92.44
86.87
86.76
93.65
90.73

(b) CoNLL dev. set with L2 regularization

Precision
87.96%
77.53%
81.30%
90.30%
85.57%

Recall
86.13%
79.30%
80.49%
93.33%
86.08%

Fβ=1
87.03
78.41
80.89
91.79
85.82

(e) CoNLL test set with L2 regularization

Precision
93.59%
93.99%
92.48%
94.81%
93.85%

Recall
92.69%
81.47%
84.61%
95.11%
89.96%

Fβ=1
93.14
87.28
88.37
94.96
91.86

(c) CoNLL dev. set with dropout
regularization

Precision
86.26%
81.52%
88.29%
92.15%
88.40%

Recall
87.74%
77.34%
81.89%
92.68%
86.45%

Fβ=1
86.99
79.37
84.97
92.41
87.42

(f) CoNLL test set with dropout
regularization

Table 6: CoNLL NER results broken down by tags and by precision, recall, and Fβ=1 . Top: development
set, bottom: test set performance.
tigating how to better optimize this non-convex regularizer online and convincingly scale it to the semisupervised setting seem to be promising future directions.

Acknowledgements
The authors would like to thank the anonymous reviewers for their comments. We gratefully acknowledge the support of the Defense Advanced Research
Projects Agency (DARPA) Broad Operational Language Translation (BOLT) program through IBM.
Any opinions, ﬁndings, and conclusions or recommendations expressed in this material are those of
the author(s) and do not necessarily reﬂect the view
of the DARPA, or the US government. S. Wager is
supported by a BC and EJ Eaves SGF Fellowship.

References
Yaser S. Abu-Mostafa. 1990. Learning from hints in
neural networks. Journal of Complexity, 6(2):192–
198.
Chris M. Bishop. 1995. Training with noise is equivalent to Tikhonov regularization. Neural computation,
7(1):108–116.
Robert Bryll, Ricardo Gutierrez-Osuna, and Francis
Quek. 2003. Attribute bagging: improving accuracy

of classiﬁer ensembles by using random feature subsets. Pattern recognition, 36(6):1291–1302.
Chris J.C. Burges and Bernhard Sch¨ lkopf. 1997. Imo
proving the accuracy and speed of support vector machines. In Advances in Neural Information Processing
Systems, pages 375–381.
Brad Efron and Robert Tibshirani. 1993. An Introduction
to the Bootstrap. Chapman & Hall, New York.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local information into information extraction systems by Gibbs sampling. In Proceedings of the 43rd annual meeting of
the Association for Computational Linguistics, pages
363–370.
Yves Grandvalet and Yoshua Bengio. 2005. Entropy
regularization. In Semi-Supervised Learning, United
Kingdom. Springer.
Geoffrey E. Hinton, Nitish Srivastava, Alex Krizhevsky,
Ilya Sutskever, and Ruslan R. Salakhutdinov.
2012. Improving neural networks by preventing
co-adaptation of feature detectors. arXiv preprint
arXiv:1207.0580.
Feng Jiao, Shaojun Wang, Chi-Hoon Lee, Russell
Greiner, and Dale Schuurmans.
2006.
Semisupervised conditional random ﬁelds for improved sequence segmentation and labeling. In Proceedings of
the 44th annual meeting of the Association for Computational Linguistics, ACL-44, pages 209–216.
Thorsten Joachims. 1999. Transductive inference for

text classiﬁcation using support vector machines. In
Proceedings of the International Conference on Machine Learning, pages 200–209.
Wei Li and Andrew McCallum. 2005. Semi-supervised
sequence modeling with syntactic topic models. In
Proceedings of the 20th national conference on Artiﬁcial Intelligence - Volume 2, AAAI’05, pages 813–
818.
Gideon S. Mann and Andrew McCallum. 2007. Simple, robust, scalable semi-supervised learning via expectation regularization. In Proceedings of the International Conference on Machine Learning.
Kiyotoshi Matsuoka. 1992. Noise injection into inputs
in back-propagation learning. Systems, Man and Cybernetics, IEEE Transactions on, 22(3):436–440.
Slav Petrov and Ryan McDonald. 2012. Overview of the
2012 shared task on parsing the web. Notes of the First
Workshop on Syntactic Analysis of Non-Canonical
Language (SANCL).
Salah Rifai, Yann Dauphin, Pascal Vincent, Yoshua Bengio, and Xavier Muller. 2011a. The manifold tangent
classiﬁer. Advances in Neural Information Processing
Systems, 24:2294–2302.
Salah Rifai, Xavier Glorot, Yoshua Bengio, and Pascal
Vincent. 2011b. Adding noise to the input of a model
trained with a regularized objective. arXiv preprint
arXiv:1104.3250.
Patrice Y. Simard, Yann A. Le Cun, John S. Denker, and
Bernard Victorri. 2000. Transformation invariance in
pattern recognition: Tangent distance and propagation.
International Journal of Imaging Systems and Technology, 11(3):181–197.
Andrew Smith, Trevor Cohn, and Miles Osborne. 2005.
Logarithmic opinion pools for conditional random
ﬁelds. In Proceedings of the 43rd Annual Meeting on
Association for Computational Linguistics, pages 18–
25. Association for Computational Linguistics.
Charles Sutton, Michael Sindelar, and Andrew McCallum. 2005. Feature bagging: Preventing weight undertraining in structured discriminative learning. Center for Intelligent Information Retrieval, U. of Massachusetts.
Erik F. Tjong Kim Sang and Fien De Meulder. 2003.
Introduction to the conll-2003 shared task: languageindependent named entity recognition. In Proceedings
of the seventh conference on Natural language learning at HLT-NAACL 2003 - Volume 4, CONLL ’03,
pages 142–147.
Laurens van der Maaten, Minmin Chen, Stephen Tyree,
and Kilian Q. Weinberger. 2013. Learning with
marginalized corrupted features. In Proceedings of the
International Conference on Machine Learning.

Stefan Wager, Sida Wang, and Percy Liang. 2013.
Dropout training as adaptive regularization. arXiv
preprint:1307.1493.
Li Wan, Matthew Zeiler, Sixin Zhang, Yann LeCun, and
Rob Fergus. 2013. Regularization of neural networks
using dropconnect. In Proceedings of the International Conference on Machine learning.
Sida Wang and Christopher D. Manning. 2013. Fast
dropout training. In Proceedings of the International
Conference on Machine Learning.

