Appeared in ICDM’09

Aspect Guided Text Categorization with Unobserved Labels
Dan Roth
Yuancheng Tu
University of Illinois at Urbana-Champaign
{danr,ytu}@illinois.edu
Abstract

well when the “document” is very short, and even worse
when the number of labels is very large. And, there is
no principled solution to the problem of dealing with new,
previously unobserved labels, beyond re-training the classiﬁers. Moreover, category labels are only treated as mutually
exclusive ﬂat symbols without any structure.

This paper proposes a novel multiclass classiﬁcation
method and exhibits its advantage in the domain of text categorization with a large label space and, most importantly,
when some of the labels were not observed in the training
data. The key insight is the introduction of intermediate aspect variables that encode properties of the labels. Aspect
variables serve as a joint representation for observed and
unobserved labels. This way the classiﬁcation problem can
be viewed as a structure learning problem with natural constraints on assignments to the aspect variables. We solve the
problem as a constrained optimization problem over multiple learners and show signiﬁcant improvement in classifying short sentences into a large label space of categories,
including previously unobserved categories.

In this paper we introduce a new method for multiclass
classiﬁcation of text documents that addresses all these issues. Speciﬁcally, it handles well the case of a very large
number of labels and, most signiﬁcantly, it can robustly
categorize text snippets into previously unobserved labels
which traditional methods cannot deal with at all. Our
model, Multi-Aspect Multiclass Classiﬁcation (MAMuC),
introduces a set of intermediate aspect variables, each representing a property of the data and its associated labels.
Rather than training a multiclass classiﬁer to predict a label,
we train a structured classiﬁer, that learns to assign values to
the aspect variables. This allows us to enforce natural constraints (hard and soft) among aspect variables—e.g, if an
aspect takes the value “turn”, it is unlikely that another aspect will take the value “restaurant”, and it is likely that one
will take either the value ‘on’ or ‘off’. We deﬁne an objective function that scores assignments to the aspect variables;
we then predict a category by choosing the one that maximizes the score of the assignment to the aspect variables,
subject to the constraints. This view constitutes a reformulation of the original classiﬁcation problem as a structured
learning problem. We consider two training paradigms of
the aspect variables. In one, we train individual models for
aspect variables, and then predict a category label by combining the aspect models’ predictions and weights via a constrained optimization framework, subject to the constraints.
In the other, we train a joint model for the aspect variables,
taking into account the constraints during training.

1. Introduction
Text Categorization is an archetypical example for multiclass classiﬁcation and has many applications, ranging from
spam ﬁltering to E-mail classiﬁcation to sentiment analysis.
Our interest is in the case where the text documents are quite
short, typically only a short text snippet of 3-10 words, and
the number of possible categories is quite large, at least a
few hundreds. In addition, unlike traditional text classiﬁcation where each label is only one simple phrase such as
shopping, we consider the case in which each of the categories is more expressive, i.e., represented as a short sentence such as ﬁnd the nearest supermarket.
One case where this situation is important is in the domain of categorizing free text or spoken commands into operating commands to devices, e.g., as a way to control devices in a car. Moreover, in such applications, it is often
the case that new labels are introduced, with minimal or no
training data. The new labels may be variations of existing labels—the system may be familiar with labels such as
“turn on the radio” and “turn off the cd player” and is now
requested to categorize also into a new label “turn off the
GPS”. Traditional text categorization techniques do not do

The key novelty in our model is that the variables dictating the decision are not explicit in the problem deﬁnition,
but rather introduced as an intermediate level in order to exploit hidden structure of the category labels. As we show,
our scheme allows us to signiﬁcantly improve over traditional state-of-the-art multiclass classiﬁcation. More significantly, one of the key advantages this model provides is
1

the ability to handle the case of new, previously unobserved
labels. While existing multiclass classiﬁcation cannot deal
with unobserved labels, our method can predict aspects of
the data. Since aspects are chosen to provide a representation of the label space, predicting the aspect allows us to approximate and often predict a label that was never observed
in the training data by determining some of its aspects. In
our application, where each label is a short sentence, a partial prediction of the label is useful, since it can trigger interaction to clarify its exact value.

straints among these variables as a way to improve aspect
predictions and, consequently, the Y values.
In our domain (see Sec. 4) the labels are operating commands such as “ﬁnd nearest Chinese restaurant” or “CD
track 3”. We make use of ﬁve types of aspects which we
call: Topic, Action, Manner, Modiﬁer and Detail; each type
can take multiple values. For the aforementioned labels,
Topic takes the values restaurant and CD, resp., and Action
takes the values ﬁnd and null, resp.
An aspect variable zij is a function, zij : X × Y →
[0, 1] which indicates the probability that the ith aspect type
takes its j th value. At evaluation time, given a document
x ∈ X, we predict the label y ∈ Y that maximizes the
score assigned to the aspect variables Z:

2. Related Work
Multiclass classiﬁcation is a central problem in machine
learning. Existing literature on MCC assumes the independence of the output labels. With the exception of some
work on MCC with hierarchies [8], this paper is the ﬁrst
to explore the latent structure of the output labels, and to
formulate MCC as a structured learning problem. While
there has been a lot of work on structured output learning
in the last few years [5, 11, 17, 18, 15], we are unaware of
any use of it as a way to resolve MCC problems as we do
here. The paradigm that comes closest to ours is that of error correcting output codes [7]. Superﬁcially, our scheme
can be viewed as one that ﬁrst generates an output code,
and then assembles it to form a legitimate category using
constrained optimization. However, there are several key
differences–the key one being that in ECOC the decomposition of the label space is done “syntactically”, without any
understanding of the output space. Consequently, the resulting binary classiﬁcation problems could be very difﬁcult
learning problems, and the scheme cannot support prediction on new, previously unobserved labels.
There are several way to train an objective function for
structured output problems. We follow a discriminative
approach and train the model both jointly and separately
within a constrained optimization framework [3, 13]. It
has been shown that, when individual components can be
learned reliably, the latter is a better training scheme. Our
work substantiates this conclusion in the current context.

y = arg max score(Z(x, y)).
ˆ
y

Our formulation follows the one developed in [14, 16, 3].
The score is a linear objective function deﬁned in Eq. 1:
m

y = arg maxy
ˆ

wi zij (x, y) −
i=1

ρk dCk (Z(x, y), 1Ck )
Ck ∈C

(1)

where:
1. zij = probability(ith aspect takes its jth value).
2. Ck : 2Z → {0, 1} are constraints over possible values
of the zij s. E.g., if C1 (z23 , z34 , z15 ) = 0, then we do
not allow the simultaneous assignment of the 3rd value
to the 2nd aspect, the 4th value to the 3rd aspect and the
5th value to the 1st aspect. More concretely, we may
not allow the Action aspect take the value ‘turn’, when
the Topic aspect takes the value ‘restaurant’. Note that
we always have a constraint ∀i, j, k : C(zij , zik ) = 0.
3. d : Z × Ck → [0, 1] measures the degree to which the
the assignment z ∈ Z violates the constraint Ck . 1Ck
denotes the set of all assignments in Z that satisfy Ck ;
d measures the distance from assignment z ∈ Z to 1Ck .
This allows us to choose a value j for each zij , taking
into account constraints among these variables and consequently choose the category y ∈ Y as the solution to the inˆ
teger linear programming in Eq. 1. This objective function
is trained discriminatively, and both wi and ρk are learned
from the training data.
We note that the introduction of the aspect representation can be viewed also as addressing the common sparsity
problem in the label space. While is it likely (and happens
often in our domain) that a label is represented by a small
number of examples, an aspect is typically represented by
more examples, thus enabling learning more robust aspect
functions.

3. Multi-Aspect Mutliclass Classiﬁcation
Text categorization is an archetypical MCC problem; the
goal is to learn a function f : X → Y where X and Y are
collections of documents and labels. It is generally assumed
that variables in Y do not have any latent structure.
The key contribution of our MAMuC model is the introduction of a collection Z = {z11 , z12 , . . . , zij } of intermediate aspect variables. Each aspect variable can be thought
of as a property of the Y labels. Aspect variables take values that are interdependent; this allows us to exploit con2

3.1. Learning and Inference

Algorithm 2 IBT: Aspect models zi∗ are learned jointly subject to
the constraints. In the on-line algorithm used, model update and global
inference (Eq. 1) are interleaved.
1: LEARNING CONSTRAINTS:
2: for each Ci do
3:
if Ci is not violated in Training Data then
4:
ρi = ∞
else
5:
6:
ρi = − log P (Ci is violated in Training Data)
7:
end if
8: end for
9: LEARNING MODELS
10: Initialize weights for zi∗
11: for each example (x, ygold ) in Training Data do
12:
Evaluate zi∗
ypred = solution of Eq. 1
13:
14:
if ypred = ygold then
for each aspect zij do
15:
16:
if (zij )pred = (zij )gold then
17:
w(zij ) = w(zij ) + zij (x, ygold ) − zij (x, ypred )
18:
end if
19:
end for
20:
end if
21: end for
22: INFERENCE at DECISION TIME:
23: Use the distribution over zi∗ and solve Eq. 1

We follow a discriminative approach and train the model
weights wi in two ways, following [13]. In both cases we
use an online training paradigm, and make use of the Averaged Perceptron Algorithm [9].
Our Learning plus Inference (L + I) training model learns
individual zi∗ classiﬁers that are unaware of the constraints.
Once we have for each zi∗ the distribution of the values it
can take, we run the inference in Eq. 1, with the constraints,
to assign the labels y.
The joint training model, Inference Based Training
(IBT), incorporates the constraints into the training, by running the inference step in Eq. 1 with each evaluation of the
classiﬁers. Model weights are updated when a mistake is
encountered after the inference procedure. At decision time,
as in L+I, we run the inference in Eq. 1, with the constraints,
to assign the labels y. The details of the algorithms are described in Algorithms 1 and 2, resp.
The constraint penalty weights ρi s in Eq. 1 are learned
independently from the weights wi [4]. Note that some of
the constraints are hard constraints; e.g., those that encode
that zi∗ takes a single value. For most of the constraints, we
set the weights ρi by calculating the corresponding violation
probability in the training data:
ρi = − log P (Ci is violated in the training data)

this paradigm has been used successfully by several authors
[12, 6, 1]. Our empirical results, comparing between these
two paradigms, (see Sec.4) agree with earlier results, and
favor decoupling the training from the inference via L+I.
In both of these learning and inference paradigms we
make use of a regularized version of the Averaged Perceptron algorithm [9], implemented within the Sparse Network of Winnow framework [2]. While classical Perceptron
comes with generalization bound related to the margin of
the data, Averaged Perceptron also comes with a PAC-like
generalization bound [9]. This linear learning algorithm is
known, both theoretically and experimentally, to be among
the best linear learning approaches and is competitive with
SVM and Logistic Regression, while being more efﬁcient in
training. It also has been shown to produce state-of-the-art
results on many natural language applications [12].
Our inference, both at decision time and during training
(when running the IBT algorithm) is implemented as an exhaustive grid search algorithm in the space constructed by
the top k outputs from each individual classiﬁer. This is facilitated by the fact that, despite the large number of values
each of the aspect variables zi∗ can take, only one of these,
zij , can be active in each instance (allowing also for a null
value). It is easy to observe that when wi is the probability
that the ith aspect takes the value j, the solution to the optimization problem is the element in Z that maximizes the
expected number of correct aspects, modulo the constraints.
This is especially important when some of the labels have
not been observed previously, as discussed in Sec. 4.3.2.
The constraints Ci used in our application can also be

(2)

The weight of a constraint that is never violated is set to ∞;
Algorithm 1 L + I: Inference refers to the decision making process
subject to global constraints, by optimizing Eq. 1. This training algorithm
decouples the Learning and Inference. Aspect models zi∗ are learned independent of each other, and constraint penalties are also learned independently from the training data.
1: LEARNING MODELS:
2: for each aspect zi∗ do
3:
zi∗ = learn(T rainingData)
4:
note: * ranges over the values of ith aspect.
5: end for
6: LEARNING CONSTRAINTS:
7: for each Ci do
8:
if Ci is not violated in Training Data then
9:
ρi = ∞
10:
else
11:
ρi = − log P (Ci is violated in Training Data)
12:
end if
13: end for
14: INFERENCE at DECISION TIME:
15: Use the distribution over zi∗ and solve Eq. 1
It has been shown that the IBT paradigm is more expressive and should ultimately perform better given sufﬁciently
many training examples [13]. However, it was also shown
that when each of the individual classiﬁers is good, L+I
performs better than IBT. In addition, training the individual classiﬁers independently requires fewer examples and
3

represented as rules such as “if event X = x , then A =
a” (corresponding to C([X = x’] and ¬[A = a]) = 0). For
comprehensibility, we present examples this way in Table 1.
With the exception of the constraints that are part of the
problem formulation (e.g., ∀i, j, k : C(zij , zik ) = 0), all
the constraints used in our application are learned semiautomatically using the following process. First we generate the Z space by mapping from the given labels; then
we use a simple association rule mining algorithm on the
Z space to ﬁnd interesting constraints. We set minimum
thresholds on support and conﬁdence and learned about 45
constraints over the space of 318 Z tuples.
Constraints
za (f ind) ∧ zt (restaurant) → zd (nearest)
zm (null) ∧ zt (store) → za (f ind)
zt (cd) ∧ za (play) ∧ zm (null) → zmo (normal)

Type
hard
soft
hard

present in the data, the value null is assigned. Examples of
data, labels and corresponding aspects are listed in Table 3.
Each of the ﬁve aspects can take multiple values. Using the
notation in Sec. 3, zij , indicates a variable with the aspect
type set to i ∈ {1, 2, 3, 4, 5} and the value of the ith type
aspect is set to j.

4.3. Evaluation and Results
We evaluate our text categorization scheme by reporting
the results of our scheme compared to the standard MCC
protocol. We evaluate in two settings. In one, all labels are
assumed to be seen in the training data. We therefore randomly select 80% of the examples for each label as training, 10% as development and the rest as test data. In the
second setting, we evaluate the ability of our method to predict labels that are not part of the training data. In this case,
we eliminate examples that correspond to 10% of the labels from the training data. These examples are then presented during testing and we measure the performance of
our model on them.
Note that classiﬁers trained using MCC cannot make any
predictions on these examples; however, since most aspects
of these labels are observed in training, we expect that our
MAMuC model will do reasonably well on them.
We evaluated the system’s performance using two measures. The ﬁrst is the standard accuracy computed as the
percentage of correctly labeled examples. The second metric is motivated by the speciﬁc application, where even a
partially labeled instance is useful; for example, it could
trigger an interaction with the passenger in order to get
a clariﬁcation. This measure, a Weighted Aspect based
Metric (WAM), is a weighted Hamming distance computed
at the aspect level, between the predicted and the correct aspect value; it assigns a weighted per-aspect score to partially
correct predictions. We formally deﬁne it in equation 3.
Formally, let n be the number of test examples, m be
the number of aspects and ωk be the weight assigned to the
k-th aspect. We denote by ai the k-th aspect of the i-th
k
i
example and by gk be the true value of the k-th aspect of
i
this example. Then, 1[ai :gk ] is an indicator variable that
k
i
takes the value 1 when ai equals gk , and 0 otherwise. The
k
WAM measure is then written as follows:

ρi
∞
2.207
∞

Table 1. Examples of soft and hard constraints. za (f ind)
means that the value of Action aspect is ﬁnd.

4. Experiments and Analysis
4.1. Data
Our experimental study is done in the domain of categorizing short text snippets provided by passengers in a car,
into operating commands to devices such as radio, ac, navigation system in the car. The data set consists of 72,483
labeled examples with 318 short navigation commands as
class labels. Table 2 shows several concrete examples from
the data set. The data was collected from the OpenMind
Indoor Common Sense Project [10].
Labels
ﬁnd nearest restaurant

display map

radio seek down

Texts
locate next diner
where is the closest restaurant
show me where I can eat nearby
show map setting
point location
route display
change the radio station
run down through the stations
tune down

Table 2. Concrete examples from the data set. Notice that the

WAM =

label carries similar meaning to the text.

1
n

n

m

m
i
ωk 1[ai :gk ]
k

i=1 k=1

ωk = 1

(3)

k=1

4.2. Aspects Variables

4.3.1 Standard Multiclass Classiﬁcation Setting

We deﬁne a collection of 5 types of aspect variables:
Topic, Action, Manner, Modiﬁer and Detail, each of which
can be thought of as a property of a text snippet and its corresponding label – the car command. If an aspect is not

The results of the setting in which all labels in test have been
observed in training are summarized in Tables 4,5 and 6.
The baseline result in Table 4, achieved with a regularized MCC Averaged Perceptron, is the best result achieved
4

Original Label(318)
climate control defrost off
display current location
ﬁnd nearest american restaurant
volume up
d v d title chapter

Input Text
stop defrosting
show location
american food
turn it up louder
disc chapter title

Topic (98)
climate control
location
restaurant
volume
dvd

Action (27)
defrost
display
ﬁnd
null
null

Manner (40)
off
null
null
up
null

Modifer (49)
null
current
american
null
title

Detail (14)
null
null
nearest
null
chapter

Table 3. Example of input text, their associated gold standard labels and the corresponding values of the ﬁve aspects. There are 318 different
labels, and the values in parenthesis above the aspect columns indicate the number of possible values each aspect can take.

Algorithm
Baseline
Aspects (No Inference)
MAMuC (IBT))
MAMuC (L+I)
Error Reduction (%)

Accuracy (%)
67.84
64.70
61.76
71.18
10.39

WAM (%)
86.14
88.94
84.16
89.65
25.32

Aspects
Topic
Action
Manner
Modiﬁer
Detail

baseline
81.55
82.72
87.35
89.51
89.59

error reduction (%)
24.88
32.35
20.79
15.64
29.68

Table 6. Improvement of individual aspect classiﬁers zi∗ with

Table 4. Comparing several approaches in the standard text cat-

inference over a MCC baseline.

egorization setting. The Baseline is learned via a MCC Averaged
Perceptron [9]. IBT is the joint training model and L+I is the decoupled model. Our best model result (MAMuC with L+I training)
is only marginally lower than human performance.

Rank
Top 1
Top 2
Top 3
Top 4
Top 5

MAMuC
86.14
88.31
89.98
91.15
92.68

structured learning algorithms, such as structured SVM [18]
but these produced even weaker results, probably since the
structure supported there is not expressive enough for our
setting.
Finally, we show results on the top k ranking of the
model prediction; table 5 shows that the model’s accuracy
is more than 88% in its top ﬁve predictions.

MAMuC Accuracy(%)
71.18
80.74
84.08
86.49
88.17

Table 5. Performance of top-k accuracy (k ∈ {1, 2, 3, 4, 5}) of
the MAMuC model.

4.3.2 Predicting Previously Unobserved Labels
The main results in this setting are summarized in Tables 7 and 8. In this experimental setting, sentences in the
test data have labels that were not observed in training. This
explains the baseline 0 performance in Table 7 – MCC algorithms cannot handle this case. However, in MAMuC, we
can predict the aspects of the text snippet provided in test,
and evaluate how much it can (partially) predict the new
label.
Table 7 summarizes the results of the MAMuC model
when we insist that our model predicts all aspects, and then
use these to construct a new label (using the inference procedure).

on this data set, when compared to other common MCC algorithms. The MAMuC result shows a signiﬁcant improvement of about 3.5% in accuracy and WAM, which is more
than 10% (25%) error reduction in accuracy (WAM, resp.).
The improvement achieved by incorporating the inference
step, as indicated in the MAMuC (L+I) row, shows the effectiveness of the global constraints in our model1 . Among
our two training paradigms, L+I is shown to perform much
better than IBT. Given these results we performed an analysis of the individual aspect classiﬁers, shown in Table 6.
Each individual classiﬁer is shown to have a pretty high
baseline accuracy. And, these results are shown to improve
signiﬁcantly when we consider each aspect’s accuracy once
global constraints were taken into account. This analysis
agrees with earlier studies on the tradeoff between L+I and
IBT training paradigms [13], which argues that L+I is likely
to perform better when the individual models are relatively
easy to learn. We note that we have experimented with other

Algorithm
Baseline
Aspect(No Inference)
MAMuC
Error Reduction (%)

Accuracy (%)
0.00
21.39
28.16
28.16

WAM (%)
58.43
69.86
70.27
28.48

Table 7. MAMuC’s ability to predict unobserved labels. Traditional classiﬁcation methods cannot make any prediction on these
examples. However MAMuC can achieve partial success by predicting the intermediate aspect representation.

1 Human

performance on this data set is around 75%. Three annotators
labeled a sample of 100 examples from this data set. The Fleiss Kappa
agreement for this experiment is 0.764, which means substantial agreement
among annotators.

5

In fact, the intermediate aspect representation can be
used in order to support partial label prediction. We do
this by considering the top n most conﬁdently predicted aspects (n ≤ 5). Table 8 shows the accuracy of a partial prediction consisting of the aspect prediction for the n most
conﬁdently predicted aspects. For example, if our model
only makes predictions on its top 2 aspects, its accuracy on
unseen labels is 82.67%. This evaluation shows that our
model is very good at predicting aspects of unobserved labels, even if it cannot “name” the label. This can be used,
for example, in an interactive setting – where partial labels
are reliably proposed by the model, and the new label is
then derived via interaction. In addition, this model can be
used to enrich the label space with new labels.
Partial Prediction
Top 1 Aspect
Top 2 Aspect
Top 3 Aspect
Top 4 Aspect
All 5 Aspect

and Rakesh Gupta for discussions of the problem at the preliminary stages of this research, and for providing us with
the data set. This research was supported partly by a grant
from Honda Research and by MIAS, a DHS-IDS Center for
Multimodal Information Access and Synthesis at UIUC.

References
[1] R. Barzilay and M. Lapata. Aggregation via Set Partitioning
for Natural Language Generation. In NAACL, 2006.
[2] A. Carlson, C. Cumby, J. Rosen, and D. Roth. The SNoW
learning architecture. Technical report, 1999.
[3] M. Chang, L. Ratinov, and D. Roth. Guiding semisupervision with constraint-driven learning. In ACL, 2007.
[4] M. Chang, L. Ratinov, and D. Roth. Constraints as prior
knowledge. In ICML Workshop on Prior Knowledge for Text
and Language Processing, pages 32–39, July 2008.
[5] M. Collins. Discriminative training methods for hidden
Markov models: Theory and experiments with perceptron
algorithms. In EMNLP, 2002.
[6] P. Denis and J. Baldridge. Joint determination of anaphoricity and coreference resolution using integer programming.
In NAACL, 2007.
[7] T. G. Dietterich and G. Bakiri. Solving multiclass learning
problems via error-correcting output codes. Journal of Artiﬁcial Intelligence Research, 2:263–286, 1995.
[8] R. El-Yaniv and N. Etzion-Rosenberg. Hierarchical multiclass decompositions with application to authorship determination. Technical report, 2004.
[9] Y. Freund and R. E. Schapire. Large margin classiﬁcation using the perceptron algorithm. Machine Learning,
37(3):277–296, 1999.
[10] R. Gupta and M. Kochenderfer. Common sense data acquisition for indoor mobile robots. In AAAI, 2004.
[11] J. Lafferty, A. McCallum, and F. Pereira. Conditional random ﬁelds: Probabilistic models for segmenting and labeling sequence data. In ICML, 2001.
[12] V. Punyakanok, D. Roth, and W. Yih. The importance of
syntactic parsing and inference in semantic role labeling.
Computational Linguistics, 34(2), 2008.
[13] V. Punyakanok, D. Roth, W. Yih, and D. Zimak. Learning
and inference over constrained output. In IJCAI, 2005.
[14] D. Roth and W. Yih. A linear programming formulation for
global inference in natural language tasks. In CoNLL, 2004.
[15] D. Roth and W. Yih. Integer linear programming inference
for conditional random ﬁelds. In Proc. of the International
Conference on Machine Learning (ICML), pages 737–744,
2005.
[16] D. Roth and W. Yih. Global inference for entity and relation identiﬁcation via a linear programming formulation. In
L. Getoor and B. Taskar, editors, Introduction to Statistical
Relational Learning. MIT Press, 2007.
[17] K. Toutanova. Competitive generative models with structure
learning for nlp classiﬁcation tasks. In EMNLP, 2006.
[18] I. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun.
Support vector learning for interdependent and structured
output spaces. In ICML, 2004.

Accuracy(%)(no inference)
92.61
82.67
65.17
43.43
21.39

Table 8. Top n partial prediction evaluation for MAMuC in the
case of previously unobserved labels. The accuracy of the partial
prediction is measured before the inference procedure.

5. Conclusion
We propose a new approach to Multiclass Classiﬁcation
of text documents that exploits the structure of the label
space as a way to improve categorization of short text snippets. Our approach works by introducing a set of intermediate aspect variables that capture properties of the text.
The key advantage of this view is that aspects can be
constrained to support the prediction of better labels. Since
values taken by aspect variables are constrained in a natural
way, we predict labels that correspond to an assignment of
values to aspects that satisﬁes all constraints.
Even more signiﬁcantly, the aspects provide a shared
representation for the label space, one that is shared also by
previously unobserved labels. Therefore, once we can predict aspects reliably, we can say something sensible about
aspects of the unobserved label too and, at least, predict
it partially. As we have shown, this yields signiﬁcant improvements on our problem and has great potential to correctly predict unobserved labels which traditional multiclass classiﬁcation methods cannot get at all.

Acknowledgments
The authors would like to thank Ming-Wei Chang,
Michael Connor for insightful discussions of the algorithm
6

