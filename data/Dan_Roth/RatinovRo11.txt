TAC 2011

GLOW TAC-KBP 2011 Entity Linking System∗

Lev Ratinov
Dan Roth
University of Illinois at Urbana-Champaign
{ratinov2/danr}@uiuc.edu

Abstract
Traditional information extraction evaluations, such as the Message Understanding
Conferences (MUC) and Automatic Content
Extraction (ACE), assess the ability to extract
information from individual documents in isolation. In practice, however, we may need to
gather information about a person or organization that is scattered among the documents of
a large collection. The TAC KBP entity linking shared task challenges the participants to
identify real-word entities and to map them to
a knowledgebase of reference entities. This
paper describes the G LOW system which participated in the competition and was based on
our earlier work (Ratinov et al., 2011).

1

Introduction

Traditional information extraction evaluations, such
as the Message Understanding Conferences (MUC)
and Automatic Content Extraction (ACE), assess the
ability to extract information from individual documents in isolation. In practice, however, we may
need to gather information about a person or organization that is scattered among the documents of a
large collection. This requires the ability to identify
∗
This research is supported by the Defense Advanced Research Projects Agency (DARPA) Machine Reading Program
under Air Force Research Laboratory (AFRL) prime contract
no. FA8750-09-C-0181 and by and by the Army Research
Laboratory (ARL) under agreement W911NF-09-2-0053. Any
opinions, ﬁndings, and conclusion or recommendations expressed in this material are those of the author(s) and do not
necessarily reﬂect the view of the DARPA, AFRL, ARL or the
US government.

the relevant documents and to integrate facts, possibly redundant, possibly complementary, possibly
in conﬂict, coming from these documents. Furthermore, we may want to use the extracted information
to augment an existing database. This requires the
ability to link individuals mentioned in a document
and information about these individuals to entries in
a data base.
The Entity Linking task in KBP is formalized
as follows (Ji et al., 2011). The organizers provide a set KB = {E1 , E2 , . . . , E|KB| } of reference entities Ei , which is a subset of Wikipedia1 .
The organizers also provide a list of queries, which
are tripples of the form (Qid , Qf orm , Qtext ), where
Qid is a reference number for the query, Qf orm
is the surface form of the query, and Qtext is the
text within which the surface form appears. Not
all queries can be mapped to KB or to Wikipedia.
The goal of the entity-linking task is to provide the
mapping to KB for those queries which can be
mapped, and (as of 2011) to cluster the rest into
equivalence classes which refer to same real-world
entities. Therefore, the TAC Entity Linking task
is a combination of a task similar to Disambiguation to Wikipedia (Ratinov et al., 2011), (Mihalcea and Csomai, 2007), (Cucerzan, 2007), (Bunescu
and Pasca, 2006), (Milne and Witten, 2008) and a
task similar to cross-document co-reference resolution (Bagga and Baldwin, 1998), (Li et al., 2005).
We note that we focus only on linking the queries
to the TAC KBP knowledgebase. For the queries
which cannot be linked, we provide a trivial solution
1

The TAC knowledge base contains 818,741 reference entities, which is about a third of 2009 Wikipedia pages.

TAC	  QUERY	  	  
*	  ID=2012	  
*	  “Ford”	  
*	  “The	  Ford	  Presiden6al	  
Library	  is	  named	  a=er	  	  
President	  Gerald	  Ford”	  

KBP	  TAC	  
Knowledgebase	  
	  

…	  
Michael	  Jordan	  
(basketball)	  
Michael	  Jackson	  (singer)	  
Gerald	  Ford	  (president)	  
…	  

1)	  Men6on	  
Iden6ﬁca6on	  
2)	  GLOW	  	  
Disambigua6on	  

“The	  [Ford]m1	  Presiden6al	  
Library	  is	  named	  a=er	  	  
President	  [Gerald	  Ford]m2”	  

(m1,	  hHp://en.wikipedia.org/wiki/Ford_Motor_Company,	  	  0.1,	  	  -­‐0.1)	  
(m2,	  hHp://en.wikipedia.org/wiki/President_Gerald_Ford,	  	  0.2,	  	  0.7)	  
QUERY	  MAPPING	  	  
3)	  GLOW	  Output	  	  
Reconcilia6on	  

Gerald	  Ford	  (president)	  

Figure 1: System architecture with an illustration of information ﬂow. The output is provided for illustration purposes
only. In reality, while the baseline model of G LOW makes a disambiguation error on [Ford]m1 , most expressive models
of G LOW link [Ford]m1 to Gerald Ford. Also, in our submitted NEQI mention identiﬁcation implementation, we mark
only a single mention [Gerald Ford]m2 for G LOW as a canonical reference mention of the query. Nevertheless, the
output in the ﬁgure is consistent with various ﬂavors of our submission.

for cross-document co-reference resolution by clustering all the queries with identical surface forms together.
Our idea to was to use G LOW, an off-the-shelf
system we have developed for a related task of Disambiguation to Wikipedia (D2W). The GLOW system takes as input a text document d and a set of
mentions M = {m1 , m2 , . . . , mN } in d, and crosslinks them to Wikipedia, which acts as a knowledge base. This is done through combining local clues (namely lexical overlap and Wikipedia title prevalence) with global coherence of the joint
cross-linking assignment (which is done by analyzing Wikipedia link structure and estimating pairwise article relatedness). The key advantage of
GLOW as reported by (Ratinov et al., 2011) is using different strategies for forming an approximate
solution to the input problem and using it as a semantic disambiguation context for all the mentions.
This allows G LOW to maintain a tractable inference by disambiguating each mention independently
while capturing important global properties. In fact,
G LOW stands for Global and LOcal Wikiﬁcation.
However, there are subtle differences between
D2W and entity linking tasks which prevent
G LOW from being applied directly. More speciﬁcally, in D2W the set of input mentions is tied to
speciﬁc locations in the text, thus potentially the
same surface form may refer to different entities.

For example, a review about a movie Titanic may
use the same surface form “Titanic” to refer both to
the ship and to the movie. In D2W, each mention referring to the ship would be linked to http://en.
wikipedia.org/wiki/RMS_Titanic, while
each mention referring to the movie would be
cross-linked to http://en.wikipedia.org/
wiki/Titanic_(1997_film). This scenario
does not occur in the TAC KBP entity-linking task
where one sense per document holds. On the other
hand, in the entity linking task, the following query
is possible ( QID, “Ford”, “The Ford Presidential
Library is named after President Gerald Ford” ).
While in D2W, the above text would typically contain two mentions: “Ford Presidential Library” and
“Gerald Ford”2 (both of which are easy to disambiguate), in the entity linking task it is necessary
to understand that in both mentions “Ford” refers
to President Gerald Ford. We note that technically, nothing precludes the D2W systems to have
nested mentions such as “[The [Ford] Presidential
Library]”, however most D2W systems are trained
either to mimic Wikipedia link structure or to disambiguate named entities, which leads to poor per2
“President Gerald Ford” is also a possible mention; the
exact mention boundaries depend on annotation guidelines. In
this work we follow the CoNLL 2003 named entity recognition
shared task standards and consistently exclude honoriﬁcs from
the named entities.

formance on most nested mentions.

2

These differences and the choice of using a D2W
component as an inference driver has dictated the
structure of our entity linking system architecture,
which we summarize in Figure 1. The entity-linking
system is composed of the following three steps:
1) Mention Identiﬁcation. Here we identify the
mentions in the query text which correspond to
the query form. We experimented with two approaches. A Simple Query Identiﬁcation (SIQI)
simply marks all the instances of the query form in
the text, while a Named Entity Query Identiﬁcation (NEQI) maps the query form to all the named
entities containing the form. In Figure 1 we show the
output of NEQI. The output of SIQI would be ”The
[Ford] Presidential Library is named after President
Gerald [Ford]”.
2) Disambiguation - this step is a straighforward
application of the GLOW system. We note that the
GLOW system assigns each mention mi a disambiguation Wikipedia title ti along with two conﬁdence scores: ri , the ranker score and li the linker
score. Roughly speaking, the ranker score indicates the conﬁdence that the selected dismbiguation
is more appropriate than the alternatives, while the
linker score is the conﬁdence that the mention can
be mapped to the knowledgebase (in the G LOW case,
Wikipedia).
3) G LOW Output Reconciliation. The NEQI mention matching approach has generated two mentions:
Ford and Gerald Ford and G LOW has mapped them
to different Wikipedia titles. We need to map the
query to a single entry in the KBP knowledgebase.
There are two challenges in this step: to select a single Wikipedia title as disambiguation and to map it
from Wikipedia to the TAC KBP knowledgebase3 .

The goal of this step is to identify expressions in
Qtext which are likely to refer to Qf orm . As we
mentioned eariler, we experimented with two methods: SIQI, a simple mention identiﬁcation based
on exact string matching, and NEQI, a named entity based mention identiﬁcation with approximate
string matching, which we discuss in this section.
The NEQI strategy is very similar to query expansion in many entity linking systems, for example (Chen et al., 2010). The difference is that in
contrast to the traditional query expansion, the reference mentions will be bound to speciﬁc locations
in the text. For example, in our running example
of Figure 1, we mark the reference mentions set
{ Ford[m1 ], Gerald Ford[m2 ]}. We note that we
will disambiguate the mentions jointly, however we
will ultimately allow each reference mention have a
different disambiguation. Therefore, in our system
even two mentions having an identical surface form
could have different disambiguations. We believe
that this architecture is more robust since it allows
us to be more ﬂexible in suggesting reference mentions for the query form, and to recover from potentially erroneous reference mentions. This architecture also allows us to be robust to documents which
do not have a “one sense per document” property.
Below we describe the NEQI method for reference mention recommendation.
1) Annotate Qtext with Illinois NER tagger (Ratinov and Roth, 2009)4 . Let N ERQf orm (Qtext ) be
the set of all named entities in Qtext which could
be matched through approximate string matching
to Qf orm . Approximate string matching we applied was acronym matching (for example, AI would
be matched with Artiﬁcial Intelligence) and simple rules for matching named entities, which allowed matching Mr. Bush to GEORGE W. BUSH.
That is, we have simple rules for discarding professional titles, honoriﬁcs, case-insensitive matching and punctuation-insensitive matching. We note
that in Figure 1 “Ford Presidential Library” would
not be matched against “Ford”.
2) If N ERQf orm (Qtext ) = ∅, let CF be the
longest string in N ERQf orm (Qtext ), let CF =

The rest of the paper is organized as follows. We
focus on mention identiﬁcation in Section 2, on disambiguation which in Section 3, and on G LOW output reconciliation in Section 4. In Section 5 we evaluate our system on the TAC KBP 2011 shared task
data and conclude.

3

The matching may not be straightforward especially if the
TAC KBP was built using a different Wikipedia dump than
G LOW.

Mention Identiﬁcation

4

Available at http://cogcomp.cs.illinois.edu/
page/software

Document text with mentions

m1 = Taiwan

φ(m1, t1)

..............

..............

m2 = China

m3 = Jiangsu Province

φ(m1, t3)
φ(m1, t2)

t1 = Taiwan

t2 = Chinese Taipei

t3 =Republic of China

t4 = China

t5 =People's Republic of China

ψ(t1, t7)

t6 = History of China

ψ(t3, t7)

t7 = Jiangsu

ψ(t5, t7)

Figure 2: Sample disambiguation to Wikipedia with three mentions formalized as bipartite matching problem. The
correct mapping from mentions to titles is marked by heavy edges

Qf orm otherwise. The purpose of this step is to
identify the “canionical form” (CF) for the query
form in the text. For example, in Figure 1, {
Gerald Ford[m2 ] is the canonical form for the query
“Ford”.
3) The G LOW system of (Ratinov et al., 2011)
does not perform an approximate string matching, it cross-links only the expressions which appeared as hyperlinks in Wikipedia. Therefore, the
G LOW would not be able to cross-link the string
LONDON to Wikipedia. In this step, we normalize
the canonical form CN to the normalized canonical form N CF . Following (Mihalcea and Csomai, 2007), we deﬁne linkability of an expression
s as the ratio of Wikipedia pages which contain s
as a hyperlink anchor to the number of Wikipedia
pages which contain s in any form. For example, 1154 Wikipedia pages contain the expression
“Michael Jordan” and out of them 959 (83%) also
contain a link anchored as “Michael Jordan” to the
Wikipedia page corresponding to the correct meaning. In constrast the expression “boarding school”
appeared in 5038 Wikipedia pages, and only 1421
(28%) of them, had a hyperlink anchored with the
surface from. To obtain the canonical normalized
surface form for CF we compare CF against the
list of all titles, redirects and hyperlink anchors in
Wikipedia. We keep only those, which appeared
in at least 10 Wikipedia pages and can be matched
using a case-insensitive, punctuation-insensitive approximate string matching heuristis. Among the set
of matched expressions, we choose the most linkable one.
4) If CF = Qf orm and CF cannot be matched

to neither Wikipedia anchors nor titles nor redirects,
we assume that an NER error has occurred and we
revert to the normalization step with CF = Qf orm .
5) We replace all the instances of CF in Qtext
by N CF , and mark these instances as out ﬁnal set
of reference mentions. The G LOW system will be
applied to this modiﬁed text.

3

Disambiguation

The disambiguation component of our entity linking
system is performed through a straighforward application of the G LOW Wikiﬁcation system of (Ratinov
et al., 2011). In this section we provide a short summary of G LOW. We refer the reader to the original
paper for the full details.
We formalize the task as ﬁnding a many-to-one
matching on a bipartite graph, with mentions forming one partition and Wikipedia titles the other (see
Figure 2). We denote the output matching as an N tuple Γ = (t1 , . . . , tN ) where ti is the output disambiguation for mention mi . With this formulation
in mind, we can write down an objective function:
The common approach is to utilize the Wikipedia
link graph to obtain an estimate pairwise relatedness
between titles ψ(ti , tj ) and to efﬁciently generate a
disambiguation context Γ , a rough approximation
to the optimal Γ∗ . We then solve the easier problem:
N

Γ∗ ≈ arg max
Γ

ψ(ti , tj )] (1)

[φ(mi , ti ) +
i=1

tj ∈Γ

Where {mi }N is the set of mentions, {ti }N is a
i=1
i=1
set of associated Wikipedia pages, φ is a local scoring function which assigns higher scores to titles

Algorithm: Disambiguate to Wikipedia
Input: document d, Mentions M = {m1 , . . . , mN }
Output: a disambiguation Γ = (t1 , . . . , tN ).
1) Let M = M ∪ { Other potential mentions in d}
2) For each mention mi ∈ M , construct a set of disambiguation candidates Ti =
{ti , . . . , ti i }, ti = null
1
j
k
3) Ranker: Find a solution Γ = (t1 , . . . , t|M | ), where ti ∈ Ti is the best non-null disambiguation of mi .
4) Linker: For each mi , assign a linker score li indicating whether mapping mi to ti improves
the objective function 1 as opposed to mapping mi to N U LL
5) Return Γ as a set of tuples: {mi , ti , ri , li }N , where ti is the title assigned to the mention
i=1
mi , ri is ranker score indicating how conﬁdent the ranker is in recommending ti , and li is the
linker score, indicating how likely the mention mi is to have a corresponding page in Wikipedia.

Figure 3: High-level pseudocode for G LOW.

Men$on	   GLOW	  Disambigua$on	  

Ranker	  Score	   Linker	  Score	  

m1

Gerald_Ford

0.1

0.1

m2

Ford_Motor_Company

0.5

-0.1

m3

Ford_Motor_Company

0.3

0.3

m4

Gerald_Ford

0.4

0.1

Reconciled	  Disambigua$on	   End	  Ranker	  
Gerald_Ford

0

Ford_Motor_Company

MaxNoLinker

0.5

Reconciled	  Disambigua$on	   End	  Ranker	  
Gerald_Ford

0.4

Ford_Motor_Company

MaxWithLinker

0

Reconciled	  Disambigua$on	   End	  Ranker	  
Gerald_Ford

0.5

Ford_Motor_Company

SumNoLinker

0.8

Reconciled	  Disambigua$on	   End	  Ranker	  
Gerald_Ford

0.5

Ford_Motor_Company

SumWithLinker

0.3

Figure 4: G LOW output reconciliation strategies.

with content similar to that of the input document, ψ
is a coherence function which assigns higher scores
to related titles in Wikipedia and Γ , a rough approximation to the optimal solution. We can solve the
equation 1 efﬁciently by ﬁnding each ti and then
mapping mi independently as in a local approach,
but still enforces some degree of coherence among
the disambiguations using Γ and ψ.
The pseudocode for the original G LOW system
is given in Figure 3. We note that the input document d and the mention set M = {m1 , . . . , mN }
which G LOW expects as input are the normalized
text of Qtext and the set of reference mentions provided by the algorithm described in Section 2. We
note that while we mark a speciﬁc set of mentions M

for G LOW to link to Wikipedia, G LOW will identify and disambiguate other expressions in the input
text as well, and use them as disambiguation context
for disambiguating M .

4

Output Reconciliation

The given an input of a document d and a set of
mentions M = {m1 , . . . , mN } the G LOW system
assigns each mention mi a Wikipedia title ti , and
two conﬁdence scores (ri , li ), where ri is the ranking conﬁdence that ti is the most appropriate disambiguation among the disambiguation candidates
proposed for mi . li is the linker score indicating
whether the objective function 1 would improve if
we map mi to N U LL instead of ti . A positive score
indicates that ti is preferred over N U LL and a negative score indicates otherwise.
Given a knowledge base KB
=
{E1 , E2 , . . . , E|KB| }, a query (Qid , Qf orm , Qtext )
and a set of tuples {(mi , ti , ri , li )}1≤i≤N returned
by G LOW , our goal is to assign a KB entry E ∗ to the
query or N U LL if no such entry can be matched.
Our ﬁrst step is selecting a single Wikipedia
page t∗ out of the set {(mi , ti , ri , li )} (possibly
N U LL). We have explored four strategies, which
are summarized in ﬁgure 4, and described formally
below.
1. MaxNoLinker : Let
i∗ = argmax{ri }

(2)

1≤i≤N

Then t∗ = ti∗ . The idea is very simple: just
select a title ti∗ which was assigned the maximum ranker conﬁdence.

Mention Identiﬁcation
Policy
SIQI
NEQI

MaxNoLinker Performance
Micro-Average B 3 Precision B 3 Recall
0.752
0.709
0.740
0.787
0.757
0.765

B 3 F1
0.724
0.761

Table 1: The utility of mention selection. The Naive mention generation strategy is marking all the mention in query
text which match exactly the query surface form. The method for mention generation proposed in Section 2 improves
the micro-average performance by 3 points and the B 3 F1 by 4 points. We note that these results were obtained
using the G LOW model trained on our internal newswire dataset rather than on the TAC data and with MaxNoLinker
solution aggregation strategy.

5

2. MaxWithLinker : Let
i∗ = argmax {ri }

(3)

1≤i≤N ∧li ≥0

Then t∗ = ti∗ . This strategy is identical to
MaxNoLinker, but we consider only the titles
which were “linked” by G LOW. If G LOW assigned a negative linker score li to the mention
mi , we discard ti from the list of possible results.
3. SumNoLinker : Let
i∗ = argmax

rj

(4)

1≤i≤N t =t
j
i

Then t∗ = ti∗ . This strategy is similar to
MaxNoLinker, except we summarize the ranker
scores for all the mentions mapped to the same
title.
4. SumWithLinker : Let
i∗ = argmax

rj

(5)

1≤i≤N ∧li ≥0 t =t ∧l ≥0
j
i j

Then t∗ = ti∗ . This strategy is identical to
SumNoLinker, but we consider only the titles
which were “linked” by G LOW. We also summarize only over “linked” mentions.
Once we mapped the query to a Wikipedia title
t∗ , our next step is to map t∗ to an entry E ∗ in the
KBP TAC knowledge base. Since G LOW and KBP
can use different versions of Wikipedia, we used
a very recent February 2011 version of Wikipedia
redirects. Therefore, a Wikipedia title t matches the
KBP TAC entry E if both redirect to the same page
in the February 2011 version of Wikipedia.

Experiments and Results

It is important to note that the ranking and the linking components of G LOW are SVM models which
have to be trained. In the results reported in (Ratinov et al., 2011), we trained the G LOW system
on Wikipedia articles themselves, training the system to mimic the Wikipedia annotation scheme.
For the TAC 2011 entity linking task, we have
also trained a G LOW model on the three publicly
available newswire Wikiﬁcation datasets described
in (Ratinov et al., 2011) as well as a collection on
97 blogs which we Wikiﬁed using the Mechanical
Turk, but have not published yet. The annotation
style wad consistent to the Wikiﬁcation works such
as (Cucerzan, 2007) and (Milne and Witten, 2008),
it was not a TAC entity linking style annotation.
Overall, our ”newswire” training dataset contained
200 documents. Since we never train on the TAC
2011 data, throughout this section, we directly report our results on the TAC 2011 evaluation dataset.
Utility of Mention Identiﬁcation
In Table 1 we compare the performance of our submitted system with the SIQI and the NEQI mention
identiﬁcation policies. We note that our submitted
results to TAC were obtained using a G LOW model
trained on our internal newswire dataset rather than
on the TAC data and with MaxNoLinker solution
aggregation strategy. The baseline is to mark all
the mentions in the query text which exactly match
the query form, we call this policy Naive mention
generation. We compared the performance of the
Naive strategy and the strategy discussed in Section 2, which as the table shows improves the microaverage performance by 3 points and the B 3 F1 by
4 points.
Utility of Solution Aggregation Strategies
In Section 4 we have mentioned that given a knowl-

Reconciliation
Policy
MaxNoLinker
MaxWithLinker
SumNoLinker
SumWithLinker

Micro-Average
0.787
0.788
0.794
0.788

NEQI Performance
B 3 Precision B 3 Recall
0.757
0.765
0.757
0.765
0.763
0.773
0.757
0.766

B 3 F1
0.761
0.761
0.768
0.762

Table 2: Comparison of the solution generation policies. We note that these results were obtained using the
G LOW model trained on our internal newswire dataset rather than on the TAC data. All approaches are competitive.

edge base KB = {E1 , E2 , . . . , E|KB| }, a query
(Qid , Qf orm , Qtext ) we use G LOW to generate a
set of tuples {(mi , ti , ri , li )}1≤i≤N . However, our
end goal is to assign a KB single entry E ∗ to the
query, and we have suggested four approaches for
generating a single solution, namely: MaxNoLinker,
MaxWithLinker, SumNoLinker, SumWithLinker. In
Table 2 we compare these approaches and conclude
that all approaches are competitive.
Ablation Feature Study The G LOW system has
several groups of features: baseline, lexical naive,
lexical re-weighted, and coherence5 . In (Ratinov et
al., 2011) we ran an ablation study on the Wikiﬁcation task, assessing the strengths and the weaknesses of each feature group. We concluded that
the baseline features provide a very strong baseline.
Lexical features lead to state-of-the-art performance,
and while adding coherence features allow to further
marginally improve the performance, the key difﬁculty was identifying when a mention refers to outof-Wikipedia entity In other words, the linker scores
are not very reliable. One of our goals of participating in the TAC KBP entity linking competition
was to see whether these statements hold true for the
TAC KBP entity linking task. In the following set
of experiments, we have used the models obtained
in (Ratinov et al., 2011) for different feature groups.
All of the models were trained on around 10K paragraphs from Wikipedia articles. In Table 3 we compare the performance of the different G LOW models using different sets of features. We note that
in all the experiments, we used our mention selection strategy from Section 2 and the SumNoLinker
single-solution generation strategy.
5

(Ratinov et al., 2011) has compared multiple approaches
to capture coherence. In this work, we only report the bestperforming approach: when disambiguating mention m, use
baseline predictions for other mentions as a semantic context.

We make several observations. First, both the
lexical features and the coherence features have improved the performance considerably over the baseline. Second, surprisingly, both the lexical and the
coherence features performed extremely competitively to one another, and combining them did not
lead to further improvement. Surprisingly, the naive
lexical features performed almost as well as the reweighted lexical features, which in (Ratinov et al.,
2011) performed signiﬁcantly better. Finally, the
best conﬁguration of models trained on the 10K
paragraphs from Wikipedia articles achieved macroaverage of 0.786 and B 3 F1 of 0.759, while the best
conﬁguration trained on 200 newswire documents
achieved achieved macro-average of 0.794 and B 3
F1 of 0.768. Which means that a system trained
on a smaller amount of newswire data and blogs
marginally outperformed a system trained on a large
amount of Wikipedia data. We hypothesize that the
majority of test document contain enough context to
easily disambiguate the mentions, as long as meaningful mentions have been identiﬁed and the correct
disambiguation appears in the disambiguation candidate list.

6

Conclusions

We have presented an approach for using the
G LOW system for the TAC KBP entity linking challenge. Our approach was based on detecting mentions matching the query form in the query text, disambiguating them to Wikipedia using G LOW and
then forming a single Wikipedia title t corresponding to the query. Finally, we have matched the assigned Wikipedia title to the KBP knowledge base
using a February 2011 set of Wikipedia redirects.
We noticed that although the G LOW system did not
use the TAC KBP entity linking data for training or

Features Used
Baseline
Naive
Re-weighted
All Lexical
Coherence
All features

NEQI SumNoLinker Performance
Micro-Average B 3 Precision B 3 Recall
0.747
0.710
0.731
Baseline+Lexical
0.784
0.749
0.764
0.786
0.753
0.766
0.786
0.752
0.766
Baseline+Global
0.780
0.749
0.760
Baseline+Local+Global
0.783
0.754
0.759

B 3 F1
0.720
0.756
0.759
0.759
0.754
0.756

Table 3: Ablation study of models from (Ratinov et al., 2011).

tuning, it achieved a surprisingly good performance.
We noticed that matching the query form against potential mentions in the query text has a major impact on the end performance, allowing us to improve
by 4 points B 3 F1 over the baseline. All reasonable strategies for reconciling potentially conﬂicting disambiguations for the identiﬁed mention set,
such as MaxNoLinker, MaxWithLinker, SumNoLinker, SumWithLinker led to similar performance.
G LOW has several feature groups. All of them performed similarly, and surprisingly a combination of
multiple lexical features or lexical and coherence
features together did not lead to an improvement
over a single feature group. We were also surprised
to discover that the G LOW system trained on 200
newswire documents outperformed the same system
when trained on 10K articles from Wikipedia. Overall, the selection of the training set did not have
much impact, and most of the performance gains
were made through our approach for detecting mentions matching the query form in the query text and
a single (either lexical or coherence) feature group.
We hypothesize that the majority of test document
contain enough context to easily disambiguate the
mentions, as long as the correct mentions have been
identiﬁed (correct being indeed matching the query
form) and the correct disambiguation appears in the
disambiguation candidate list.

References
Amit Bagga and Breck Baldwin. 1998. Entity-based
cross-document coreferencing using the vector space
model. In ACL.
R. C. Bunescu and M. Pasca. 2006. Using encyclope-

dic knowledge for named entity disambiguation. In
EACL.
Z. Chen, S. Tamang, A. Lee, X. Li, W. Lin, J. Artiles,
M. Snover, M. Passantino, and H. Ji. 2010. Cunyblender tac-kbp2010 entity linking and slot ﬁlling system description. In Text Analytics Conference.
S. Cucerzan. 2007. Large-scale named entity disambiguation based on Wikipedia data. In EMNLPCoNLL.
H. Ji, R. Grishman, and H. T. Dang. 2011. An overview
of the tac2011 knowledge base population track. In
TAC.
X. Li, P. Morie, and D. Roth. 2005. Semantic integration
in text: From ambiguous names to identiﬁable entities.
AI Magazine. Special Issue on Semantic Integration.
R. Mihalcea and A. Csomai. 2007. Wikify!: linking documents to encyclopedic knowledge. In CIKM.
D. Milne and I. H. Witten. 2008. Learning to link with
wikipedia. In CIKM.
L. Ratinov and D. Roth. 2009. Design challenges
and misconceptions in named entity recognition. In
CoNLL.
L. Ratinov, D. Downey, M. Anderson, and D. Roth.
2011. Local and global algorithms for disambiguation
to wikipedia. In ACL.

