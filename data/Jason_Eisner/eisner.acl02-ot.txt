Appeared in Proceedings of the 40th Annual Meeting of the Association
for Computational Linguistics (ACL 2002), pp. 56-63, 2002.

Comprehension and Compilation in Optimality Theory∗
Jason Eisner
Department of Computer Science
Johns Hopkins University
Baltimore, MD, USA 21218-2691
jason@cs.jhu.edu
Abstract
This paper ties up some loose ends in ﬁnite-state Optimality
Theory. First, it discusses how to perform comprehension under Optimality Theory grammars consisting of ﬁnite-state constraints. Comprehension has not been much studied in OT; we
show that unlike production, it does not always yield a regular
set, making ﬁnite-state methods inapplicable. However, after
giving a suitably ﬂexible presentation of OT, we show carefully how to treat comprehension under recent variants of OT
in which grammars can be compiled into ﬁnite-state transducers. We then unify these variants, showing that compilation is
possible if all components of the grammar are regular relations,
including the harmony ordering on scored candidates. A side
beneﬁt of our construction is a far simpler implementation of
directional OT (Eisner, 2000).

1

Introduction

To produce language is to convert utterances from
their underlying (“deep”) form to a surface form.
Optimality Theory or OT (Prince and Smolensky,
1993) proposes to describe phonological production
as an optimization process. For an underlying x,
a speaker purportedly chooses the surface form z
so as to maximize the harmony of the pair (x, z).
Broadly speaking, (x, z) is harmonic if z is “easy”
to pronounce and “similar” to x. But the precise harmony measure depends on the language; according
to OT, it can be speciﬁed by a grammar of ranked
desiderata known as constraints.
According to OT, then, production maps each underlying form to its best possible surface pronunciation. It is akin to the function that maps each child x
to his or her most ﬂattering outﬁt z. Different children look best in different clothes, and for an oddly
shaped child x, even the best conceivable outﬁt z
may be an awkward compromise between style and
ﬁt—that is, between ease of pronunciation and similarity to x.
Language comprehension is production in reverse. In OT, it maps each outﬁt z to the set of chil∗

Thanks to Kie Zuraw for asking about comprehension; to
Ron Kaplan for demanding an algebraic construction before he
believed directional OT was ﬁnite-state; and to others whose
questions convinced me that this paper deserved to be written.

dren x for whom that outﬁt is optimal, i.e., is at least
as ﬂattering as any other outﬁt z ′ :
PRODUCE(x)
COMPREHEND(z)

= {z : (∄z ′ ) (x, z ′ ) > (x, z)}
= {x : z ∈

PRODUCE(x)}

′

= {x : (∄z ) (x, z ′ ) > (x, z)}
In general z and z ′ may range over inﬁnitely many
possible pronunciations. While the formulas above
are almost identical, comprehension is in a sense
more complex because it varies both the underlying
and surface forms. While PRODUCE(x) considers
all pairs (x, z ′ ), COMPREHEND(z) must for each x
consider all pairs (x, z ′ ). Of course, this nested definition does not preclude computational shortcuts.
This paper has three modest goals:
1. To show that OT comprehension does in fact
present a computational problem that production
does not. Even when the OT grammar is required to
be ﬁnite-state, so that production can be performed
with ﬁnite-state techniques, comprehension cannot
in general be performed with ﬁnite-state techniques.
2. To consider recent constructions that cut through
this problem (Frank and Satta, 1998; Karttunen,
1998; Eisner, 2000; Gerdemann and van Noord,
2000). By altering or approximating the OT
formalism—that is, by hook or by crook—these constructions manage to compile OT grammars into
ﬁnite-state transducers. Transducers may readily be
inverted to do comprehension as easily as production. We carefully lay out how to use them for comprehension in realistic circumstances (in the presence of correspondence theory, lexical constraints,
hearer uncertainty, and phonetic postprocessing).
3. To give a uniﬁed treatment in the extended ﬁnitestate calculus of the constructions referenced above.
This clariﬁes their meaning and makes them easy to
implement. For example, we obtain a transparent algebraic version of Eisner’s (2000) unbearably technical automaton construction for his proposed formalism of “directional OT.”

The treatment shows that all the constructions
emerge directly from a generalized presentation of
OT, in which the crucial fact is that the harmony ordering on scored candidates is a regular relation.

2

Previous Work on Comprehension

Work focusing on OT comprehension—or even
mentioning it—has been surprisingly sparse. While
the recent constructions mentioned in §1 can easily
be applied to the comprehension problem, as we will
explain, they were motivated primarily by a desire to
pare back OT’s generative power to that of previous
rewrite-rule formalisms (Johnson, 1972).
Fosler (1996) noted the existence of the OT comprehension task and speculated that it might succumb to heuristic search. Smolensky (1996) proposed to solve it by optimizing the underlying form,
COMPREHEND(z)

?

= {x : (∄x′ ) (x′ , z) > (x, z)}

Hale and Reiss (1998) pointed out in response that
any comprehension-by-optimization strategy would
have to arrange for multiple optima: after all, phonological comprehension is a one-to-many mapping
(since phonological production is many-to-one).1
The correctness of Smolensky’s proposal (i.e.,
whether it really computes COMPREHEND) depends
on the particular harmony measure. It can be made
to work, multiple optima and all, if the harmony
measure is constructed with both production and
comprehension in mind. Indeed, for any phonology,
it is trivial to design a harmony measure that both
production and comprehension optimize. (Just deﬁne the harmony of (x, z) to be 1 or 0 according
to whether the mapping x → z is in the language!)
But we are really only interested in harmony measures that are deﬁned by OT-style grammars (rankings of “simple” constraints). In this case Smolensky’s proposal can be unworkable. In particular, §4
will show that a ﬁnite-state production grammar in
classical OT need not be invertible by any ﬁnite-state
comprehension grammar.
1
Hale & Reiss’s criticism may be speciﬁc to phonology
and syntax. For some phenomena in semantics, pragmatics,
and even morphology, Blutner (1999) argues for a one-to-one
form-meaning mapping in which marked forms express marked
meanings. He deliberately uses bidirectional optimization to
rule out many-to-one cases: roughly speaking, an (x, z) pair is
grammatical for him only if z is optimal given x and vice-versa.

3

A General Presentation of OT

This section (graphically summarized in Fig. 1) lays
out a generalized version of OT’s theory of production, introducing some notational and representational conventions that may be useful to others and
will be important below. In particular, all objects
are represented as strings, or as functions that map
strings to strings. This will enable us to use ﬁnitestate techniques later.
The underlying form x and surface form z are
represented as strings. We often refer to these strings
as input and output. Following Eisner (1997), each
candidate (x, z) is also represented as a string y.
The notation (x, z) that we have been using so far
for candidates is actually misleading, since in fact
the candidates y that are compared encode more than
just x and z. They also encode a particular alignment
or correspondence between x and z. For example,
if x = abdip and z = a[di][bu], then a typical
candidate would be encoded
y = aab0[ddii][pb0u]
which speciﬁes that a corresponds to a, b was
deleted (has no surface correspondent), voiceless p
surfaces as voiced b, etc. The harmony of y might
depend on this alignment as well as on x and z (just
as an outﬁt might ﬁt worse when worn backwards).
Because we are distinguishing underlying and
surface material by using disjoint alphabets Σ =
{a, b, . . .} and ∆ = {[, ], a, b, . . .},2 it is easy to
extract the underlying and surface forms (x and z)
from y.
Although the above example assumes that x and
z are simple strings of phonemes and brackets, nothing herein depends on that assumption. Autosegmental representations too can be encoded as strings
(Eisner, 1997).
In general, an OT grammar consists of 4 components: a constraint ranking, a harmony ordering,
and generating and pronouncing functions. The constraint ranking is the language-speciﬁc part of the
grammar; the other components are often supposed
to be universal across languages.
The generating function G EN maps any x ∈ Σ∗
to the (nonempty) set of candidates y whose underlying form is x. In other words, G EN just inserts
2
An alternative would be to distinguish them by odd and
even positions in the string.

x

G EN

C

C

C

P RON

n
1
2
−→ Y0 (x) −→ Y1 (x) −→ Y2 (x) · · · −→ Yn (x) −→ Z(x)

underlying form x∈Σ∗

sets of candidates y∈(Σ∪∆)∗

set of surface forms z∈∆∗

prune
Ci
Ci
delete ⋆
¯
¯
where Yi−1 (x) −→ Yi (x) really means Yi−1 (x) −→ Yi (x) −→ optimal subset of Yi (x) −→ Yi (x)
y ∈(Σ∪∆∪{⋆})∗
¯

y∈(Σ∪∆)∗

y∈(Σ∪∆)∗

Figure 1: This paper’s view of OT production. In the second line, Ci inserts ⋆’s into candidates; then the candidates with suboptimal
starrings are pruned away, and ﬁnally the ⋆’s are removed from the survivors.

arbitrary substrings from ∆∗ amongst the characters of x, subject to any restrictions on what constitutes a legitimate candidate y.3 (Legitimacy might
for instance demand that y’s surface material z have
matched, non-nested left and right brackets, or even
that z be similar to x in terms of edit distance.)
A constraint ranking is simply a sequence
C1 , C2 , . . . Cn of constraints. Let us take each
Ci to be a function that scores candidates y by
annotating them with violation marks ⋆. For example, a N O D ELETE constraint would map y =
aab0c0[ddii][pb0u] to y =N O D ELETE(y) =
¯
aab⋆0c⋆0[ddii][pb0u], inserting a ⋆ after each
underlying phoneme that does not correspond to any
surface phoneme. This unconventional formulation
is needed for new approaches that care about the exact location of the ⋆’s. In traditional OT only the
number of ⋆’s is important, although the locations
are sometimes shown for readability.
Finally, OT requires a harmony ordering ≻
on scored candidates y ∈ (Σ ∪ ∆ ∪ {⋆})∗ . In
¯
traditional OT, y is most harmonic when it con¯
tains the fewest ⋆’s. For example, among candidates scored by N O D ELETE, the most harmonic
ones are the ones with the fewest deletions; many
candidates may tie for this honor. §6 considers
other harmony orderings, a possibility recognized
by Prince and Smolensky (1993) (≻ corresponds to
their H -E VAL). In general ≻ may be a partial order: two competing candidates may be equally harmonic or incomparable (in which case both can
survive), and candidates with different underlying
forms never compete at all.
Production under such a grammar is a matter of
successive ﬁltering by the constraints C1 , . . . Cn .
Given an underlying form x, let
Y0 (x) = G EN(x)

(1)

3
It is never really necessary for G EN to enforce such restrictions, since they can equally well be enforced by the top-ranked
constraint C1 (see below).

Yi (x) = {y ∈ Yi−1 (x) :
′

(2)
′

(∄y ∈ Yi−1 (x)) Ci (y ) ≻ Ci (y)}
The set of optimal candidates is now Yn (x). Extracting z from each y ∈ Yn (x) gives the set Z(x)
or P RODUCE(x) of acceptable surface forms:
Z(x) = {P RON(y) : y ∈ Yn (x)} ⊆ ∆∗

(3)

P RON denotes the simple pronunciation function
that extracts z from y. It is the counterpart to G EN:
just as G EN ﬂeshes out x ∈ Σ∗ into y by inserting
symbols of ∆, P RON slims y down to z ∈ ∆∗ by
removing symbols of Σ.
Notice that Yn ⊆ Yn−1 ⊆ . . . ⊆ Y0 . The only
candidates y ∈ Yi−1 that survive ﬁltering by Ci are
the ones that Ci considers most harmonic.
The above notation is general enough to handle
some of the important variations of OT, such as
Paradigm Uniformity and Sympathy Theory. In particular, one can deﬁne G EN so that each candidate
y encodes not just an alignment between x and z,
but an alignment among x, z, and some other strings
that are neither underlying nor surface. These other
strings may represent the surface forms for other
members of the same morphological paradigm, or
intermediate throwaway candidates to which z is
sympathetic. Production still optimizes y, which
means that it simultaneously optimizes z and the
other strings.

4

Comprehension in Finite-State OT

This section assumes OT’s traditional harmony ordering, in which the candidates that survive ﬁltering
by Ci are the ones into which Ci inserts fewest ⋆’s.
Much computational work on OT has been conducted within a ﬁnite-state framework (Ellison,
1994), in keeping with a tradition of ﬁnite-state
phonology (Johnson, 1972; Kaplan and Kay, 1994).4
4

The tradition already included (inviolable) phonological

Finite-state OT is a restriction of the formalism discussed above. It speciﬁcally assumes that
G EN, C1 , . . . Cn , and P RON are all regular relations,
meaning that they can be described by ﬁnite-state
transducers. G EN is a nondeterministic transducer
that maps each x to multiple candidates y. The other
transducers map each y to a single y or z.
¯
These ﬁnite-state assumptions were proposed
(in a different and slightly weaker form) by
Ellison (1994). Their empirical adequacy has been
defended by Eisner (1997).
In addition to having the right kind of power linguistically, regular relations are closed under various relevant operations and allow (efﬁcient) parallel
processing of regular sets of strings. Ellison (1994)
exploited such properties to give a production algorithm for ﬁnite-state OT. Given x and a ﬁnite-state
OT grammar, he used ﬁnite-state operations to construct the set Yn (x) of optimal candidates, represented as a ﬁnite-state automaton.
Ellison’s construction demonstrates that Yn is always a regular set. Since P RON is regular, it follows
that PRODUCE(x) = Z(x) is also a regular set.
We now show that COMPREHEND(z), in constrast, need not be a regular set. Let Σ = {a, b},
∆ = {[, ], a, b, . . .} and suppose that G EN allows
candidates like the ones in §3, in which parts of the
string may be bracketed between [ and ]. The crucial grammar consists of two ﬁnite-state constraints.
C2 penalizes a’s that fall between brackets (by inserting ⋆ next to each one) and also penalizes b’s
that fall outside of brackets. It is dominated by C1 ,
which penalizes brackets that do not fall at either
edge of the string. Note that this grammar is completely permissive as to the number and location of
surface characters other than brackets.
If x contains more a’s than b’s, then PRODUCE(x)
ˆ
is the set ∆∗ of all unbracketed surface forms, where
ˆ
∆ is ∆ minus the bracket symbols. If x contains
ˆ
fewer a’s than b’s, then PRODUCE(x) = [∆∗ ].
And if a’s and b’s appear equally often in x, then
PRODUCE(x) is the union of the two sets.
Thus, while the x-to-z mapping is not a regular
relation under this grammar, at least PRODUCE(x)
is a regular set for each x—just as ﬁnite-state OT
constraints, notably Koskenniemi’s (1983) two-level model,
which like OT used ﬁnite-state constraints on candidates y that
encoded an alignment between underlying x and surface z.

ˆ
guarantees. But for any unbracketed z ∈ ∆∗ , such
as z = abc, COMPREHEND(z) is not regular: it is
the set of underlying strings with # of a’s ≥ # of b’s.
This result seems to eliminate any hope of handling OT comprehension in a ﬁnite-state framework. It is interesting to note that both OT and
current speech recognition systems construct ﬁnitestate models of production and deﬁne comprehension as the inverse of production. Speech recognizers do correctly implement comprehension via
ﬁnite-state optimization (Pereira and Riley, 1997).
But this is impossible in OT because OT has a more
complicated production model. (In speech recognizers, the most probable phonetic or phonological
surface form is not presumed to have suppressed its
competitors.)
One might try to salvage the situation by barring
constraints like C1 or C2 from the theory as linguistically implausible. Unfortunately this is unlikely
to succeed. Primitive OT (Eisner, 1997) already restricts OT to something like a bare minimum of constraints, allowing just two simple constraint families
that are widely used by practitioners of OT. Yet even
these primitive constraints retain enough power to
simulate any ﬁnite-state constraint. In any case, C1
and C2 themselves are fairly similar to “domain”
constraints used to describe tone systems (Cole and
Kisseberth, 1994). While C2 is somewhat odd in
that it penalizes two distinct conﬁgurations at once,
one would obtain the same effect by combining three
separately plausible constraints: C2 requires a’s between brackets (i.e., in a tone domain) to receive surface high tones, C3 requires b’s outside brackets to
receive surface high tones, and C4 penalizes all surface high tones.5
Another obvious if unsatisfying hack would impose heuristic limits on the length of x, for example by allowing the comprehension system to return
the approximation COMPREHEND(z) ∩ {x : |x| ≤
2 · |z|}. This set is ﬁnite and hence regular, so per5
Since the surface tones indicate the total number of a’s and
b’s in the underlying form, COMPREHEND(z) is actually a ﬁnite
set in this version, hence regular. But the non-regularity argument does go through if the tonal information in z is not available to the comprehension system (as when reading text without diacritics); we cover this case in §5. (One can assume that
some lower-ranked constraints require a special sufﬁx before ],
so that the bracket information need not be directly available to
the comprehension system either.)

haps it can be produced by some ﬁnite-state method,
although the automaton to describe the set might be
large in some cases.
Recent efforts to force OT into a fully ﬁnite-state
mold are more promising. As we will see, they identify the problem as the harmony ordering ≻, rather
than the space of constraints or the potential inﬁnitude of the answer set.

5

Regular-Relation Comprehension

Since COMPREHEND(z) need not be a regular set
in traditional OT, a corollary is that COMPREHEND
and its inverse PRODUCE are not regular relations.
That much was previously shown by Markus Hiller
and Paul Smolensky (Frank and Satta, 1998), using
similar examples.
However, at least some OT grammars ought to describe regular relations. It has long been hypothesized that all human phonologies are regular relations, at least if one omits reduplication, and this is
necessarily true of phonologies that were successfully described with pre-OT formalisms (Johnson,
1972; Koskenniemi, 1983).
Regular relations are important for us because
they are computationally tractable. Any regular relation can be implemented as a ﬁnite-state transducer
T , which can be inverted and used for comprehension as well as production. PRODUCE(x) = T (x) =
range(x ◦ T ), and COMPREHEND(z) = T −1 (z) =
domain(T ◦ z).
We are therefore interested in compiling OT
grammars into ﬁnite-state transducers—by hook or
by crook. §6 discusses how; but ﬁrst let us see how
such compilation is useful in realistic situations.
Any practical comprehension strategy must recognize that the hearer does not really perceive the
entire surface form. After all, the surface form contains phonetically invisible material (e.g., syllable
and foot boundaries) and makes phonetically imperceptible distinctions (e.g., two copies of a tone versus one doubly linked copy). How to comprehend in
this case?
The solution is to modify P RON to “go all the
way”—to delete not only underlying material but
also phonetically invisible material. Indeed, P RON
can also be made to perform any purely phonetic
processing. Each output z of PRODUCE is now not a

phonological surface form but a string of phonemes
or spectrogram segments. So long as P RON is a regular relation (perhaps a nondeterministic or probabilistic one that takes phonetic variation into account), we will still be able to construct T and use it
for production and comprehension as above. 6
How about the lexicon? When the phonology can
be represented as a transducer, COMPREHEND(z) is
a regular set. It contains all inputs x that could have
produced output z. In practice, many of these inputs are not in the lexicon, nor are they possible
novel words. One should restrict to inputs that appear in the lexicon (also a regular set) by intersecting
COMPREHEND(z) with the lexicon. For novel words
this intersection will be empty; but one can ﬁnd the
possible underlying forms of the novel word, for
learning’s sake, by intersecting COMPREHEND(z)
with a larger (inﬁnite) regular set representing all
forms satisfying the language’s lexical constraints.
There is an alternative treatment of the lexicon.
G EN can be extended “backwards” to incorporate
morphology just as P RON was extended “forwards”
to incorporate phonetics. On this view, the input
x is a sequence of abstract morphemes, and G EN
performs morphological preprocessing to turn x into
possible candidates y. G EN looks up each abstract
morpheme’s phonological string ∈ Σ∗ from the lexicon,7 then combines these phonological strings by
concatenation or template merger, then nondeterministically inserts surface material from ∆∗ . Such
a G EN can plausibly be built up (by composition)
as a regular relation from abstract morpheme sequences to phonological candidates. This regularity,
as for P RON, is all that is required.
Representing a phonology as a transducer T has
additional virtues. T can be applied efﬁciently
to any input string x, whereas Ellison (1994) or
Eisner (1997) requires a fresh automaton construction for each x. A nice trick is to build T without
6
Pereira and Riley (1997) build a speech recognizer by composing a probabilistic ﬁnite-state language model, a ﬁnite-state
pronouncing dictionary, and a probabilistic ﬁnite-state acoustic
model. These three components correspond precisely to the input to G EN, the traditional OT grammar, and P RON, so we are
simply suggesting the same thing in different terminology.
7
Nondeterministically in the case of phonologically conditioned allomorphs: INDEFINITE APPLE → {Λæpl, ænæpl} ⊆
Σ∗ . This yields competing candidates that differ even in their
underlying phonological material.

P RON and apply it to all conceivable x’s in parallel, yielding the complete set of all optimal candidates Yn (Σ∗ ) = x∈Σ∗ Yn (x). If Y and Y ′ denote
the sets of optimal candidates under two grammars,
then (Y ∩ ¬Y ′ ) ∪ (Y ′ ∩ ¬Y ) yields the candidates
that are optimal under only one grammar. Applying
G EN−1 or P RON to this set ﬁnds the regular set of
underlying or surface forms that the two grammars
would treat differently; one can then look for empirical cases in this set, in order to distinguish between
the two grammars.

6

Theorem on Compiling OT

Why are OT phonologies not always regular relations? The trouble is that inputs may be arbitrarily long, and so may accrue arbitrarily large
numbers of violations. Traditional OT (§4) is
supposed to distinguish all such numbers. Consider syllabiﬁcation in English, which prefers
to syllabify the long input bi bambam . . . bam
k copies

as [bi][bam][bam] . . . [bam] (with k codas)
rather than [bib][am][bam] . . . [bam] (with
k + 1 codas). N O C ODA must therefore distinguish
annotated candidates y with k ⋆’s (which are opti¯
mal) from those with k + 1 ⋆’s (which are not). It
requires a (≥ k + 2)-state automaton to make this
distinction by looking only at the ⋆’s in y . And if k
¯
can be arbitrarily large, then no ﬁnite-state automaton will handle all cases.
Thus, constraints like N O C ODA do not allow an
upper bound on k for all x ∈ Σ∗ . Of course, the minimal number of violations k of a constraint is ﬁxed
given the underlying form x, which is useful in production.8 But comprehension is less fortunate: we
cannot bound k given only the surface form z. In
the grammar of §4, COMPREHEND(abc) included
underlying forms whose optimal candidates had arbitrarily large numbers of violations k.
Now, in most cases, the effect of an OT grammar can be achieved without actually counting anything. (This is to be expected since rewrite-rule
8
Ellison (1994) was able to construct P RODUCE(x) from x.
One can even build a transducer for P RODUCE that is correct on
all inputs that can achieve ≤ K violations and returns ∅ on other
inputs (signalling that the transducer needs to be recompiled
with increased K). Simply use the construction of (Frank and
Satta, 1998; Karttunen, 1998), composed with a hard constraint
that the answer must have ≤ K violations.

grammars were previously written for the same
phonologies, and they did not use counting!) This
is possible despite the above arguments because
for some grammars, the distinction between optimal and suboptimal y can be made by looking at
¯
the non-⋆ symbols in y rather than trying to count
¯
the ⋆’s. In our N O C ODA example, a surface substring such as . . . ib⋆][a. . . might signal that y is
¯
suboptimal because it contains an “unnecessary”
coda. Of course, the validity of this conclusion
depends on the grammar and speciﬁcally the constraints C1 , . . . Ci−1 ranked above N O C ODA, since
whether that coda is really unnecessary depends on
¯
whether Yi−1 also contains the competing candidate
. . . i][ba . . . with fewer codas.
But as we have seen, some OT grammars do have
effects that overstep the ﬁnite-state boundary (§4).
Recent efforts to treat OT with transducers have
therefore tried to remove counting from the formalism. We now unify such efforts by showing that they
all modify the harmony ordering ≻.
§4 described ﬁnite-state OT grammars as ones
where G EN, P RON, and the constraints are regular
relations. We claim that if the harmony ordering ≻
is also a regular relation on strings of (Σ∪∆∪{⋆})∗ ,
then the entire grammar (PRODUCE) is also regular.
We require harmony orderings to be compatible
with G EN: an ordering must treat y ′ , y as incompa¯ ¯
rable (neither is ≻ the other) if they were produced
from different underlying forms. 9
To make the notation readable let us denote the ≻
relation by the letter H. Thus, a transducer for H
accepts the pair (¯′ , y ) if and only if y ′ ≻ y .
y ¯
¯
¯
The construction is inductive. Y0 = G EN is regular by assumption. If Yi−1 is regular, then so is Yi
since (as we will show)
¯
¯
Yi = (Yi ◦ ¬range(Yi ◦ H)) ◦ D
(4)
def

¯
where Yi = Yi−1 ◦ Ci and maps x to the set of
starred candidates that Ci will prune; ¬ denotes the
complement of a regular language; and D is a transducer that removes all ⋆’s. Therefore PRODUCE =
Yn ◦ P RON is regular as claimed.
9

For example, the harmony ordering of traditional OT is
{(¯′ , y ) : y ′ has the same underlying form as, but contains
y ¯
¯
fewer ⋆’s than, y }. If we were allowed to drop the same¯
underlying-form condition then the ordering would become regular, and then our claim would falsely imply that all traditional
ﬁnite-state OT grammars were regular relations.

We now summarize the main proposals from the
It remains to derive (4). Equation (2) implies
′
′
¯
Ci (Yi (x)) = {¯ ∈ Yi (x) : (∄¯ ∈ Yi (x)) y ≻ y } (5) literature (see §1), propose operator names, and cast
y ¯
y
¯
¯
them in the general framework.
′
′
¯
¯
= Yi (x) − {¯ : (∃¯ ∈ Yi (x)) y ≻ y } (6)
y
y
¯
¯
¯
¯
= Yi (x) − H(Yi (x)) (7) • Y o C: Inviolable constraint (Koskenniemi,
¯i (x)) as “starred candidates that 1983; Bird, 1995), implemented by composition.
One can read H(Y
are worse than other starred candidates,” i.e., subop- • Y o+ C: Counting constraint (Prince and
timal. The set difference (7) leaves only the optimal Smolensky, 1993): more violations is more disharcandidates. We now see
monic. No ﬁnite-state implementation possible.
(x, y ) ∈ Yi ◦ Ci ⇔ y ∈ Ci (Yi (x))
¯
¯
(8) • Y oo C: Binary approximation (Karttunen,
¯
⇔ y ∈ Yi (x), y ∈ H(Yi (x))
¯ ¯
¯
[by (7)]
(9) 1998; Frank and Satta, 1998). All candidates with
¯
⇔ y ∈ Yi (x), (∄z)¯ ∈ H(Yi (z)) [see below](10) any violations are equally disharmonic. Imple¯ ¯
y
∗
∗ +
¯ ¯
¯
⇔ (x, y ) ∈ Yi , y ∈ range(Yi ◦ H)
¯
(11) mented by G = (Σ (ǫ : ⋆)Σ ) , which relates underlying forms without violations to the same forms
¯
¯
⇔ (x, y ) ∈ Yi ◦ ¬range(Yi ◦ H)
¯
(12)
with violations.
¯
¯
therefore Yi ◦ Ci = Yi ◦ ¬range(Yi ◦ H)
(13) • Y oo C: 3-bounded approximation (Karttunen,
3
and composing both sides with D yields (4). To jus- 1998; Frank and Satta, 1998). Like o+ , but all
tify (9) ⇔ (10) we must show when y ∈ Yi (x) that candidates with ≥ 3 violations are equally dishar¯ ¯
¯
¯
y ∈ H(Yi (x)) ⇔ (∃z)¯ ∈ H(Yi (z)). For the ⇒ monic. G is most easily described with a transducer
¯
y
¯
direction, just take z = x. For ⇐, y ∈ H(Yi (z)) that keeps count of the input and output ⋆’s so far, on
¯
′ ∈ Y (z))¯′ ≻ y ; but then x = z
¯i
means that (∃¯
y
y
¯
a scale of 0, 1, 2, ≥ 3. Final states are those whose
¯i (x))), since if not, our compatibil- output count exceeds their input count on this scale.
(giving y ∈ H(Y
¯
¯
ity requirement on H would have made y ′ ∈ Yi (z) • Y o⊂ C: Matching or subset approximation
¯
incomparable with y ∈ Yi (x).
¯ ¯
(Gerdemann and van Noord, 2000). A candidate is
Extending the pretty notation of (Karttunen, more disharmonic than another if it has stars in all
1998), we may use (4) to deﬁne a left-associative the same locations and some more besides.11 Here
generalized optimality operator ooH :
G = ((Σ|⋆)∗ (ǫ : ⋆)(Σ|⋆)∗ )+ .
def
Y ooH C = (Y ◦C ◦¬range(Y ◦C ◦H))◦D (14) • Y o> C: Left-to-right directional evaluation (Eisner, 2000). A candidate is more disharmonic than
Then for any regular OT grammar, PRODUCE =
another if in the leftmost position where they differ
G EN ooH C1 ooH C2 · · · ooH Cn ◦ P RON
(ignoring surface characters), it has a ⋆. This revises
OT’s “do only when necessary” mantra to “do only
and can be inverted to get COMPREHEND. More
generally, different constraints can usefully be ap- when necessary and then as late as possible” (even
if delaying ⋆’s means suffering more of them later).
plied with different H’s (Eisner, 2000).
Here G = (Σ|⋆)∗ (ǫ : ⋆)+ (Σ(⋆∗ × ⋆∗ ))∗ . Unlike
The algebraic construction above is inspired by a
version that Gerdemann and van Noord (2000) give the other proposals, here two forms can both be opfor a particular variant of OT. Their regular expres- timal only if they have exactly the same pattern of
sions can be used to implement it, simply replacing violations with respect to their underlying material.
• Y <o C: Right-to-left directional evaluation.
their add_violation by our H.
Typically, H ignores surface characters when “Do only when necessary and then as early as possicomparing starred candidates. So H can be written ble.” Here G is the reverse of the G used in o> .
as elim(∆) ◦ G ◦ elim(∆)−1 where elim(∆) is a
transducer that removes all characters of ∆. To satisfy the compatibility requirement on H, G should
be a subset of the relation (Σ| ⋆ |(ǫ : ⋆)|(⋆ : ǫ))∗ .10
10

This transducer regexp says to map any symbol in Σ ∪ {⋆}
to itself, or insert or delete ⋆—and then repeat.

The novelty of the matching and directional proposals is their attention to where the violations fall.
Eisner’s directional proposal (o>, <o) is the only
11
Many candidates are incomparable under this ordering, so
Gerdemann and van Noord also showed how to weaken the notation of “same location” in order to approximate o+ better.

(a) x =bantodibo
(b)
N O C ODA
[ban][to][di][bo]
ban⋆todibo
[ban][ton][di][bo]
ban⋆to⋆dibo
[ban][to][dim][bon]
ban⋆todi⋆bo⋆
[ban][ton][dim][bon]
ban⋆to⋆di⋆bo⋆

(c)

C1 N O C ODA
*!



*
**
***!
***!*

C1

σ1

*!

(d)

*
*



σ2

σ4

*

*

*

*

*!

*
*

σ3

*!

Figure 2: Counting vs. directionality. [Adapted from (Eisner, 2000).] C1 is some high-ranked constraint that kills the most faithful
candidate; N O C ODA dislikes syllable codas. (a) Surface material of the candidates. (b) Scored candidates for G to compare.
Surface characters but not ⋆’s have been removed by elim(∆). (c) In traditional evaluation o+ , G counts the ⋆’s. (d) Directional
evaluation o> gets a different result, as if N O C ODA were split into 4 constraints evaluating the syllables separately. More
accurately, it is as if N O C ODA were split into one constraint per underlying letter, counting the number of ⋆’s right after that letter.

one defended on linguistic as well as computational
grounds. He argues that violation counting (o+) is
a bug in OT rather than a feature worth approximating, since it predicts unattested phenomena such as
“majority assimilation” (Bakovi´ , 1999; Lombardi,
c
1999). Conversely, he argues that comparing violations directionally is not a hack but a desirable feature, since it naturally predicts “iterative phenomena” whose description in traditional OT (via Generalized Alignment) is awkward from both a linguistic
and a computational point of view. Fig. 2 contrasts
the traditional and directional harmony orderings.
Eisner (2000) also proved that o> was a regular operator for directional H, by making use of a
rather different insight, but that machine-level construction was highly technical. The new algebraic
construction is simple and can be implemented with
a few regular expressions, as for any other H.

7

Conclusion

See the itemized points in §1 for a detailed summary.
In general, this paper has laid out a clear, general
framework for ﬁnite-state OT systems, and used it to
obtain positive and negative results about the understudied problem of comprehension. Perhaps these
results will have some bearing on the development
of realistic learning algorithms.
The paper has also established sufﬁcient conditions for a ﬁnite-state OT grammar to compile into a
ﬁnite-state transducer. It should be easy to imagine
new variants of OT that meet these conditions.

References
Eric Bakovi´ . 1999. Assimilation to the unmarked. Rutc
gers Optimality Archive ROA-340., August.
Steven Bird. 1995. Computational Phonology: A
Constraint-Based Approach. Cambridge.

Reinhard Blutner. 1999. Some aspects of optimality in
natural language interpretation. In Papers on Optimality Theoretic Semantics. Utrecht.
J. Cole and C. Kisseberth. 1994. An optimal domains
theory of harmony. Studies in the Linguistic Sciences,
24(2).
Jason Eisner. 1997. Efﬁcient generation in primitive Optimality Theory. In Proc. of ACL/EACL.
Jason Eisner. 2000. Directional constraint evaluation in
Optimality Theory. In Proc. of COLING.
T. Mark Ellison. 1994. Phonological derivation in Optimality Theory. In Proc. of COLING
J. Eric Fosler. 1996. On reversing the generation process
in Optimality Theory. Proc. of ACL Student Session.
R. Frank and G. Satta. 1998. Optimality Theory and the
generative complexity of constraint violability. Computational Linguistics, 24(2):307–315.
D. Gerdemann and G. van Noord. 2000. Approximation and exactness in ﬁnite-state Optimality Theory. In
Proc. of ACL SIGPHON Workshop.
Mark Hale and Charles Reiss. 1998. Formal and empirical arguments concerning phonological acquisition.
Linguistic Inquiry, 29:656–683.
C. Douglas Johnson. 1972. Formal Aspects of Phonological Description. Mouton.
R. Kaplan and M. Kay. 1994. Regular models of phonological rule systems. Comp. Ling., 20(3).
L. Karttunen. 1998. The proper treatment of optimality
in computational phonology. In Proc. of FSMNLP.
Kimmo Koskenniemi. 1983. Two-level morphology: A
general computational model for word-form recognition and production. Publication 11, Dept. of General
Linguistics, University of Helsinki.
Linda Lombardi. 1999. Positional faithfulness and voicing assimilation in Optimality Theory. Natural Language and Linguistic Theory, 17:267–302.
Fernando C. N. Pereira and Michael Riley. 1997. Speech
recognition by composition of weighted ﬁnite automata. In E. Roche and Y. Schabes, eds., Finite-State
Language Processing. MIT Press.
A. Prince and P. Smolensky. 1993. Optimality Theory:
Constraint interaction in generative grammar. Ms.,
Rutgers and U. of Colorado (Boulder).
Paul Smolensky.
1996.
On the comprehension/production dilemma in child language. Linguistic
Inquiry, 27:720–731.

