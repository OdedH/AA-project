5th Workshop on Tree-Adjoining Grammars and Related Formalisms (TAG+5), Paris, 25–27 May 2000

A Faster Parsing Algorithm
for Lexicalized Tree-Adjoining Grammars
and Giorgio Satta

¡

 

Jason Eisner

¢

Dept. of Computer Science
University of Rochester
P.O. Box 270226
Rochester, NY 14627-0226 USA
jason@cs.rochester.edu
Dip. di Elettronica e Informatica
Universit` di Padova
a
via Gradenigo 6/A
I-35131 Padova, Italy
satta@dei.unipd.it

£
Abstract

This paper points out some computational inefﬁciencies of standard TAG parsing algorithms
when applied to LTAGs. We propose a novel algorithm with an asymptotic improvement, from
to
, where is the input length and
are grammar constants
that are independent of vocabulary size.

'
!

§

  ' ¥ $ "  ¥
210)(&§%#!¨§¦¤

 © ¥
¨§¦¤

Introduction
Lexicalized Tree-Adjoining Grammars (LTAGs) were ﬁrst introduced in (Schabes et al., 1988)
as a variant of Tree-Adjoining Grammars (TAGs) (Joshi, 1987). In LTAGs each elementary tree
is specialized for some individual lexical item. Following the original proposal, LTAGs have
been used in several state-of-the-art, real-world parsers; see for instance (Abeill´ & Candito,
e
2000) and (Doran et al., 2000).
Like link grammar (Sleator & Temperley, 1991) and lexicalized formalisms from the statistical
parsing literature (Collins, 1997; Charniak, 1997; Alshawi, 1996; Eisner, 1996) LTAGs provide
two main recognized advantages over more standard non-lexicalized formalisms:

3

subcategorization can be speciﬁed separately for each word; and
each word can restrict the anchors (head words) of its arguments and adjuncts, encoding
lexical preferences as well as some effects of semantics and world knowledge.

3

To give a simple example, consider the verb walk, which is usually intransitive but can take an
object in some restricted cases. An LTAG can easily specify the acceptability of sentence Mary
walks the dog by associating walk with a transitive elementary tree that selects for an indirect
object tree anchored at word dog (and some other words within a limited range).

J. Eisner, G. Satta
LTAGs are large because they include separate trees for each word in the vocabulary. However,
parsing need consider only those trees of the grammar that are associated with the lexical symbols in the input string. While this strategy reduces parsing time in all practical cases, since the
input string length tends to be considerably smaller than the grammar size, it also introduces
an additional factor in the runtime that depends on the input string length. This problem was
recognized early in (Schabes et al., 1988), but a precise computational analysis has not been
provided in the literature, up to the authors’ knowledge. See (Eisner, 1997; Eisner, 2000) for
related discussion on different lexicalized formalisms.
In this work we reconsider LTAG parsing in the above perspective. Under standard assumptions,
we show that existing LTAG parsing methods perform with
worst case running time,
where and are smallish constants depending on the grammar and is the input string. As our
main result we present an
time algorithm, a considerable improvement
over the standard LTAG parsing methods.

¡
 © ¢  ¡    ¦¤
¥
 



 §¦¢  ¡ ! ¥%#  £  ¡  ¨¥ ¤
  ' ¤$"  



1. The problem
We assume the reader is familiar with TAGs, LTAGs and related notions (Joshi, 1987; Joshi &
Schabes, 1992). Each node in an elementary tree is associated with a selectional constraint
representing a (possibly empty) set of elementary trees that can be adjoined at (we
treat substitution as adjunction at a childless node). We deﬁne the size of as
.
The size of an LTAG , written
, is the sum of the sizes of all nodes in the elementary trees
of .

 © 
  &§¥   ¨ 
§

§

§

   

 &§¥ ¨
©



(
 ( I V
I
E I C A
QP' D' B@
E'
 
£  ¡ (TAS(R
C
EC A
FD' B@
"7

¡
%
% 98#65¡ ¢ ¡  ¡ 21£0)' &
  ( & ( % (
¡
$ #"!¡
  ¢  ¡   ¨¥ ¤  ¡ '  ¥ 
 



 ¡' ¥ 
 

  ¢  ¡  ¡ '  ¥ ¥ ¤
 
¡

  £¡      ¦¤
    ¥



Standard parsing algorithms for TAGs have running time
, where
is the input
grammar and is the input string. As already mentioned in the introduction, LTAGs allow
more selective parsing strategies, resulting in
running time, for some function
that is independent of the size of the vocabulary treated by (hence typically much
less than
). In order to give an estimate of the factor
, let us deﬁne as the maximum
number of nodes in an elementary tree (of ), and as the maximum number of elementary
trees that are anchored in a common lexical item. We argue below that
is
.

 
 ¡' ¥ 

We need to introduce some additional notation. We write
to denote the substring of from
position to position ,
. (Position is the boundary between the -th and
the
-th symbols of .) We write
for
. In the grammar, assume some arbitrary
ordering for the elementary trees with a given anchor and for the nodes of each elementary tree,
denotes the -th elementary tree anchored
with the root node always being the ﬁrst. Then
at ,
denotes its root node, and
denotes its -th node (for
,
,
).

 
 43% ¥
%

(
 ( C U
C A G
D' B@ H¡

By “tree” we now mean an elementary or derived tree that may contain a foot-node. The most
time-expensive step in TAG and LTAG tabular parsing is the recognition of adjunction at nodes
dominating a foot-node. Say that we have constructed a subtree that is rooted at the node
, which may be an internal node of some elementary tree, and covers substrings
and
. Say also that we have constructed a complete tree rooted at
, covering suband
. In a tabular method these two analyses can be represented, respectively,
strings
by the items
and
.1 In items, the subscript on a node
indicates that no further adjunction is allowed to take place there (i.e., adjunction has already

Y #"X¡



E  ' b D' 8B@
C bA

a

ytx& ' & ' % ' rb% ' PE  ' b D' vBp@i
b
w C bA

t 's
u& W' qr' % ' EQP' D' Bp@i
I C A
d "$
h$ hg¡

"fedc¡
$ H¡
"`
E I C A
QW' D' B@
1

Top-down tabular algorithms, and those that enforce the valid-preﬁx property, might use more indices in item
representations, in addition to those shown in our example. In some cases this may damage the asymptotic runtime.

A Faster LTAG Parsing Algorithm

E I C A
QW' D' B@

occurred or has been explicitly declined). Adjunction of at the node
is then carried
out as illustrated by the following abstract inference rule (see for instance (Vijay-Shanker &
Joshi, 1985; Vijay-Shanker & Weir, 1993)):

a

 © £  ¡ ) ¨¥ ¤
  
  $   #¢  ¡

"  

  !  

¥

  ¦  £  ¡ '      ¥    ¦¤
 
 ¤ 


  
 ¢  ¡ ) ¨¥ ¤  b¡ '  ¥ 
E I C A
QP' D' B@
E A
FC ' @
I
C C
¦ "
©¨ §¦` ¡ Y P¥¡ 
G
H¡
"¤
& £b &  9% ¢c¡¥%G ¥ d $ hg¡ f"edH¡
¡
b ¡ "$

   s ¡  
d Gc¡
b & £  ¡ & Wfqs '¥ q ' % ' b % ¥ 
'
 ¡' ¥ 
E I C A
QW' D' B@
ytx&
b
's
W' rq' rb% ' PQW' D' BAp@i
wE I C
dh$ `H¡ Y p"ed¡
"
§tx& s %b ' w E
b
A
Bp@i
tI'
E I C ©
QP' D' BA@ ¥ ¨   E b D' b BA@ bt & ' & ' % ' %b ' PE  ' ' P' D'qb ' A (PQPWD'Cs ' q' % ' QW!D' BA i@
C
w b C i@ u& '
E I' C

Item
represents a new partial analysis spanning
adjunction is possible at node
in this analysis.

and

(1)

; no further

In order to bound
, let us ﬁx positions
and . Then step (1) can be executed
a number of times bounded by
. This is because
can
freely range within
or
,
can freely range within
or
, since the anchor
of
tree
might not be dominated by node
; also, , and can assume any values
within their respective ranges. We therefore conclude that
.
,
the terminal
Note that a better upper bound would be given by
alphabet (vocabulary) of , since each anchor can assume no more than
different values.
However, in practical applications we have
, and therefore in this paper we will
always use the former bound. We then conclude that standard LTAG parsing algorithms run
with a worst case time of
.

2. A novel algorithm
This section improves upon the time upper bound reported in 1. The result is achieved by
splitting step (1) into three substeps. (A similar method may be applied to speed up parsing of
lexicalized context-free grammars (Eisner & Satta, 1999).)

%

We start by observing that at step (1) we simultaneously carry out two tests on the trees under
analysis:
is found in the selectional constraint

$ H¡ Y #"H¡
"`
E I' C ©
QW!D' BA@ ¥ ¨

d "$
h$ hg¡ p"ed5¡

E b D' b @
C A

we check that the tree yield
,
“wraps around” the tree yield
the two copies of match and likewise .

&

3

we check that the tree

,

; and

, i.e., that

%

3

To some extent, the two computations above can be carried out independently of each other.
More precisely, the result of the check on the selectional constraint does not depend on the
value of positions and . Furthermore, once the check has been carried out, we can do away
with the anchor position , since this information is not used by the wrapping test or mentioned
in the result of step (1).

b¥A
s

q

In order to implement the above idea, we deﬁne two new kinds of items, which we write as
and
. Item
packages together all items of
the form
. Similarly, item
packages together all items
of the form
such that
. We can then replace
step (1) with the following three steps:
(2)

(3)

t x& %
t
A
C Bp@i i
t s '' E % ' IQI D C A
u& ' W'q ' QW' W' ' D' Bp@i
E
bt & %

EQIW't bt C & ' A& @ ¥ % ¨   E b C ' b BABA@ i @i
' & ' u' & %b ' PE  % ' b D' b BAp@i A i@
 © %b
t ' w EC I
t' u& ' ' % ' ' QWD'wI Q' W!p@DiC i '
t E ' E DI ' B
t t x& ' & ' % ' %b ' PQW!D''(C ' B'@&A ' i i ' QW' t uC & ' ' % ' QW' C ' @ i i
b
t E I A
C A
wE I'

t bt
E I' C ©
QW!D' BA@ ¥ ¨   E b C ' b BA@ ytb & ' & ' % ' rb% '

& ' & ' % ' %b ' PQP' D' Bp@i i
wE I C A
w C A t E I C A
PE  ' b D' b Bp@i t u& ' % ' QP' D' Bp@i i

J. Eisner, G. Satta

ytx& rb%
b
A
 i@
t 's
u& W' rq' % ' QW' DW'sC' ' BrqA p@' i ' t Pbt w Q' W&I ' ' %C '' %b (PQW' D' Bp@i i
&E
E I
' wE I C A

(4)

A computational analysis similar to the one carried out in 1 shows the following overall time
costs: step (2) takes time
, step (3) takes time
and step (4) takes time
. Thus the overall time cost for all the above steps is
.


"
§¦¢  ¡ '! ¤¥$%#   ¢  ¡  ¨¥ ¤
 
  £  ¡ )  ¦¤%
   ¥

 ¢  ¡   ¦¤
  ¥
 

 ¢  ¡   ¦¤
  ¥
¡

All the remaining steps in standard LTAG tabular parsing algorithms that have not been considered here can easily be accommodated within the indicated upper bound. Thus, steps (2) to (4)
can be integrated into a standard LTAG parser, providing a new parsing algorithm for LTAG
with worst case running time
.

 §¦¢  ¡ ! ¥%"  £  ¡  ¨¥ ¤
  ' ¤$  

3. Discussion
We have discussed standard LTAG, in which every elementary tree has exactly one lexical anchor. Multiply anchored trees can be handled straightforwardly and without additional cost: for
the analysis, simply consider one anchor to be primary when deﬁning the grammar constant
and when naming the tree
. The parse table should be seeded with all of a tree’s anchor
nodes if and only if all those anchor words appear in the input in the correct order. (Recall
that it was always possible to construct subtrees over substrings that do not include the primary
anchor.)



E D' B@
C A

¡

Our inference rules enforce the traditional prohibition against multiple adjunctions at the same
node (Vijay-Shanker & Joshi, 1985). This prohibition has been questioned on linguistic grounds
(Schabes & Shieber, 1994), since for example a verb may need to select lexically for each of its
multiple PP adjuncts. To relax the prohibition it is sufﬁcient to drop the symbol throughout
the rules.



 
¢  ¡

 

Our algorithm is an asymptotic improvement for any values of , , and
. However, we really
have in mind grammars where is a smallish constant, much smaller than the vocabulary size.
In particular, we do not expect a word to anchor multiple elementary trees that have the same
labeled internal structure as one another, differing only in their selectional constraints. Thus,
the selectional constraints at each node in an elementary tree only depend on the tree’s head and
the internal structure of the tree itself. Grammars satisfying this requirement have been called
node-dependent or SLG(2) in (Carroll & Weir, 1997), and bilexical in (Eisner, 1997; Eisner &
Satta, 1999; Eisner, 2000). If we drop the above assumption, the grammar can capture lexical
relations of arity larger than two. For instance, in an LTAG which is not bilexical, a verb
could anchor many instances of the basic transitive-sentence elementary tree, in each of which
the selectional constraint at the object node required a speciﬁc object tree (with a speciﬁc head).
In this case, the selectional constraint at each
tree’s subject node would depend on both
and its required object, thus establishing a relation between three lexical elements. Moreover,
an upstairs verb
could select for certain of these
trees and thereby restrict both
and
the head of ’s object, again establishing a relation between three lexical elements. This style
of grammar can dramatically increase as a function of the vocabulary size. To overcome this
one would again have to substitute some factor that depends on the input string length.

7



7

¢

7

¢

7

7

¢

¢

¤

¢







¢

7

¢

Even in the bilexical grammars we expect, where is unrelated to vocabulary size, can still
be somewhat large in broad-coverage grammars such as those cited in the introduction, which
include large tree families for each word. The literature describes some further tricks for efﬁciency in this case. Similar trees in the same family may be made to share structure (Evans

A Faster LTAG Parsing Algorithm
& Weir, 1997; Carroll et al., 1998). “Supertagging” techniques (Srinivas & Joshi, 1999; Chen
et al., 1999) use contextual probabilities to eliminate some elementary trees heuristically before
parsing begins. Alternatively, under a stochastic LTAG (Resnik, 1992; Schabes, 1992), one may
prune away unpromising items, such as those with low inside probability. It should be possible
to combine any of these tricks with our technique.

References
´
A BEILL E A. & C ANDITO M.-H. (2000). FTAG: A lexicalized tree-adjoining grammar for french. In
´
A. A BEILL E & O. R AMBOW, Eds., Tree-Adjoining Grammar. Stanford, CA: CSLI Lecture Notes. To
appear.
A LSHAWI H. (1996). Head automata and bilingual tiling: Translation with minimal representations. In
Proc. of the 34 ACL, p. 167–176, Santa Cruz, CA.
¡
¢ 

C ARROLL J., N ICOLOV N., S HAUMYAN O., S METS M. & W EIR D. (1998). Grammar compaction and
computation sharing in automaton-based parsing. In Proceedings of the 1 Workshop on Tabulation in
Parsing and Deduction (TAPD’98), p. 16–25, Paris, France.
  £

C ARROLL J. & W EIR D. (1997). Encoding frequency information in lexicalized grammars. In Proceedings of the 5 Int. Workshop on Parsing Technologies, MIT, Cambridge, MA.
¡
¢ 

C HARNIAK E. (1997). Statistical parsing with a context-free grammar and word statistics. In Proc. of
AAAI-97, Menlo Park, CA.
C HEN J., S RINIVAS B. & V IJAY-S HANKER K. (1999). New models for improving supertag disambiguation. In Proc. of the 9 EACL, p. 188–195, Bergen, Norway.
¡
¤ 

¡
¢ 

C OLLINS M. (1997). Three generative, lexicalised models for statistical parsing. In Proc. of the 35
ACL, Madrid, Spain.

D ORAN C., H OCKEY B., S ARKAR A., S RINIVAS B. & X IA F. (2000). Evolution of the XTAG system.
´
In A. A BEILL E & O. R AMBOW, Eds., Tree-Adjoining Grammar. Stanford, CA: CSLI Lecture Notes.
To appear.
E ISNER J. (1996). Three new probabilistic models for dependency parsing: An exploration. In Proc. of
the 16 COLING, p. 340–345, Copenhagen, Denmark.
¡
¢ 

¡
¢ 

E ISNER J. (1997). Bilexical grammars and a cubic-time probabilistic parser. In Proceedings of the 5
Int. Workshop on Parsing Technologies, MIT, Cambridge, MA.

E ISNER J. (2000). Bilexical grammars and a cubic-time probabilistic parser. In H. C. B UNT & A.
N IJHOLT, Eds., New Developments in Natural Language Parsing. Kluwer.
E ISNER J. & S ATTA G. (1999). Efﬁcient parsing for bilexical context-free grammars and head automaton grammars. In Proc. of the 37 ACL, p. 457–464, College Park, Maryland.
¡
¢ 

E VANS R. & W EIR D. (1997). Automata-based parsing for lexicalized grammars. In Proceedings of
the 5 Int. Workshop on Parsing Technologies, MIT, Cambridge, MA.
¡
¢ 

J OSHI A. K. (1987). An introduction to tree adjoining grammars. In A. M ANASTER -R AMER, Ed.,
Mathematics of Language. Amsterdam: John Benjamins.
J OSHI A. K. & S CHABES Y. (1992). Tree adjoining grammars and lexicalized grammars. In M. N IVAT
& A. P ODELSKY, Eds., Tree Automata and Languages. Amsterdam, The Netherlands: Elsevier.
R ESNIK P. (1992). Probabilistic tree-adjoining grammar as a framework for statistical natural language
processing. In Proc. of the 14 COLING, p. 418–424, Nantes, France.
¡
¢ 

¡
¢ 

S CHABES Y. (1992). Stochastic lexicalized tree-adjoining grammars. In Proc. of the 14 COLING, p.
426–432, Nantes, France.

J. Eisner, G. Satta
´
S CHABES Y., A BEILL E A. & J OSHI A. K. (1988). Parsing strategies with ’lexicalized’ grammars:
Application to tree adjoining grammars. In Proc. of the 12 COLING, p. 578–583, Budapest, Hungary.
¡
¢ 

S CHABES Y. & S HIEBER S. M. (1994). An alternative conception of tree-adjoining derivation. Computational Linguistics, 20 (1), p. 91–118.
S LEATOR D. & T EMPERLEY D. (1991). Parsing English with a Link Grammar. Computer Science
Technical Report CMU-CS-91-196, Carnegie Mellon University.
S RINIVAS B. & J OSHI A. (1999). Supertagging: An approach to almost parsing. Computational Linguistics, 20 (3), p. 331–378.
V IJAY-S HANKER K. & J OSHI A. K. (1985). Some computational properties of tree adjoining grammars.
In
Meeting of the Association for Computational Linguistics, p. 82–93, Chicago, Illinois.
¥£ ¡
¦¤¢ 

V IJAY-S HANKER K. & W EIR D. J. (1993). Parsing some constrained grammar formalisms. Computational Linguistics, 19 (4), p. 591–636.

