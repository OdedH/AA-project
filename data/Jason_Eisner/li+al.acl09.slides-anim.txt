Variational Decoding for Statistical
Machine Translation
Zhifei Li, Jason Eisner, and Sanjeev Khudanpur
Center for Language and Speech Processing
Computer Science Department
Johns Hopkins University

1
Monday, August 17, 2009

Spurious Ambiguity

•

Statistical models in MT exhibit spurious
ambiguity

•

•
•

Many different derivations (e.g., trees or
segmentations) generate the same translation string

Regular phrase-based MT systems

•

phrase segmentation ambiguity

Tree-based MT systems

•

derivation tree ambiguity
2

Monday, August 17, 2009

Spurious Ambiguity in Phrase Segmentations

3
Monday, August 17, 2009

Spurious Ambiguity in Phrase Segmentations
机器 翻译 软件

3
Monday, August 17, 2009

Spurious Ambiguity in Phrase Segmentations
machine translation software

机器 翻译 软件

3
Monday, August 17, 2009

Spurious Ambiguity in Phrase Segmentations
machine translation software

机器 翻译 软件
machine translation

3
Monday, August 17, 2009

Spurious Ambiguity in Phrase Segmentations
machine translation software

机器 翻译 软件
machine translation software

3
Monday, August 17, 2009

Spurious Ambiguity in Phrase Segmentations
machine translation software

机器 翻译 软件
machine translation software

机器 翻译 软件

3
Monday, August 17, 2009

Spurious Ambiguity in Phrase Segmentations
machine translation software

机器 翻译 软件
machine translation software

机器 翻译 软件
machine

3
Monday, August 17, 2009

Spurious Ambiguity in Phrase Segmentations
machine translation software

机器 翻译 软件
machine translation software

机器 翻译 软件
machine translation software

3
Monday, August 17, 2009

Spurious Ambiguity in Phrase Segmentations
machine translation software

机器 翻译 软件
machine translation software

机器 翻译 软件
machine translation software

机器
machine
3
Monday, August 17, 2009

Spurious Ambiguity in Phrase Segmentations
machine translation software

机器 翻译 软件
machine translation software

机器 翻译 软件
machine translation software

机器 翻译
machine translation
3
Monday, August 17, 2009

Spurious Ambiguity in Phrase Segmentations
machine translation software

机器 翻译 软件
machine translation software

机器 翻译 软件
machine translation software

机器 翻译 软件
machine translation software
3
Monday, August 17, 2009

Spurious Ambiguity in Phrase Segmentations
machine translation software

机器 翻译 软件
machine translation software

•
•

机器 翻译 软件
machine translation software

机器 翻译 软件
machine translation software
3
Monday, August 17, 2009

Same output:
“machine translation software”
Three different phrase
segmentations

Spurious Ambiguity in Phrase Segmentations
machine translation software

机器 翻译 软件
machine translation software

•
•

机器 翻译 软件

Same output:
“machine translation software”
Three different phrase
segmentations

machine translation software
machine transfer software

机器 翻译 软件
machine translation software
3
Monday, August 17, 2009

Spurious Ambiguity in Derivation Trees

4
Monday, August 17, 2009

Spurious Ambiguity in Derivation Trees
机器 翻译 软件

4
Monday, August 17, 2009

Spurious Ambiguity in Derivation Trees
机器 翻译 软件

S->(机器, machine)

4
Monday, August 17, 2009

Spurious Ambiguity in Derivation Trees
机器 翻译 软件

S->(机器, machine)

S->(翻译, translation)

4
Monday, August 17, 2009

Spurious Ambiguity in Derivation Trees
机器 翻译 软件

S->(机器, machine)

S->(翻译, translation) S->(软件, software)

4
Monday, August 17, 2009

Spurious Ambiguity in Derivation Trees
机器 翻译 软件

S->(S0 S1, S0 S1)
S->(机器, machine)

S->(翻译, translation) S->(软件, software)

4
Monday, August 17, 2009

Spurious Ambiguity in Derivation Trees
机器 翻译 软件
S->(S0 S1, S0 S1)
S->(S0 S1, S0 S1)
S->(机器, machine)

S->(翻译, translation) S->(软件, software)

4
Monday, August 17, 2009

Spurious Ambiguity in Derivation Trees
机器 翻译 软件
S->(S0 S1, S0 S1)
S->(S0 S1, S0 S1)
S->(机器, machine)

S->(翻译, translation) S->(软件, software)
S->(S0 S1, S0 S1)
S->(S0 S1, S0 S1)

S->(机器, machine)

S->(翻译, translation) S->(软件, software)

4
Monday, August 17, 2009

Spurious Ambiguity in Derivation Trees
机器 翻译 软件
S->(S0 S1, S0 S1)
S->(S0 S1, S0 S1)
S->(机器, machine)

S->(翻译, translation) S->(软件, software)
S->(S0 S1, S0 S1)
S->(S0 S1, S0 S1)

S->(机器, machine)

S->(翻译, translation) S->(软件, software)

S->(S0 翻译 S1, S0 translation S1)

S->(机器, machine)
Monday, August 17, 2009

翻译

4

S->(软件, software)

Spurious Ambiguity in Derivation Trees
机器 翻译 软件

•

Same output:
“machine translation software”

•

Three different derivation trees

S->(S0 S1, S0 S1)
S->(S0 S1, S0 S1)
S->(机器, machine)

S->(翻译, translation) S->(软件, software)
S->(S0 S1, S0 S1)
S->(S0 S1, S0 S1)

S->(机器, machine)

S->(翻译, translation) S->(软件, software)

S->(S0 翻译 S1, S0 translation S1)

S->(机器, machine)
Monday, August 17, 2009

翻译

4

S->(软件, software)

Maximum A Posterior (MAP) Decoding

5

Monday, August 17, 2009

Maximum A Posterior (MAP) Decoding
translation
string

red translation
blue translation
green translation

5

Monday, August 17, 2009

Maximum A Posterior (MAP) Decoding
translation
string

derivation

red translation
blue translation
green translation

5

Monday, August 17, 2009

Maximum A Posterior (MAP) Decoding
translation
string

derivation probability

0.16
0.14
0.14
0.13
0.12
0.11
0.10
0.10

red translation
blue translation
green translation

5

Monday, August 17, 2009

Maximum A Posterior (MAP) Decoding
translation
string

derivation probability

MAP

0.16
0.14
0.14
0.13
0.12
0.11
0.10
0.10

red translation
blue translation
green translation

5

Monday, August 17, 2009

Maximum A Posterior (MAP) Decoding
translation
string

derivation probability

MAP

0.16
0.14
0.14
0.13
0.12
0.11
0.10
0.10

red translation
blue translation
green translation

•

Exact MAP decoding

5

Monday, August 17, 2009

Maximum A Posterior (MAP) Decoding
translation
string

derivation probability

MAP

0.16
0.14
0.14
0.13
0.12
0.11
0.10
0.10

red translation
blue translation
green translation

•

Exact MAP decoding
y∗

= arg
= arg

max

y∈Trans(x)

p(y|x)

max

y∈Trans(x)

p(y, d|x)
d∈D(x,y)
5

Monday, August 17, 2009

• x: Foreign sentence
• y: English sentence
• d: derivation

Maximum A Posterior (MAP) Decoding
translation
string

derivation probability

MAP

0.16
0.14
0.14
0.13
0.12
0.11
0.10
0.10

red translation
blue translation
green translation

•

Exact MAP decoding
y∗

= arg
= arg

max

y∈Trans(x)

p(y|x)

max

y∈Trans(x)

p(y, d|x)
d∈D(x,y)
5

Monday, August 17, 2009

• x: Foreign sentence
• y: English sentence
• d: derivation

Maximum A Posterior (MAP) Decoding
translation
string

derivation probability

MAP

0.16
0.14
0.14
0.13
0.12
0.11
0.10
0.10

red translation
blue translation
green translation

•

Exact MAP decoding
y∗

= arg
= arg

max

y∈Trans(x)

p(y|x)

max

y∈Trans(x)

p(y, d|x)
d∈D(x,y)
6

Monday, August 17, 2009

• x: Foreign sentence
• y: English sentence
• d: derivation

Maximum A Posterior (MAP) Decoding
translation
string

red translation

derivation probability

MAP

0.16
0.14
0.14
0.13
0.12
0.11
0.10
0.10

0.28

blue translation
green translation

•

Exact MAP decoding
y∗

= arg
= arg

max

y∈Trans(x)

p(y|x)

max

y∈Trans(x)

p(y, d|x)
d∈D(x,y)
6

Monday, August 17, 2009

• x: Foreign sentence
• y: English sentence
• d: derivation

Maximum A Posterior (MAP) Decoding
translation
string

red translation

derivation probability

MAP

0.16
0.14
0.14
0.13
0.12
0.11
0.10
0.10

0.28

blue translation
green translation

•

Exact MAP decoding
y∗

= arg
= arg

max

y∈Trans(x)

p(y|x)

max

y∈Trans(x)

p(y, d|x)
d∈D(x,y)
7

Monday, August 17, 2009

• x: Foreign sentence
• y: English sentence
• d: derivation

Maximum A Posterior (MAP) Decoding
translation
string

derivation probability

MAP

red translation

0.28

blue translation

0.16
0.14
0.14
0.13
0.12
0.11
0.10
0.10

0.28

green translation

•

Exact MAP decoding
y∗

= arg
= arg

max

y∈Trans(x)

p(y|x)

max

y∈Trans(x)

p(y, d|x)
d∈D(x,y)
7

Monday, August 17, 2009

• x: Foreign sentence
• y: English sentence
• d: derivation

Maximum A Posterior (MAP) Decoding
translation
string

derivation probability

MAP

red translation

0.28

blue translation

0.16
0.14
0.14
0.13
0.12
0.11
0.10
0.10

0.28

green translation

•

Exact MAP decoding
y∗

= arg
= arg

max

y∈Trans(x)

p(y|x)

max

y∈Trans(x)

p(y, d|x)
d∈D(x,y)
8

Monday, August 17, 2009

• x: Foreign sentence
• y: English sentence
• d: derivation

Maximum A Posterior (MAP) Decoding
translation
string

derivation probability

MAP

red translation

0.28

blue translation

0.28

green translation

0.16
0.14
0.14
0.13
0.12
0.11
0.10
0.10

0.44

•

Exact MAP decoding
y∗

= arg
= arg

max

y∈Trans(x)

p(y|x)

max

y∈Trans(x)

p(y, d|x)
d∈D(x,y)
8

Monday, August 17, 2009

• x: Foreign sentence
• y: English sentence
• d: derivation

Maximum A Posterior (MAP) Decoding
translation
string

derivation probability

MAP

red translation

0.28

blue translation

0.28

green translation

0.16
0.14
0.14
0.13
0.12
0.11
0.10
0.10

0.44

•

Exact MAP decoding
y∗

= arg
= arg

max

y∈Trans(x)

p(y|x)

max

y∈Trans(x)

p(y, d|x)
d∈D(x,y)
9

Monday, August 17, 2009

• x: Foreign sentence
• y: English sentence
• d: derivation

Maximum A Posterior (MAP) Decoding
translation
string

derivation probability

MAP

red translation

0.28

blue translation

0.28

green translation

0.16
0.14
0.14
0.13
0.12
0.11
0.10
0.10

0.44

•

Exact MAP decoding
y∗

= arg
= arg

max

y∈Trans(x)

p(y|x)

max

y∈Trans(x)

p(y, d|x)
d∈D(x,y)
9

Monday, August 17, 2009

• x: Foreign sentence
• y: English sentence
• d: derivation

Maximum A Posterior (MAP) Decoding
translation
string

derivation probability

MAP

red translation

0.28

blue translation

0.28

green translation

0.16
0.14
0.14
0.13
0.12
0.11
0.10
0.10

0.44

•

Exact MAP decoding
y∗

= arg
= arg

max

y∈Trans(x)

p(y|x)

max

y∈Trans(x)

p(y, d|x)
d∈D(x,y)
9

Monday, August 17, 2009

• x: Foreign sentence
• y: English sentence
• d: derivation

Hypergraph as a search space

Monday, August 17, 2009

Hypergraph as a search space

S 0,4
S→ X0 , X0
S→ X0 , X0

X 0,4 the · · · cat

X 0,4 a · · · mat

X→ X0 de X1 , X1 on X0
X→ X0 de X1 , X0 ’s X1

X→ X0 de X1 , X1 of X0

X→ X0 de X1 , X0 X1

X 0,2 the · · · mat
X→ dianzi shang, the mat

dianzi0 shang1
Monday, August 17, 2009

X 3,4 a · · · cat
X→ mao, a cat

de2

mao3

Hypergraph as a search space
A hypergraph is a compact structure
to encode exponentially many trees.

S 0,4
S→ X0 , X0
S→ X0 , X0

X 0,4 the · · · cat

X 0,4 a · · · mat

X→ X0 de X1 , X1 on X0
X→ X0 de X1 , X0 ’s X1

X→ X0 de X1 , X1 of X0

X→ X0 de X1 , X0 X1

X 0,2 the · · · mat
X→ dianzi shang, the mat

dianzi0 shang1
Monday, August 17, 2009

X 3,4 a · · · cat
X→ mao, a cat

de2

mao3

Hypergraph as a search space

S 0,4
S→ X0 , X0
S→ X0 , X0

X 0,4 the · · · cat

X 0,4 a · · · mat

X→ X0 de X1 , X1 on X0
X→ X0 de X1 , X0 ’s X1

X→ X0 de X1 , X1 of X0

X→ X0 de X1 , X0 X1

X 0,2 the · · · mat
X→ dianzi shang, the mat

dianzi0 shang1
Monday, August 17, 2009

X 3,4 a · · · cat
X→ mao, a cat

de2

mao3

Hypergraph as a search space

S 0,4
S→ X0 , X0
S→ X0 , X0

X 0,4 the · · · cat

X 0,4 a · · · mat

X→ X0 de X1 , X1 on X0
X→ X0 de X1 , X0 ’s X1

Probabilistic
Hypergraph
X 0,2 the · · · mat

X→ X0 de X1 , X0 X1

X→ dianzi shang, the mat

dianzi0 shang1
Monday, August 17, 2009

X→ X0 de X1 , X1 of X0

X 3,4 a · · · cat

X→ mao, a cat

de2

mao3

Hypergraph as a search space
The hypergraph deﬁnes a probability distribution
over derivation trees, i.e. p(y, d | x),

S 0,4
S→ X0 , X0
S→ X0 , X0

X 0,4 the · · · cat

X 0,4 a · · · mat

X→ X0 de X1 , X1 on X0
X→ X0 de X1 , X0 ’s X1

Probabilistic
Hypergraph
X 0,2 the · · · mat

X→ X0 de X1 , X0 X1

X→ dianzi shang, the mat

dianzi0 shang1
Monday, August 17, 2009

X→ X0 de X1 , X1 of X0

X 3,4 a · · · cat

X→ mao, a cat

de2

mao3

Hypergraph as a search space
The hypergraph deﬁnes a probability distribution
over derivation trees, i.e. p(y, d | x),
and also a distribution (implicit)
over strings, i.e. p(y | x).
S 0,4
S→ X0 , X0
S→ X0 , X0

X 0,4 the · · · cat

X 0,4 a · · · mat

X→ X0 de X1 , X1 on X0
X→ X0 de X1 , X0 ’s X1

Probabilistic
Hypergraph
X 0,2 the · · · mat

X→ X0 de X1 , X0 X1

X→ dianzi shang, the mat

dianzi0 shang1
Monday, August 17, 2009

X→ X0 de X1 , X1 of X0

X 3,4 a · · · cat

X→ mao, a cat

de2

mao3

Hypergraph as a search space
The hypergraph deﬁnes a probability distribution
over derivation trees, i.e. p(y, d | x),
and also a distribution (implicit)
• Exact MAP decoding
y ∗ = arg max p(y|x)
over strings, i.e. p(y | x).
y∈HG(x)
=

S 0,4
S→ X0 , X0
S→ X0 , X0

X 0,4 the · · · cat

X 0,4 a · · · mat

X→ X0 de X1 , X1 on X0
X→ X0 de X1 , X0 ’s X1

Probabilistic
Hypergraph
X 0,2 the · · · mat

X→ X0 de X1 , X0 X1

X→ dianzi shang, the mat

dianzi0 shang1
Monday, August 17, 2009

X→ X0 de X1 , X1 of X0

X 3,4 a · · · cat

X→ mao, a cat

de2

mao3

arg max

y∈HG(x)

p(y, d|x)
d∈D(x,y)

exponential size
NP-hard (Sima’an 1996)

Decoding with spurious ambiguity?

•

Maximum a posterior (MAP) decoding

• Viterbi approximation
• N-best approximation (crunching) (May and
Knight 2006)

Monday, August 17, 2009

Viterbi Approximation
translation
string

derivation probability

MAP

red translation

0.28

blue translation

0.28

green translation

0.16
0.14
0.14
0.13
0.12
0.11
0.10
0.10

0.44

• Viterbi approximation
Monday, August 17, 2009

=

arg

=

y∗

max

max p(y, d|x)

Y(arg max p(y, d|x))

y∈Trans(x) d∈D(x,y)
d∈D(x)

Viterbi Approximation
translation
string

derivation probability

MAP Viterbi

red translation

0.28

blue translation

0.28

green translation

0.16
0.14
0.14
0.13
0.12
0.11
0.10
0.10

0.44

• Viterbi approximation
Monday, August 17, 2009

=

arg

=

y∗

max

max p(y, d|x)

Y(arg max p(y, d|x))

y∈Trans(x) d∈D(x,y)
d∈D(x)

Viterbi Approximation
translation
string

derivation probability

MAP Viterbi

red translation

0.28

blue translation

0.28

green translation

0.16
0.14
0.14
0.13
0.12
0.11
0.10
0.10

0.44

• Viterbi approximation
Monday, August 17, 2009

=

arg

=

y∗

max

max p(y, d|x)

Y(arg max p(y, d|x))

y∈Trans(x) d∈D(x,y)
d∈D(x)

Viterbi Approximation
translation
string

derivation probability

MAP Viterbi

red translation

0.28

0.16

blue translation

0.28

0.14

green translation

0.44

0.16
0.14
0.14
0.13
0.12
0.11
0.10
0.10

0.13

• Viterbi approximation
Monday, August 17, 2009

=

arg

=

y∗

max

max p(y, d|x)

Y(arg max p(y, d|x))

y∈Trans(x) d∈D(x,y)
d∈D(x)

Viterbi Approximation
translation
string

derivation probability

MAP Viterbi

red translation

0.28

0.16

blue translation

0.28

0.14

green translation

0.44

0.16
0.14
0.14
0.13
0.12
0.11
0.10
0.10

0.13

• Viterbi approximation
Monday, August 17, 2009

=

arg

=

y∗

max

max p(y, d|x)

Y(arg max p(y, d|x))

y∈Trans(x) d∈D(x,y)
d∈D(x)

N-best Approximation
translation
string

MAP Viterbi

red translation

0.28
0.28

0.14

green translation

0.44

0.16
0.14
0.14
0.13
0.12
0.11
0.10
0.10

0.16

blue translation

derivation probability

0.13

• N-best approximation (crunching) (May and Knight 2006)
y

∗

Monday, August 17, 2009

= arg

max

y∈Trans(x)

p(y, d|x)
d∈D(x,y)∩ND(x)

N-best Approximation
translation
string

4-best
MAP Viterbi
crunching

red translation

0.28

0.16

blue translation

0.28

0.14

green translation

0.44

derivation probability

0.13

0.16
0.14
0.14
0.13
0.12
0.11
0.10
0.10

• N-best approximation (crunching) (May and Knight 2006)
y

∗

Monday, August 17, 2009

= arg

max

y∈Trans(x)

p(y, d|x)
d∈D(x,y)∩ND(x)

N-best Approximation
translation
string

4-best
MAP Viterbi
crunching

red translation

0.28

0.16

blue translation

0.28

0.14

green translation

0.44

derivation probability

0.13

0.16
0.14
0.14
0.13
0.12
0.11
0.10
0.10

• N-best approximation (crunching) (May and Knight 2006)
y

∗

Monday, August 17, 2009

= arg

max

y∈Trans(x)

p(y, d|x)
d∈D(x,y)∩ND(x)

N-best Approximation
translation
string

4-best
MAP Viterbi
crunching

red translation

0.28

0.16

0.16

blue translation

0.28

0.14

0.28

green translation

0.44

0.13

derivation probability

0.13

0.16
0.14
0.14
0.13
0.12
0.11
0.10
0.10

• N-best approximation (crunching) (May and Knight 2006)
y

∗

Monday, August 17, 2009

= arg

max

y∈Trans(x)

p(y, d|x)
d∈D(x,y)∩ND(x)

N-best Approximation
translation
string

4-best
MAP Viterbi
crunching

red translation

0.28

0.16

0.16

blue translation

0.28

0.14

0.28

green translation

0.44

0.13

derivation probability

0.13

0.16
0.14
0.14
0.13
0.12
0.11
0.10
0.10

• N-best approximation (crunching) (May and Knight 2006)
y

∗

Monday, August 17, 2009

= arg

max

y∈Trans(x)

p(y, d|x)
d∈D(x,y)∩ND(x)

MAP vs. Approximations
translation
string

4-best
MAP Viterbi
crunching

red translation

0.28

0.16

0.16

blue translation

0.28

0.14

0.28

green translation

0.44

0.13

0.13

Monday, August 17, 2009

derivation probability

0.16
0.14
0.14
0.13
0.12
0.11
0.10
0.10

MAP vs. Approximations
translation
string

4-best
MAP Viterbi
crunching

red translation

0.28

0.16

0.16

blue translation

0.28

0.14

0.28

green translation

0.44

0.13

derivation probability

0.13

•

Exact MAP decoding under spurious ambiguity is intractable

Monday, August 17, 2009

0.16
0.14
0.14
0.13
0.12
0.11
0.10
0.10

MAP vs. Approximations
translation
string

4-best
MAP Viterbi
crunching

red translation

0.28

0.16

0.16

blue translation

0.28

0.14

0.28

green translation

0.44

0.13

derivation probability

0.13

•
•

Exact MAP decoding under spurious ambiguity is intractable
Viterbi and crunching are efﬁcient, but ignore most derivations

Monday, August 17, 2009

0.16
0.14
0.14
0.13
0.12
0.11
0.10
0.10

MAP vs. Approximations
translation
string

4-best
MAP Viterbi
crunching

red translation

0.28

0.16

0.16

blue translation

0.28

0.14

0.28

green translation

0.44

0.13

derivation probability

0.13

•
•
•

Exact MAP decoding under spurious ambiguity is intractable
Viterbi and crunching are efﬁcient, but ignore most derivations
Our goal: develop an approximation that considers all the
derivations but still allows tractable decoding

Monday, August 17, 2009

0.16
0.14
0.14
0.13
0.12
0.11
0.10
0.10

Variational Decoding

18
Monday, August 17, 2009

Variational Decoding
Decoding using Variational approximation

18
Monday, August 17, 2009

Variational Decoding
Decoding using Variational approximation
Decoding using a sentence-speciﬁc
approximate distribution

18
Monday, August 17, 2009

Variational Decoding for MT: an Overview

Monday, August 17, 2009

Variational Decoding for MT: an Overview
Sentence-speciﬁc decoding

Monday, August 17, 2009

Variational Decoding for MT: an Overview
Sentence-speciﬁc decoding
Three steps:

Monday, August 17, 2009

Variational Decoding for MT: an Overview
Sentence-speciﬁc decoding
Three steps:

1

Generate a hypergraph

Monday, August 17, 2009

Variational Decoding for MT: an Overview
Sentence-speciﬁc decoding
Three steps:

1

Generate a hypergraph

Foreign
sentence x

Monday, August 17, 2009

Variational Decoding for MT: an Overview
Sentence-speciﬁc decoding
Three steps:

1

Generate a hypergraph

Foreign
sentence x

Monday, August 17, 2009

SMT

Variational Decoding for MT: an Overview
Sentence-speciﬁc decoding
Three steps:

1

Generate a hypergraph

S 0,4
S→ X0 , X0
S→ X0 , X0

Foreign
sentence x

X 0,4 the · · · cat

SMT

X 0,4 a · · · mat

X→ X0 de X1 , X1 on X0
X→ X0 de X1 , X0 ’s X1

X→ X0 de X1 , X1 of X

X→ X0 de X1 , X0 X1

X 0,2 the · · · mat
X→ dianzi shang, the mat

dianzi0 shang1

Monday, August 17, 2009

X 3,4 a · · · cat
X→ mao, a cat

de2

mao3

Variational Decoding for MT: an Overview
Sentence-speciﬁc decoding
Three steps:

1

Generate a hypergraph

S 0,4
S→ X0 , X0
S→ X0 , X0

Foreign
sentence x

X 0,4 the · · · cat

SMT

X 0,4 a · · · mat

p(y, d | x)

X→ X0 de X1 , X1 on X0

X→ X0 de X1 , X0 ’s X1

X→ X0 de X1 , X1 of X

X→ X0 de X1 , X0 X1

X 0,2 the · · · mat
X→ dianzi shang, the mat

dianzi0 shang1

Monday, August 17, 2009

X 3,4 a · · · cat
X→ mao, a cat

de2

mao3

Variational Decoding for MT: an Overview
Sentence-speciﬁc decoding
Three steps:

1

Generate a hypergraph

S 0,4
S→ X0 , X0
S→ X0 , X0

Foreign
sentence x

X 0,4 the · · · cat

SMT

X 0,4 a · · · mat

p(y, d | x)

X→ X0 de X1 , X1 on X0

X→ X0 de X1 , X0 ’s X1

X→ X0 de X1 , X1 of X

X→ X0 de X1 , X0 X1

p(y | x)
X 0,2 the · · · mat
X→ dianzi shang, the mat

dianzi0 shang1

Monday, August 17, 2009

X 3,4 a · · · cat
X→ mao, a cat

de2

mao3

Variational Decoding for MT: an Overview
Sentence-speciﬁc decoding
Three steps:

1

MAP decoding under P is
intractable

Generate a hypergraph

S 0,4
S→ X0 , X0
S→ X0 , X0

Foreign
sentence x

X 0,4 the · · · cat

SMT

X 0,4 a · · · mat

p(y, d | x)

X→ X0 de X1 , X1 on X0

X→ X0 de X1 , X0 ’s X1

X→ X0 de X1 , X1 of X

X→ X0 de X1 , X0 X1

p(y | x)
X 0,2 the · · · mat
X→ dianzi shang, the mat

dianzi0 shang1

Monday, August 17, 2009

X 3,4 a · · · cat
X→ mao, a cat

de2

mao3

1

S 0,4
S→ X0 , X0
S→ X0 , X0

X 0,4 the · · · cat

X 0,4 a · · · mat

X→ X0 de X1 , X1 on X0

p(y, d | x)
X→ X0 de X1 , X0 ’s X1

X→ X0 de X1 , X1 of X0

X→ X0 de X1 , X0 X1

X 0,2 the · · · mat
X→ dianzi shang, the mat

dianzi0 shang1

Monday, August 17, 2009

X 3,4 a · · · cat
X→ mao, a cat

de2

mao3

Generate a hypergraph

1

S 0,4
S→ X0 , X0
S→ X0 , X0

X 0,4 the · · · cat

X 0,4 a · · · mat

X→ X0 de X1 , X1 on X0

p(y, d | x)
X→ X0 de X1 , X0 ’s X1

X→ X0 de X1 , X1 of X0

X→ X0 de X1 , X0 X1

X 0,2 the · · · mat
X→ dianzi shang, the mat

dianzi0 shang1

Monday, August 17, 2009

X 3,4 a · · · cat
X→ mao, a cat

de2

mao3

Generate a hypergraph

1

S 0,4
S→ X0 , X0
S→ X0 , X0

X 0,4 the · · · cat

X 0,4 a · · · mat

X→ X0 de X1 , X1 on X0

p(y, d | x)
X→ X0 de X1 , X0 ’s X1

X→ X0 de X1 , X1 of X0

X→ X0 de X1 , X0 X1

X 0,2 the · · · mat
X→ dianzi shang, the mat

dianzi0 shang1

2

Monday, August 17, 2009

X 3,4 a · · · cat
X→ mao, a cat

de2

mao3

Generate a hypergraph

1

S 0,4
S→ X0 , X0
S→ X0 , X0

X 0,4 the · · · cat

X 0,4 a · · · mat

X→ X0 de X1 , X1 on X0

p(y, d | x)
X→ X0 de X1 , X0 ’s X1

X→ X0 de X1 , X1 of X0

X→ X0 de X1 , X0 X1

X 0,2 the · · · mat

X 3,4 a · · · cat

X→ dianzi shang, the mat

X→ mao, a cat

dianzi0 shang1

2

de2

mao3

S 0,4
S→ X0 , X0
S→ X0 , X0

X 0,4 the · · · cat

X 0,4 a · · · mat

X→ X0 de X1 , X1 on X0

p(y, d | x)
X→ X0 de X1 , X0 ’s X1

X→ X0 de X1 , X1 of X0

X→ X0 de X1 , X0 X1

X 0,2 the · · · mat
X→ dianzi shang, the mat

dianzi0 shang1

Monday, August 17, 2009

X 3,4 a · · · cat
X→ mao, a cat

de2

mao3

Generate a hypergraph

1

S 0,4
S→ X0 , X0
S→ X0 , X0

X 0,4 the · · · cat

X 0,4 a · · · mat

X→ X0 de X1 , X1 on X0

p(y, d | x)
X→ X0 de X1 , X0 ’s X1

X→ X0 de X1 , X1 of X0

X→ X0 de X1 , X0 X1

X 0,2 the · · · mat

X 3,4 a · · · cat

X→ dianzi shang, the mat

X→ mao, a cat

dianzi0 shang1

2

de2

mao3

S 0,4
S→ X0 , X0
S→ X0 , X0

X 0,4 the · · · cat

X 0,4 a · · · mat

X→ X0 de X1 , X1 on X0

p(y, d | x)
X→ X0 de X1 , X0 ’s X1

X→ X0 de X1 , X1 of X0

X→ X0 de X1 , X0 X1

X 0,2 the · · · mat
X→ dianzi shang, the mat

dianzi0 shang1

Monday, August 17, 2009

Generate a hypergraph

X 3,4 a · · · cat
X→ mao, a cat

de2

mao3

Estimate a model
from the hypergraph

q*(y | x)

1

S 0,4
S→ X0 , X0
S→ X0 , X0

X 0,4 the · · · cat

X 0,4 a · · · mat

X→ X0 de X1 , X1 on X0

p(y, d | x)
X→ X0 de X1 , X0 ’s X1

X→ X0 de X1 , X1 of X0

X→ X0 de X1 , X0 X1

X 0,2 the · · · mat

X 3,4 a · · · cat

X→ dianzi shang, the mat

X→ mao, a cat

dianzi0 shang1

2

de2

mao3

S 0,4
S→ X0 , X0
S→ X0 , X0

X 0,4 the · · · cat

X 0,4 a · · · mat

X→ X0 de X1 , X1 on X0

p(y, d | x)
X→ X0 de X1 , X0 ’s X1

X→ X0 de X1 , X1 of X0

X→ X0 de X1 , X0 X1

X 0,2 the · · · mat
X→ dianzi shang, the mat

dianzi0 shang1

Monday, August 17, 2009

Generate a hypergraph

X 3,4 a · · · cat
X→ mao, a cat

de2

mao3

Estimate a model
from the hypergraph

q* is an n-gram model
over output strings.

q*(y | x)

1

S 0,4
S→ X0 , X0
S→ X0 , X0

X 0,4 the · · · cat

X 0,4 a · · · mat

X→ X0 de X1 , X1 on X0

p(y, d | x)
X→ X0 de X1 , X0 ’s X1

X→ X0 de X1 , X1 of X0

X→ X0 de X1 , X0 X1

X 0,2 the · · · mat

X 3,4 a · · · cat

X→ dianzi shang, the mat

X→ mao, a cat

dianzi0 shang1

2

de2

mao3

S 0,4
S→ X0 , X0
S→ X0 , X0

X 0,4 the · · · cat

X 0,4 a · · · mat

X→ X0 de X1 , X1 on X0

p(y, d | x)
X→ X0 de X1 , X0 ’s X1

X→ X0 de X1 , X1 of X0

X→ X0 de X1 , X0 X1

X 0,2 the · · · mat
X→ dianzi shang, the mat

dianzi0 shang1

Monday, August 17, 2009

Generate a hypergraph

X 3,4 a · · · cat
X→ mao, a cat

de2

mao3

Estimate a model
from the hypergraph

q* is an n-gram model
over output strings.

q*(y | x)
≈∑d∈D(x,y) p(y,d|x)

1

S 0,4
S→ X0 , X0
S→ X0 , X0

X 0,4 the · · · cat

X 0,4 a · · · mat

X→ X0 de X1 , X1 on X0

p(y, d | x)
X→ X0 de X1 , X0 ’s X1

X→ X0 de X1 , X1 of X0

X→ X0 de X1 , X0 X1

X 0,2 the · · · mat

X 3,4 a · · · cat

X→ dianzi shang, the mat

X→ mao, a cat

dianzi0 shang1

2

de2

mao3

S 0,4
S→ X0 , X0
S→ X0 , X0

X 0,4 the · · · cat

X 0,4 a · · · mat

X→ X0 de X1 , X1 on X0

p(y, d | x)
X→ X0 de X1 , X0 ’s X1

X→ X0 de X1 , X1 of X0

X→ X0 de X1 , X0 X1

X 0,2 the · · · mat
X→ dianzi shang, the mat

dianzi0 shang1

3

Monday, August 17, 2009

Generate a hypergraph

X 3,4 a · · · cat
X→ mao, a cat

de2

mao3

Estimate a model
from the hypergraph

q* is an n-gram model
over output strings.

q*(y | x)
≈∑d∈D(x,y) p(y,d|x)

1

S 0,4
S→ X0 , X0
S→ X0 , X0

X 0,4 the · · · cat

X 0,4 a · · · mat

X→ X0 de X1 , X1 on X0

p(y, d | x)
X→ X0 de X1 , X0 ’s X1

X→ X0 de X1 , X1 of X0

X→ X0 de X1 , X0 X1

X 0,2 the · · · mat

X 3,4 a · · · cat

X→ dianzi shang, the mat

X→ mao, a cat

dianzi0 shang1

2

de2

mao3

S 0,4
S→ X0 , X0
S→ X0 , X0

X 0,4 the · · · cat

X 0,4 a · · · mat

X→ X0 de X1 , X1 on X0

p(y, d | x)
X→ X0 de X1 , X0 ’s X1

X→ X0 de X1 , X1 of X0

X→ X0 de X1 , X0 X1

X 0,2 the · · · mat

Estimate a model
from the hypergraph

X 3,4 a · · · cat

X→ dianzi shang, the mat

de2

mao3

S 0,4
S→ X0 , X0
S→ X0 , X0

X 0,4 the · · · cat

X 0,4 a · · · mat

X→ X0 de X1 , X1 on X0

q*(y | x)
X→ X0 de X1 , X0 ’s X1

X→ X0 de X1 , X1 of X0

X→ X0 de X1 , X0 X1

X 0,2 the · · · mat
X→ dianzi shang, the mat

dianzi0 shang1

Monday, August 17, 2009

X 3,4 a · · · cat
X→ mao, a cat

de2

mao3

q* is an n-gram model
over output strings.

q*(y | x)
≈∑d∈D(x,y) p(y,d|x)

X→ mao, a cat

dianzi0 shang1

3

Generate a hypergraph

Decode using q*
on the hypergraph

Variational Inference

21
Monday, August 17, 2009

•

Variational Inference
We want to do inference under p, but it is intractable

21
Monday, August 17, 2009

•

Variational Inference
We want to do inference under p, but it is intractable
y

∗

=

arg max p(y|x)
y

21
Monday, August 17, 2009

•

Variational Inference
We want to do inference under p, but it is intractable
y

•

∗

=

arg max p(y|x)
y

Instead, we derive a simpler distribution q*

21
Monday, August 17, 2009

•

Variational Inference
We want to do inference under p, but it is intractable
y

•

∗

=

arg max p(y|x)
y

Instead, we derive a simpler distribution q*
q

∗

=

arg min KL(p||q)
q∈Q

21
Monday, August 17, 2009

•

Variational Inference
We want to do inference under p, but it is intractable
y

•

=

arg max p(y|x)
y

Instead, we derive a simpler distribution q*
q

•

∗

∗

=

arg min KL(p||q)
q∈Q

Then, we will use q* as a surrogate for p in inference

21
Monday, August 17, 2009

•

Variational Inference
We want to do inference under p, but it is intractable
y

•

=

arg max p(y|x)
y

Instead, we derive a simpler distribution q*
q

•

∗

∗

=

arg min KL(p||q)
q∈Q

Then, we will use q* as a surrogate for p in inference
y

∗

=

arg max q (y | x)
y

∗

21
Monday, August 17, 2009

•

Variational Inference
We want to do inference under p, but it is intractable
y

•

=

arg max p(y|x)
y

Instead, we derive a simpler distribution q*
q

•

∗

∗

=

arg min KL(p||q)

P

q∈Q

Then, we will use q* as a surrogate for p in inference
y

∗

=

arg max q (y | x)
y

∗

21
Monday, August 17, 2009

•

Variational Inference
We want to do inference under p, but it is intractable
y

•

=

arg max p(y|x)
y

Instead, we derive a simpler distribution q*
q

•

∗

∗

=

arg min KL(p||q)

P

p

q∈Q

Then, we will use q* as a surrogate for p in inference
y

∗

=

arg max q (y | x)
y

∗

21
Monday, August 17, 2009

•

Variational Inference
We want to do inference under p, but it is intractable
y

•

∗

=

arg max p(y|x)
y

Instead, we derive a simpler distribution q*
q

∗

=

arg min KL(p||q)

P

p

q∈Q

Q

•

Then, we will use q* as a surrogate for p in inference
y

∗

=

arg max q (y | x)
y

∗

21
Monday, August 17, 2009

•

Variational Inference
We want to do inference under p, but it is intractable
y

•

∗

=

arg max p(y|x)
y

Instead, we derive a simpler distribution q*
q

∗

=

arg min KL(p||q)

P

p

q∈Q

Q

•

Then, we will use q* as a surrogate for p in inference
y

∗

=

arg max q (y | x)
y

∗

21
Monday, August 17, 2009

•

Variational Inference
We want to do inference under p, but it is intractable
y

•

∗

=

arg max p(y|x)
y

Instead, we derive a simpler distribution q*
q

∗

=

arg min KL(p||q)

P

p

q∈Q

Q

•

q*

Then, we will use q* as a surrogate for p in inference
y

∗

=

arg max q (y | x)
y

∗

21
Monday, August 17, 2009

Variational Approximation

•

q*: an approximation having minimum distance to p
q∗

=

arg min KL(p||q)
q∈Q

22
Monday, August 17, 2009

a family of distributions

Variational Approximation

•

q*: an approximation having minimum distance to p
q∗

=

=

arg min KL(p||q)
q∈Q

arg min
q∈Q

y∈Trans(x)

22
Monday, August 17, 2009

a family of distributions

p
plog
q

Variational Approximation

•

q*: an approximation having minimum distance to p
q∗

=

=

=

arg min KL(p||q)

arg min
q∈Q

y∈Trans(x)

arg min
q∈Q

y∈Trans(x)

22
Monday, August 17, 2009

a family of distributions

q∈Q

p
plog
q

(plogp − plogq)

Variational Approximation

•

q*: an approximation having minimum distance to p
q∗

=

=

=

arg min KL(p||q)

arg min
q∈Q

y∈Trans(x)

arg min
q∈Q

y∈Trans(x)

22
Monday, August 17, 2009

a family of distributions

q∈Q

p
plog
q

(plogp − plogq)

constant

Variational Approximation

•

q*: an approximation having minimum distance to p
q∗

=

=

=
=

arg min KL(p||q)

arg min
q∈Q

y∈Trans(x)

arg min
q∈Q

y∈Trans(x)

p
plog
q

(plogp − plogq)

arg max
q∈Q

plogq
y∈Trans(x)

22
Monday, August 17, 2009

a family of distributions

q∈Q

constant

Variational Approximation

•

q*: an approximation having minimum distance to p
q∗

=

=

=
=

•

arg min KL(p||q)

arg min
q∈Q

y∈Trans(x)

arg min
q∈Q

y∈Trans(x)

p
plog
q

(plogp − plogq)

arg max

Three questions

q∈Q

plogq
y∈Trans(x)

22
Monday, August 17, 2009

a family of distributions

q∈Q

constant

Variational Approximation

•

q*: an approximation having minimum distance to p
q∗

=

=

=
=

•

arg min KL(p||q)

arg min
q∈Q

y∈Trans(x)

arg min
q∈Q

y∈Trans(x)

(plogp − plogq)

q∈Q

plogq
y∈Trans(x)

how to parameterize q?

22
Monday, August 17, 2009

p
plog
q

arg max

Three questions

•

a family of distributions

q∈Q

constant

Variational Approximation

•

q*: an approximation having minimum distance to p
q∗

=

=

=
=

•

arg min KL(p||q)

arg min
q∈Q

y∈Trans(x)

arg min
q∈Q

y∈Trans(x)

(plogp − plogq)

q∈Q

plogq
y∈Trans(x)

how to parameterize q?
how to estimate q*?
22

Monday, August 17, 2009

p
plog
q

arg max

Three questions

•
•

a family of distributions

q∈Q

constant

Variational Approximation

•

q*: an approximation having minimum distance to p
q∗

=

=

=
=

•

arg min KL(p||q)

arg min
q∈Q

Monday, August 17, 2009

y∈Trans(x)

arg min
q∈Q

y∈Trans(x)

p
plog
q

(plogp − plogq)

arg max

Three questions

•
•
•

a family of distributions

q∈Q

q∈Q

plogq
y∈Trans(x)

how to parameterize q?
how to estimate q*?
how to use q* for decoding?
22

constant

Parameterization of q∈Q

23
Monday, August 17, 2009

•

Parameterization of q∈Q
Naturally, we parameterize q as an n-gram model

23
Monday, August 17, 2009

•

Parameterization of q∈Q
Naturally, we parameterize q as an n-gram model

•

The probability of a string is a product of the
probabilities of those n-grams appearing in that string

23
Monday, August 17, 2009

•

Parameterization of q∈Q
Naturally, we parameterize q as an n-gram model

•

The probability of a string is a product of the
probabilities of those n-grams appearing in that string

3-gram model

23
Monday, August 17, 2009

•

Parameterization of q∈Q
Naturally, we parameterize q as an n-gram model

•

The probability of a string is a product of the
probabilities of those n-grams appearing in that string

3-gram model

y: a b c d e f

23
Monday, August 17, 2009

•

Parameterization of q∈Q
Naturally, we parameterize q as an n-gram model

•

The probability of a string is a product of the
probabilities of those n-grams appearing in that string

3-gram model
q(y)

=

y: a b c d e f

q(a) · q(b|a) · q(c|ab) · q(d|bc) · q(e|cd) · q(f |de)

23
Monday, August 17, 2009

•

Parameterization of q∈Q
Naturally, we parameterize q as an n-gram model

•

The probability of a string is a product of the
probabilities of those n-grams appearing in that string

3-gram model
q(y)

=

y: a b c d e f

q(a) · q(b|a) · q(c|ab) · q(d|bc) · q(e|cd) · q(f |de)

Other ways of parameterizations are possible!

23
Monday, August 17, 2009

•

Parameterization of q∈Q
Naturally, we parameterize q as an n-gram model

•

The probability of a string is a product of the
probabilities of those n-grams appearing in that string

3-gram model
q(y)

=

y: a b c d e f

q(a) · q(b|a) · q(c|ab) · q(d|bc) · q(e|cd) · q(f |de)

24
Monday, August 17, 2009

•

Parameterization of q∈Q
Naturally, we parameterize q as an n-gram model

•

The probability of a string is a product of the
probabilities of those n-grams appearing in that string

3-gram model
q(y)

=

y: a b c d e f

q(a) · q(b|a) · q(c|ab) · q(d|bc) · q(e|cd) · q(f |de)

how to estimate these n-gram probabilities?

24
Monday, August 17, 2009

•

Estimation of q*∈Q
Variational approximation
q

∗

=

arg max
q∈Q

25
Monday, August 17, 2009

plogq
y∈Trans(x)

•
•

Estimation of q*∈Q
Variational approximation
q

∗

=

arg max
q∈Q

plogq
y∈Trans(x)

q* is a maximum likelihood estimate (MLE)
where p is the empirical distribution

25
Monday, August 17, 2009

•

Estimation of q*∈Q
Variational approximation
q

∗

=

arg max
q∈Q

•

plogq
y∈Trans(x)

q* is a maximum likelihood estimate (MLE)
where p is the empirical distribution
But in our case, p is deﬁned not by a corpus, but by
a hypergraph for a given test sentence!

25
Monday, August 17, 2009

•

Estimation of q*∈Q
Variational approximation
q

=

∗

arg max
q∈Q

•

plogq
y∈Trans(x)

q* is a maximum likelihood estimate (MLE)
where p is the empirical distribution
But in our case, p is deﬁned not by a corpus, but by
a hypergraph for a given test sentence!
S 0,4
S→ X0 , X0
S→ X0 , X0

X 0,4 the · · · cat

X 0,4 a · · · mat

X→ X0 de X1 , X1 on X0
X→ X0 de X1 , X0 ’s X1

X→ X0 de X1 , X1 of X0

X→ X0 de X1 , X0 X1

X 0,2 the · · · mat
X→ dianzi shang, the mat

dianzi0 shang1
Monday, August 17, 2009

X 3,4 a · · · cat
X→ mao, a cat

de2

mao3 25

•

Estimation of q*∈Q
Variational approximation
q

=

∗

arg max
q∈Q

•

plogq
y∈Trans(x)

q* is a maximum likelihood estimate (MLE)
where p is the empirical distribution
But in our case, p is deﬁned not by a corpus, but by
a hypergraph for a given test sentence!
S 0,4
S→ X0 , X0
S→ X0 , X0

X 0,4 the · · · cat

X 0,4 a · · · mat

X→ X0 de X1 , X1 on X0
X→ X0 de X1 , X0 ’s X1

X→ X0 de X1 , X1 of X0

X→ X0 de X1 , X0 X1

X 0,2 the · · · mat
X→ dianzi shang, the mat

dianzi0 shang1
Monday, August 17, 2009

X 3,4 a · · · cat
X→ mao, a cat

de2

mao3 25

estimate

•

Estimation of q*∈Q
Variational approximation
q

=

∗

arg max
q∈Q

•

plogq
y∈Trans(x)

q* is a maximum likelihood estimate (MLE)
where p is the empirical distribution
But in our case, p is deﬁned not by a corpus, but by
a hypergraph for a given test sentence!
S 0,4
S→ X0 , X0
S→ X0 , X0

X 0,4 the · · · cat

X 0,4 a · · · mat

X→ X0 de X1 , X1 on X0
X→ X0 de X1 , X0 ’s X1

X→ X0 de X1 , X1 of X0

X→ X0 de X1 , X0 X1

X 0,2 the · · · mat
X→ dianzi shang, the mat

dianzi0 shang1
Monday, August 17, 2009

X 3,4 a · · · cat
X→ mao, a cat

de2

mao3 25

estimate
bi-gram model

•

Estimation of q*∈Q
Variational approximation
q

=

∗

arg max
q∈Q

•

plogq
y∈Trans(x)

q* is a maximum likelihood estimate (MLE)
where p is the empirical distribution
But in our case, p is deﬁned not by a corpus, but by
a hypergraph for a given test sentence!
S 0,4
S→ X0 , X0
S→ X0 , X0

X 0,4 the · · · cat

X 0,4 a · · · mat

X→ X0 de X1 , X1 on X0
X→ X0 de X1 , X0 ’s X1

X 0,2 the · · · mat

dianzi0 shang1
Monday, August 17, 2009

bi-gram model

X→ X0 de X1 , X1 of X0

X→ X0 de X1 , X0 X1

X→ dianzi shang, the mat

estimate

X 3,4 a · · · cat
X→ mao, a cat

de2

mao3 25

•

brute force

•

Estimation of q*∈Q
Variational approximation
q

=

∗

arg max
q∈Q

•

plogq
y∈Trans(x)

q* is a maximum likelihood estimate (MLE)
where p is the empirical distribution
But in our case, p is deﬁned not by a corpus, but by
a hypergraph for a given test sentence!
S 0,4
S→ X0 , X0
S→ X0 , X0

X 0,4 the · · · cat

X 0,4 a · · · mat

X→ X0 de X1 , X1 on X0
X→ X0 de X1 , X0 ’s X1

X 0,2 the · · · mat

dianzi0 shang1
Monday, August 17, 2009

bi-gram model

X→ X0 de X1 , X1 of X0

X→ X0 de X1 , X0 X1

X→ dianzi shang, the mat

estimate

X 3,4 a · · · cat
X→ mao, a cat

de2

mao3 25

•
•

brute force
dynamic programming

Estimating q* from a hypergraph: brute force
S 0,4
S→ X0 , X0
S→ X0 , X0

X 0,4 the · · · cat

X 0,4 a · · · mat

S→ X0 X0
S→ X0 ,,X0

X→ X0 de X1 , X1 on X0

X→X X1 ,de de X of X
X→ X0 X→0XXX’s ,X1 0, ’s XX0
X→ de 0 de 0 11 XX1X X1 1
X→ XXde0XX1X1 on 0X→ X0 de X1 , X1 of X0
, ,1
0
0
X→ X0 de X1 , X0 X1
X→ dianzi shang, the mat
X→ dianzi shang, the mat
X 0,2 the · · · mat

dianzi0 shang1
0
1

X→ dianzi shang, the mat

dianzi0 shang1

X→ mao, a cat
X→ mao, a cat

de2
de2

X 3,4 a · · · cat

mao3
mao3

X→ mao, a cat

de2

mao3

26
Monday, August 17, 2009

Estimating q* from a hypergraph: brute force
S 0,4
S→ X0 , X0
S→ X0 , X0

X 0,4 the · · · cat

X 0,4 a · · · mat

Bi-gram estimation:

S→ X0 X0
S→ X0 ,,X0

X→ X0 de X1 , X1 on X0

X→X X1 ,de de X of X
X→ X0 X→0XXX’s ,X1 0, ’s XX0
X→ de 0 de 0 11 XX1X X1 1
X→ XXde0XX1X1 on 0X→ X0 de X1 , X1 of X0
, ,1
0
0
X→ X0 de X1 , X0 X1
X→ dianzi shang, the mat
X→ dianzi shang, the mat
X 0,2 the · · · mat

dianzi0 shang1
0
1

X→ dianzi shang, the mat

dianzi0 shang1

X→ mao, a cat
X→ mao, a cat

de2
de2

X 3,4 a · · · cat

mao3
mao3

X→ mao, a cat

de2

mao3

26
Monday, August 17, 2009

Estimating q* from a hypergraph: brute force
S 0,4
S→ X0 , X0
S→ X0 , X0

X 0,4 the · · · cat

X 0,4 a · · · mat

Bi-gram estimation:

S→ X0 X0
S→ X0 ,,X0

‣ unpack the hypergraph

X→ X0 de X1 , X1 on X0

X→X X1 ,de de X of X
X→ X0 X→0XXX’s ,X1 0, ’s XX0
X→ de 0 de 0 11 XX1X X1 1
X→ XXde0XX1X1 on 0X→ X0 de X1 , X1 of X0
, ,1
0
0
X→ X0 de X1 , X0 X1
X→ dianzi shang, the mat
X→ dianzi shang, the mat
X 0,2 the · · · mat

dianzi0 shang1
0
1

X→ dianzi shang, the mat

dianzi0 shang1

X→ mao, a cat
X→ mao, a cat

de2
de2

X 3,4 a · · · cat

mao3
mao3

X→ mao, a cat

de2

mao3

26
Monday, August 17, 2009

Estimating q* from a hypergraph: brute force
S 0,4
S→ X0 , X0
S→ X0 , X0

X 0,4 the · · · cat

Bi-gram estimation:

X 0,4 a · · · mat

‣ unpack the hypergraph

X→ X0 de X1 , X1 on X0
X→ X0 de X1 , X0 ’s X1

X→ X0 de X1 , X1 of X0

X→ X0 de X1 , X0 X1

S→ X0 , X0
X 0,2 the · · · mat

X→ X0 de X1 , X0 X1

X 3,4 a · · · cat

X→ dianzi shang, the mat

X→ mao, a cat

dianzi0 shang1

de2

mao3

X→ dianzi shang, the mat

S→ X0 , X0

dianzi0 shang1

X→ X0 de X1 , X0 ’s X1
X→ dianzi shang, the mat

dianzi0 shang1

S→ X0 , X0

dianzi0 shang1

mao3

S→ X0 , X0

X→ dianzi shang, the mat

dianzi0 shang1

mao3
27

Monday, August 17, 2009

mao3

X→ X0 de X1 , X1 of X0

X→ mao, a cat

de2

de2

X→ mao, a cat

de2

X→ X0 de X1 , X1 on X0
X→ dianzi shang, the mat

X→ mao, a cat

X→ mao, a cat

de2

mao3

Estimating q* from a hypergraph: brute force
S 0,4
S→ X0 , X0
S→ X0 , X0

X 0,4 the · · · cat

Bi-gram estimation:

X 0,4 a · · · mat

‣ unpack the hypergraph

X→ X0 de X1 , X1 on X0
X→ X0 de X1 , X0 ’s X1

X→ X0 de X1 , X1 of X0

X→ X0 de X1 , X0 X1

S→ X0 , X0
X 0,2 the · · · mat

X→ X0 de X1 , X0 X1

X 3,4 a · · · cat

X→ dianzi shang, the mat

X→ mao, a cat

dianzi0 shang1

de2

mao3

X→ dianzi shang, the mat

S→ X0 , X0

dianzi0 shang1

X→ X0 de X1 , X0 ’s X1
X→ dianzi shang, the mat

dianzi0 shang1

S→ X0 , X0

X→ dianzi shang, the mat

dianzi0 shang1

a cat on the mat
Monday, August 17, 2009

mao3

S→ X0 , X0
X→ X0 de X1 , X1 of X0
X→ dianzi shang, the mat

dianzi0 shang1

mao3
27

mao3

X→ mao, a cat

de2

X→ mao, a cat

de2

de2

the mat a cat

the mat ‘s a cat

X→ X0 de X1 , X1 on X0

X→ mao, a cat

X→ mao, a cat

de2

mao3

a cat of the mat

Estimating q* from a hypergraph: brute force
S 0,4
S→ X0 , X0
S→ X0 , X0

X 0,4 the · · · cat

Bi-gram estimation:

X 0,4 a · · · mat

‣ unpack the hypergraph

X→ X0 de X1 , X1 on X0
X→ X0 de X1 , X0 ’s X1

X→ X0 de X1 , X1 of X0

X→ X0 de X1 , X0 X1

p=2/8

S→ X0 , X0
X 0,2 the · · · mat

X→ X0 de X1 , X0 X1

X 3,4 a · · · cat

X→ dianzi shang, the mat

p=3/8

X→ mao, a cat

dianzi0 shang1

de2

mao3

S→ X0 , X0

dianzi0 shang1

X→ X0 de X1 , X0 ’s X1
X→ dianzi shang, the mat

p=1/8

dianzi0 shang1

S→ X0 , X0

X→ dianzi shang, the mat

dianzi0 shang1

de2

Monday, August 17, 2009

mao3

X→ X0 de X1 , X1 of X0

dianzi0 shang1
27

mao3

p=2/8

S→ X0 , X0

X→ dianzi shang, the mat

mao3

a cat on the mat

de2

X→ mao, a cat

de2

X→ mao, a cat

X→ mao, a cat

the mat a cat

the mat ‘s a cat

X→ X0 de X1 , X1 on X0

X→ dianzi shang, the mat

X→ mao, a cat

de2

mao3

a cat of the mat

Estimating q* from a hypergraph: brute force
a cat on the mat

p=1/8

the mat ‘s a cat

p=3/8

the mat a cat

p=2/8

a cat of the mat

p=2/8

28
Monday, August 17, 2009

Estimating q* from a hypergraph: brute force
a cat on the mat

p=1/8

Bi-gram estimation:

‣ unpack the hypergraph
the mat ‘s a cat

p=3/8

the mat a cat

p=2/8

a cat of the mat

p=2/8

28
Monday, August 17, 2009

Estimating q* from a hypergraph: brute force
a cat on the mat

p=1/8

Bi-gram estimation:

‣ unpack the hypergraph
the mat ‘s a cat

p=3/8

the mat a cat

of each bigram

p=2/8

a cat of the mat

‣ accumulate the soft-count

p=2/8

28
Monday, August 17, 2009

Estimating q* from a hypergraph: brute force
a cat on the mat

p=1/8

Bi-gram estimation:

‣ unpack the hypergraph
the mat ‘s a cat

p=3/8

the mat a cat

of each bigram

p=2/8

a cat of the mat

‣ accumulate the soft-count

p=2/8

‣ normalize the counts

28
Monday, August 17, 2009

Estimating q* from a hypergraph: brute force
a cat on the mat

p=1/8

Bi-gram estimation:

‣ unpack the hypergraph
the mat ‘s a cat

p=3/8

the mat a cat

of each bigram

p=2/8

a cat of the mat

‣ accumulate the soft-count

p=2/8

‣ normalize the counts
Pr(on | cat)=1/8
Pr(</s> | cat)=5/8
Pr(of | cat)=2/8
28

Monday, August 17, 2009

Estimating q* from a hypergraph: brute force
a cat on the mat

p=1/8

Bi-gram estimation:

‣ unpack the hypergraph
the mat ‘s a cat

p=3/8

the mat a cat

of each bigram

p=2/8

a cat of the mat

‣ accumulate the soft-count

p=2/8

‣ normalize the counts
Pr(on | cat)=1/8
Pr(</s> | cat)=5/8
Pr(of | cat)=2/8
28

Monday, August 17, 2009

Estimating q* from a hypergraph:
dynamic programming
S 0,4
S→ X0 , X0
S→ X0 , X0

X 0,4 the · · · cat

X 0,4 a · · · mat

X→ X0 de X1 , X1 on X0
X→ X0 de X1 , X0 ’s X1

X→ X0 de X1 , X1 of X0

X→ X0 de X1 , X0 X1

X 0,2 the · · · mat
X→ dianzi shang, the mat

dianzi0 shang1

X 3,4 a · · · cat
X→ mao, a cat

de2

mao3

29
Monday, August 17, 2009

Estimating q* from a hypergraph:
dynamic programming
Bi-gram estimation:

S 0,4
S→ X0 , X0
S→ X0 , X0

X 0,4 the · · · cat

X 0,4 a · · · mat

X→ X0 de X1 , X1 on X0
X→ X0 de X1 , X0 ’s X1

X→ X0 de X1 , X1 of X0

X→ X0 de X1 , X0 X1

X 0,2 the · · · mat
X→ dianzi shang, the mat

dianzi0 shang1

X 3,4 a · · · cat
X→ mao, a cat

de2

mao3

29
Monday, August 17, 2009

Estimating q* from a hypergraph:
dynamic programming
Bi-gram estimation:

S 0,4
S→ X0 , X0
S→ X0 , X0

X 0,4 the · · · cat

X 0,4 a · · · mat

hypergraph

X→ X0 de X1 , X1 on X0
X→ X0 de X1 , X0 ’s X1

X→ X0 de X1 , X1 of X0

X→ X0 de X1 , X0 X1

X 0,2 the · · · mat
X→ dianzi shang, the mat

dianzi0 shang1

‣ run inside-outside on the

X 3,4 a · · · cat
X→ mao, a cat

de2

mao3

29
Monday, August 17, 2009

Estimating q* from a hypergraph:
dynamic programming
Bi-gram estimation:

S 0,4
S→ X0 , X0
S→ X0 , X0

X 0,4 the · · · cat

X 0,4 a · · · mat

hypergraph

X→ X0 de X1 , X1 on X0
X→ X0 de X1 , X0 ’s X1

X→ X0 de X1 , X1 of X0

X→ X0 de X1 , X0 X1

X 0,2 the · · · mat
X→ dianzi shang, the mat

dianzi0 shang1

‣ run inside-outside on the

X 3,4 a · · · cat

‣ accumulate the soft-count of
each bigram at each hyperedge

X→ mao, a cat

de2

mao3

29
Monday, August 17, 2009

Estimating q* from a hypergraph:
dynamic programming
Bi-gram estimation:

S 0,4
S→ X0 , X0
S→ X0 , X0

X 0,4 the · · · cat

X 0,4 a · · · mat

hypergraph

X→ X0 de X1 , X1 on X0
X→ X0 de X1 , X0 ’s X1

X→ X0 de X1 , X1 of X0

X→ X0 de X1 , X0 X1

X 0,2 the · · · mat
X→ dianzi shang, the mat

dianzi0 shang1

‣ run inside-outside on the

X 3,4 a · · · cat

‣ accumulate the soft-count of
each bigram at each hyperedge

X→ mao, a cat

de2

mao3

‣ normalize the counts

29
Monday, August 17, 2009

Decoding using q*∈Q

30
Monday, August 17, 2009

Decoding using q*∈Q

• Rescore the hypergraph HG(x)

30
Monday, August 17, 2009

Decoding using q*∈Q

• Rescore the hypergraph HG(x)
y

∗

=

arg max q (y|x)
y∈HG(x)

30
Monday, August 17, 2009

∗

Decoding using q*∈Q

• Rescore the hypergraph HG(x)
y

∗

=

arg max q (y|x)
y∈HG(x)

30
Monday, August 17, 2009

∗

Decoding using q*∈Q

• Rescore the hypergraph HG(x)
y

∗

=

arg max q (y|x)
y∈HG(x)

30
Monday, August 17, 2009

∗

q* is an n-gram model.

Decoding using q*∈Q

• Rescore the hypergraph HG(x)
y

•

∗

=

arg max q (y|x)
y∈HG(x)

q* is an n-gram model.

have efﬁcient dynamic programming algorithms

•

score the hypergraph using an n-gram model

30
Monday, August 17, 2009

∗

Decoding using q*∈Q

• Rescore the hypergraph HG(x)
y

•

∗

=

arg max q (y|x)
y∈HG(x)

∗

q* is an n-gram model.

have efﬁcient dynamic programming algorithms

•

score the hypergraph using an n-gram model

John already told you how to do this☺
30
Monday, August 17, 2009

KL divergences under different variational models
q

∗

=

arg min KL(p||q) = H(p, q) − H(p)
q∈Q

31
Monday, August 17, 2009

KL divergences under different variational models
q

∗

=

Measure
bits/word
MT’04
MT’05

arg min KL(p||q) = H(p, q) − H(p)
q∈Q

H(p)
1.36
1.37

∗
q1

0.97
0.94

31
Monday, August 17, 2009

KL(p||·)
∗
∗
q2
q3
0.32 0.21
0.32 0.21

∗
q4

0.17
0.17

KL divergences under different variational models
q

∗

=

Measure
bits/word
MT’04
MT’05

arg min KL(p||q) = H(p, q) − H(p)
q∈Q

H(p)
1.36
1.37

∗
q1

0.97
0.94

KL(p||·)
∗
∗
q2
q3
0.32 0.21
0.32 0.21

∗
q4

0.17
0.17

• The larger the order n is, the smaller the KL
divergence is!

• The reduction of KL divergence happens
mostly when switching from unigram to
bigram
31
Monday, August 17, 2009

KL divergences under different variational models
q

∗

=

Measure
bits/word
MT’04
MT’05

arg min KL(p||q) = H(p, q) − H(p)
q∈Q

H(p)
1.36
1.37

∗
q1

0.97
0.94

32
Monday, August 17, 2009

KL(p||·)
∗
∗
q2
q3
0.32 0.21
0.32 0.21

∗
q4

0.17
0.17

KL divergences under different variational models
q

∗

=

Measure
bits/word
MT’04
MT’05

arg min KL(p||q) = H(p, q) − H(p)
q∈Q

H(p)
1.36
1.37

∗
q1

0.97
0.94

KL(p||·)
∗
∗
q2
q3
0.32 0.21
0.32 0.21

∗
q4

0.17
0.17

How to compute them on a hypergraph?
see (Li and Eisner, EMNLP’09)

32
Monday, August 17, 2009

BLEU scores when using a single
variational n-gram model
Decoding scheme
Viterbi
1gram
2gram
3gram
4gram

MT’04
35.4
25.9
36.1
36.0
35.8

33
Monday, August 17, 2009

MT’05
32.6
24.5
33.4
33.1
32.9

BLEU scores when using a single
variational n-gram model
Decoding scheme
Viterbi
1gram
2gram
3gram
4gram

•

MT’04
35.4
25.9
36.1
36.0
35.8

unigram performs very badly

33
Monday, August 17, 2009

MT’05
32.6
24.5
33.4
33.1
32.9

BLEU scores when using a single
variational n-gram model
Decoding scheme
Viterbi
1gram
2gram
3gram
4gram

•
•

MT’04
35.4
25.9
36.1
36.0
35.8

unigram performs very badly
bigram achieves best BLEU scores

33
Monday, August 17, 2009

MT’05
32.6
24.5
33.4
33.1
32.9

BLEU scores when using a single
variational n-gram model
Decoding scheme
Viterbi
1gram
2gram
3gram
4gram

•
•

MT’04
35.4
25.9
36.1
36.0
35.8

unigram performs very badly
bigram achieves best BLEU scores

33
Monday, August 17, 2009

MT’05
32.6
24.5
33.4
33.1
32.9

???

BLEU scores when using a single
variational n-gram model
Decoding scheme
Viterbi
1gram
2gram
3gram
4gram

•
•

MT’04
35.4
25.9
36.1
36.0
35.8

MT’05
32.6
24.5
33.4
33.1
32.9

unigram performs very badly
bigram achieves best BLEU scores

modeling error in p
33
Monday, August 17, 2009

???

34
Monday, August 17, 2009

BLEU cares about both low- and high-order
n-gram matches

34
Monday, August 17, 2009

BLEU cares about both low- and high-order
n-gram matches

•

Interpolating variational n-gram model for different n
y = arg max
∗

y∈HG(x)

34
Monday, August 17, 2009

n

θn · log

∗
qn (y

| x)

BLEU cares about both low- and high-order
n-gram matches

•

Interpolating variational n-gram model for different n
y = arg max
∗

y∈HG(x)

n

θn · log

∗
qn (y

| x)

Viterbi and variational are different ways in
approximating p

34
Monday, August 17, 2009

BLEU cares about both low- and high-order
n-gram matches

•

Interpolating variational n-gram model for different n
y = arg max
∗

y∈HG(x)

n

θn · log

∗
qn (y

| x)

Viterbi and variational are different ways in
approximating p
y = arg max
∗

y∈HG(x)

n

θn · log

∗
qn (y

34
Monday, August 17, 2009

| x) + θv · log pViterbi (y | x)

BLEU cares about both low- and high-order
n-gram matches

•

Interpolating variational n-gram model for different n
y = arg max
∗

y∈HG(x)

n

θn · log

∗
qn (y

| x)

Viterbi and variational are different ways in
approximating p
y = arg max
∗

y∈HG(x)

n

θn · log

∗
qn (y

34
Monday, August 17, 2009

| x) + θv · log pViterbi (y | x)

Minimum Bayes Risk (MBR)
decoding?
(Tromble et al. 2008)
(Denero et al. 2009)

35
Monday, August 17, 2009

Minimum Risk Decoding

•

Maximum A Posterior (MAP) decoding

•

ﬁnd the most probable translation string
y

•

∗

=

arg max p(y|x)
y∈HG(x)

Minimum risk decoding

•

ﬁnd the consensus translation string
y

∗

=

Risk(y)

arg min Risk(y)
y∈HG(x)

=
y

L(y, y )p(y |x)
36

Monday, August 17, 2009

Variational Decoding(VD) vs. MBR (Tromble et al. 2008)

37
Monday, August 17, 2009

Variational Decoding(VD) vs. MBR (Tromble et al. 2008)

spurious ambiguity

37
Monday, August 17, 2009

Variational Decoding(VD) vs. MBR (Tromble et al. 2008)

VD
spurious ambiguity

37
Monday, August 17, 2009

consensus

Variational Decoding(VD) vs. MBR (Tromble et al. 2008)

VD
spurious ambiguity

37
Monday, August 17, 2009

consensus

Variational Decoding(VD) vs. MBR (Tromble et al. 2008)

MBR

VD
spurious ambiguity

37
Monday, August 17, 2009

consensus

Variational Decoding(VD) vs. MBR (Tromble et al. 2008)

MBR

Interpolated VD

VD
spurious ambiguity

37
Monday, August 17, 2009

consensus

Variational Decoding(VD) vs. MBR (Tromble et al. 2008)

MBR

Interpolated VD

VD
spurious ambiguity

Both BLEU metric and our variational distributions
happen to use n-gram dependencies.
37
Monday, August 17, 2009

•

Variational decoding with interpolation
y

=

∗

arg max

y∈HG(x)

qn (y | x) =

n

| x)

cw (y)

w∈Wn

q(r(w) | h(w), x)

•

θn · log

∗
qn (y

q(r(w) | h(w), x)
y

=
y

cw (y )p(y | x)

ch(w) (y )p(y | x)

Minimum risk decoding (Tromble et al. 2008)
y

∗

=

arg max

gn (y | x) =
g(w | x) =

y∈HG(x)

w∈Wn

y

n

θn · gn (y | x)

g(w | x)cw (y)

δw (y )p(y | x)
38

Monday, August 17, 2009

•

Variational decoding with interpolation
y

=

∗

arg max

y∈HG(x)

qn (y | x) =

n

| x) decision rule

cw (y)

w∈Wn

q(r(w) | h(w), x)

•

θn · log

∗
qn (y

q(r(w) | h(w), x)
y

=
y

cw (y )p(y | x)

ch(w) (y )p(y | x)

Minimum risk decoding (Tromble et al. 2008)
y

∗

=

arg max

gn (y | x) =
g(w | x) =

y∈HG(x)

w∈Wn

y

n

θn · gn (y | x)

g(w | x)cw (y)

δw (y )p(y | x)
38

Monday, August 17, 2009

decision rule

•

Variational decoding with interpolation
y

=

∗

arg max

y∈HG(x)

qn (y | x) =

n

| x) decision rule

cw (y)

w∈Wn

q(r(w) | h(w), x)

•

θn · log

∗
qn (y

q(r(w) | h(w), x)
y

=
y

n-gram model

cw (y )p(y | x)

ch(w) (y )p(y | x)

Minimum risk decoding (Tromble et al. 2008)
y

∗

=

arg max

gn (y | x) =
g(w | x) =

y∈HG(x)

w∈Wn

y

n

θn · gn (y | x)

g(w | x)cw (y)

δw (y )p(y | x)
38

Monday, August 17, 2009

decision rule
n-gram model

•

Variational decoding with interpolation
y

=

∗

arg max

y∈HG(x)

qn (y | x) =

n

| x) decision rule

cw (y)

w∈Wn

q(r(w) | h(w), x)

•

θn · log

∗
qn (y

q(r(w) | h(w), x)
y

=
y

cw (y )p(y | x)

ch(w) (y )p(y | x)

n-gram model

n-gram probability

Minimum risk decoding (Tromble et al. 2008)
y

∗

=

arg max

gn (y | x) =
g(w | x) =

y∈HG(x)

w∈Wn

y

n

θn · gn (y | x)

g(w | x)cw (y)

δw (y )p(y | x)
38

Monday, August 17, 2009

decision rule
n-gram model

n-gram probability

•

Variational decoding with interpolation
y

=

∗

arg max

y∈HG(x)

qn (y | x) =

n

| x)

cw (y)

w∈Wn

q(r(w) | h(w), x)

•

θn · log

∗
qn (y

q(r(w) | h(w), x)
y

=
y

cw (y )p(y | x)

ch(w) (y )p(y | x)

Minimum risk decoding (Tromble et al. 2008)
y

∗

=

arg max

gn (y | x) =
g(w | x) =

y∈HG(x)

w∈Wn

y

n

θn · gn (y | x)

g(w | x)cw (y)

δw (y )p(y | x)
39

Monday, August 17, 2009

•

Variational decoding with interpolation
y

=

∗

arg max

y∈HG(x)

qn (y | x) =

n

| x)

cw (y)

w∈Wn

q(r(w) | h(w), x)

•

θn · log

∗
qn (y

q(r(w) | h(w), x)
y

=
y

cw (y )p(y | x)

ch(w) (y )p(y | x)

Minimum risk decoding (Tromble et al. 2008)
y

∗

=

arg max

gn (y | x) =
g(w | x) =

y∈HG(x)

w∈Wn

y

n

θn · gn (y | x)

g(w | x)cw (y)

δw (y )p(y | x)
39

Monday, August 17, 2009

non-probabilistic
very expensive to
compute

BLEU Results on Chinese-English
NIST MT Tasks
Decoding scheme
Viterbi
MBR (K=1000)
Crunching (N =10000)
Crunching+MBR (N =10000)
Variational (1to4gram+wp+vt)

40
Monday, August 17, 2009

MT’04
35.4
35.8
35.7
35.8
36.6

MT’05
32.6
32.7
32.8
32.7
33.5

BLEU Results on Chinese-English
NIST MT Tasks
Decoding scheme
Viterbi
MBR (K=1000)
Crunching (N =10000)
Crunching+MBR (N =10000)
Variational (1to4gram+wp+vt)

•

MT’04
35.4
35.8
35.7
35.8
36.6

MT’05
32.6
32.7
32.8
32.7
33.5

variational decoding improves over Viterbi, MBR, and crunching

40
Monday, August 17, 2009

Conclusions

• Exact MAP decoding with spurious ambiguity is
intractable

• Viterbi or N-best approximations are efﬁcient,
but ignore most derivations

• We developed a variational approximation, which
considers all derivations but still allows tractable
decoding

• Our variational decoding improves a state of the
art baseline

41
Monday, August 17, 2009

Future directions

• The MT pipeline is full of intractable problems
•

variational approximation is a principled way to tackle
these problems

• Decoding with spurious ambiguity is a common
problem in many other NLP applications

•
•
•
•

Models with latent variables
Data oriented parsing (DOP)
Hidden Markov Models (HMM)
......
42

Monday, August 17, 2009

Thank you!
谢谢！

43
Monday, August 17, 2009

Joshua
44
Monday, August 17, 2009

1

S 0,4
S→ X0 , X0
S→ X0 , X0

X 0,4 the · · · cat

X 0,4 a · · · mat

X→ X0 de X1 , X1 on X0

p(y, d | x)
X→ X0 de X1 , X0 ’s X1

X→ X0 de X1 , X1 of X0

X→ X0 de X1 , X0 X1

X 0,2 the · · · mat

X 3,4 a · · · cat

X→ dianzi shang, the mat

X→ mao, a cat

dianzi0 shang1

2

de2

mao3

S 0,4
S→ X0 , X0
S→ X0 , X0

X 0,4 the · · · cat

X 0,4 a · · · mat

X→ X0 de X1 , X1 on X0

p(y, d | x)
X→ X0 de X1 , X0 ’s X1

X→ X0 de X1 , X1 of X0

X→ X0 de X1 , X0 X1

X 0,2 the · · · mat

Estimate a model
from the hypergraph

X 3,4 a · · · cat

X→ dianzi shang, the mat

de2

mao3

S 0,4
S→ X0 , X0
S→ X0 , X0

X 0,4 the · · · cat

X 0,4 a · · · mat

X→ X0 de X1 , X1 on X0

q*(y | x)
X→ X0 de X1 , X0 ’s X1

X→ X0 de X1 , X1 of X0

X→ X0 de X1 , X0 X1

X 0,2 the · · · mat
X→ dianzi shang, the mat

dianzi0 shang1

Monday, August 17, 2009

X 3,4 a · · · cat
X→ mao, a cat

de2

mao3

q* is an n-gram model
over output strings.

q*(y | x)
≈∑d∈D(x,y) p(y,d|x)

X→ mao, a cat

dianzi0 shang1

3

Generate a hypergraph

Decode using q*
on the hypergraph

