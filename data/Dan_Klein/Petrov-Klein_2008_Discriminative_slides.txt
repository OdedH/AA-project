Discriminative Log-Linear Grammars with Latent Variables
Slav Petrov and Dan Klein
University of California, Berkeley
Training

Motivation

Results

Grammar Learning

Generative Training

Discriminative Training

The observed treebank categories are too coarse
because the rewrite probabilities depend on context.

Maximize the joint likelihood:

Maximize the conditional likelihood:

Ljoint (θ) = log

S
All NPs

VP

NP
PRP VBD

NPs under S

.
NP

11%

9%

9%

heard DT

i

i

θ = argmax log

Lcond (θ) = log

t:Ti

θ

9%

Pθ (t, wi )

∗

23%

i

7%

6%

She

Pθ (t, wi )

i

Pθ (Ti |wi ) = log

Pθ (t|wi )
i

θ = argmax log

Pθ (t|wi )

∗

θ

t:Ti

i

t:Ti

4%

DT NN

NP PP

PRP

DT NN

PRP

NP PP

DT NN

The parameters can be learned with an Expectation Maximization
algorithm. The E-Step involves computing expectations over derivations
corresponding to the observed trees. These expectations are
normalized in the M-Step to update the rewrite probabilities:

PRP

the noise

φX→γ =

Given a treebank over a set of categories
learn an optimally reﬁned grammar for parsing.

PRP-x VBD-x

The parameters can be learned with a numerical gradient based
method (e.g. L-BFGS). Computing the gradient involves calculating
expectations over derivations corresponding to the observed trees,
as well as over all possible trees:

Eθ [fX→γ (t)|T ]
T Eθ [fX→γ (t)|T ]

∂Lcond (θ)
=
∂θX→γ

Computing expectations over derivations corresponding to
the observed trees can be done in linear time
(in the number of words).

.-x
NP-x

T
γ

S-x
VP-x

91 %

Eθ [fX→γ (t)|Ti ] − Eθ [fX→γ (t)|wi ]

i

Computing expectations over derivations corresponding to all possible
trees involves parsing the training corpus, which requires cubic time
(in the number of words).

90%
89%

89 %

88%

87 %

87%

83 %

Discriminative

81 %

Generative

40%

State-of-the-art
(Petrov et al. '06)

37%

79 %
77 %

Exact Match

75 %

Grammars with Latent Variables

NP-x

Parsing performance (F1-score)

85 %

NN
NP PP

F1-score

t:Ti

NPs under VP

21%

.

Pθ (Ti , wi ) = log

Grammars were trained on the Wall Street Journal section of the
Penn Treebank using the standard splits. The training set contains
roughly 1M words in 40K sentences.

34%

1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16
Number of subcategories

31%

Discriminative training is superior to generative training
in terms of F1-score and exact match.

Time per training iteration
2000 min

.
1500 min

She

heard

No Pruning

DT-x NN-x
1000 min

the

noise

Coarse-to-Fine Pruning
Cached Pruning

Efﬁcient Estimation

500 min
0 min

Automatic Grammar Reﬁnement

Hierarchical Splitting

Feature Count Approximation

Reﬁne the observed trees with latent variables
and learn subcategories.

Repeatedly split each category in two and retrain
the grammar, initializing with the previous grammar.

Use predictions from hierarchical coarse-to-ﬁne parsing to
prune unlikely chart items, setting their expectations to zero.

the
a
The

S-1
NP-10

S
NP

VP-11

VP

was

.

PRP-2 VBD-16 ADJP-14 .

.

He

was

PRP VBD ADJP .
He

Grammar G
S1 → NP10 VP11
S2 → NP17 VP23
NP10 → PRP2
NP17 → PRP2
...

right

right
S-2

NP-17

VP-23

Lexicon
PRP2 → She
VBD16 → was
VBD12 → was
...

.

PRP-2 VBD-12 ADJP-11 .

Parse Tree T
Sentence w

He

was

right

Parse Derivations ti : T

1
Pθ (t|w) =
Z(θ, w)

e
X→γ∈t

θX→γ

?
?
?
?

1
θ T f (t)
=
e
Z(θ, w)

...

VP NP QP

Coarse-to-Fine pruning gives a tremendous speed-up over
exhaustive parsing, but only cached pruning makes large scale
training of discriminative grammars practically feasible.

...

Number of non-zero features

that
this
some

6M

the
a
The

G1

...

VP1 VP2 NP1 NP2

...

5M
4M

?
?
?

Model Parameters θ

G0

1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16
Number of subcategories

this
that
That

some
all
those

the
The
a

a
the
an

G2

...

VP3 VP4

...

NP3 NP4

3M
2M

this
that
another

That
This
each

some
all
those

these
both
Some

the
a
The

The
A
No

a
an
the

a
an
the

1M

G3

...

VP6 VP7

...

NP3 NP4

...

L2-regularization
L1-regularization

0M
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16
Number of subcategories

Linguistically interpretable subcategories emerge
in the course of hierarchical reﬁnement.

Coarse-to-ﬁne pruning leads to few brackets with non-zero feature counts.
In cached pruning only the ﬁnest level of the hierarchy is updated.

L1-regularization leads to extremely sparse grammars without
decreasing the parsing performance.

