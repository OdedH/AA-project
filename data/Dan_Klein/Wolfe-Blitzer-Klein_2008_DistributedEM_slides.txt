Fully Distributed EM for Very Large Datasets
Jason Wolfe

Aria Haghighi

Computer Science Division
UC Berkeley

Dan Klein

Overview

Task: unsupervised learning via EM

‫ﺍﻟﻟﻮﻮﻻﻳﺎﺎﺕ ﺍﺍﳌﳌﺘﺤﺪﺓﺓ‬
‫ﺍ ﻻ ﻳ ﺕ ﺘﺤﺪ‬
‫ﺴﻀ ﺤ‬
‫ﻮ‬
‫ﺍﺗﻟﺗﺴﻻﻳﻳﺎﺎﺕﻴﺍﺍﳌﳌﺘﺘﻣﺆﺆﺪﺓﺓﺮﺮ‬
‫ﺍﻟﻮﻻﺘ ﺕ ﻒ ﺤﺪﲤ‬
‫ﺗﺗﺴﺘﺘﺘﻀﻴﻴﻒﻣﻣﺆ ﲤﺮ‬
‫ﺴ ﺍﻟﻟﺴﻒﻡﻡ ﻓﻓ ﲤﺮ‬
‫ﻀﺴﻒ ﻣﺆﲤ‬
‫ﻀﻴ ﻼ ﻰ‬
‫ﺍﻟﺍﺴ ﻼ ﻓ ﻰ‬
‫ﺍﺍﻟﺴ ﻕﻡﺍ ﻓﻰ‬
‫ﺍﻟﻟﺸﺮﺮﻼﻡﺍﻻﻭﻰﺳﻂ‬
‫ﺸﻼ ﻻﻭ‬
‫ﺍﻟﺸﺮ ﻕ ﺍﻻﻭ ﺳﻂ‬
‫ﺍﻟ ﻓﻰﻕﺍﻻﺳ ﺳﻂ‬
‫ﻓ ﺮ ﺍ ﺳ ﺳﻂ‬
‫ﺸﻰﻕﺍﻻﻻﻭﺒﺒﻮﻮﻉ‬
‫ﻓﻰ ﺍﻻﺳﺒﻮ ﻉ‬
‫ﻓﻰ ﺍﻻﻟﺳﺩﻡ ﻉ‬
‫ﺍﻟﻘﺎﺩﺒﻮﻉ‬
‫ﺎ‬
‫ﺍﻟﺍﻘﻘﺩ ﻡ‬
‫ﺍﻟﻘﺎﺎﺩ ﻡ‬
‫ﻡ‬

US Hosts
US Hosts
US Hosts
US Hosts
Middle
Middle
Middle
Middle
East Peace
East Peace
East Peace
East Peace
Conference
Conference
Conference
Conference
Next Week
Next Week
Next Week
Next Week

Focus: models w/ many local parameters
(relevant to few datums)

millions of
parameters 

244

0
0

1

2

3

millions of data points

Approach: fully distributed, localized EM
parameter locality → less bandwidth

useful
work
communication
overhead

Overview

Task: unsupervised learning via EM

‫ﺍﻟﻟﻮﻮﻻﻳﺎﺎﺕ ﺍﺍﳌﳌﺘﺤﺪﺓﺓ‬
‫ﺍ ﻻ ﻳ ﺕ ﺘﺤﺪ‬
‫ﺴﻀ ﺤ‬
‫ﻮ‬
‫ﺍﺗﻟﺗﺴﻻﻳﻳﺎﺎﺕﻴﺍﺍﳌﳌﺘﺘﻣﺆﺆﺪﺓﺓﺮﺮ‬
‫ﺍﻟﻮﻻﺘ ﺕ ﻒ ﺤﺪﲤ‬
‫ﺗﺗﺴﺘﺘﺘﻀﻴﻴﻒﻣﻣﺆ ﲤﺮ‬
‫ﺴ ﺍﻟﻟﺴﻒﻡﻡ ﻓﻓ ﲤﺮ‬
‫ﻀﺴﻒ ﻣﺆﲤ‬
‫ﻀﻴ ﻼ ﻰ‬
‫ﺍﻟﺍﺴ ﻼ ﻓ ﻰ‬
‫ﺍﺍﻟﺴ ﻕﻡﺍ ﻓﻰ‬
‫ﺍﻟﻟﺸﺮﺮﻼﻡﺍﻻﻭﻰﺳﻂ‬
‫ﺸﻼ ﻻﻭ‬
‫ﺍﻟﺸﺮ ﻕ ﺍﻻﻭ ﺳﻂ‬
‫ﺍﻟ ﻓﻰﻕﺍﻻﺳ ﺳﻂ‬
‫ﻓ ﺮ ﺍ ﺳ ﺳﻂ‬
‫ﺸﻰﻕﺍﻻﻻﻭﺒﺒﻮﻮﻉ‬
‫ﻓﻰ ﺍﻻﺳﺒﻮ ﻉ‬
‫ﻓﻰ ﺍﻻﻟﺳﺩﻡ ﻉ‬
‫ﺍﻟﻘﺎﺩﺒﻮﻉ‬
‫ﺎ‬
‫ﺍﻟﺍﻘﻘﺩ ﻡ‬
‫ﺍﻟﻘﺎﺎﺩ ﻡ‬
‫ﻡ‬

US Hosts
US Hosts
US Hosts
US Hosts
Middle
Middle
Middle
Middle
East Peace
East Peace
East Peace
East Peace
Conference
Conference
Conference
Conference
Next Week
Next Week
Next Week
Next Week

Focus: models w/ many local parameters
(relevant to few datums)

millions of
parameters 

244

0
0

1

2

3

millions of data points

Approach: fully distributed, localized EM
parameter locality → less bandwidth

useful
work
communication
overhead

Overview

Task: unsupervised learning via EM

‫ﺍﻟﻟﻮﻮﻻﻳﺎﺎﺕ ﺍﺍﳌﳌﺘﺤﺪﺓﺓ‬
‫ﺍ ﻻ ﻳ ﺕ ﺘﺤﺪ‬
‫ﺴﻀ ﺤ‬
‫ﻮ‬
‫ﺍﺗﻟﺗﺴﻻﻳﻳﺎﺎﺕﻴﺍﺍﳌﳌﺘﺘﻣﺆﺆﺪﺓﺓﺮﺮ‬
‫ﺍﻟﻮﻻﺘ ﺕ ﻒ ﺤﺪﲤ‬
‫ﺗﺗﺴﺘﺘﺘﻀﻴﻴﻒﻣﻣﺆ ﲤﺮ‬
‫ﺴ ﺍﻟﻟﺴﻒﻡﻡ ﻓﻓ ﲤﺮ‬
‫ﻀﺴﻒ ﻣﺆﲤ‬
‫ﻀﻴ ﻼ ﻰ‬
‫ﺍﻟﺍﺴ ﻼ ﻓ ﻰ‬
‫ﺍﺍﻟﺴ ﻕﻡﺍ ﻓﻰ‬
‫ﺍﻟﻟﺸﺮﺮﻼﻡﺍﻻﻭﻰﺳﻂ‬
‫ﺸﻼ ﻻﻭ‬
‫ﺍﻟﺸﺮ ﻕ ﺍﻻﻭ ﺳﻂ‬
‫ﺍﻟ ﻓﻰﻕﺍﻻﺳ ﺳﻂ‬
‫ﻓ ﺮ ﺍ ﺳ ﺳﻂ‬
‫ﺸﻰﻕﺍﻻﻻﻭﺒﺒﻮﻮﻉ‬
‫ﻓﻰ ﺍﻻﺳﺒﻮ ﻉ‬
‫ﻓﻰ ﺍﻻﻟﺳﺩﻡ ﻉ‬
‫ﺍﻟﻘﺎﺩﺒﻮﻉ‬
‫ﺎ‬
‫ﺍﻟﺍﻘﻘﺩ ﻡ‬
‫ﺍﻟﻘﺎﺎﺩ ﻡ‬
‫ﻡ‬

US Hosts
US Hosts
US Hosts
US Hosts
Middle
Middle
Middle
Middle
East Peace
East Peace
East Peace
East Peace
Conference
Conference
Conference
Conference
Next Week
Next Week
Next Week
Next Week

Focus: models w/ many local parameters
(relevant to few datums)

millions of
parameters 

244

0
0

1

2

3

millions of data points

Approach: fully distributed, localized EM
parameter locality → less bandwidth

useful
work
communication
overhead

Outline

Running example: IBM Model 1 for word alignment

Naive distributed EM

Efﬁciently distributed EM

Word alignment for machine translation

Goal: parallel sentences →
word-level translation model
Parameters θs t :
probability that Spanish word s
translates to English word t

 θla the

 θ
 la chair



 θla table

θ
θ=
 silla the
 θsilla chair



 θ
 mesa the


θmesa table

la silla

la mesa

the chair

the table

corpus of parallel sentences

Word alignment for machine translation

Goal: parallel sentences →
word-level translation model
Parameters θs t :
probability that Spanish word s
translates to English word t

 θla the

 θ
 la chair



 θla table

θ
θ=
 silla the
 θsilla chair



 θ
 mesa the


θmesa table

la silla

la mesa

the chair

the table

corpus of parallel sentences

la silla

la mesa

the chair

the table

possible alignment arcs

Word alignment for machine translation

Goal: parallel sentences →
word-level translation model
Parameters θs t :
probability that Spanish word s
translates to English word t

 θla the

 θ
 la chair



 θla table

θ
θ=
 silla the
 θsilla chair



 θ
 mesa the


θmesa table

=
=
=
=
=
=
=

1.0
0.0
0.0
0.0
1.0
0.0
1.0

la silla

la mesa

the chair

the table

corpus of parallel sentences

la silla

la mesa

the chair

the table

possible alignment arcs

la silla

la mesa

the chair

the table

unobserved true alignments

IBM Model 1 for word alignment

a Steve no le gustan las ferias grandes
Steve does not like big ferris wheels
?
?
? ? ?
?
?

each target word is generated by exactly
one source word chosen u.a.r

IBM Model 1: a simple generative model
For each target position i, independently
choose a source index ai u.a.r.
choose a target word Ti ∼ θSai ·

IBM Model 1 for word alignment

a Steve no le gustan las ferias grandes
Steve does not like big ferris wheels
?
?
? ? ?
?
?

each target word is generated by exactly
one source word chosen u.a.r

IBM Model 1: a simple generative model
For each target position i, independently
choose a source index ai u.a.r.
choose a target word Ti ∼ θSai ·

IBM Model 1 for word alignment

a Steve no le gustan las ferias grandes
Steve does not like big ferris wheels
?
? ? ?
?
?

each target word is generated by exactly
one source word chosen u.a.r

IBM Model 1: a simple generative model
For each target position i, independently
choose a source index ai u.a.r.
choose a target word Ti ∼ θSai ·

IBM Model 1 for word alignment

a Steve no le gustan las ferias grandes
Steve does not like big ferris wheels
?
? ? ?
?
?

each target word is generated by exactly
one source word chosen u.a.r

IBM Model 1: a simple generative model
For each target position i, independently
choose a source index ai u.a.r.
choose a target word Ti ∼ θSai ·

IBM Model 1 for word alignment

a Steve no le gustan las ferias grandes
Steve does not like big ferris wheels
? ? ?
?
?

each target word is generated by exactly
one source word chosen u.a.r

IBM Model 1: a simple generative model
For each target position i, independently
choose a source index ai u.a.r.
choose a target word Ti ∼ θSai ·

IBM Model 1 for word alignment

a Steve no le gustan las ferias grandes
Steve does not like big ferris wheels
? ? ?
?
?

each target word is generated by exactly
one source word chosen u.a.r

IBM Model 1: a simple generative model
For each target position i, independently
choose a source index ai u.a.r.
choose a target word Ti ∼ θSai ·

IBM Model 1 for word alignment

a Steve no le gustan las ferias grandes
Steve does not like big ferris wheels
? ?
?
?

each target word is generated by exactly
one source word chosen u.a.r

IBM Model 1: a simple generative model
For each target position i, independently
choose a source index ai u.a.r.
choose a target word Ti ∼ θSai ·

IBM Model 1 for word alignment

a Steve no le gustan las ferias grandes
Steve does not like big ferris wheels
? ?
?
?

each target word is generated by exactly
one source word chosen u.a.r

IBM Model 1: a simple generative model
For each target position i, independently
choose a source index ai u.a.r.
choose a target word Ti ∼ θSai ·

IBM Model 1 for word alignment

a Steve no le gustan las ferias grandes
Steve does not like big ferris wheels
?
?
?

each target word is generated by exactly
one source word chosen u.a.r

IBM Model 1: a simple generative model
For each target position i, independently
choose a source index ai u.a.r.
choose a target word Ti ∼ θSai ·

IBM Model 1 for word alignment

a Steve no le gustan las ferias grandes
Steve does not like big ferris wheels

each target word is generated by exactly
one source word chosen u.a.r

IBM Model 1: a simple generative model
For each target position i, independently
choose a source index ai u.a.r.
choose a target word Ti ∼ θSai ·

EM algorithm for IBM Model 1
θ ← some initial guess

θla the =.33, θla chair =.33,
θla table =.33, θsilla the =.5,...

EM algorithm for IBM Model 1
θ ← some initial guess

Iterate:
1

E-step: estimate alignment counts η
1

compute posteriors p(ai |θ)

θla the =.33, θla chair =.33,
θla table =.33, θsilla the =.5,...

.33
.33+.5

= .4

la silla

.5
.6 = .33+.5

the chair

EM algorithm for IBM Model 1
θ ← some initial guess

Iterate:
1

E-step: estimate alignment counts η
1

θla the =.33, θla chair =.33,
θla table =.33, θsilla the =.5,...

.33
.33+.5

= .4

compute posteriors p(ai |θ)

la silla

la silla

.4

la mesa

.6
the chair

.5
.6 = .33+.5

the chair

.4

.6
the table

EM algorithm for IBM Model 1
θla the =.33, θla chair =.33,
θla table =.33, θsilla the =.5,...

θ ← some initial guess

Iterate:
1

.33
.33+.5

E-step: estimate alignment counts η
compute posteriors p(ai |θ)
aggregate into expected counts ηs
(expected # times s t under θ)

1
2

ηs

t

←
C

θs t
i θSi

t

t

= .4

la silla

la silla

.4

la mesa

.6
the chair

.5
.6 = .33+.5

the chair

.4

.6
the table

ηla the =.8, ηla chair =.4,
ηla table =.4, ηsilla the =.6,...

EM algorithm for IBM Model 1
θla the =.33, θla chair =.33,
θla table =.33, θsilla the =.5,...

θ ← some initial guess

Iterate:
1

E-step: estimate alignment counts η
compute posteriors p(ai |θ)
aggregate into expected counts ηs
(expected # times s t under θ)

1
2

ηs

t

θs t
i θSi

←
C

2

.33
.33+.5

t

M-step: normalize η to get new ML θ
θs

t

←

ηs t
t ηs

t

t

= .4

la silla

la silla

.4

la mesa

.6
the chair

.5
.6 = .33+.5

the chair

.4

.6
the table

ηla the =.8, ηla chair =.4,
ηla table =.4, ηsilla the =.6,...
θla the =.5, θla chair =.25,
θla table =.25, θsilla the =.5,...

EM example continued

E-Step 1
la silla

la mesa

the chair

the table

EM example continued

E-Step 2
la silla

la mesa

the chair

the table

EM example continued

E-Step 3
la silla

la mesa

the chair

the table

EM example continued

E-Step 4
la silla

la mesa

the chair

the table

EM example continued

E-Step 5
la silla

la mesa

the chair

the table

EM example continued

E-Step ∞
la silla

la mesa

the chair

the table

UN Arabic English TIDES v2 corpus

‫ﺍﻟﻟﻮﻮﻻﻳﺎﺎﺕ ﺍﺍﳌﳌﺘﺤﺪﺓﺓ‬
‫ﺍ ﻻ ﻳ ﺕ ﺘﺤﺪ‬
‫ﺴﻀ ﺤ‬
‫ﻮ‬
‫ﺍﺗﻟﺗﺴﻻﻳﻳﺎﺎﺕﻴﺍﺍﳌﳌﺘﺘﻣﺆﺆﺪﺓﺓﺮﺮ‬
‫ﺍﻟﻮﻻﺘ ﺕ ﻒ ﺤﺪﲤ‬
‫ﺗﺗﺴﺘﺘﺘﻀﻴﻴﻒﻣﻣﺆ ﲤﺮ‬
‫ﺴ ﺍﻟﻟﺴﻒﻡﻡ ﻓﻓ ﲤﺮ‬
‫ﻀﺴﻒ ﻣﺆﲤ‬
‫ﻀﻴ ﻼ ﻰ‬
‫ﺍﻟﺍﺴ ﻼﻡ ﻓ ﻰ‬
‫ﺍﺍﺸﺮﻼﻡﺍﻻﻰ‬
‫ﺍﻟﻟﻟﺴ ﻕ ﺍ ﻓﻰﺳﻂ‬
‫ﺍﻟ ﺸﺮ ﻼ ﺍ ﻻﻭﻭﺳﻂ‬
‫ﻕ‬
‫ﺍﻟ ﺸﻰﻕﺍﻻﻻﻭﺳﻂ‬
‫ﺸﻰ ﺍﻻﻭﺳﻂ‬
‫ﻓﻓﺮﻕﺍﻻ ﺳﺒﺒﻮﻮﻉ‬
‫ﻓ ﺮ ﺍﻻ ﺳﺒﻮ ﻉ‬
‫ﻓ ﻰ ﺍﻻﻟ ﺳﺩﻡ ﻉ‬
‫ﻰ ﺍﻟﻘﺎﺩﺒﻮﻉ‬
‫ﻘﺎ‬
‫ﺍﻟﺍﻘﺳ ﻡ‬
‫ﺍﻟﻘﺎﺎﺩﻡ‬
‫ﺩﻡ‬

US Hosts
US Hosts
US Hosts
US Hosts
Middle
Middle
Middle
Middle
East Peace
East Peace
East Peace
East Peace
Conference
Conference
Conference
Conference
Next Week
Next Week
Next Week
Next Week

2.9 million sentence pairs from UN proceedings
243 million unique word pairs
(translations possible in some sentence pair)
243 M parameters in θ
243 M counts in η

Even ﬁtting all (indexed) parameters in
32-bit memory can be challenging

Outline

Running example: IBM Model 1 for word alignment

Naive distributed EM

Efﬁciently distributed EM

Previous approach: distributing the E-step
la silla
the chair

1

E-step computations distribute easily
partition data over k nodes
alignments independent given θ

E-step 1
2

3

Reduce,
M-step

Nodes communicate partial counts
to central Reduce node
Reduce node does global M-step

4

Reduce sends new parameters back
Remaining problems:

E-step 2
la mesa

Memory at Reduce node
C-step (communication) bandwidth:
5.5 B numbers per iteration
(on full dataset with 20 nodes)

the table

(Chu et al. 2006, Dyer et al. 2008, Newman et al. 2008, ...)

Previous approach: distributing the E-step
la silla
the chair

1

E-step computations distribute easily
partition data over k nodes
alignments independent given θ

E-step 1
2

3

Reduce,
M-step

Nodes communicate partial counts
to central Reduce node
Reduce node does global M-step

4

Reduce sends new parameters back
Remaining problems:

E-step 2
la mesa

Memory at Reduce node
C-step (communication) bandwidth:
5.5 B numbers per iteration
(on full dataset with 20 nodes)

the table

(Chu et al. 2006, Dyer et al. 2008, Newman et al. 2008, ...)

Previous approach: distributing the E-step
la silla
the chair

1

partition data over k nodes
alignments independent given θ

E-step 1

ηla the
ηla chair
ηsilla the
ηsilla chair
Reduce,
M-step

ηla the
ηla table
ηmesa the
ηmesa table

E-step 2
la mesa

E-step computations distribute easily

2

Nodes communicate partial counts
to central Reduce node

3

Reduce node does global M-step

4

Reduce sends new parameters back
Remaining problems:
Memory at Reduce node
C-step (communication) bandwidth:
5.5 B numbers per iteration
(on full dataset with 20 nodes)

the table

(Chu et al. 2006, Dyer et al. 2008, Newman et al. 2008, ...)

Previous approach: distributing the E-step
la silla
the chair

1

partition data over k nodes
alignments independent given θ

E-step 1

ηla the
ηla chair
ηsilla the
ηsilla chair
Reduce,
M-step

ηla the
ηla table
ηmesa the
ηmesa table

E-step 2
la mesa

E-step computations distribute easily

2

Nodes communicate partial counts
to central Reduce node

3

Reduce node does global M-step

4

Reduce sends new parameters back
Remaining problems:
Memory at Reduce node
C-step (communication) bandwidth:
5.5 B numbers per iteration
(on full dataset with 20 nodes)

the table

(Chu et al. 2006, Dyer et al. 2008, Newman et al. 2008, ...)

Previous approach: distributing the E-step
la silla
the chair

1

partition data over k nodes
alignments independent given θ

E-step 1

ηla the
ηla chair
ηsilla the
ηsilla chair

All θ
(7 params)

E-step 2
la mesa

2

Nodes communicate partial counts
to central Reduce node

3

All θ
(7 params)

Reduce,
M-step

ηla the
ηla table
ηmesa the
ηmesa table

E-step computations distribute easily

Reduce node does global M-step

4

Reduce sends new parameters back
Remaining problems:
Memory at Reduce node
C-step (communication) bandwidth:
5.5 B numbers per iteration
(on full dataset with 20 nodes)

the table

(Chu et al. 2006, Dyer et al. 2008, Newman et al. 2008, ...)

Previous approach: distributing the E-step
la silla
the chair

1

partition data over k nodes
alignments independent given θ

E-step 1

ηla the
ηla chair
ηsilla the
ηsilla chair

All θ
(7 params)

E-step 2
la mesa

2

Nodes communicate partial counts
to central Reduce node

3

All θ
(7 params)

Reduce,
M-step

ηla the
ηla table
ηmesa the
ηmesa table

E-step computations distribute easily

Reduce node does global M-step

4

Reduce sends new parameters back
Remaining problems:
Memory at Reduce node
C-step (communication) bandwidth:
5.5 B numbers per iteration
(on full dataset with 20 nodes)

the table

(Chu et al. 2006, Dyer et al. 2008, Newman et al. 2008, ...)

Speedup (on 200K total sentence pairs)
Iteration time vs. # of E-step nodes
MapReduce

Iteration time (s)

250

M-Step
C-Step
E-Step

200
150
100
50
0
1 2

5

10

# of nodes

20

Common practical solutions

Memory and bandwidth are real problems in practice
Workarounds
Use less data
Ignore rare words
Train on independent chunks
Swap to disk
Distribute over multiple machines

Outline

Running example: IBM Model 1 for word alignment

Naive distributed EM

Efﬁciently distributed EM

Distributing the M-step locally
Distribute M-step alongside E-step
la silla
the chair
E-step 1
M-step 1

Nodes store only needed params,
compute them locally
Reduce passes back counts
Don’t need to hear about
irrelevant source words

Reduce

Don’t need to tell (or hear) about
purely local source words
Need to hear everything about each
source word: M-step denominator

E-step 2
M-step 2
la mesa
the table

θs

t

←

ηs t
t ηs

t

Bandwidth savings: 30%

Distributing the M-step locally
Distribute M-step alongside E-step
la silla
the chair
E-step 1
M-step 1

θla the
θla chair
θsilla the
θsilla chair

Nodes store only needed params,
compute them locally
Reduce passes back counts
Don’t need to hear about
irrelevant source words
Don’t need to tell (or hear) about
purely local source words

Reduce

Need to hear everything about each
source word: M-step denominator
E-step 2
M-step 2
la mesa
the table

θla the
θla table
θmesa the
θmesa table

θs

t

←

ηs t
t ηs

t

Bandwidth savings: 30%

Distributing the M-step locally
Distribute M-step alongside E-step
la silla
the chair
E-step 1
M-step 1

θla the
θla chair
θsilla the
θsilla chair

Nodes store only needed params,
compute them locally
Reduce passes back counts
Don’t need to hear about
irrelevant source words
Don’t need to tell (or hear) about
purely local source words

Reduce

Need to hear everything about each
source word: M-step denominator
E-step 2
M-step 2
la mesa
the table

θla the
θla table
θmesa the
θmesa table

θs

t

←

ηs t
t ηs

t

Bandwidth savings: 30%

Distributing the M-step locally
Distribute M-step alongside E-step
la silla
the chair
E-step 1
M-step 1

θla the
θla chair
θsilla the
θsilla chair

Reduce passes back counts

ηla the
ηla chair
ηsilla the
ηsilla chair

Don’t need to hear about
irrelevant source words
Don’t need to tell (or hear) about
purely local source words

Reduce

ηla the
ηla table
ηmesa the
ηmesa table
E-step 2
M-step 2
la mesa
the table

Nodes store only needed params,
compute them locally

Need to hear everything about each
source word: M-step denominator
θla the
θla table
θmesa the
θmesa table

θs

t

←

ηs t
t ηs

t

Bandwidth savings: 30%

Distributing the M-step locally
Distribute M-step alongside E-step
la silla
the chair
E-step 1
M-step 1

θla the
θla chair
θsilla the
θsilla chair

Reduce passes back counts

ηla the
ηla chair
ηsilla the
ηsilla chair

Don’t need to hear about
irrelevant source words
Don’t need to tell (or hear) about
purely local source words

Reduce

ηla the
ηla table
ηmesa the
ηmesa table
E-step 2
M-step 2
la mesa
the table

Nodes store only needed params,
compute them locally

Need to hear everything about each
source word: M-step denominator
θla the
θla table
θmesa the
θmesa table

θs

t

←

ηs t
t ηs

t

Bandwidth savings: 30%

Distributing the M-step locally
Distribute M-step alongside E-step
la silla
the chair
E-step 1
M-step 1

ηla the
ηla chair
ηsilla the
ηsilla chair

ηla
ηla
ηla

θla the
θla chair
θsilla the
θsilla chair
the
chair
table

ηsilla the
ηsilla chair
ηmesa the
ηmesa table

Reduce

ηla the
ηla table
ηmesa the
ηmesa table

ηla
ηla
ηla

E-step 2
M-step 2
la mesa
the table

the
chair
table

ηsilla the
ηsilla chair
ηmesa the
ηmesa table

θla the
θla table
θmesa the
θmesa table

Nodes store only needed params,
compute them locally
Reduce passes back counts
Don’t need to hear about
irrelevant source words
Don’t need to tell (or hear) about
purely local source words
Need to hear everything about each
source word: M-step denominator
θs

t

←

ηs t
t ηs

t

Bandwidth savings: 30%

Distributing the M-step locally
Distribute M-step alongside E-step
la silla
the chair
E-step 1
M-step 1

ηla the
ηla chair
ηsilla the
ηsilla chair

ηla
ηla
ηla

θla the
θla chair
θsilla the
θsilla chair
the
chair
table

ηsilla the
ηsilla chair
ηmesa the
ηmesa table

Reduce

ηla the
ηla table
ηmesa the
ηmesa table

ηla
ηla
ηla

E-step 2
M-step 2
la mesa
the table

the
chair
table

ηsilla the
ηsilla chair
ηmesa the
ηmesa table

θla the
θla table
θmesa the
θmesa table

Nodes store only needed params,
compute them locally
Reduce passes back counts
Don’t need to hear about
irrelevant source words
Don’t need to tell (or hear) about
purely local source words
Need to hear everything about each
source word: M-step denominator
θs

t

←

ηs t
t ηs

t

Bandwidth savings: 30%

Distributing the M-step locally
Distribute M-step alongside E-step
la silla
the chair
E-step 1
M-step 1

ηla the
ηla chair
ηsilla the
ηsilla chair

ηla
ηla
ηla

θla the
θla chair
θsilla the
θsilla chair
the
chair
table

ηsilla the
ηsilla chair
ηmesa the
ηmesa table

Reduce

ηla the
ηla table
ηmesa the
ηmesa table

ηla
ηla
ηla

E-step 2
M-step 2
la mesa
the table

the
chair
table

ηsilla the
ηsilla chair
ηmesa the
ηmesa table

θla the
θla table
θmesa the
θmesa table

Nodes store only needed params,
compute them locally
Reduce passes back counts
Don’t need to hear about
irrelevant source words
Don’t need to tell (or hear) about
purely local source words
Need to hear everything about each
source word: M-step denominator
θs

t

←

ηs t
t ηs

t

Bandwidth savings: 30%

Distributing the M-step locally
Distribute M-step alongside E-step
la silla
the chair
E-step 1
M-step 1

ηla the
ηla chair
ηsilla the
ηsilla chair

ηla
ηla
ηla

θla the
θla chair
θsilla the
θsilla chair
the
chair
table

ηsilla the
ηsilla chair
ηmesa the
ηmesa table

Reduce

ηla the
ηla table
ηmesa the
ηmesa table

ηla
ηla
ηla

E-step 2
M-step 2
la mesa
the table

the
chair
table

ηsilla the
ηsilla chair
ηmesa the
ηmesa table

θla the
θla table
θmesa the
θmesa table

Nodes store only needed params,
compute them locally
Reduce passes back counts
Don’t need to hear about
irrelevant source words
Don’t need to tell (or hear) about
purely local source words
Need to hear everything about each
source word: M-step denominator
θs

t

←

ηs t
t ηs

t

Bandwidth savings: 30%

Distributing the M-step locally
Distribute M-step alongside E-step
la silla
the chair
E-step 1
M-step 1

ηla the
ηla chair
ηsilla the
ηsilla chair

ηla
ηla
ηla

θla the
θla chair
θsilla the
θsilla chair
the
chair
table

ηsilla the
ηsilla chair
ηmesa the
ηmesa table

Reduce

ηla the
ηla table
ηmesa the
ηmesa table

ηla
ηla
ηla

E-step 2
M-step 2
la mesa
the table

the
chair
table

ηsilla the
ηsilla chair
ηmesa the
ηmesa table

θla the
θla table
θmesa the
θmesa table

Nodes store only needed params,
compute them locally
Reduce passes back counts
Don’t need to hear about
irrelevant source words
Don’t need to tell (or hear) about
purely local source words
Need to hear everything about each
source word: M-step denominator
θs

t

←

ηs t
t ηs

t

Bandwidth savings: 30%

Distributing the M-step locally
Distribute M-step alongside E-step
la silla
the chair
E-step 1
M-step 1

ηla the
ηla chair
ηsilla the
ηsilla chair

ηla
ηla
ηla

θla the
θla chair
θsilla the
θsilla chair
the
chair
table

ηsilla the
ηsilla chair
ηmesa the
ηmesa table

Reduce

ηla the
ηla table
ηmesa the
ηmesa table

ηla
ηla
ηla

E-step 2
M-step 2
la mesa
the table

the
chair
table

ηsilla the
ηsilla chair
ηmesa the
ηmesa table

θla the
θla table
θmesa the
θmesa table

Nodes store only needed params,
compute them locally
Reduce passes back counts
Don’t need to hear about
irrelevant source words
Don’t need to tell (or hear) about
purely local source words
Need to hear everything about each
source word: M-step denominator
θs

t

←

ηs t
t ηs

t

Bandwidth savings: 30%

Distributing the M-step locally
Distribute M-step alongside E-step
la silla
the chair
E-step 1
M-step 1

ηla the
ηla chair
ηsilla the
ηsilla chair

ηla
ηla
ηla

θla the
θla chair
θsilla the
θsilla chair
the
chair
table

ηsilla the
ηsilla chair
ηmesa the
ηmesa table

Reduce

ηla the
ηla table
ηmesa the
ηmesa table

ηla
ηla
ηla

E-step 2
M-step 2
la mesa
the table

the
chair
table

ηsilla the
ηsilla chair
ηmesa the
ηmesa table

θla the
θla table
θmesa the
θmesa table

Nodes store only needed params,
compute them locally
Reduce passes back counts
Don’t need to hear about
irrelevant source words
Don’t need to tell (or hear) about
purely local source words
Need to hear everything about each
source word: M-step denominator
θs

t

←

ηs t
t ηs

t

Bandwidth savings: 30%

Augmenting η to increase locality
la silla
the chair

ηla
ηla

E-step 1
M-step 1
the
chair

θla the
θla chair
θsilla the
θsilla chair
ηla
ηla
ηla

the
chair
table

ηla
ηla
ηla

the
table

E-step 2
M-step 2
la mesa
the table

M-step becomes θs

t

←

ηs t
ηs

Increases locality

Reduce

ηla
ηla

Augment η with redundant
ηs = t ηs t in E-step

the
chair
table

θla the
θla table
θmesa the
θmesa table

Total bandwidth savings: 84%
(bigger if more nodes)
Similar tricks for other models

Augmenting η to increase locality
la silla
the chair

ηla the
ηla chair
ηla
ηsilla

E-step 1
M-step 1

θla the
θla chair
θsilla the
θsilla chair
ηla the
ηla chair
ηla table
ηla
ηsilla

Reduce

ηla the
ηla table
ηla
ηmesa
E-step 2
M-step 2
la mesa
the table

ηla the
ηla chair
ηla table
ηla
ηmesa
θla the
θla table
θmesa the
θmesa table

Augment η with redundant
ηs = t ηs t in E-step
M-step becomes θs

t

←

ηs t
ηs

Increases locality
Total bandwidth savings: 84%
(bigger if more nodes)
Similar tricks for other models

Augmenting η to increase locality
la silla
the chair

ηla the
ηla chair
ηla
ηsilla

E-step 1
M-step 1

θla the
θla chair
θsilla the
θsilla chair
ηla the
ηla chair
ηla table
ηla
ηsilla

Reduce

ηla the
ηla table
ηla
ηmesa
E-step 2
M-step 2
la mesa
the table

ηla the
ηla chair
ηla table
ηla
ηmesa
θla the
θla table
θmesa the
θmesa table

Augment η with redundant
ηs = t ηs t in E-step
M-step becomes θs

t

←

ηs t
ηs

Increases locality
Total bandwidth savings: 84%
(bigger if more nodes)
Similar tricks for other models

Augmenting η to increase locality
la silla
the chair
E-step 1
M-step 1

ηla
ηla

the

ηla
ηla

θla the
θla chair
θsilla the
θsilla chair

the

M-step becomes θs

the

t

←

ηs t
ηs

Increases locality

Reduce

ηla
ηla

Augment η with redundant
ηs = t ηs t in E-step

ηla
ηla

E-step 2
M-step 2
la mesa
the table

the

θla the
θla table
θmesa the
θmesa table

Total bandwidth savings: 84%
(bigger if more nodes)
Similar tricks for other models

Choice of C-step topology
la silla
the chair
E-step 1
M-step 1

ηla
ηla

the

ηla
ηla

E-step 2
M-step 2
la mesa
the table

θla the
θla chair
θsilla the
θsilla chair

the

θla the
θla table
θmesa the
θmesa table

No need for separate Reduce nodes
By choosing connectivity, can trade off
bandwidth
latency
locality
...

Choice of C-step topology
la silla
the chair
E-step 1
M-step 1

ηla
ηla

the

ηla
ηla

θla the
θla chair
θsilla the
θsilla chair

No need for separate Reduce nodes
By choosing connectivity, can trade off
bandwidth
latency
locality
...
EM
Node 1

EM
Node 2

la mesa
the table

EM
Node 2

EM
Node 5

EM
Node 3

EM
Node 4

EM
Node 3

EM
Node 4

E-step 2
M-step 2

EM
Node 1

the

EM
Node 1

θla the
θla table
θmesa the
θmesa table

EM
Node 2

EM
Node 3

EM
Node 1

EM
Node 5

EM
Node 4

EM
Node 2

EM
Node 3

EM
Node 4

EM
Node 1

EM
Node 2

EM
Node 5

EM
Node 4

EM
Node 3

EM
Node 5

A LL PAIRS topology

AllPairs
EM
Node 1

EM
Node 3

EM
Node 2

EM
Node 4

Iteration time (s)

250

M-Step
C-Step
E-Step

200
150
100
50
0
12

5

10
# of nodes

Total Bandwidth: 3.6 B counts per iteration

20

J UNCTION T REE topology
Nodes embedded in
arbitrary tree structure
EM
Node 1

EM
Node 2

EM
Node 3

Messages contain counts needed by
nodes in both subtrees
Tree can optimize for
EM
Node 5

EM
Node 4

bandwidth
locality
...

We use maximum spanning tree to
heuristically minimize bandwidth
Future work: multiple trees

J UNCTION T REE topology

JunctionTree

EM
Node 2

EM
Node 3

EM
Node 5

EM
Node 4

Iteration time (s)

250
EM
Node 1

M-Step
C-Step
E-Step

200
150
100
50
0
12

5

10
# of nodes

Total Bandwidth: 1.4 B counts per iteration

20

Locality in other models

Ex: Latent Dirichlet Allocation (LDA) for topic modeling
Parameters: unigram distributions for each topic p(w|t)
Topic-word parameters local
Similar augmentation trick to Model 1
Details and results in paper

Also applies to other EM models, beyond EM
Word locality is extremely common in NLP applications
Variational inference
Other computations that make sparse use of expectations

Conclusion

A fully distributed, maximally localized EM algorithm
exploits parameter locality for signiﬁcant speedup
is general; just deﬁne η for each datum
is ﬂexible with respect to communication topology

Many further improvements possible
intelligent partitioning of data
running E- and C-steps in parallel
better topologies (e.g., multiple trees)
exploiting approximate sparsity/locality
...

