Automated reconstruction of ancient languages using
probabilistic models of sound change
Alexandre Bouchard-Côtéa,1, David Hallb, Thomas L. Grifﬁthsc, and Dan Kleinb
a
Department of Statistics, University of British Columbia, Vancouver, BC V6T 1Z4, Canada; bComputer Science Division and cDepartment of Psychology,
University of California, Berkeley, CA 94720

ancestral

| computational | diachronic

R

econstruction of the protolanguages from which modern languages are descended is a difﬁcult problem, occupying historical linguists since the late 18th century. To solve this problem
linguists have developed a labor-intensive manual procedure called
the comparative method (1), drawing on information about the
sounds and words that appear in many modern languages to hypothesize protolanguage reconstructions even when no written
records are available, opening one of the few possible windows
to prehistoric societies (2, 3). Reconstructions can help in understanding many aspects of our past, such as the technological
level (2), migration patterns (4), and scripts (2, 5) of early societies.
Comparing reconstructions across many languages can help reveal
the nature of language change itself, identifying which aspects
of language are most likely to change over time, a long-standing
question in historical linguistics (6, 7).
In many cases, direct evidence of the form of protolanguages is
not available. Fortunately, owing to the world’s considerable linguistic diversity, it is still possible to propose reconstructions by
leveraging a large collection of extant languages descended from a
single protolanguage. Words that appear in these modern languages can be organized into cognate sets that contain words suspected to have a shared ancestral form (Table 1). The key
observation that makes reconstruction from these data possible is
that languages seem to undergo a relatively limited set of regular
sound changes, each applied to the entire vocabulary of a language
at speciﬁc stages of its history (1). Still, several factors make reconstruction a hard problem. For example, sound changes are often
context sensitive, and many are string insertions and deletions.
In this paper, we present an automated system capable of
large-scale reconstruction of protolanguages directly from words
that appear in modern languages. This system is based on a
probabilistic model of sound change at the level of phonemes,
www.pnas.org/cgi/doi/10.1073/pnas.1204678110

building on work on the reconstruction of ancestral sequences
and alignment in computational biology (8–12). Several groups
have recently explored how methods from computational biology
can be applied to problems in historical linguistics, but such work
has focused on identifying the relationships between languages
(as might be expressed in a phylogeny) rather than reconstructing
the languages themselves (13–18). Much of this type of work has
been based on binary cognate or structural matrices (19, 20), which
discard all information about the form that words take, simply indicating whether they are cognate. Such models did not have the
goal of reconstructing protolanguages and consequently use a representation that lacks the resolution required to infer ancestral
phonetic sequences. Using phonological representations allows us
to perform reconstruction and does not require us to assume that
cognate sets have been fully resolved as a preprocessing step. Representing the words at each point in a phylogeny and having a model
of how they change give a way of comparing different hypothesized
cognate sets and hence inferring cognate sets automatically.
The focus on problems other than reconstruction in previous
computational approaches has meant that almost all existing
protolanguage reconstructions have been done manually. However, to obtain more accurate reconstructions for older languages,
large numbers of modern languages need to be analyzed. The
Proto-Austronesian language, for instance, has over 1,200 descendant languages (21). All of these languages could potentially
increase the quality of the reconstructions, but the number of
possibilities increases considerably with each language, making it
difﬁcult to analyze a large number of languages simultaneously.
The few previous systems for automated reconstruction of protolanguages or cognate inference (22–24) were unable to handle
this increase in computational complexity, as they relied on deterministic models of sound change and exact but intractable
algorithms for reconstruction.
Being able to reconstruct large numbers of languages also
makes it possible to provide quantitative answers to questions
about the factors that are involved in language change. We demonstrate the potential for automated reconstruction to lead to
novel results in historical linguistics by investigating a speciﬁc
hypothesized regularity in sound changes called functional load.
The functional load hypothesis, introduced in 1955, asserts that
sounds that play a more important role in distinguishing words are
less likely to change over time (6). Our probabilistic reconstruction
of hundreds of protolanguages in the Austronesian phylogeny
provides a way to explore this question quantitatively, producing
compelling evidence in favor of the functional load hypothesis.

Author contributions: A.B.-C., D.H., T.L.G., and D.K. designed research; A.B.-C. and D.H.
performed research; A.B.-C. and D.H. contributed new reagents/analytic tools; A.B.-C.,
D.H., T.L.G., and D.K. analyzed data; and A.B.-C., D.H., T.L.G., and D.K. wrote the paper.
The authors declare no conﬂict of interest.
This article is a PNAS Direct Submission. N.C. is a guest editor invited by the Editorial
Board.
Freely available online through the PNAS open access option.
1

To whom correspondence should be addressed. E-mail: bouchard@stat.ubc.ca.

This article contains supporting information online at www.pnas.org/lookup/suppl/doi:10.
1073/pnas.1204678110/-/DCSupplemental.

PNAS Early Edition | 1 of 6

PSYCHOLOGICAL AND
COGNITIVE SCIENCES

One of the oldest problems in linguistics is reconstructing the
words that appeared in the protolanguages from which modern
languages evolved. Identifying the forms of these ancient languages makes it possible to evaluate proposals about the nature
of language change and to draw inferences about human history.
Protolanguages are typically reconstructed using a painstaking
manual process known as the comparative method. We present a
family of probabilistic models of sound change as well as algorithms for performing inference in these models. The resulting
system automatically and accurately reconstructs protolanguages
from modern languages. We apply this system to 637 Austronesian
languages, providing an accurate, large-scale automatic reconstruction of a set of protolanguages. Over 85% of the system’s reconstructions are within one character of the manual reconstruction
provided by a linguist specializing in Austronesian languages. Being able to automatically reconstruct large numbers of languages
provides a useful way to quantitatively explore hypotheses about the
factors determining which sounds in a language are likely to change
over time. We demonstrate this by showing that the reconstructed
Austronesian protolanguages provide compelling support for a hypothesis about the relationship between the function of a sound
and its probability of changing that was ﬁrst proposed in 1955.

COMPUTER SCIENCES

Edited by Nick Chater, University of Warwick, Coventry, United Kingdom, and accepted by the Editorial Board December 22, 2012 (received for review
March 19, 2012)

Table 1. Sample of reconstructions produced by the system

*Complete sets of reconstructions can be found in SI Appendix.
†
Randomly selected by stratiﬁed sampling according to the Levenshtein edit distance Δ.
‡
Levenshtein distance to a reference manual reconstruction, in this case the reconstruction of Blust (42).
§
The colors encode cognate sets.
{
We use this symbol for encoding missing data.

Model
We use a probabilistic model of sound change and a Monte Carlo
inference algorithm to reconstruct the lexicon and phonology of
protolanguages given a collection of cognate sets from modern
languages. As in other recent work in computational historical
linguistics (13–18), we make the simplifying assumption that each
word evolves along the branches of a tree of languages, reﬂecting
the languages’ phylogenetic relationships. The tree’s internal
nodes are languages whose word forms are not observed, and the
leaves are modern languages. The output of our system is a posterior probability distribution over derivations. Each derivation
contains, for each cognate set, a reconstructed transcription of
ancestral forms, as well as a list of sound changes describing the
transformation from parent word to child word. This representation is rich enough to answer a wide range of queries that would
normally be answered by carrying out the comparative method
manually, such as which sound changes were most prominent
along each branch of the tree.
We model the evolution of discrete sequences of phonemes,
using a context-dependent probabilistic string transducer (8).
Probabilistic string transducers efﬁciently encode a distribution
over possible changes that a string might undergo as it changes
through time. Transducers are sufﬁcient to capture most types of
regular sound changes (e.g., lenitions, epentheses, and elisions) and
can be sensitive to the context in which a change takes place. Most
types of changes not captured by transducers are not regular (1) and
are therefore less informative (e.g., metatheses, reduplications, and
haplologies). Unlike simple molecular InDel models used in computational biology such as the TKF91 model (25), the parameterization of our model is very expressive: Mutation probabilities are
context sensitive, depending on the neighboring characters, and
each branch has its own set of parameters. This context-sensitive
and branch-speciﬁc parameterization plays a central role in our
system, allowing explicit modeling of sound changes.
Formally, let τ be a phylogenetic tree of languages, where each
language is linked to the languages that descended from it. In
such a tree, the modern languages, whose word forms will be
observed, are the leaves of τ. The most recent common ancestor
of these modern languages is the root of τ. Internal nodes of the
tree (including the root) are protolanguages with unobserved
word forms. Let L denote all languages, modern and otherwise.
All word forms are assumed to be strings in the International
Phonetic Alphabet (IPA).
We assume that word forms evolve along the branches of the
tree τ. However, it is usually not the case that a word belonging
to each cognate set exists in each modern language—words are
lost or replaced over time, meaning that words that appear in the
root languages may not have cognate descendants in the languages at the leaves of the tree. For the moment, we assume there
is a known list of C cognate sets. For each c ∈ f1; . . . ; Cg let LðcÞ
2 of 6 | www.pnas.org/cgi/doi/10.1073/pnas.1204678110

denote the subset of modern languages that have a word form in
the cth cognate set. For each set c ∈ f1; . . . ; Cg and each language
ℓ ∈ LðcÞ, we denote the modern word form by wcℓ . For cognate set
c, only the minimal subtree τðcÞ containing LðcÞ and the root is
relevant to the reconstruction inference problem for that set.
Our model of sound change is based on a generative process
deﬁned on this tree. From a high-level perspective, the generative process is quite simple. Let c be the index of the current
cognate set, with topology τðcÞ. First, a word is generated for the
root of τðcÞ, using an (initially unknown) root language model
(i.e., a probability distribution over strings). The words that appear at other nodes of the tree are generated incrementally, using
a branch-speciﬁc distribution over changes in strings to generate
each word from the word in the language that is its parent in τðcÞ.
Although this distribution differs across branches of the tree,
making it possible to estimate the pattern of changes involved in
the transition from one language to another, it remains the same
for all cognate sets, expressing changes that apply stochastically
to all words. The probabilities of substitution, insertion and deletion are also dependent on the context in which the change
occurs. Further details of the distributions that were used and
their parameterization appear in Materials and Methods.
The ﬂexibility of our model comes at the cost of having literally
millions of parameters to set, creating challenges not found in
most computational approaches to phylogenetics. Our inference
algorithm learns these parameters automatically, using established principles from machine learning and statistics. Speciﬁcally, we use a variant of the expectation-maximization algorithm
(26), which alternates between producing reconstructions on the
basis of the current parameter estimates and updating the parameter estimates on the basis of those reconstructions. The
reconstructions are inferred using an efﬁcient Monte Carlo inference algorithm (27). The parameters are estimated by optimizing a cost function that penalizes complexity, allowing us to
obtain robust estimates of large numbers of parameters. See SI
Appendix, Section 1 for further details of the inference algorithm.
If cognate assignments are not available, our system can be
applied just to lists of words in different languages. In this case it
automatically infers the cognate assignments as well as the
reconstructions. This setting requires only two modiﬁcations to
the model. First, because cognates are not available, we index the
words by their semantic meaning (or gloss) g, and there are thus
G groups of words. The model is then deﬁned as in the previous
case, with words indexed as wgℓ . Second, the generation process is
augmented with a notion of innovation, wherein a word wgℓ′ in
some language ℓ′ may instead be generated independently from
its parent word wgℓ . In this instance, the word is generated from
a language model as though it were a root string. In effect, the
tree is “cut” at a language when innovation happens, and so the
word begins anew. The probability of innovation in any given
Bouchard-Côté et al.

Random
Automated
Fig. 1. Quantitative validation of
Effect of tree
Effect of tree size
modern
reconstruction
reconstructions and identiﬁcation
quality
Agreement between
of some important factors inﬂutwo linguists
0.55
encing reconstruction quality. (A)
0.500
Reconstruction error rates for a
0.5
baseline (which consists of picking
0.375
0.45
one modern word at random), our
system, and the amount of dis0.250
0.4
agreement between two linguist’s
manual reconstructions. Reconstruc0.125
0.35
tion error rates are Levenshtein
N/A
distances normalized by the mean
0.3
0
0
10
20
30
40
50
60
word form length so that errors
Number of modern languages
can be compared across languages.
Agreement between linguists was computed on only Proto-Oceanic because the dataset used lacked multiple reconstructions for other protolanguages. (B) The
effect of the topology on the quality of the reconstruction. On one hand, the difference between reconstruction error rates obtained from the system that ran
on an uninformed topology (ﬁrst and second) and rates obtained from the system that ran on an informed topology (third and fourth) is statistically signiﬁcant.
On the other hand, the corresponding difference between a ﬂat tree and a random binary tree is not statistically signiﬁcant, nor is the difference between using
the consensus tree of ref. 41 and the Ethnologue tree (29). This suggests that our method has a certain robustness to moderate topology variations. (C) Reconstruction error rate as a function of the number of languages used to train our automatic reconstruction system. Note that the error is not expected to
go down to zero, perfect reconstruction being generally unidentiﬁable. The results in A and B are directly comparable: In fact, the entry labeled “Ethnologue”
in B corresponds to the green Proto-Austronesian entry in A. The results in A and B and those in C are not directly comparable because the evaluation in C
is restricted to those cognates with at least one reﬂex in the smallest evaluation set (to make the curve comparable across the horizontal axis of C).

Protolanguage Reconstructions. To test our system, we applied it to
a large-scale database of Austronesian languages, the Austronesian Basic Vocabulary Database (ABVD) (28). We used a previously established phylogeny for these languages, the Ethnologue
tree (29) (we also describe experiments with other trees in Fig. 1).
For this ﬁrst test of our system we also used the cognate sets
provided in the database. The dataset contained 659 languages at
the time of download (August 7, 2010), including a few languages
outside the Austronesian family and some manually reconstructed
protolanguages used for evaluation. The total data comprised
142,661 word forms and 7,708 cognate sets. The goal was to reconstruct the word in each protolanguage that corresponded to
each cognate set and to infer the patterns of sound changes along
each branch in the phylogeny. See SI Appendix, Section 2 for
further details of our simulations.
We used the Austronesian dataset to quantitatively evaluate the
performance of our system by comparing withheld words from
known languages with automatic reconstructions of those words.
The Levenshtein distance between the held-out and reconstructed
forms provides a measure of the number of errors in these
reconstructions. We used this measure to show that using more
languages helped reconstruction and also to assess the overall
performance of our system. Speciﬁcally, we compared the system’s
error rate on the ancestral reconstructions to a baseline and also to
the amount of divergence between the reconstructions of two
linguists (Fig. 1A). Given enough data, the system can achieve
reconstruction error rates close to the level of disagreement between manual reconstructions. In particular, most reconstructions
perfectly agree with manual reconstructions, and only a few contain big errors. Refer to Table 1 for examples of reconstructions.
See SI Appendix, Section 3 for the full lists.
We also present in Fig. 1B the effect of the tree topology on
reconstruction quality, reiterating the importance of using informative topologies for reconstruction. In Fig. 1C, we show that
the accuracy of our method increases with the number of observed Oceanic languages, conﬁrming that large-scale inference
is desirable for automatic protolanguage reconstruction: Reconstruction improved statistically signiﬁcantly with each increase

Cognate Recovery. Previous reconstruction systems (22) required
that cognate sets be provided to the system. However, the creation of these large cognate databases requires considerable annotation effort on the part of linguists and often requires that at
least some reconstruction be done by hand. To demonstrate that
our model can accurately infer cognate sets automatically, we
used a version of our system that learns which words are cognate,
starting only from raw word lists and their meanings. This system
uses a faster but lower-ﬁdelity model of sound change to infer
correspondences. We then ran our reconstruction system on
cognate sets that our cognate recovery system found. See SI Appendix, Section 1 for details.
This version of the system was run on all of the Oceanic languages in the ABVD, which comprise roughly half of the Austronesian languages. We then evaluated the pairwise precision
(the fraction of cognate pairs identiﬁed by our system that are also
in the set of labeled cognate pairs), pairwise recall (the fraction of
labeled cognate pairs identiﬁed by our system), and pairwise F1
measure (deﬁned as the harmonic mean of precision and recall)
for the cognates found by our system against the known cognates
that are encoded in the ABVD. We also report cluster purity,
which is the fraction of words that are in a cluster whose known
cognate group matches the cognate group of the cluster. See SI
Appendix, Section 2.3 for a detailed description of the metrics.
Using these metrics, we found that our system achieved a precision of 0.844, recall of 0.621, F1 of 0.715, and cluster purity of
0.918. Thus, over 9 of 10 words are correctly grouped, and our
system errs on the side of undergrouping words rather than clustering words that are not cognates. Because the null hypothesis in
historical linguistics is to deem words to be unrelated unless
proved otherwise, a slight undergrouping is the desired behavior.
PNAS Early Edition | 3 of 6

COMPUTER SCIENCES

except from 32 to 64 languages, where the average edit distance
improvement was 0.05.
For comparison, we also evaluated previous automatic reconstruction methods. These previous methods do not scale to
large datasets so we performed comparisons on smaller subsets
of the Austronesian dataset. We show in SI Appendix, Section 2
that our method outperforms these baselines.
We analyze the output of our system in more depth in Fig. 2
A–C, which shows the system learned a variety of realistic sound
changes across the Austronesian family (30). In Fig. 2D, we show
the most frequent substitution errors in the Proto-Austronesian
reconstruction experiments. See SI Appendix, Section 5 for
details and similar plots for the most common incorrect insertions and deletions.

PSYCHOLOGICAL AND
COGNITIVE SCIENCES

Kubokot

Lungga
Luqa
Kus Hoav
a
aghe
Rov Nduke a
Simbo
Patpatar
iana

Kuanua

Results
Our results address three questions about the performance of our
system. First, how well does it reconstruct protolanguages? Second,
how well does it identify cognate sets? Finally, how can this approach
be used to address outstanding questions in historical linguistics?

am

C

Reconstruction error rate

S a a roa
Ts ou
P uy uma
Ka v a la n

Bas ai
CentralAmi
S ira y a
Pazeh
S a is ia t
Ya ka n
Bajo
M a pun
S a ma lS ia s i
Ina ba knon
S uba nunS in
S uba nonS io
W e s te rnBuk

ke
NehanHapSolos o
re
ba a rov ga
M M
Vangunu
Ughele
Ghanong

Reconstruction error rate

Ta ne ma

Te a nu
Va no

Buma
As umboa
Ta nimbili
Ne mba o
S e ima t
WL ou
uv ulu
Na una
SL eis a Tita
iv ipon
L ikum
Levei

gau

L oniu
M us s
au
Ka s ira
Gima Ira h
Buli
M or n
M iny
a ifuin
BigaMis
As
ool
W a rope
Numfor
n
Ma
ra u
AmbaiYapen
W inde
s iW
NorthBabar a n
Da
Da i we ra
Tela Da we
M
Imroing a s bua
E
E a mpla
s
wa
S e tM a s
CentralMas e s
rili
la
S outhE
W es
Ta tDa a s
ranga
UjirNAru matB
r
Nga
Ges
iborS nB a
er
W
a
E la tubela Ar
SHituAmbon
tKe
ouAmanaTe
Amahai iB
es
Paulohi
Alune
M
urnatenAl
Bonfia
W
erinama
S oboy
BuruNamro
M
a mboru
Ngadha o
M
Pondok
a nggarai
l
Kodi
W
Bima
Soa ejewa
Savu
W
a nukaka Ta
GauraNg
na

CiuliAtaya
S quliqAtay
S e diq
Bunun
Tha o ng
F a v orla
Ruka i n
a iwa
PKa na ka na bu

M noboIlia
M a a noboW e
st
M a noboDiba
M a noboAtad
M a noboAta
u
a noboTigw
MM noboKa
M a noboS
a
Binukid ala
ra

Va
u
iGhon
ris
Haku
Va
ingga
s an e
S is Nis
BabatanaT
Teop
Taiof

Tanga
M
S ia
a goriS
Ka
r
nda
Vilirupu s
M out
otu
M
eL a
keo
Roro la
Kuni
Gabadi
Doura
Kiliv
M is
ima
Dobuanila
Ga W edau
pa
pa
iwa
M Ubir
Guma olima
Diodio
wa
M a na
is
S uain
Auhelawa u
Sa
liba
Ali
W oge
o
Ka iriru
Kis
Ka y
Riwo
upula
S obeuK
Ta rpia i
Kov
M a le
M e nge e
u
n
Biliau
Gea tuka
M da
ge d
r

Kubokot

Lungga
Luqa
Kus Hoav
a
aghe
Rov Nduke a
Simbo
Patpatar
iana

Kuanua

Tanga
M
S ia
a goriS
Ka
r
nda
Vilirupu s
M out
otu
M
eL a
keo
Roro la
Kuni
Gabadi
Doura
MKiliv
is ima
Dobuanila
Ga W edau
pa
pa
iwa
M Ubir
Guma olima
Diodio
wa
M a na
is
S uain
Auhelawa u
Sa
liba
Ali
W oge
o
Ka iriru
Kis
Ka y
Riwo
upula
S obeuK
Ta rpia i
Kov
M a le
M e nge e
u
n
Biliau
Gea tuka
M da
ge d
r

M e gia
Bilibil
r
Ka ulongAuV
S e ngs
e ng
L a moga
Amara
iM ul
Aria
NumbaM ouk
miS ib
Ya
W a be m
mpa
P una nKe la ir
Bukat
Ka y a nUma Ju
M ori
W olio
Baree
Bangga iW di

r
ma
Roma
s tDa s e
Ea
tine nimba
L e mdena
Ya
Ke iTa
ru
S e la iIria
Koiwa
Chamorro
ring
L a mpung
Kome
ja
S unda
ngRe b
ga
Re ja
TboliTa da lB
bili
Ta ga
Korona niB
nga
S a ra
a ra
BilaanKoro
BilaanS

Ira nun
M a ra
na o
Sa
Bali s a k
M a ma
nwa
Ilonggo
Hiliga
W a y non
Butua
Ta usra y W a ra
non
ugJolo y
S uriga
AklanonBis
onon
Cebuano
Ka
M a la
Ta ga ga n
ns a
logAnt
ka
BikolNa
Ta
gba
ga
Tagba nwa C
BatakPalaw Ka
nwaAb
Pa
la
Banggi
S W wa nB
Ha P
a la
nunoo a t
Pa
wa
laua
DaS inghi
no
Ka yakBa n
tinga
DayakNgaju
ka
n t
M Ka
e rina
dorih
M
Tunjung Ma
a
MKe a ny
rinc a la
e la
n
M
y uS
i
Ogan
e
Indones la y
L owM u a ra
Banjares
a la
M M
ian y
M e la a la
inangkab
y uBrun
PhanRan y B eM
Chru
a ha
M HainanCh
oken
s
Iban
a
gCh
Gayo

L ungaLunga
BabatanaKa v
ghua
Va
Babatana
BabatanaLo
ngga
i
Se
BabatanaA Ririo
ris

language is initially unknown and must be learned automatically
along with the other branch-speciﬁc model parameters.

Bouchard-Côté et al.

n

n
Nage
Kambera
r
PalueNitu
n
Anakalang
e ka
SPulauArgu
RotiTerma
Atonimbai ra le
a
M
le
TetunTerik
mak holotI
ma
Ke ma
La
a
ng
SLika
da
i
Ke
E ra
Talur
Aputai i
P era
Tugun
Iliunrua
Se
Nila unr
Te a
Kis

Ta lis e
Tolo
Gha ri
Gha riNdi
Ta lis e M a la
lis e M oli
Ta Bugotu
M bughotuDh
se
Ya peVitu
i
iB il
L a ka la
Na ka na
ututu
M aBarok
s
kL a ma
M a da unglk
L ihirSTiga
Tia ng
Na lik
t
Wes
Ka ra
gTung
Tunga M a da ra
Banoni
ka Ys
G
KilokaKokota
nga
Blabla ngas
a ma l
Blablana Kia
e
ba
L a ghuS
ZaringeL
oro
Ma
oP
a
ringeTat
Ngga Km
M a ringe
Bilur
Ma
ChekeHolo
a uro
a
Uruav u
M onoF
Tora
M onoAlu
M ono

s un
ur
M
r
Idaan
on
BunduDu
Timugon
ong
Bintulu
KelabitBa hL uk l
a
BerawanL uM n a
Belait na na
an
Kenylaa ha
e
M L
e
EngganoM
Nias dures c
EngganoB
a
TobaBatak
M
atanBas
an
IvItbayat y
Babuy
Irarala
yaten
Itba amorong
ay
Is
as
nga
Iv
lBoto
Imorod mpa n
mi
y
YaS a mba
pa
nKa
oBa
Ka
ha
lla
Ifuga ha nKe
Ka lla loi
n
ina nI
Ka
Iniba s
nga
P a kiduge k
Ka
oAmga
IlongotKa ta
Ifuga a
oB
Guin
Ifuga
BontokGuiny No
Bontoc na
Ka nka wGui
Balanga
Ka linga
ng
gB inon
Itne
Ga dda
g
AttaPamplo
Is negDiba s
Agta ga tCa
Duma no
s ia n
Iloka a
oly ne
s
Yogy a nea la y oP
vrnM
OldJa
W e s te
s eSo
Bugine
a loh
Ma ka s s a r
MTa e S Tora ja
bu
S a ngirTa
S a ngir a ra
S a ngilS
Bantik ng
Ka idipa
Goronta loH
Bola a ngM on
P opa lia te
Bone ra
W una
M una Ka tobu
Tonte mboa n
Tons e a

s un
ur
M

ar
Idaan
on
BunduDu
Timugon
ong
Bintulu hL uk
KelabitB
a
BerawanL uM n a l
Belait na na
an
Kenylaa ha
e
M L
e
EngganoM
Nias dures c
EngganoB
a
TobaBatak
M
atanBas
an
IvItbayat y
Babuy
Irarala
yaten
Itba amorong
ay
Is
as
nga
Iv
lBoto
Imorod mpa n
mi
y
YaS a mba
pa
nKa
oBa
Ka
ha
lla
Ifuga ha nKe
Ka lla loi
n
ina nI
Ka
Iniba s
nga
P a kiduge k
Ka
oAmga
IlongotKa ta
Ifuga a
oB
Guin
Ifuga
BontokGuiny No
Bontoc na
Ka nka wGui
Balanga
Ka linga
ng
gB inon
Itne
Ga dda
g
AttaPamplo
gDiba
Is ne
s
Agta ga tCa
Duma no
s ia n
Iloka a
oly ne
s
Yogy a nea la y oP
vrnM
OldJa
W e s te
s eSo
Bugine
a loh
Ma ka s s a r
MTa e S Tora ja
bu
S a ngirTa
S a ngir a ra
S a ngilS
Bantik ng
Ka idipa
Goronta loH
Bola a ngM on
P opa lia te
Bone ra
W una
M una Ka tobu
Tonte mboa n
Tons e a

am

umban
a T
tS
Eas
Baliledo
Lamboy
Ende
LioFlores

Sa
nta
FFaa ga
Ana
ga niRihu
BauroP niAguf
a
Ka wa V
hua
Aros
Aros
Aros iTawat
S a iOneib i
Ka nta Ca
hua
F a M ta l
BauroHaga a mi
ni
BauroBaroo
unu
Saa
AreareW
AuluVil
AreareMaas
a ia
Dorio

Saa Saa
Ulawa
Oroha
S
S a a a a S a a Vill
UkiNiM
a
L onggu
L
L a uW a uNorth
a la de
KwaF a ta
ra a S k
M ba e eleola
le le a
Lau
M ba e ngguu
L a ngaKwa io
la nga
Kwa
Toa mba itai

Ngge la
L e ngoP a rip
L e ngoGha im
L e ngo
Gha riNgini
Gha riNgge r
Ta lis e Koo
Ta lis e P ole
Gha riTa nda
M bira o
Gha riNgga e
M a la ngo

M e gia
Bilibil
r
Ka ulongAuV
S e ngs
e ng
L a moga
Amara
iM ul
Aria
NumbaM ouk
miS ib
Ya
W a be m
mpa
P una nKe la ir
Bukat
Ka y a nUma Ju
M ori
W olio
Baree
Bangga iW di

Ta ne ma

Te a nu
Va no

L oniu
M us s
au
Ka s ira
Gima Ira h
Buli
M or n
M iny
a ifuin
BigaMis
As
ool
W a rope
Numfor
n
Ma
ra
AmbaiYapen
W inde u
s iW
NorthBabar a n
Da
Da i we ra
Tela Da we
M
Imroing a s bua
E
E a mpla
s
wa
S e tM a s
CentralMas e s
rili
la
S outhE
W es
Ta tDa a s
ra
UjirNAru matB
Nga nga r
Ges
iborS nB a
er
W
a
E la tubela Ar
SHituAmbon
tKe
ouAmanaTe
Amahai iB
es
Paulohi
Alune
M
urnatenAl
Bonfia
W
erinama
S oboy
BuruNamro
M
a mboru
Ngadha o
M
Pondok
a nggarai
l
Kodi
W
Bima
Soa ejewa
Savu
W
a nukaka Ta
GauraNg
na

gau

SakaoPo

S a a roa
Ts ou
P uy uma
Ka v a la n

Bas ai
CentralAmi
S ira y a
Pazeh
S a is ia t
Ya ka n
Bajo
M a pun

ke
NehanHapSolos o
re
ba a rov ga
M M
Vangunu
Ughele
Ghanong

Kiriba
e
Ponapea
oleai
okiles
Pingilapes
MW
PuloAnna e
oroles
ipanCaro
S a ons
kes
S
Chuukes eAK
ortloc
Carolinianlese n
M
e
oleaians
ChuukesW
Satawa
eei
rtO
PuloAnna
Naman
Puluwate Avava
Nev

CiuliAtaya
S quliqAtay
S e diq
Bunun
Tha o ng
F a v orla
Ruka i n
a iwa
PKa na ka na bu

S a ma lS ia s i
Ina ba knon
S uba nunS in
S uba nonS io
W e s te rnBuk
M noboIlia
M a a noboW e
st
M a noboDiba
M a noboAtad
M a noboAtau
a noboTigw
MM noboKa
M a noboS
a
Binukid ala
ra

Va
u
iGhon
ris
Haku
Va
ingga
s an e
S is
BabatanaT
Nis Teop
Taiof

s
uru
Nalle
Nelemwa a ie
ha
a rs Kus ti n

M

Nes nei
Nahav Tape
Namakir e
aq
Nguna Nati
SouthEfa
Orkon
Paames
M
Raga
wotlap
PeteraraM
AmbrymSo
te
eSou

ArakiSouthM a
ota
M ut
Sy
e re
eE
i
rroma
Le
TannaSouth n
na Ura
Kwa
kel
Tawaroga
me
ra

r
ma
Roma
s tDa sna
e
Ea
tine nimba
L e mde
Ya
Ke iTa
ru
S e la iIria
Koiwa
Chamorro
ring
L a mpung
Kome
ja
S unda
ngRe b
ga
Re ja
TboliTa da lB
bili
Ta ga
Korona niB
nga
S a ra
a ra
BilaanKoro
BilaanS

Ira nun
M a ra
na o
Sa
Bali s a k
M a ma
nwa
Ilonggo
Hiliga
W a y non
Butua
Ta usra y W a ra
non
ugJolo y
S uriga
AklanonBis
onon
Cebuano
Ka
M a la
Ta ga ga n
ns a
logAnt
ka
BikolNa
Ta
gba
ga
Ta
gba nwa C
BatakPalaw Ka
nwa
Pa
Ab
la
Banggi
S W wa nB
Ha P
a la
nunoo a t
Pa
wa
la ua
DaS inghi
no
Ka yakBa n
tinga
Da
ka
n
M Ka yakNgaju t
e rinaMa
dorih
M
Tunjung
a
MKe a ny
rinc a la
e la
n
M
y uS
i
Ogan
e
Indones la y
L owM u a ra
Banjares
a la
M M
ian y
M e la a la
inangkab
y uBrun
PhanRan y B eM
Chru
a ha
Moken
HainanCh
s
Iban
a
gCh
Gayo

L ungaLunga
BabatanaKa v
ghua
Va
Babatana
BabatanaLo
ngga
i
Se
BabatanaA Ririo
ris

i
Ia a
Nengone
Canala
Ja we

AnejomA

n

n
Nage
Kambera
r
PalueNitu
n
Anakalang
e ka
SPulauArgu
RotiTerma
Atonimbai ra le
a
k
M
le
TetunTerik
ma holotI
ma
Ke ma
La
a
ng
SLika
da
i
Ke
E ra
Talur
Aputai i
ra
Pe
Tugun
Iliunrua
Se
Nila r
a
Teun
Kis

Ta lis e
Tolo
Gha ri
Gha riNdi
Ta lis e M a la
Ta lis e M oli
Bugotu
M bughotuDh
se
Ya peVitu
i
iB il
L a ka la
Na ka na
ututu
M aBarok
s
kL a ma
M a da ungl
L ihirSTiga k
Tia ng
Na lik
t
Wes
Ka ra
gTung
Tunga M a da ra
Banoni
ka Ys
G
KilokaKokota
nga
Blabla ngas
a ma l
Blablana Kia
e
ba L
L a ghuS
Zaringe
oro
Ma
oP Tat
ringe
Ngga Kma
M a ringe
Bilur
Ma
ChekeHolo
a uro
a
Uruav u
M onoF
Tora
M onoAlu
M ono

Tikopia
Bellona n
a
S a moa s n
nuiE iia
Ra pa wa v a
Ha re
nga a n
Ma
rques nth
M a hitia n
Ta
o
nM
Ra rotonga n
Ta hitia
Rurutua
nihiki
M a motu n
Tua e nrhy
P
hiti
Taa ori
M
ij
n
s ternF
W e Rotuma
Dehu

umban
a T
tS
Eas
Baliledo
Lamboy
Ende
LioFlores

ArakiSouthM a
ota
M ut
Sy
e re
eE
i
rroma
Le
Ura
TannaSouth n
Kwa na kel
Tawaroga
me
ra

Sa
nta
FFaa ga
Ana
ga niRihu
BauroP niAguf
a
Ka wa V
hua
Aros
Aros
Aros iTawat
S a iOneib i
Ka nta Ca
hua
F a M ta l
BauroHaga a mi
ni
BauroBaroo
unu
Saa
AreareW
AuluVil
AreareMaas
a ia
Dorio

Saa Saa
Ulawa
Oroha
S
S a a a a S a a Vill
UkiNiM
a
L onggu
L
L a uW a uNorth
a la de
KwaF a ta
ra a S k
M ba e eleola
le le a
Lau
M ba e ngguu
L a ngaKwa io
la nga
Kwa
Toa mba itai

Ngge
L e ngoP a la
L e ngoGha rip
im
L e ngo
Gha riNgini
Gha riNgge r
Ta lis e Koo
Ta lis e P ole
Gha riTa nda
M bira o
Gha riNgga e
M a la ngo

F ijia nB a u
Niue
Uv eaEas t
Tonga n
Tokela u
P uka puka
L ua ngiua
Nukuoro r
Ka pinga ma
S ika ia na
Tuv a lu
Ta kuu
uTa u t
Va e a ka E a s
F utuna
es t
Uv eaW le M
e
Ifira M Aniwe
s
utuna lle
F Re nne
e
E ma
Anuta

SakaoPo

Kiriba
e
Ponapea
oleai
okiles
Pingilapes
MW
PuloAnna e
oroles
ipanCaro
S a ons
kes
S
Chuukes eAK
ortloc
Carolinianlese n
M
e
oleaians
ChuukesW
Satawa
eei
rtO
PuloAnna
Naman
Puluwate Avava
Nev

s
uru
Nalle
Nelemwa a ie
ha
a rs Kus ti n

M

i
Ia a
Nengone
Canala
Ja we

Tikopia
Bellona n
a
S a moa s n
nuiE iia
Ra pa wa v a
Ha re
nga a n
Ma
rques nth
M a hitia n
Ta
o
nM
Ra rotonga n
Ta hitia
Rurutua
nihiki
M a motu n
Tua e nrhy
P
hiti
Taa ori
M
ij
n
ternF
W es
Rotuma
Dehu

AnejomA

Nes nei
Nahav Tape
Namakir e
aq
Nguna Nati
SouthEfat
Orkon
Paames
M
Raga
wotlap
PeteraraM
AmbrymSo
e
eSou

Buma
As umboa
Ta nimbili
Ne mba o
S e ima t
WL ou
uv ulu
Na una
SL eis a Tita
iv ipon
L ikum
Levei

B

F ijia nB a u
Niue
Uv eaEas t
Tonga n
Tokela u
P uka puka
L ua ngiua
Nukuoro r
Ka pinga ma
S ika ia na
Tuv a lu
Ta kuu
uTa u
Va e a ka E a s t
F utuna
es t
Uv eaW le M
e
Ifira M Aniwe
s
utuna lle
F Re nne
e
E ma
Anuta

Reconstruction error rate

A

B

i y

11

n

pb

t d

10

8

a

2

f v

s z
5

kg

q

4

9

h

x
6

( 666)

D

ii

)
( 448

3)

i

( 177)

( 683)

2)
( 12

( 563)

)
( 396

( 56)

)
( 464

( 541
( 395 )
( 305 )
)

( 409)
( 280)
( 183)

( 482
)

( 712
( 421 )
)

( 261
( 655 )
( 590 )
( 293 )
( 501 )
)

( 723
)

6)
( 15

( 753)
( 404)

)
( 554

( 470)

( 594
( 342 )
)

R o e s te
W o ri

( 523)

)
( 633 )
( 341

( 270)

( 631)

( 243)
( 735)

Ra m o a n
S a ona
e ll

L
P uka puk a
Toke la u
Uv e a E a s t
Tonga n
Niue
F ijia nBa u
Te a n

C

7

3

( 524)
( 680)
( 702)
( 682)
( 459)

( 77)
( 519
)

( 158)
( 521)

)
( 477

( 736
)

un
us ur
n uD M
on ar
aa
on
Id nd ugbi tB
Bu m la u nL
g
ul
Ti
Ke nt ra wa L on uk
Bi la it ah M
al
Be ny au
Be
an an
Ke el an an oM an
oB
M ah
L gg an ta k
)
En gg Ba e
( 677 )
En as
es c
( 276
Ni ba ur Ba s
)
To ad an
( 65)
( 62)
M at at an
( 217
( 99)
Iv ay y y
Itb bu la en
Ba ra at ng
Ira ay oro
)
)
Itb am ay
)
) 169 )
)
( 397
Is as d
(
( 138
( 303 ( 168
( 242 )
to
Iv oro
( 78)
( 237
Im mi ba lBo ga
( 39) )
( 66) )
an
Yaam mp a y n
( 234 )
( 278
( 238
S pa oB nKa
( 235)
Ka ga ha
e
( 240)
Ifu lla ha nK
( 228)
n
Ka lla loi
( 222)
Ka ba a s ina I
)
( 257)
en
Ini
ng
( 241
)
( 258)
P a kid ug a kga
( 167 ) )
( 561)
Ka ng otK
( 455
( 256)
( 263)
( 679
Ilo a oAm ta n
( 752)
Ifug a oBa
( 232)
Gui n
Ifug tok
( 64)
( 251)
Bon toc Gui No
( 55)
( 220)
na y
( 227)
Bon
( 89)
w
( 221)
( 497)
Ka nka Gui
nga
( 87)
( 498)
Ba la a on
( 113)
( 88)
Ka ling
( 219)
gBin
ng o
( 226)
Itne
( 262)
( 619)
Ga dda mpl g
Pa
( 90)
( 254)
Atta gDib a
( 239)
( 41)
( 478)
Is ne
s
( 184)
Agta a ga tCa
( 30)
Dum no
( 236)
( 605)
n
( 110)
( 255)
Iloka a
( 2)
ne s ia
( 144)
Yogy v a ne s la y oP oly
( 216)
OldJate rnM a
W es es eSo
( 469)
( 471)
Bugin
M a lohs s a r
( 754)
( 93)
( 468)
M a kaTora ja
( 488)
( 354)
Ta e S
bu
( 224)
( 94)
S a ngirTa
( 567)
( 344)
S a ngir a ra
( 637)
( 566)
S a ngilS
( 565)
( 611)
Ba ntik ng
( 476)
Ka idipa
( 248)
( 50)
Goronta loH
( 568)
( 202)
( 201)
Bola a ngM on
( 518)
P opa lia
( 84)
( 203)
( 85)
Bone ra te
( 691)
( 747)
W una
( 423)
( 739)
( 424)
M una Ka tobu
( 685)
Tonte mboa n
( 466)
( 684)
Tons e a
( 46)
( 51)
Ba ngga iW di
( 746)
( 418)
Ba re e
W olio
( 271)
( 96)
M ori
( 529)
Ka y a nUma Ju
( 213)
Buka t
( 715)
P una nKe
( 749)
W a mpa la i
( 484)
Ya be m r
( 422)
Numba
( 67)
( 624)
( 20)
M ouk miS ib
( 309)
( 7)
Aria
( 451)
( 462)
( 713)
L a moga
( 500)
( 585)
Ama ra iM ul
( 268)
S e ngs
( 61)
( 73)
e
( 475)
Ka ulongng
( 394)
AuV
Bilibil
( 188)
( 401)
M e gia
( 292)
( 575)
( 72)
( 387)
( 353)
r
Ge
( 539)
M ada ge d
( 574)
tuka
r
Bilia
( 579)
( 272)
Me u
( 661)
( 250)
M a nge n
( 600)
Kov le u
( 249)
( 4)
e
Ta rpia
( 627)
( 358)
S obe
( 480)
Ka
i
( 343)
( 284)
( 499)
Riwy upu la
( 743)
( 31)
uK
Ka o
( 558)
Kis iriru
( 205)
( 463)
( 626)
W
( 104)
Ali oge o
Auh
( 17)
( 281)
( 508
S a e la
( 140)
)
( 141)
S ualiba wa
( 412)
( 16)
M u
( 720)
Gu a is
( 695)
Dio ma in
( 185)
M dio wa na
Ub oli
Ga ir ma
W pa
Do ed pa iwa
( 143)
M bu au
( 295
Kil is iman
)
Ga iv a
Do ba ila
Ku ura di
Ro ni
M ro
L ek
Vi al a eo
M lir
M ot up
Ka ag u u
( 117
)

( 502
)

( 544
Ta nd or
( 593
iS
S
( 437 ) )
Kuia rng as ou
( 296 )
t
Pa an a
( 212
Ro
( 335 ) )
Si tp ua
( 336
)
Nd m v ia atar
( 294
)
bo na
Ku
)
Ho uk

(Fig. S3)

m
1

( 481)

( 467
)
( 111
)

( 350
)

o

B o p ia
( 63)
T ik a e
( 675)
E m ta
( 163)
An u n e ll e s e
( 13)
R e n n a An iw
( 537)
F u tuM e le M
( 180)
( 182)
)
Ifi ra W e s t
( 218
703)
(
Uv e a a E a s t
( 181)
F utunka uT a u
( 704)
Va e a u
( 647)
Ta ku
( 694)
Tu v a lu
( 592)
S ika ia nama r
( 162)
( 264)
Ka pin ga
( 483)
Nuk uoro
( 333)
ua ngiu a

( 114
)

( 444
( 659
( 12)
)
)

( 709
)

( 112
)

( 179)

( 187
)

( 730
)

Lu avs ag e
Lu ng
Ku qa a he
bo ga
ko
ta

n
ba
umo a
T
s tS ed y s
Ea lilm bo re
lo
Ba
La oFde
n
Li ge be raitu g
En m
Na lu eNal an un
Ka ak
rg an
Pa
Anek aruA m
S la er i
Pu tiT i
ik
)
Ro on ba er
)
e
At am nT
( 326 )
( 165
M tu ak eral otI
( 428
Te m al ol
Keam ah
)
L am
( 542
L ik a ng
( 29) )
S da
( 356 )
)
Kerai
( 669 )
( 149 )
E lur i
)
( 277
( 44)
( 308 )
Ta utai
( 582 )
( 525
Ap
( 166 )
n
)
)
P era
)
gu
( 259
( 731
Tu n a
( 307 )
( 496
)
( 11)
( 306 )
Iliu
( 170)
( 155
( 591
S eru
( 653)
a
( 273)
( 14)
Nil un
( 506)
Te a r
( 589)
( 690)
Kis ma ma r
( 456)
( 223)
Ro s tDa s e
)
E a tine e na
( 479
L e md nim ba
( 457)
)
( 285)
)
Ya iTa
( 740)
( 540)
( 670)
( 178
( 461
Ke la ru
S e wa iIria
Koi mo rro
( 671)
( 751)
Cha ung
( 274)
L a mp ring
( 286)
e
( 623)
Kom a e ja
( 145)
( 322)
( 676)
S undngR b
Re jaliTa ga
( 275)
Tbo bili lB
( 583)
Ta ga na da
Koro nga niB
( 310)
S a ra nKoro
( 290)
( 665)
Bila a nS a ra
( 638)
617)
(
( 291)
Bila a ta y a
( 573)
( 288)
CiuliA ta y
( 70)
S quliqA
( 71)
( 664)
S e diq
( 133)
Bunun
( 625)
( 127)
( 81)
( 580)
( 509)
Tha o ng
( 634)
F a v orla
( 535)
( 672)
( 28)
Ruka i n
( 176)
( 74)
P a iwaka na bu
( 100)
Ka na
( 737)
( 260)
S a a roa
( 545)
( 553)
( 492)
Ts ou
( 687)
P uy uma
( 688)
( 269)
Ka v a la n
( 530)
( 54)
( 472)
Ba s a i
Ce ntra lAmi
( 147)
( 109)
( 596)
S ira y a
( 504)
P a ze h
( 556)
S a is ia t
( 750)
( 559)
Ya ka n
( 40)
( 632)
( 91)
Ba jo
( 375)
( 560)
( 230)
M a pun
( 629)
S a ma lS ia s i
( 630)
( 628)
Ina ba knon
( 620)
S uba nunS in
( 733)
( 728)
( 362)
S uba nonS io
( 123)
( 370)
W e s te rnBuk
( 365)
( 366)
M a noboW
( 43)
e
( 27)
M a noboIlia s t
( 363)
( 614)
( 377)
M a noboDib
( 364)
( 79)
M a noboAta a
( 369)
( 367)
( 368)
M a noboAt d
( 376)
( 355)
M a noboT a u
( 576)
( 233)
( 405)
M a noboK igw
( 42)
( 118)
M a noboS a la
( 80)
Binuk
a ra
( 121)
M a ra id
na
( 507)
Ira nun o
( 493)
( 613)
( 253)
( 311)
( 372)
Sas
( 717)
( 225)
( 639)
( 3)
( 495)
Ba li a k
( 102)
( 210)
( 52)
( 69)
( 107)
M a ma
( 208)
( 635)
Ilong nwa
( 103)
Hilig go
( 662)
W a raa y non
( 252)
( 371)
Butu y W a
( 641)
( 727)
Ta us a non ra y
( 640)
S urig ugJo
( 151
( 57)
Akl a onolo
( 693 )
( 494)
( 47)
Ce a non n
)
( 547)
( 595)
Ka bua noBis
( 136)
M la ga
( 349
( 615)
Ta a ns a n
)
( 245)
Bikga logka
Ta olN Ant
( 126
Ta gb a a ga
( 410 )
( 215
Ba gb a nwa C
)
( 403
P ta kP nw Ka
)
( 337 )
( 267)
Baa la wa a laa Ab
( 137)
( 327
)
S ng nB w
)
at
HaW P a gi
Pa nu la
( 407
S la no wa no
( 279
( 125 )
Daing ua o
( 400 )
( 206 )
Ka y hi n
( 398 )
)
Da tinak Ba
( 487 )
y akga ka
( 331 )
Ka
( 231 )
t
M do Ngn
( 48) )
M er rih aju
( 348
Tu aa in aM
( 399
)
Ke nj ny
)
( 511
M rin un an al a
( 130
)
M el c g
)
Og el ay i
L anay uS ar
u
In ow
a
Ba do M
M nj ne al ay
M al ar
M el ay ess ia
Ph in ay Ba eMn
Ch
an uB
Ha ruan Ra gk ruha s
M
Ib okin an
ng ab n
Ga an en
Ch a
Ch
yo
am

a
4) M h i ti n
( 37 )
2
T a n rh yo tu
( 64 )
5
P e a m ik i
( 50
9) T u n ih n
( 68
a
o
1) M a
( 36
ru tu n M n
6) R u itia
5)
( 54
ga
h
( 64
3) T a
to nn th
( 64
a ro
( 534) R h itia s a n
a
e
a
( 644) T a rq u
a re v
( 383) M
a ng n s
( 359) M wa ii a
iE a
( 209) Ha a n u
( 384)
p

( 119
)

)
( 557

e

3)
( 54 4)
( 73

)
)
( 528
( 527 )
( 745 ) )
( 577
( 132 )
( 419 )
( 106 )
( 131

)
( 520

)
( 32)445 )
( 433
(

a
le gg
he on u
un o
Ug an
Gh ng ov re ke
)
Va ar s
e
M balo
( 696 ) )
f
M
ap
( 193
So io nH
( 706 ) )
( 382
Ta op
( 391
Te has an a Tu
)
Ne s
( 646 )
)
Ni ku gg na n
( 668 )
Hais in ta ho
( 154
( 439 )
)
S ba iG
( 458
( 602 )
Ba ris i
Va ris
( 572
Av
)
)
Va rio gg a na
( 438 ) ( 597
Ri en ta
S ba ua a o
( 38) )
( 207
)
( 711 )
Ba gh tan aL a
( 440
( 710 )
Va ba tan aK ga
( 538 )
Ba ba tan un
( 584
Ba ba L
( 35) )
Baun ga
( 705
L on ooAlu
( 34)
M on u
( 37)
)
M ra a uro
( 36)
( 129
)
To a v a
( 334)
( 610
( 413)
Uruon oF
( 414)
M ur eHolo a
( 686)
Bil ek ge Km t
( 700)
Ch a rin ge Ta
( 415)
M a rin oro l
( 128)
M a oP e L e
( 416)
( 379)
Ngg ring Kia s
( 381)
( 452)
M a ba naS a ma
( 75)
( 380)
Za ghu nga
( 157)
G
( 755)
L a bla
( 302)
Bla bla nga
( 82)
Bla ota Ys
( 732)
( 83)
Kok ka ka
( 289)
( 571)
i
Kilo
( 282)
( 124)
Ba nonra
ng
M a da gTu
ga
Tun W e s t
( 340)
Ka ra
( 49)
( 692)
Na lik
( 265)
Tia ng
k
( 431)
Tiga ungl s
( 673)
L ihirS kL a ma
( 674)
M a da
( 316)
( 324)
Ba rok
( 339)
M a ututuiBil
( 53)
Na ka nai
( 388)
( 338)
L a ka la
( 430)
Vitu
( 304)
Ya pe s e
Dh
( 741)
M bughotu
( 393)
( 714)
Bugotu
( 95)
( 92)
Ta lis e M oli
( 651)
( 650)
Ta lis e M a la
( 195)
Gha riNdi
( 194)
Gha ri
( 681)
Tolo
( 648)
Ta lis e
( 347)
M a la ngo
( 204)
( 196)
Gha riNgga e
( 190)
( 392)
M bira o
( 199)
Gha riTa nda
( 652)
Ta lis e P ole
( 197)
( 649)
Gha riNgge r
Ta lis e Koo
( 198)
( 319)
Gha riNgini
( 189)
( 320)
L e ngo
( 321)
L e ngoGha im
( 453)
L e ngoP
( 678)
Ngge la a rip
( 298)
Toa mba ita
( 618)
( 312)
Kwa i
( 299)
L a nga
( 473)
( 390)
Kwa io la nga
( 313)
M ba e
( 389)
L a u ngguu
( 345)
( 301)
M ba
( 175)
Kwa e le le a
( 315)
( 328)
F a tara a e S ol
( 314)
le
( 346)
L a uW ka
a
( 551)
L a uNor la de
( 550)
L ongg th
( 621)
( 490)
u
Saa
UkiN
( 552)
S
aa
iM
( 548)
OrohS a a Vill a
( 142)
Saa a
( 18)
S a a Ula wa
( 19)
Dori
( 58)
( 549)
Are o
( 59)
( 564)
( 172)
Are a re M
( 247)
S a a re W a a s
( 570)
a
Ba a Aul uVi ia
( 22)
Ba uro Ba l
( 23)
F uro Ha roo
( 21)
( 657)
Kaa ga ni unu
( 246)
S a hua M
( 60)
( 171
Aro nta Ca a
( 173)
)
( 174)
Aro s iOn ta mi
( 569)
Aro s iTa eib l
( 658)( 663)
Ka s i
wa
( 300)
t
Ba hu
( 318)
F a uro a
( 726
( 636
F agga P a wa
)
( 699 )
niA
S
a niR
V
)
Taan taA gu
( 150
Ta wa naihu f
( 15)
)
Kw nn rog
( 402
L am aS a
( 10) )
( 420
S en eraou th
( 510 )
Ur y eEak
( 491 )
Ar a rroel
( 427 )
m
M ak
( 531 )
Amerei iS ou an
)
M
th
Pe ot br
Pa te a y m S
M am ra
ou
ra
Ra wo
t
So ga tla es M a
eS
Or ut
ou
Ng ko hE p
Na
Na un n fa te
Na m a
Ne ti ak ir
Ta s ha
An pe e v aq
ej
om
An
ei
( 607
( 489 )
( 454
( 432 ) )
( 434
( 429 )
)
)

( 352
)

ProtoAustronesian

( 447
)

(Fig. S4)

u
12

( 533) 2)
( 56

(Fig. S2)

(Fig. S5)

tO
or
oP
ka a i
se
Sa av eean te n
Av v m wa na n e
Ne lu An ia s K
Na lo
ea le s eAs
Pu
Pu ol ta wake ke n
c
W
Sa uutlo ia s e
Ch or linke es o
M rouu orol ar
Ca
Chon spa nCna
S ai An i e
S lo ea s s
)
Pu ol ile pe
( 603 )
W ok ila an
( 555 )
M ng pe i
( 526 )
Pi na at
s
)
( 744 )
Po rib aie lle
( 411 )
( 351
Ki s ha
( 512 )
Ku ars
( 514
)
M uru wa
( 516
)
Na lem
( 515
Ne we la
Ja na
Ca a i on e
)
( 441)
Ia ng
( 406
( 244)
)
Ne hu a n ij
( 283
De tum nF
( 297)
Ro es ter
( 385)
)
( 474)
W a ori i
( 522
M hit y n
( 105)
( 374)
Ta nrh tu
( 642)
436)
P e mo iki
(
( 505)
( 543)
( 446)
( 689) Tua nih a n o
( 734)
a
utua nM n
( 214)
( 361) M
( 443)
( 546) Rurhiti nga
( 645)
( 139)
( 643) Ta roto nth
332)
( 534) Ra hiti a s a n
(
( 644) Ta a rque
( 122)
( 725)
re v a
( 536)
( 383) M
a nga n s
( 156)
( 359) M
wa iia a
nuiE
( 209) Ha
( 384)
Ra pa n
S a moa
Be llona
( 533)
( 63)
pia
( 562)
Tiko e
( 675)
E ma
( 163)
Anuta lle s e
( 13)
Re nne Aniw
( 537)
F utuna le M
( 180)
( 481)
e
( 182)
Ifira MW e s t
( 218)
( 513)
( 703)
Uv e a E a s t
( 146)
( 116)
( 181)
F utuna uTa u
( 704)
Va e a ka
( 647)
( 563)
Ta kuu
( 694)
Tuv a lu
( 592)
S ika ia na r
( 162)
( 264)
Ka pinga ma
( 483)
Nukuoro
( 333)
L ua ngiua
( 524)
P uka puka
( 680)
Toke la u
( 702)
Uv e a E a s t
( 682)
Tonga n
( 683)
( 459)
Niue
( 177)
F ijia nBa u
( 666)
Te a nu
( 708)
( 707)
Va no
( 654)
( 159)
Ta ne ma
( 98)
( 26)
Buma
( 701)
( 656)
As umboa
( 738)
( 442)
Ta nimbili
( 581)
( 1)
Ne mba o
( 748)
( 616)
S e ima t
( 330)
( 160)
W uv ulu
( 435)
( 426)
( 153)
L ou
( 373)
( 317)
Na una
( 598)
( 729)
L e ipon
( 608)
( 325)
( 609)
S iv is a
( 329)
( 323)
L ikum Tita
( 266)
Lev e
( 200)
( 417)
L oniui
( 108)
( 97)
M us s
( 532)
Ka s ira a u
( 718)
( 408)
Gima Ira h
( 33)
( 485)
( 68)
Buli n
( 465)
( 25)
M or
( 120)
M iny
( 724)
Biga a ifuin
( 24)
( 378)
As M is ool
( 612)
( 8)
( 460)
W a rope
( 742)
( 135)
Num
( 622)
n
( 134)
M a for
Am ra u
ba
W ind iYa
( 386)
( 667)
( 229)
( 152
Nor e s pe n
( 45)
( 164)
)
Da thB iW a
( 660)
( 148)
Da we ra a ba r n
( 697)
( 588)
Da we
Te i
( 450)
( 101
( 115)
( 601
( 586
Imr la M a
( 606)
)
( 192)
oin s bua
)
)
E mp
( 161)
E a la g
( 486
s
S eri tM wa s
)
Ce li a s e
la
( 587
( 86)
( 191)
S ntra
( 357
( 722
)
( 719)
( 360
W ou thE lM
( 449 )
)
( 9)
( 698
Ta es tDa a s a s
( 517 )
( 287 ) )
Uji ra ng matB
)
( 6)
( 721
( 76) )
Ng rNA a
( 503
( 599 )
( 211)
Ge aib ru nB ar
)
( 578
( 604
( 716 )
W s er orS
( 186 )
)
( 5)
E atu
Ar
)
( 425
)
HitlatKebe
)
la
S ouuA iBe
mb
Am Am
Pa ah anons
Al ul
M un ohai aTe
Bo ur e i
W nf na
Bu er ia te nA
S ru in
l
M ob Na am
M am oy m a
Ng an
o ro
Po ad ggbo
l
Ko nd ha ar ru
W
ai
Bi ej di ok
So m ew
Sa a a aT
W
an
Ga anv u
a
ur uk
aN ak
gg a
au

A

Fraction of substitution errors

Fig. 2. Analysis of the output of our system in more depth. (A) An Austronesian phylogenetic tree from ref. 29 used in our analyses. Each quadrant is
available in a larger format in SI Appendix, Figs. S2–S5, along with a detailed table of sound changes (SI Appendix, Table S5). The numbers in parentheses
attached to each branch correspond to rows in SI Appendix, Table S5. The colors and numbers in parentheses encode the most prominent sound change along
each branch, as inferred automatically by our system in SI Appendix, Section 4. (B) The most supported sound changes across the phylogeny, with the width of
links proportional to the support. Note that the standard organization of the IPA chart into columns and rows according to place, manner, height, and
backness is only for visualization purposes: This information was not encoded in the model in this experiment, showing that the model can recover realistic
cross-linguistic sound change trends. All of the arcs correspond to sound changes frequently used by historical linguists: sonorizations /p/ > /b/ (1) and /t/ > /d/
(2), voicing changes (3, 4), debuccalizations /f/ > /h/ (5) and /s/ > /h/ (6), spirantizations /b/ > /v/ (7) and /p/ > /f/ (8), changes of place of articulation (9, 10), and
vowel changes in height (11) and backness (12) (1). Whereas this visualization depicts sound changes as undirected arcs, the sound changes are actually
represented with directionality in our system. (C) Zooming in a portion of the Oceanic languages, where the Nuclear Polynesian family (i) and Polynesian
family (ii) are visible. Several attested sound changes such as debuccalization to Maori and place of articulation change /t/ > /k/ to Hawaiian (30) are successfully localized by the system. (D) Most common substitution errors in the PAn reconstructions produced by our system. The ﬁrst phoneme in each pair
ðx; yÞ represents the reference phoneme, followed by the incorrectly hypothesized one. Most of these errors could be plausible disagreements among human
experts. For example, the most dominant error (p, v) could arise over a disagreement over the phonemic inventory of Proto-Austronesian, whereas vowels are
common sources of disagreement.

Because we are ultimately interested in reconstruction, we then
compared our reconstruction system’s ability to reconstruct words
given these automatically determined cognates. Speciﬁcally, we
took every cognate group found by our system (run on the Oceanic
subclade) with at least two words in it. Then, we automatically
reconstructed the Proto-Oceanic ancestor of those words, using
our system. For evaluation, we then looked at the average Levenshtein distance from our reconstructions to the known reconstructions described in the previous sections. This time, however,
we average per modern word rather than per cognate group, to
provide a fairer comparison. (Results were not substantially different when averaging per cognate group.) Compared with reconstruction from manually labeled cognate sets, automatically
identiﬁed cognates led to an increase in error rate of only 12.8%
and with a signiﬁcant reduction in the cost of curating linguistic
databases. See SI Appendix, Fig. S1 for the fraction of words with
each Levenshtein distance for these reconstructions.
Functional Load. To demonstrate the utility of large-scale recon-

struction of protolanguages, we used the output of our system to
investigate an open question in historical linguistics. The functional load hypothesis (FLH), introduced 1955 (6), claims that
the probability that a sound will change over time is related to
the amount of information provided by a sound. Intuitively, if
two phonemes appear only in words that are differentiated from
one another by at least one other sound, then one can argue that
no information is lost if those phonemes merge together, because no new ambiguous forms can be created by the merger.
A ﬁrst step toward quantitatively testing the FLH was taken in
1967 (7). By deﬁning a statistic that formalizes the amount of
information lost when a language undergoes a certain sound
change—on the basis of the proportion of words that are discriminated by each pair of phonemes—it became possible to
evaluate the empirical support for the FLH. However, this initial
4 of 6 | www.pnas.org/cgi/doi/10.1073/pnas.1204678110

investigation was based on just four languages and found little
evidence to support the hypothesis. This conclusion was criticized by several authors (31, 32) on the basis of the small number
of languages and sound changes considered, although they provided no positive counterevidence.
Using the output of our system, we collected sound change
statistics from our reconstruction of 637 Austronesian languages,
including the probability of a particular change as estimated by our
system. These statistics provided the information needed to give
a more comprehensive quantitative evaluation of the FLH, using
a much larger sample than previous work (details in SI Appendix,
Section 2.4). We show in Fig. 3 A and B that this analysis provides
clear quantitative evidence in favor of the FLH. The revealed
pattern would not be apparent had we not been able to reconstruct
large numbers of protolanguages and supply probabilities of different kinds of change taking place for each pair of languages.
Discussion
We have developed an automated system capable of large-scale
reconstruction of protolanguage word forms, cognate sets, and
sound change histories. The analysis of the properties of hundreds of ancient languages performed by this system goes far
beyond the capabilities of any previous automated system and
would require signiﬁcant amounts of manual effort by linguists.
Furthermore, the system is in no way restricted to applications
like assessing the effects of functional load: It can be used as
a tool to investigate a wide range of questions about the structure
and dynamics of languages.
In developing an automated system for reconstructing ancient
languages, it is by no means our goal to replace the careful
reconstructions performed by linguists. It should be emphasized
that the reconstruction mechanism used by our system ignores
many of the phenomena normally used in manual reconstructions. We have mentioned limitations due to the transducer
Bouchard-Côté et al.

Fig. 3. Increasing the number of languages we can
1
1
106
reconstruct gives new ways to approach questions in
historical linguistics, such as the effect of functional
0.8
0.8
load on the probability of merging two sounds. The
104
plots shown are heat maps where the color encodes
0.6
0.6
the log of the number of sound changes that fall
into a given two-dimensional bin. Each sound
102
change x > y is encoded as a pair of numbers in the
0.4
0.4
unit square, ðl; mÞ, as explained in Materials and
Methods. To convey the amount of noise one could
1
0.2
0.2
expect from a study with the number of languages
that King previously used (7), we ﬁrst show in A the
0
0
0
heat map visualization for four languages. Next, we
0
0
2x10-5
2x10-5
4x10-5
6x10-5
8x10-5
1x10-4
4x10-5
6x10-5
8x10-5
1x10-4
show the same plot for 637 Austronesian languages
Functional load
Functional load
in B. Only in this latter setup is structure clearly
visible: Most of the points with high probability of merging can be seen to have comparatively low functional load, providing evidence in favor of the
functional load hypothesis introduced in 1955. See SI Appendix, Section 2.4 for details.

Materials and Methods
This section provides a more detailed speciﬁcation of our probabilistic
model. See SI Appendix, Section 1.2 for additional content on the algorithm and simulations.
Distributions. The conditional distributions over pairs of evolving strings are
speciﬁed using a lexicalized stochastic string transducer (33).
Consider a language ℓ′ evolving to ℓ for cognate set c. Assume we have a
word form x = wcℓ′ . The generative process for producing y = wcℓ works as
follows. First, we consider x to be composed of characters x1 x2 . . . xn , with
the ﬁrst and last ones being a special boundary symbol x1 = # ∈ Σ, which is
never deleted, mutated, or created. The process generates y = y1 y2 . . . yn in n
chunks yi ∈ Σ* ; i ∈ f1; . . . ; ng, one for each xi . The yi s may be a single character, multiple characters, or even empty. To generate yi , we deﬁne a mutation Markov chain that incrementally adds zero or more characters to an
initially empty yi . First, we decide whether the current phoneme in the top
word t = xi will be deleted, in which case yi = e (the probabilities of the

Bouchard-Côté et al.

Parameterization. Instead of directly estimating the transition probabilities of
the mutation Markov chain (which could be done, in principle, by taking them
to be the parameters of a collection of multinomial distributions) we express
them as the output of a multinomial logistic regression model (36). This

PNAS Early Edition | 5 of 6

COMPUTER SCIENCES

decisions taken in this process depend on a context to be speciﬁed shortly).
If t is not deleted, we choose a single substitution character in the bottom
word. We write S = Σ∪​ fζg for this set of outcomes, where ζ is the special
outcome indicating deletion. Importantly, the probabilities of this multinomial can depend on both the previous character generated so far (i.e., the
rightmost character p of yi−1 ) and the current character in the previous
generation string (t), providing a way to make changes context sensitive.
This multinomial decision acts as the initial distribution of the mutation
Markov chain. We consider insertions only if a deletion was not selected in
the ﬁrst step. Here, we draw from a multinomial over S , where this time the
special outcome ζ corresponds to stopping insertions, and the other elements of S correspond to symbols that are appended to yi . In this case, the
conditioning environment is t = xi and the current rightmost symbol p in yi .
Insertions continue until ζ is selected. We use θS;t;p;ℓ and θI;t;p;ℓ to denote the
probabilities over the substitution and insertion decisions in the current
branch ℓ′ → ℓ. A similar process generates the word at the root ℓ of a tree or
when an innovation happens at some language ℓ, treating this word as
a single string y1 generated from a dummy ancestor t = x1 . In this case, only
the insertion probabilities matter, and we separately parameterize these
probabilities with θR;t;p;ℓ . There is no actual dependence on t at the root or
innovative languages, but this formulation allows us to unify the parameterization, with each θω;t;p;ℓ ∈ RjΣj+1 , where ω ∈ fR; S; Ig. During cognate inference, the decision to innovate is controlled by a simple Bernoulli random
variable ngℓ for each language in the tree. When known cognate groups are
assumed, ncℓ is set to 0 for all nonroot languages and to 1 for the root
language. These Bernoulli distributions have parameters νℓ .
Mutation distributions conﬁned in the family of transducers miss certain
phylogenetic phenomena. For example, the process of reduplication (as in
“bye-bye”, for example) is a well-studied mechanism to derive morphological and lexical forms that is not explicitly captured by transducers. The same
situation arises in metatheses (e.g., Old English frist > English ﬁrst). However, these changes are generally not regular and therefore less informative
(1). Moreover, because we are using a probabilistic framework, these events
can still be handled in our system, even though their costs will simply not be
as discounted as they should be.
Note also that the generative process described in this section does not
allow explicit dependencies to the next character in ℓ. Relaxing this assumption can be done in principle by using weighted transducers, but at the
cost of a more computationally expensive inference problem (caused by the
transducer normalization computation) (34). A simpler approach is to use
the next character in the parent ℓ′ as a surrogate for the next character in ℓ.
Using the context in the parent word is also more aligned to the standard
representation of sound change used in historical linguistics, where the
context is deﬁned on the parent as well.
More generally, dependencies limited to a bounded context on the parent
string can be incorporated in our formalism. By bounded, we mean that it
should be possible to ﬁx an integer k beforehand such that all of the
modeled dependencies are within k characters to the string operation. The
caveat is that the computational cost of inference grows exponentially in k.
We leave open the question of handling computation in the face of unbounded dependencies such as those induced by harmony (35).

PSYCHOLOGICAL AND
COGNITIVE SCIENCES

formalism but other limitations include the lack of explicit modeling of changes at the level of the phoneme inventories used by
a language and the lack of morphological analysis. Challenges
speciﬁc to the cognate inference task, for example difﬁculties with
polymorphisms, are also discussed in more detail in SI Appendix.
Another limitation of the current approach stems from the assumption that languages form a phylogenetic tree, an assumption
violated by borrowing, dialect variation, and creole languages.
However, we believe our system will be useful to linguists in
several ways, particularly in contexts where there are large numbers of languages to be analyzed. Examples might include using
the system to propose short lists of potential sound changes and
correspondences across highly divergent word forms.
An exciting possible application of this work is to use the
model described here to infer the phylogenetic relationships
between languages jointly with reconstructions and cognate sets.
This will remove a source of circularity present in most previous
computational work in historical linguistics. Systems for inferring
phylogenies such as ref. 13 generally assume that cognate sets are
given as a ﬁxed input, but cognacy as determined by linguists is in
turn motivated by phylogenetic considerations. The phylogenetic
tree hypothesized by the linguist is therefore affecting the tree
built by systems using only these cognates. This problem can be
avoided by inferring cognates at the same time as a phylogeny,
something that should be possible using an extended version of
our probabilistic model.
Our system is able to reconstruct the words that appear in
ancient languages because it represents words as sequences of
sounds and uses a rich probabilistic model of sound change. This
is an important step forward from previous work applying computational ideas to historical linguistics. By leveraging the full
sequence information available in the word forms in modern
languages, we hope to see in historical linguistics a breakthrough
similar to the advances in evolutionary biology prompted by the
transition from morphological characters to molecular sequences
in phylogenetic analysis.

B
Merger posterior

Merger posterior

A

Using these features and parameter sharing, the logistic regression model
deﬁnes the transition probabilities of the mutation process and the root
language model to be

model speciﬁes a distribution over transition probabilities by assigning
weights to a set of features that describe properties of the sound changes
involved. These features provide a more coherent representation of the
transition probabilities, capturing regularities in sound changes that reﬂect
the underlying linguistic structure.
We used the following feature templates: OPERATION, which identiﬁes
whether an operation in the mutation Markov chain is an insertion, a deletion, a substitution, a self-substitution (i.e., of the form x > y, x = y), or the
end of an insertion event; MARKEDNESS, which consists of language-speciﬁc
n-gram indicator functions for all symbols in Σ (during reconstruction, only
unigram and bigram features are used for computational reasons; for cognate
inference, only unigram features are used); FAITHFULNESS, which consists of
indicators for mutation events of the form 1 [ x > y ], where x ∈ Σ, y ∈ S .
Feature templates similar to these can be found, for instance, in the work of
refs. 37 and 38, in the context of string-to-string transduction models used in
computational linguistics. This approach to specifying the transition probabilities produces an interesting connection to stochastic optimality theory (39,
40), where a logistic regression model mediates markedness and faithfulness
of the production of an output form from an underlying input form.
Data sparsity is a signiﬁcant challenge in protolanguage reconstruction.
Although the experiments we present here use an order of magnitude more
languages than previous computational approaches, the increase in observed
data also brings with it additional unknowns in the form of intermediate
protolanguages. Because there is one set of parameters for each language,
adding more data is not sufﬁcient to increase the quality of the reconstruction; it is important to share parameters across different branches in the
tree to beneﬁt from having observations from more languages. We used the
following technique to address this problem: We augment the parameterization to include the current language (or language at the bottom of the
current branch) and use a single, global weight vector instead of a set of
branch-speciﬁc weights. Generalization across branches is then achieved
by using features that ignore ℓ, whereas branch-speciﬁc features depend
on ℓ. Similarly, all of the features in OPERATION, MARKEDNESS, and
FAITHFULNESS have universal and branch-speciﬁc versions.

ACKNOWLEDGMENTS. This work was supported by Grant IIS-1018733 from
the National Science Foundation.

1. Hock HH (1991) Principles of Historical Linguistics (Mouton de Gruyter, The Hague,
Netherlands).
2. Ross M, Pawley A, Osmond M (1998) The Lexicon of Proto Oceanic: The Culture and
Environment of Ancestral Oceanic Society (Paciﬁc Linguistics, Canberra, Australia).
3. Diamond J (1999) Guns, Germs, and Steel: The Fates of Human Societies (WW Norton,
New York).
4. Nichols J (1999) Archaeology and Language: Correlating Archaeological and Linguistic
Hypotheses, eds Blench R, Spriggs M (Routledge, London).
5. Ventris M, Chadwick J (1973) Documents in Mycenaean Greek (Cambridge Univ Press,
Cambridge, UK).
6. Martinet A (1955) Économie des Changements Phonétiques [Economy of phonetic
sound changes] (Maisonneuve & Larose, Paris).
7. King R (1967) Functional load and sound change. Language 43:831–852.
8. Holmes I, Bruno WJ (2001) Evolutionary HMMs: A Bayesian approach to multiple
alignment. Bioinformatics 17(9):803–820.
9. Miklós I, Lunter GA, Holmes I (2004) A “Long Indel” model for evolutionary sequence
alignment. Mol Biol Evol 21(3):529–540.
10. Suchard MA, Redelings BD (2006) BAli-Phy: Simultaneous Bayesian inference of
alignment and phylogeny. Bioinformatics 22(16):2047–2048.
11. Liberles DA, ed (2007) Ancestral Sequence Reconstruction (Oxford Univ Press, Oxford, UK).
12. Paten B, et al. (2008) Genome-wide nucleotide-level mammalian ancestor reconstruction. Genome Res 18(11):1829–1843.
13. Gray RD, Jordan FM (2000) Language trees support the express-train sequence of
Austronesian expansion. Nature 405(6790):1052–1055.
14. Ringe D, Warnow T, Taylor A (2002) Indo-European and computational cladistics.
Trans Philol Soc 100:59–129.
15. Evans SN, Ringe D, Warnow T (2004) Inference of Divergence Times as a Statistical Inverse Problem, McDonald Institute Monographs, eds Forster P, Renfrew C (McDonald
Institute, Cambridge, UK).
16. Gray RD, Atkinson QD (2003) Language-tree divergence times support the Anatolian
theory of Indo-European origin. Nature 426(6965):435–439.
17. Nakhleh L, Ringe D, Warnow T (2005) Perfect phylogenetic networks: A new methodology for reconstructing the evolutionary history of natural languages. Language
81:382–420.
18. Bryant D (2006) Phylogenetic Methods and the Prehistory of Languages, eds Forster P,
Renfrew C (McDonald Institute for Archaeological Research, Cambridge, UK), pp
111–118.
19. Daumé H III, Campbell L (2007) A Bayesian model for discovering typological implications. Assoc Comput Linguist 45:65–72.
20. Dunn M, Levinson S, Lindstrom E, Reesink G, Terrill A (2008) Structural phylogeny
in historical linguistics: Methodological explorations applied in Island Melanesia.
Language 84:710–759.

21. Lynch J, ed (2003) Issues in Austronesian (Paciﬁc Linguistics, Canberra, Australia).
22. Oakes M (2000) Computer estimation of vocabulary in a protolanguage from word
lists in four daughter languages. J Quant Linguist 7:233–244.
23. Kondrak G (2002) Algorithms for Language Reconstruction. PhD thesis (Univ of Toronto, Toronto).
24. Ellison TM (2007) Bayesian identiﬁcation of cognates and correspondences. Assoc
Comput Linguist 45:15–22.
25. Thorne JL, Kishino H, Felsenstein J (1991) An evolutionary model for maximum likelihood alignment of DNA sequences. J Mol Evol 33(2):114–124.
26. Dempster AP, Laird NM, Rubin DB (1977) Maximum likelihood from incomplete data
via the EM algorithm. J R Stat Soc B 39:1–38.
27. Bouchard-Côté A, Jordan MI, Klein D (2009) Efﬁcient inference in phylogenetic InDel
trees. Adv Neural Inf Process Syst 21:177–184.
28. Greenhill SJ, Blust R, Gray RD (2008) The Austronesian basic vocabulary database:
From bioinformatics to lexomics. Evol Bioinform Online 4:271–283.
29. Lewis MP, ed (2009) Ethnologue: Languages of the World (SIL International, Dallas,
TX, 16th Ed).
30. Lyovin A (1997) An Introduction to the Languages of the World (Oxford Univ Press,
Oxford, UK).
31. Hockett CF (1967) The quantiﬁcation of functional load. Word 23:320–339.
32. Surendran D, Niyogi P (2006) Competing Models of Linguistic Change. Evolution and
Beyond (Benjamins, Amsterdam).
33. Varadarajan A, Bradley RK, Holmes IH (2008) Tools for simulating evolution of aligned
genomic regions with integrated parameter estimation. Genome Biol 9(10):R147.
34. Mohri M (2009) Handbook of Weighted Automata, Monographs in Theoretical
Computer Science, eds Droste M, Kuich W, Vogler H (Springer, Berlin).
35. Hansson GO (2007) On the evolution of consonant harmony: The case of secondary
articulation agreement. Phonology 24:77–120.
36. McCullagh P, Nelder JA (1989) Generalized Linear Models (Chapman & Hall, London).
37. Dreyer M, Smith JR, Eisner J (2008) Latent-variable modeling of string transductions
with ﬁnite-state methods. Empirical Methods on Natural Language Processing 13:
1080–1089.
38. Chen SF (2003) Conditional and joint models for grapheme-to-phoneme conversion.
Eurospeech 8:2033–2036.
39. Goldwater S, Johnson M (2003) Learning OT constraint rankings using a maximum
entropy model. Proceedings of the Workshop on Variation Within Optimality Theory
eds Spenader J, Eriksson A, Dahl Ö (Stockholm University, Stockholm) pp 113–122.
40. Wilson C (2006) Learning phonology with substantive bias: An experimental and
computational study of velar palatalization. Cogn Sci 30(5):945–982.
41. Gray RD, Drummond AJ, Greenhill SJ (2009) Language phylogenies reveal expansion
pulses and pauses in Paciﬁc settlement. Science 323(5913):479–483.
42. Blust R (1999) Subgrouping, circularity and extinction: Some issues in Austronesian
comparative linguistics. Inst Linguist Acad Sinica 1:31–94.

6 of 6 | www.pnas.org/cgi/doi/10.1073/pnas.1204678110

θω;t;p;ℓ = θω;t;p;ℓ ðξ; λÞ =

expfÆλ; f ðω; t; p; ℓ; ξÞæg
× μðω; t; ξÞ;
Zðω; t; p; ℓ; λÞ

[1]

where ξ ∈ S , f : fS; I; Rg × Σ × Σ × L × S → Rk is the feature function (which
indicates which features apply for each event), Æ · ; · æ denotes inner product,
and λ ∈ Rk is a weight vector. Here, k is the dimensionality of the feature space
of the logistic regression model. In the terminology of exponential families, Z
and μ are the normalization function and the reference measure, respectively:
Zðω; t; p; ℓ; λÞ =

X

È
À
ÁÉ
exp Æλ; f ω; t; p; ℓ; ξ′ æ

ξ′∈S

8
> 0 if ω = S; t = #; ξ ≠ #
>
<
0 if ω = R; ξ = ζ
μðω; t; ξÞ =
>
> 0 if ω ≠ R; ξ = #
:
1  o:w:
Here, μ is used to handle boundary conditions, ensuring that the resulting
probability distribution is well deﬁned.
During cognate inference, the innovation Bernoulli random variables νgℓ
are similarly parameterized, using a logistic regression model with two kinds
of features: a global innovation feature κ global ∈ R and a language-speciﬁc
feature κℓ ∈ R. The likelihood function for each νgℓ then takes the form
νgℓ =

1
É:
È
1 + exp −κ global − κ ℓ

[2]

Bouchard-Côté et al.

