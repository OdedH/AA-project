Hierarchical Search
for Parsing

Adam Pauls and Dan Klein

Motivation

•

Modern parsers users very large grammars
(millions of rules!)

•

Coarse-to-Fine has proven successful (Charniak and

•

Multi-level or Hierarchical Coarse-to-Fine works
even better (Charniak and Johnson 2005, Petrov and Klein 2007)

•

In this talk, we explore an optimal hierarchical
search method: Hierarchical A*

Caraballo 1998)

Hierarchical Setting

fast parsing is fun .

Hierarchical Setting

S
NP
JJ

VP
NN VBZ JJ

.

fast parsing is fun .

Hierarchical Setting

S
NP
JJ

VP
NN VBZ JJ

.

fast parsing is fun .

Fine Grammar

Hierarchical Setting

S
Coarse Grammar

NP
JJ

VP
NN VBZ JJ

.

fast parsing is fun .

Fine Grammar

Hierarchical Setting
.
.
.

Coarser Grammar

S
Coarse Grammar

NP
JJ

VP
NN VBZ JJ

.

fast parsing is fun .

Fine Grammar

Hierarchical Setting
.
.
.

Coarser Grammar

S
Coarse Grammar

NP

VP
VP

JJ

NN VBZ JJ

.

fast parsing is fun .

Fine Grammar

VBZ JJ

Hierarchical Setting
.
.
.

Coarser Grammar

V
S
Coarse Grammar

NP

V

J

project

VP

VP
JJ

NN VBZ JJ

.

fast parsing is fun .

Fine Grammar

VBZ JJ

Agenda-Based Search

Agenda

Agenda-Based Search
“edges”
VP[2,4]
S[0,2]
.
.
.

Agenda

Agenda-Based Search
“edges”
VP[2,4]
S[0,2]
.
.
.

VB
JJ

Agenda

NP

S

S
VB
NN

VP
VB

Chart

VB
NN

NP
.

Agenda-Based Search
“edges”

S[0,2]
.
.
.

VP[2,4]

VB
JJ

Agenda

NP

S

S
VB
NN

VP
VB

Chart

VB
NN

NP
.

Agenda-Based Search
“edges”

S[0,2]
.
.
.

NP[0,2] VP[2,4]

VB
JJ

Agenda

NP

S

S
VB
NN

VP
VB

Chart

VB
NN

NP
.

Agenda-Based Search
“edges”
S
S[0,2]
.
.
.

NP
VP
NP[0,2] VP[2,4]

VB
JJ

Agenda

NP

S

S
VB
NN

VP
VB

Chart

VB
NN

NP
.

Agenda-Based Search
“edges”

S[0,4]
S

S[0,2]
.
.
.

NP
VP
NP[0,2] VP[2,4]

VB
JJ

Agenda

NP

S

S
VB
NN

VP
VB

Chart

VB
NN

NP
.

Agenda-Based Search
“edges”
S[0,2]
S[0,4]
.
.
.

VP[2,4]

VB
JJ

Agenda

NP

S

S
VB
NN

VP
VB

Chart

VB
NN

NP
.

Agenda-Based Search
“edges”
S[0,2]
S[0,4]
.
.
.

VB
JJ

Agenda

NP

S
VB
NN

VP

VP
VB

Chart

S
VB
NN

NP
.

Building Edges

Building Edges

VP

NP

1

3

3

5

Building Edges

NP

βL
1

VP

βR
3

3

5

Building Edges

S

w

NP
NP

VP
VP

βL
1

βR
3

3

5

Building Edges

S

w

NP
NP

VP
VP

βL
1

βR
3

3

5

Building Edges

S

S

w

NP
NP

VP
VP

βL
1

β

βR
3

3

5

1

5

Building Edges

S

S

w

NP
NP

VP
VP

βL
1

β
=βL +βR+w

βR
3

3

5

1

5

Building Edges

S

priority:

w

NP
NP

VP
VP

βL
1

S

β
=βL +βR+w

βR
3

3

5

1

5

Building Edges

S

priority:

w

NP
NP

β

VP
VP

βL
1

3

β

Uniform Cost
Search

βR
3

S

5

=βL +βR+w
1

5

Building Edges

S

priority:

w

NP
NP

β + h(S[1,5])

VP
VP

βL
1

S

β

A*

=βL +βR+w

βR
3

3

5

1

5

Heuristics

•

h is a heuristic which lower bounds the Viterbi
outside cost α

Heuristics

•

h is a heuristic which lower bounds the Viterbi
outside cost α

h(VP[2,4]) ≤ α(VP[2,4])

Heuristics

•

h is a heuristic which lower bounds the Viterbi
outside cost α

h(VP[2,4]) ≤ α(VP[2,4])

VP

fast parsing

is fun

.

Heuristics

•

h is a heuristic which lower bounds the Viterbi
outside cost α

h(VP[2,4]) ≤ α(VP[2,4])

VP

fast parsing

is fun

.

Heuristics

•

h is a heuristic which lower bounds the Viterbi
outside cost α

h(VP[2,4]) ≤ α(VP[2,4])

VP

fast parsing

VBZ

JJ

is fun

.

Heuristics

•

h is a heuristic which lower bounds the Viterbi
outside cost α

h(VP[2,4]) ≤ α(VP[2,4])

VP
β

fast parsing

is fun

.

Heuristics

•

h is a heuristic which lower bounds the Viterbi
outside cost α

h(VP[2,4]) ≤ α(VP[2,4])

VP
β

fast parsing

is fun

.

Heuristics

•

h is a heuristic which lower bounds the Viterbi
outside cost α

h(VP[2,4]) ≤ α(VP[2,4])
S

VP
NP
JJ

NN

fast parsing

β

is fun

.

.

Heuristics

•

h is a heuristic which lower bounds the Viterbi
outside cost α

h(VP[2,4]) ≤ α(VP[2,4])

α
VP
β

fast parsing

is fun

.

Outside Scores

•

We can get lower bounds on α from coarse
grammars

Outside Scores

•

We can get lower bounds on α from coarse
grammars

α
VP
.

Outside Scores

•

We can get lower bounds on α from coarse
grammars

α′
V

≤

α
VP
.

coarse

ﬁne

Outside Scores

•

We can get lower bounds on α from coarse
grammars

α′
V

α

≤

VP
.

coarse

•

ﬁne

How do we compute these outside scores?

Building Outside Edges

α
VP

0

3

5

n

Building Outside Edges

α
VP

0

3

5

n

Building Outside Edges

αO
S
0

1

5

n

α
VP

0

3

5

n

Building Outside Edges

αO
S
0

1

S

NP

w

5

n

α
VP
VP

0

3

5

n

Building Outside Edges

αO
S
0

1

S

w

NP

5

n

α
VP

NP[1,3]

VP

βL
1

3

0

3

5

n

Building Outside Edges

αO
S
0

1

S

w

NP

5

n

α
VP

=αO +w + βL

NP[1,3]

VP

βL
1

3

0

3

5

n

Hierarchical A* (Felzenswalb and McAllester 2007)

•

Basic Idea:
• build both inside and outside edges as needed

using same agenda
• use coarse outside scores as heuristics for ﬁne

inside edges

Hierarchically Building Inside Edges

S

S

w

NP

β+h(S[1,5])
VP

NP

VP

βL
1

β
=βL +βR+w

βR
3

3

5

1

5

Hierarchically Building Inside Edges

α′
S
0

1

5

S

S

w

NP

β+h(S[1,5])
VP

NP

VP

βL
1

n

β
=βL +βR+w

βR
3

3

5

1

5

Hierarchically Building Inside Edges

α′
S
0

1

5

S

S

w

NP

β+h(S[1,5])
VP

NP

VP

βL
1

n

β
=βL +βR+w

βR
3

3

5

1

5

Hierarchically Building Inside Edges

α′
S
0

1

5

S

S

β + α′

w

NP

VP

NP

VP

βL
1

n

β
=βL +βR+w

βR
3

3

5

1

5

HA*

HA*

Agenda

HA*

Agenda

Charts

HA*
inside

Agenda

Charts

HA*
inside

Agenda

outside

Charts

HA*
outside

coarser

inside

Agenda

Charts

HA*
outside

coarser

inside

Agenda

Charts

HA*
outside

coarser

inside

Agenda

Charts

HA*
outside

coarser

inside

Agenda

Charts

HA*
outside

coarser

inside

.
.
.

Agenda

Charts

HA*
outside

coarser

inside

.
.
.

Agenda

Charts

HA*
outside

coarser

inside

.
.
.

Agenda

Charts

HA*
outside

coarser

inside

.
.
.

Agenda

Charts

HA*
outside

coarser

inside

.
.
.

Agenda

Charts

HA*
outside

coarser

inside

.
.
.

Agenda

Charts

HA*
outside

coarser

inside

.
.
.

Agenda

Charts

HA*
outside

coarser

inside

.
.
.

Agenda

Charts

HA*
outside

coarser

inside

.
.
.

Agenda

Charts

Coarse-to-Fine

•

Prune edges in ﬁne grammar based on posteriors from
coarse grammar

.

Coarse-to-Fine

•

Prune edges in ﬁne grammar based on posteriors from
coarse grammar

•

We use Viterbi posteriors for pruning (Petrov and Klein 2007)

.

Coarse-to-Fine

•

Prune edges in ﬁne grammar based on posteriors from
coarse grammar

•

We use Viterbi posteriors for pruning (Petrov and Klein 2007)

β (e) + α (e) ≤ threshold

.

Agenda-Based CTF

•

(Hierarchical) CTF can also be thought of as an
instance of agenda-based parsing with

priority(e) =

β(e) β (e) + α (e) ≤ threshold
∞
otherwise

Agenda-Based CTF

•

(Hierarchical) CTF can also be thought of as an
instance of agenda-based parsing with

priority(e) =

•

β(e) β (e) + α (e) ≤ threshold
∞
otherwise

This reformulation makes architectures directly
comparable

HA* vs. HCTF Qualitatively
HA*

HCTF

HA* vs. HCTF Qualitatively
HA*

‣ optimal

HCTF

‣ makes search errors

HA* vs. HCTF Qualitatively
HA*

HCTF

‣ optimal

‣ makes search errors

‣ uses coarse grammars

‣ uses coarse grammars

to prioritize search

to prune search

HA* vs. HCTF Qualitatively
HA*

HCTF

‣ optimal

‣ makes search errors

‣ uses coarse grammars

‣ uses coarse grammars

‣ speed determined by

‣ speed determined by

to prioritize search

tightness of heuristic

to prune search
threshold

HA* vs. HCTF Qualitatively
HA*

HCTF

‣ optimal

‣ makes search errors

‣ uses coarse grammars

‣ uses coarse grammars

‣ speed determined by

‣ speed determined by

‣ min over rules

‣ average over rules

to prioritize search

tightness of heuristic

to prune search
threshold

Experimental Setup #1

•

Use the state-split grammars of Petrov et al. 2006

•

Train on WSJ Sections 2-21, and use 6 split
iterations, which creates 7 grammars

Experimental Setup #1

•

Use the state-split grammars of Petrov et al. 2006

•

Train on WSJ Sections 2-21, and use 6 split
iterations, which creates 7 grammars

NP

0-split

Experimental Setup #1

•

Use the state-split grammars of Petrov et al. 2006

•

Train on WSJ Sections 2-21, and use 6 split
iterations, which creates 7 grammars

NP-1
NP
NP-2
0-split

1-split

Experimental Setup #1

•

Use the state-split grammars of Petrov et al. 2006

•

Train on WSJ Sections 2-21, and use 6 split
iterations, which creates 7 grammars
NP-1
NP-1
NP
NP-2

NP-2
NP-3
NP-4

0-split

1-split

2-split

Experimental Setup #1

•

Use the state-split grammars of Petrov et al. 2006

•

Train on WSJ Sections 2-21, and use 6 split
iterations, which creates 7 grammars
NP-1
NP-1
NP
NP-2

NP-2
NP-3
NP-4

0-split

1-split

2-split

......

State-Split Projections

S-26

6-split
NP-21

VP-14

State-Split Projections

S-14
NP-12

5-split
VP-6

S-26

6-split
NP-21

VP-14

State-Split Projections
S-7

4-split
NP-5

VP-6

S-14
NP-12

5-split
VP-6

S-26

6-split
NP-21

VP-14

State-Split Projections
S-7

4-split
NP-5

VP-6

S-14
NP-12

5-split
VP-6

S-26

6-split
NP-21

VP-14

One-Level CTF vs. A*

•

Only one coarse grammar (the 3-split)

•

CTF is faster than A*, but makes search errors
500
424.00

Time

375

Makes 2%
search errors

250

125

0

86.60
8.83
Exhaustive

A*

CTF

Hierarchies

•

How do HCTF and HA* scale with size of hierarchy?

Hierarchies

•

How do HCTF and HA* scale with size of hierarchy?
3,6-split
3-6 split
0-6 split

9.00

8.83
7.12

Time

6.75
4.50
1.98

2.25
0

HCTF

Hierarchies

•

How do HCTF and HA* scale with size of hierarchy?
3,6-split
3-6 split
0-6 split

9.00

8.83

90.0
7.12

67.5
Time

Time

6.75
4.50
1.98

2.25
0

HCTF

86.60
58.80 60.10

45.0
22.5
0

HA*

Why A*?

•

CTF is faster, and extends to hierarchies nicely, so why A*?

1. If you really don’t want to make search errors

Cost of Optimality: State-Split Grammars

50.0
10.0
2.0
0.5

Edges pushed (billions)

500.0

HCTF Speed vs. Search Errors

0.65

0.70

0.75

0.80

0.85

0.90

0.95

Fraction of sentences without search errors

1.00

Cost of Optimality: State-Split Grammars

2.0

10.0

50.0

HA*

0.5

Edges pushed (billions)

500.0

HCTF Speed vs. Search Errors

0.65

0.70

0.75

0.80

0.85

0.90

0.95

Fraction of sentences without search errors

1.00

Cost of Optimality: State-Split Grammars
HCTF Speed vs. Search Errors

2.0

10.0

50.0

HA*

0.5

Edges pushed (billions)

500.0

99%

0.65

0.70

0.75

0.80

0.85

0.90

0.95

Fraction of sentences without search errors

1.00

Why A*?

•

CTF is faster, and extends to hierarchies nicely, so why A*?

1. If you really don’t want to make search errors
2. For some problems, we can ﬁnd very efﬁcient, tight heuristics

• In this case, A* is very fast

Experimental Setup #2

•

We use the factored lexicalized grammar of Klein
and Manning (2003)

•

They construct a lexicalized grammar as the crossproduct of a dependency grammar and PCFG

Experimental Setup #2

•

We use the factored lexicalized grammar of Klein
and Manning (2003)

•

They construct a lexicalized grammar as the crossproduct of a dependency grammar and PCFG

S

wp
NP

VP
PCFG

Experimental Setup #2

•

We use the factored lexicalized grammar of Klein
and Manning (2003)

•

They construct a lexicalized grammar as the crossproduct of a dependency grammar and PCFG
is-VB

S

⊗

wp
NP

VP
PCFG

wd
parsing-NN

is-VB

Dependency Grammar

Experimental Setup #2

•

We use the factored lexicalized grammar of Klein
and Manning (2003)

•

They construct a lexicalized grammar as the crossproduct of a dependency grammar and PCFG
is-VB

S

⊗

wp
NP

VP
PCFG

=

wd
parsing-NN

is-VB

Dependency Grammar

S-is-VB

wp + wd
NP-parsing-NN

VP-is-VB

Lexicalized Grammar

Cost of Optimality: Lexicalized Grammar

6
5

A*

4

A*

3

Edges pushed (billions)

7

8

CTF Speed vs. Optimality

0.65

0.70

0.75

0.80

0.85

0.90

Fraction of sentences without search errors

0.95

1.00

Conclusions
•

Coarse-to-Fine is much faster for reasonable number
of search errors

•

Hierarchical Coarse-to-Fine effectively exploits
multilevel hierarchies, Hierarchical A* does not

•

Hierarchical A* is the right choice if
•

optimality is desired

•

heuristics are very tight

Thank you

