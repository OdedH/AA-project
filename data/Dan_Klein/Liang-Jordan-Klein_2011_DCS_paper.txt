Learning Dependency-Based Compositional Semantics
Percy Liang
UC Berkeley
pliang@cs.berkeley.edu

Michael I. Jordan
UC Berkeley
jordan@cs.berkeley.edu

Dan Klein
UC Berkeley
klein@cs.berkeley.edu

(parameters)

Compositional question answering begins by
mapping questions to logical forms, but training a semantic parser to perform this mapping
typically requires the costly annotation of the
target logical forms. In this paper, we learn
to map questions to answers via latent logical forms, which are induced automatically
from question-answer pairs. In tackling this
challenging learning problem, we introduce a
new semantic representation which highlights
a parallel between dependency syntax and efﬁcient evaluation of logical forms. On two standard semantic parsing benchmarks (G EO and
J OBS), our system obtains the highest published accuracies, despite requiring no annotated logical forms.

1

(world)

θ

Abstract

w

Semantic Parsing

Evaluation

y

x

z

(question)

(logical form)

(answer)

state with the
largest area

∗∗

Alaska

x1

state
1
1

z ∼ pθ (z | x)

area
c

y= z

w

argmax

Figure 1: Our probabilistic model: a question x is
mapped to a latent logical form z, which is then evaluated
with respect to a world w (database of facts), producing
an answer y. We represent logical forms z as labeled
trees, induced automatically from (x, y) pairs.

Introduction

What is the total population of the ten largest capitals in the US? Answering these types of complex
questions compositionally involves ﬁrst mapping the
questions into logical forms (semantic parsing). Supervised semantic parsers (Zelle and Mooney, 1996;
Tang and Mooney, 2001; Ge and Mooney, 2005;
Zettlemoyer and Collins, 2005; Kate and Mooney,
2007; Zettlemoyer and Collins, 2007; Wong and
Mooney, 2007; Kwiatkowski et al., 2010) rely on
manual annotation of logical forms, which is expensive. On the other hand, existing unsupervised semantic parsers (Poon and Domingos, 2009) do not
handle deeper linguistic phenomena such as quantiﬁcation, negation, and superlatives.
As in Clarke et al. (2010), we obviate the need
for annotated logical forms by considering the endto-end problem of mapping questions to answers.
However, we still model the logical form (now as a
latent variable) to capture the complexities of language. Figure 1 shows our probabilistic model:

We want to induce latent logical forms z (and parameters θ) given only question-answer pairs (x, y),
which is much cheaper to obtain than (x, z) pairs.
The core problem that arises in this setting is program induction: ﬁnding a logical form z (over an
exponentially large space of possibilities) that produces the target answer y. Unlike standard semantic
parsing, our end goal is only to generate the correct
y, so we are free to choose the representation for z.
Which one should we use?
The dominant paradigm in compositional semantics is Montague semantics, which constructs
lambda calculus forms in a bottom-up manner. CCG
is one instantiation (Steedman, 2000), which is used
by many semantic parsers, e.g., Zettlemoyer and
Collins (2005). However, the logical forms there
can become quite complex, and in the context of
program induction, this would lead to an unwieldy
search space. At the same time, representations such
as FunQL (Kate et al., 2005), which was used in

Clarke et al. (2010), are simpler but lack the full expressive power of lambda calculus.
The main technical contribution of this work is
a new semantic representation, dependency-based
compositional semantics (DCS), which is both simple and expressive (Section 2). The logical forms in
this framework are trees, which is desirable for two
reasons: (i) they parallel syntactic dependency trees,
which facilitates parsing and learning; and (ii) evaluating them to obtain the answer is computationally
efﬁcient.
We trained our model using an EM-like algorithm
(Section 3) on two benchmarks, G EO and J OBS
(Section 4). Our system outperforms all existing
systems despite using no annotated logical forms.

2

Semantic Representation

We ﬁrst present a basic version (Section 2.1) of
dependency-based compositional semantics (DCS),
which captures the core idea of using trees to represent formal semantics. We then introduce the full
version (Section 2.2), which handles linguistic phenomena such as quantiﬁcation, where syntactic and
semantic scope diverge.
We start with some deﬁnitions, using US geography as an example domain. Let V be the set of all
values, which includes primitives (e.g., 3, CA ∈ V)
as well as sets and tuples formed from other values
(e.g., 3, {3, 4, 7}, (CA, {5}) ∈ V). Let P be a set
of predicates (e.g., state, count ∈ P), which are
just symbols.
A world w is mapping from each predicate p ∈
P to a set of tuples; for example, w(state) =
{(CA), (OR), . . . }. Conceptually, a world is a relational database where each predicate is a relation
(possibly inﬁnite). Deﬁne a special predicate ø with
w(ø) = V. We represent functions by a set of inputoutput pairs, e.g., w(count) = {(S, n) : n = |S|}.
As another example, w(average) = {(S, x) :
¯
x = |S1 |−1 x∈S1 S(x)}, where a set of pairs S
¯
is treated as a set-valued function S(x) = {y :
(x, y) ∈ S} with domain S1 = {x : (x, y) ∈ S}.
The logical forms in DCS are called DCS trees,
where nodes are labeled with predicates, and edges
are labeled with relations. Formally:
Deﬁnition 1 (DCS trees) Let Z be the set of DCS
trees, where each z ∈ Z consists of (i) a predicate

Relations R
j
j

Σ
Xi

(join)
(aggregate)
(execute)

E
Q
C

(extract)
(quantify)
(compare)

Table 1: Possible relations appearing on the edges of a
DCS tree. Here, j, j ∈ {1, 2, . . . } and i ∈ {1, 2, . . . }∗ .

z.p ∈ P and (ii) a sequence of edges z.e1 , . . . , z.em ,
each edge e consisting of a relation e.r ∈ R (see
Table 1) and a child tree e.c ∈ Z.
We write a DCS tree z as p; r1 : c1 ; . . . ; rm : cm .
Figure 2(a) shows an example of a DCS tree. Although a DCS tree is a logical form, note that it looks
like a syntactic dependency tree with predicates in
place of words. It is this transparency between syntax and semantics provided by DCS which leads to
a simple and streamlined compositional semantics
suitable for program induction.
2.1

Basic Version

The basic version of DCS restricts R to join and aggregate relations (see Table 1). Let us start by considering a DCS tree z with only join relations. Such
a z deﬁnes a constraint satisfaction problem (CSP)
with nodes as variables. The CSP has two types of
constraints: (i) x ∈ w(p) for each node x labeled
with predicate p ∈ P; and (ii) xj = yj (the j-th
component of x must equal the j -th component of
y) for each edge (x, y) labeled with jj ∈ R.
A solution to the CSP is an assignment of nodes
to values that satisﬁes all the constraints. We say a
value v is consistent for a node x if there exists a
solution that assigns v to x. The denotation z w (z
evaluated on w) is the set of consistent values of the
root node (see Figure 2 for an example).
Computation We can compute the denotation
z w of a DCS tree z by exploiting dynamic programming on trees (Dechter, 2003). The recurrence
is as follows:
j

j

p; j1 : c1 ; · · · ; jm : cm
m
1

(1)

w

m

= w(p) ∩

{v : vji = tji , t ∈ ci

w }.

i=1

At each node, we compute the set of tuples v consistent with the predicate at that node (v ∈ w(p)), and

Example: major city in California

number of
major cities
∗∗

1 1
1

major

1

loc
2
1

λc ∃m ∃ ∃s .
city(c) ∧ major(m)∧
loc( ) ∧ CA(s)∧
c1 = m1 ∧ c1 = 1 ∧

1
1

∗∗

= s1

average

1
1

2

1
2

count

city

∗∗

1
2

z = city; 1 : major ; 1 : loc; 2 : CA
1
1
1

average population of
major cities

∗∗

Σ

Σ

city

population

1
1

1
1

major

city

CA

(a) DCS tree

(b) Lambda calculus formula

(c) Denotation: z

w

= {SF, LA, . . . }

1
1

major

Figure 2: (a) An example of a DCS tree (written in both
the mathematical and graphical notation). Each node is
labeled with a predicate, and each edge is labeled with a
relation. (b) A DCS tree z with only join relations encodes a constraint satisfaction problem. (c) The denotation of z is the set of consistent values for the root node.

for each child i, the ji -th component of v must equal
the ji -th component of some t in the child’s denotation (t ∈ ci w ). This algorithm is linear in the
number of nodes times the size of the denotations.1
Now the dual importance of trees in DCS is clear:
We have seen that trees parallel syntactic dependency structure, which will facilitate parsing. In
addition, trees enable efﬁcient computation, thereby
establishing a new connection between dependency
syntax and efﬁcient semantic evaluation.
Aggregate relation DCS trees that only use join
relations can represent arbitrarily complex compositional structures, but they cannot capture higherorder phenomena in language. For example, consider the phrase number of major cities, and suppose
that number corresponds to the count predicate.
It is impossible to represent the semantics of this
phrase with just a CSP, so we introduce a new aggregate relation, notated Σ. Consider a tree Σ : c ,
whose root is connected to a child c via Σ. If the denotation of c is a set of values s, the parent’s denotation is then a singleton set containing s. Formally:
Σ:c

w

={ c

w }.

(2)

Figure 3(a) shows the DCS tree for our running
example. The denotation of the middle node is {s},

(a) Counting

(b) Averaging

Figure 3: Examples of DCS trees that use the aggregate
relation (Σ) to (a) compute the cardinality of a set and (b)
take the average over a set.

where s is all major cities. Having instantiated s as
a value, everything above this node is an ordinary
CSP: s constrains the count node, which in turns
constrains the root node to |s|.
A DCS tree that contains only join and aggregate relations can be viewed as a collection of treestructured CSPs connected via aggregate relations.
The tree structure still enables us to compute denotations efﬁciently based on (1) and (2).
2.2

Full Version

The basic version of DCS described thus far handles a core subset of language. But consider Figure 4: (a) is headed by borders, but states needs
to be extracted; in (b), the quantiﬁer no is syntactically dominated by the head verb borders but needs
to take wider scope. We now present the full version of DCS which handles this type of divergence
between syntactic and semantic scope.
The key idea that allows us to give semanticallyscoped denotations to syntactically-scoped trees is
as follows: We mark a node low in the tree with a
mark relation (one of E, Q, or C). Then higher up in
the tree, we invoke it with an execute relation Xi to
create the desired semantic scope.2
This mark-execute construct acts non-locally, so
to maintain compositionality, we must augment the

1

Inﬁnite denotations (such as < w ) are represented as implicit sets on which we can perform membership queries. The
intersection of two sets can be performed as long as at least one
of the sets is ﬁnite.

2

Our mark-execute construct is analogous to Montague’s
quantifying in, Cooper storage, and Carpenter’s scoping constructor (Carpenter, 1998).

California borders which states?

Alaska borders no states.

Some river traverses every city.

city traversed by no rivers

∗∗

∗∗

∗∗

∗∗

∗∗

x1

x1

x12

x21

x12

border

border

traverse

traverse

1 2

1 2

1 2

1 2

1

1

CA

1

state

AK

1

1

1

1

city
1
e

1

∗∗

2

state

river

city

river

city

e

q

q

q

q

q

1
1

∗∗

no

some

every

some

every

river

(narrow)

traverse

(wide)

q

no

(a) Extraction (e)

state bordering
the most states

(b) Quantiﬁcation (q)

(c) Quantiﬁer ambiguity (q, q)

state bordering
more states than Texas

state bordering
the largest state

(d) Quantiﬁcation (q, e)

Every state’s
largest city is major.

∗∗

∗∗

state

∗∗

x12

x12

1
1

x12

x1

state

state

border

state

major

1
e

∗∗

1
e

1

1
e

∗∗

x2

1

border

∗∗

2
1

2
1

x12

2
1

state

state

state

state

loc

c

c

1
1

2
1

c

argmax

more

size

state

argmax

border

∗∗

2
1

1

∗∗

1
e

∗∗

1

size

border

city
1 1
1

3
1

(e) Superlative (c)

c

c

q

TX

argmax

argmax

1

size

every

(f) Comparative (c)

(absolute)
(relative)
(g) Superlative ambiguity (c)

(h) Quantiﬁcation+Superlative (q, c)

Figure 4: Example DCS trees for utterances in which syntactic and semantic scope diverge. These trees reﬂect the
syntactic structure, which facilitates parsing, but importantly, these trees also precisely encode the correct semantic
scope. The main mechanism is using a mark relation (E, Q, or C) low in the tree paired with an execute relation (Xi )
higher up at the desired semantic point.

denotation d = z w to include any information
about the marked nodes in z that can be accessed
by an execute relation later on. In the basic version, d was simply the consistent assignments to the
root. Now d contains the consistent joint assignments to the active nodes (which include the root
and all marked nodes), as well as information stored
about each marked node. Think of d as consisting
of n columns, one for each active node according to
a pre-order traversal of z. Column 1 always corresponds to the root node. Formally, a denotation is
deﬁned as follows (see Figure 5 for an example):
Deﬁnition 2 (Denotations) Let D be the set of denotations, where each d ∈ D consists of

where each store α contains a mark relation
α.r ∈ {E, Q, C, ø}, a base denotation α.b ∈
D ∪ {ø}, and a child denotation α.c ∈ D ∪ {ø}.
We write d as A; (r1 , b1 , c1 ); . . . ; (rn , bn , cn ) . We
use d{ri = x} to mean d with d.ri = d.αi .r = x
(similar deﬁnitions apply for d{αi = x}, d{bi = x},
and d{ci = x}).
The denotation of a DCS tree can now be deﬁned
recursively:
p
p; e; jj

:c

w
w

= {[v] : v ∈ w(p)}; ø ,

(3)

= p; e

w

j,j

(4)

c

w,

p; e; Σ : c

w

= p; e

w

∗,∗

Σ( c

w) ,

(5)

• a set of arrays d.A, where each array a =
[a1 , . . . , an ] ∈ d.A is a sequence of n tuples
(ai ∈ V ∗ ); and

p; e; Xi : c

w

= p; e

w

∗,∗

Xi ( c

w ),

(6)

p; e; E : c

w

= M( p; e

w , E , c),

(7)

p; e; C : c

w

= M( p; e

w , C , c),

(8)

• a list of n stores d.α = (d.α1 , . . . , d.αn ),

p; Q : c; e

w

= M( p; e

w , Q , c).

(9)

state
1
1

border
2
1

state
1
1

size
c

argmax

DCS tree

·

w

column 1 column 2
(TX,2.7e5)
(OK)
(TX,2.7e5)
(NM)
A:
(CA,1.6e5)
(NV)
···
···
r:
ø
c
size w
b:
ø
argmax w
c:
ø
Denotation

Figure 5: Example of the denotation for a DCS tree with
a compare relation C. This denotation has two columns,
one for each active node—the root node state and the
marked node size.

full deﬁnition of join is as follows:
A; α

j,j

A ;α

= A ; α + α [1,−ø],

A = {a + a : a ∈ A, a ∈ A , a1j = a1j }. (10)
Aggregate The aggregate operation takes a denotation and forms a set out of the tuples in the ﬁrst
column for each setting of the rest of the columns:
Σ ( A; α ) = A ∪ A ; α

(11)

A = {[S(a), a2 , . . . , an ] : a ∈ A}
S(a) = {a1 : [a1 , a2 , . . . , an ] ∈ A}
A = {[∅, a2 , . . . , an ] : ¬∃a1 , a ∈ A,
∀2 ≤ i ≤ n, [ai ] ∈ d.bi [1].A}.

The base case is deﬁned in (3): if z is a single node with predicate p, then the denotation of z
has one column with the tuples w(p) and an empty
store. The other six cases handle different edge relations. These deﬁnitions depend on several operations ( j,j , Σ, Xi , M) which we will deﬁne shortly,
but let us ﬁrst get some intuition.
Let z be a DCS tree. If the last child c of z’s
root is a join (jj ), aggregate (Σ), or execute (Xi ) relation ((4)–(6)), then we simply recurse on z with c
removed and join it with some transformation (identity, Σ, or Xi ) of c’s denotation. If the last (or ﬁrst)
child is connected via a mark relation E, C (or Q),
then we strip off that child and put the appropriate
information in the store by invoking M.
We now deﬁne the operations j,j , Σ, Xi , M.
Some helpful notation: For a sequence v =
(v1 , . . . , vn ) and indices i = (i1 , . . . , ik ), let vi =
(vi1 , . . . , vik ) be the projection of v onto i; we write
v−i to mean v[1,...,n]\i . Extending this notation to
denotations, let A; α [i] = {ai : a ∈ A}; αi .
Let d[−ø] = d[−i], where i are the columns with
empty stores. For example, for d in Figure 5, d[1]
keeps column 1, d[−ø] keeps column 2, and d[2, −2]
swaps the two columns.
Join The join of two denotations d and d with respect to components j and j (∗ means all components) is formed by concatenating all arrays a of d
with all compatible arrays a of d , where compatibility means a1j = a1j . The stores are also concatenated (α + α ). Non-initial columns with empty
stores are projected away by applying ·[1,−ø]. The

2.2.1 Mark and Execute
Now we turn to the mark (M) and execute (Xi )
operations, which handles the divergence between
syntactic and semantic scope. In some sense, this is
the technical core of DCS. Marking is simple: When
a node (e.g., size in Figure 5) is marked (e.g., with
relation C), we simply put the relation r, current denotation d and child c’s denotation into the store of
column 1:
M(d, r, c) = d{r1 = r, b1 = d, c1 = c

w }.

(12)

The execute operation Xi (d) processes columns
i in reverse order. It sufﬁces to deﬁne Xi (d) for a
single column i. There are three cases:
Extraction (d.ri = E) In the basic version, the
denotation of a tree was always the set of consistent values of the root node. Extraction allows us to return the set of consistent values of a
marked non-root node. Formally, extraction simply moves the i-th column to the front: Xi (d) =
d[i, −(i, ø)]{α1 = ø}. For example, in Figure 4(a),
before execution, the denotation of the DCS tree
is {[(CA, OR), (OR)], . . . }; ø; (E, state w , ø) ;
after applying X1 , we have {[(OR)], . . . }; ø .
Generalized Quantiﬁcation (d.ri = Q) Generalized quantiﬁers are predicates on two sets, a restrictor A and a nuclear scope B. For example,
w(no) = {(A, B) : A ∩ B = ∅} and w(most) =
1
{(A, B) : |A ∩ B| > 2 |A|}.
In a DCS tree, the quantiﬁer appears as the
child of a Q relation, and the restrictor is the parent (see Figure 4(b) for an example). This information is retrieved from the store when the

quantiﬁer in column i is executed. In particular, the restrictor is A = Σ (d.bi ) and the nuclear scope is B = Σ (d[i, −(i, ø)]). We then
apply d.ci to these two sets (technically, denotations) and project away the ﬁrst column: Xi (d) =
((d.ci 1,1 A) 2,1 B) [−1].
For the example in Figure 4(b), the denotation of the DCS tree before execution is
∅; ø; (Q, state w , no w ) . The restrictor
set (A) is the set of all states, and the nuclear scope
(B) is the empty set. Since (A, B) exists in no, the
ﬁnal denotation, which projects away the actual pair,
is {[ ]} (our representation of true).
Figure 4(c) shows an example with two interacting quantiﬁers. The quantiﬁer scope ambiguity is
resolved by the choice of execute relation; X12 gives
the narrow reading and X21 gives the wide reading.
Figure 4(d) shows how extraction and quantiﬁcation
work together.
Comparatives and Superlatives (d.ri = C) To
compare entities, we use a set S of (x, y) pairs,
where x is an entity and y is a number. For superlatives, the argmax predicate denotes pairs of
sets and the set’s largest element(s): w(argmax) =
{(S, x∗ ) : x∗ ∈ argmaxx∈S1 max S(x)}. For comparatives, w(more) contains triples (S, x, y), where
x is “more than” y as measured by S; formally:
w(more) = {(S, x, y) : max S(x) > max S(y)}.
In a superlative/comparative construction, the
root x of the DCS tree is the entity to be compared,
the child c of a C relation is the comparative or superlative, and its parent p contains the information
used for comparison (see Figure 4(e) for an example). If d is the denotation of the root, its i-th column
contains this information. There are two cases: (i) if
the i-th column of d contains pairs (e.g., size in
Figure 5), then let d = ø w 1,2 d[i, −i], which
reads out the second components of these pairs; (ii)
otherwise (e.g., state in Figure 4(e)), let d =
ø w 1,2 count w 1,1 Σ (d[i, −i]), which
counts the number of things (e.g., states) that occur
with each value of the root x. Given d , we construct
a denotation S by concatenating (+i ) the second and
ﬁrst columns of d (S = Σ (+2,1 (d {α2 = ø})))
and apply the superlative/comparative: Xi (d) =
( ø w 1,2 (d.ci 1,1 S)){α1 = d.α1 }.
Figure 4(f) shows that comparatives are handled

using the exact same machinery as superlatives. Figure 4(g) shows that we can naturally account for
superlative ambiguity based on where the scopedetermining execute relation is placed.

3

Semantic Parsing

We now turn to the task of mapping natural language
utterances to DCS trees. Our ﬁrst question is: given
an utterance x, what trees z ∈ Z are permissible? To
deﬁne the search space, we ﬁrst assume a ﬁxed set
of lexical triggers L. Each trigger is a pair (x, p),
where x is a sequence of words (usually one) and p
is a predicate (e.g., x = California and p = CA).
We use L(x) to denote the set of predicates p triggered by x ((x, p) ∈ L). Let L( ) be the set of
trace predicates, which can be introduced without
an overt lexical trigger.
Given an utterance x = (x1 , . . . , xn ), we deﬁne
ZL (x) ⊂ Z, the set of permissible DCS trees for
x. The basic approach is reminiscent of projective
labeled dependency parsing: For each span i..j, we
build a set of trees Ci,j and set ZL (x) = C0,n . Each
set Ci,j is constructed recursively by combining the
trees of its subspans Ci,k and Ck ,j for each pair of
split points k, k (words between k and k are ignored). These combinations are then augmented via
a function A and ﬁltered via a function F , to be speciﬁed later. Formally, Ci,j is deﬁned recursively as
follows:
Ci,j = F A L(xi+1..j ) ∪

T1 (a, b))

.

i≤k≤k <j
a∈Ci,k
b∈Ck ,j

(13)
In (13), L(xi+1..j ) is the set of predicates triggered
by the phrase under span i..j (the base case), and
Td (a, b) = Td (a, b) ∪ T d (b, a), which returns all
ways of combining trees a and b where b is a descendant of a (Td ) or vice-versa (T d ). The former is
deﬁned recursively as follows: T0 (a, b) = ∅, and
{ a; r : b } ∪ Td−1 (a, p; r : b ).

Td (a, b) =
r∈R
p∈L( )

The latter (T k ) is deﬁned similarly. Essentially,
Td (a, b) allows us to insert up to d trace predicates between the roots of a and b. This is useful for modeling relations in noun compounds (e.g.,

California cities), and it also allows us to underspecify L. In particular, our L will not include verbs or
prepositions; rather, we rely on the predicates corresponding to those words to be triggered by traces.
The augmentation function A takes a set of trees
and optionally attaches E and Xi relations to the
root (e.g., A( city ) = { city , city; E : ø }).
The ﬁltering function F rules out improperly-typed
trees such as city; 0 : state . To further reduce
0
the search space, F imposes a few additional constraints, e.g., limiting the number of marked nodes
to 2 and only allowing trace predicates between arity 1 predicates.
Model We now present our discriminative semantic parsing model, which places a log-linear
distribution over z ∈ ZL (x) given an utterance x. Formally, pθ (z | x) ∝ eφ(x,z) θ ,
where θ and φ(x, z) are parameter and feature vectors, respectively. As a running example, consider x = city that is in California and z =
city; 1 : loc; 2 : CA , where city triggers city
1
1
and California triggers CA.
To deﬁne the features, we technically need to
augment each tree z ∈ ZL (x) with alignment
information—namely, for each predicate in z, the
span in x (if any) that triggered it. This extra information is already generated from the recursive deﬁnition in (13).
The feature vector φ(x, z) is deﬁned by sums of
ﬁve simple indicator feature templates: (F1 ) a word
triggers a predicate (e.g., [city, city]); (F2 ) a word
is under a relation (e.g., [that, 1]); (F3 ) a word is un1
der a trace predicate (e.g., [in, loc]); (F4 ) two predicates are linked via a relation in the left or right
direction (e.g., [city, 1, loc, RIGHT]); and (F5 ) a
1
predicate has a child relation (e.g., [city, 1]).
1
Learning Given a training dataset D containing (x, y) pairs, we deﬁne the regularized
marginal
log-likelihood
objective
O(θ) =
(x,y)∈D log pθ ( z w = y | x, z ∈
ZL (x)) − λ θ 2 , which sums over all DCS trees z
2
that evaluate to the target answer y.
Our model is arc-factored, so we can sum over all
DCS trees in ZL (x) using dynamic programming.
However, in order to learn, we need to sum over
{z ∈ ZL (x) : z w = y}, and unfortunately, the

additional constraint z w = y does not factorize.
We therefore resort to beam search. Speciﬁcally, we
truncate each Ci,j to a maximum of K candidates
sorted by decreasing score based on parameters θ.
˜
Let ZL,θ (x) be this approximation of ZL (x).
Our learning algorithm alternates between (i) using the current parameters θ to generate the K-best
˜
set ZL,θ (x) for each training example x, and (ii)
optimizing the parameters to put probability mass
on the correct trees in these sets; sets containing no correct answers are skipped. Formally, let
˜
O(θ, θ ) be the objective function O(θ) with ZL (x)
˜
˜
replaced with ZL,θ (x). We optimize O(θ, θ ) by
setting θ(0) = 0 and iteratively solving θ(t+1) =
˜
argmaxθ O(θ, θ(t) ) using L-BFGS until t = T . In all
experiments, we set λ = 0.01, T = 5, and K = 100.
After training, given a new utterance x, our system
outputs the most likely y, summing out the latent
˜
logical form z: argmaxy pθ(T ) (y | x, z ∈ ZL,θ(T ) ).

4

Experiments

We tested our system on two standard datasets, G EO
and J OBS. In each dataset, each sentence x is annotated with a Prolog logical form, which we use
only to evaluate and get an answer y. This evaluation is done with respect to a world w. Recall that
a world w maps each predicate p ∈ P to a set of
tuples w(p). There are three types of predicates in
P: generic (e.g., argmax), data (e.g., city), and
value (e.g., CA). G EO has 48 non-value predicates
and J OBS has 26. For G EO, w is the standard US
geography database that comes with the dataset. For
J OBS, if we use the standard Jobs database, close to
half the y’s are empty, which makes it uninteresting.
We therefore generated a random Jobs database instead as follows: we created 100 job IDs. For each
data predicate p (e.g., language), we add each possible tuple (e.g., (job37, Java)) to w(p) independently with probability 0.8.
We used the same training-test splits as Zettlemoyer and Collins (2005) (600+280 for G EO and
500+140 for J OBS). During development, we further held out a random 30% of the training sets for
validation.
Our lexical triggers L include the following: (i)
predicates for a small set of ≈ 20 function words
(e.g., (most, argmax)), (ii) (x, x) for each value

System
Clarke et al. (2010) w/answers
Clarke et al. (2010) w/logical forms
Our system (DCS with L)
Our system (DCS with L+ )

Accuracy
73.2
80.4
78.9
87.2

Table 2: Results on G EO with 250 training and 250
test examples. Our results are averaged over 10 random
250+250 splits taken from our 600 training examples. Of
the three systems that do not use logical forms, our two
systems yield signiﬁcant improvements. Our better system even outperforms the system that uses logical forms.

predicate x in w (e.g., (Boston, Boston)), and
(iii) predicates for each POS tag in {JJ, NN, NNS}
(e.g., (JJ, size), (JJ, area), etc.).3 Predicates
corresponding to verbs and prepositions (e.g.,
traverse) are not included as overt lexical triggers, but rather in the trace predicates L( ).
We also deﬁne an augmented lexicon L+ which
includes a prototype word x for each predicate appearing in (iii) above (e.g., (large, size)), which
cancels the predicates triggered by x’s POS tag. For
G EO, there are 22 prototype words; for J OBS, there
are 5. Specifying these triggers requires minimal
domain-speciﬁc supervision.
Results We ﬁrst compare our system with Clarke
et al. (2010) (henceforth, S EM R ESP), which also
learns a semantic parser from question-answer pairs.
Table 2 shows that our system using lexical triggers
L (henceforth, DCS) outperforms S EM R ESP (78.9%
over 73.2%). In fact, although neither DCS nor
S EM R ESP uses logical forms, DCS uses even less supervision than S EM R ESP. S EM R ESP requires a lexicon of 1.42 words per non-value predicate, WordNet features, and syntactic parse trees; DCS requires
only words for the domain-independent predicates
(overall, around 0.5 words per non-value predicate),
POS tags, and very simple indicator features. In
fact, DCS performs comparably to even the version
of S EM R ESP trained using logical forms. If we add
prototype triggers (use L+ ), the resulting system
(DCS+ ) outperforms both versions of S EM R ESP by
a signiﬁcant margin (87.2% over 73.2% and 80.4%).
3

We used the Berkeley Parser (Petrov et al., 2006) to perform POS tagging. The triggers L(x) for a word x thus include
L(t) where t is the POS tag of x.

System
Tang and Mooney (2001)
Wong and Mooney (2007)
Zettlemoyer and Collins (2005)
Zettlemoyer and Collins (2007)
Kwiatkowski et al. (2010)
Kwiatkowski et al. (2010)
Our system (DCS with L)
Our system (DCS with L+ )

G EO
79.4
86.6
79.3
81.6
88.2
88.9
88.6
91.1

J OBS
79.8
–
79.3
–
–
–
91.4
95.0

Table 3: Accuracy (recall) of systems on the two benchmarks. The systems are divided into three groups. Group
1 uses 10-fold cross-validation; groups 2 and 3 use the independent test set. Groups 1 and 2 measure accuracy of
logical form; group 3 measures accuracy of the answer;
but there is very small difference between the two as seen
from the Kwiatkowski et al. (2010) numbers. Our best
system improves substantially over past work, despite using no logical forms as training data.

Next, we compared our systems (DCS and DCS+ )
with the state-of-the-art semantic parsers on the full
dataset for both G EO and J OBS (see Table 3). All
other systems require logical forms as training data,
whereas ours does not. Table 3 shows that even DCS,
which does not use prototypes, is comparable to the
best previous system (Kwiatkowski et al., 2010), and
by adding a few prototypes, DCS+ offers a decisive
edge (91.1% over 88.9% on G EO). Rather than using lexical triggers, several of the other systems use
IBM word alignment models to produce an initial
word-predicate mapping. This option is not available to us since we do not have annotated logical
forms, so we must instead rely on lexical triggers
to deﬁne the search space. Note that having lexical
triggers is a much weaker requirement than having
a CCG lexicon, and far easier to obtain than logical
forms.
Intuitions How is our system learning? Initially,
the weights are zero, so the beam search is essentially unguided. We ﬁnd that only for a small fraction of training examples do the K-best sets contain
any trees yielding the correct answer (29% for DCS
on G EO). However, training on just these examples is enough to improve the parameters, and this
29% increases to 66% and then to 95% over the next
few iterations. This bootstrapping behavior occurs
naturally: The “easy” examples are processed ﬁrst,
where easy is deﬁned by the ability of the current

model to generate the correct answer using any tree.
Our system learns lexical associations between
words and predicates. For example, area (by virtue
of being a noun) triggers many predicates: city,
state, area, etc. Inspecting the ﬁnal parameters
(DCS on G EO), we ﬁnd that the feature [area, area]
has a much higher weight than [area, city]. Trace
predicates can be inserted anywhere, but the features favor some insertions depending on the words
present (for example, [in, loc] has high weight).
The errors that the system makes stem from multiple sources, including errors in the POS tags (e.g.,
states is sometimes tagged as a verb, which triggers
no predicates), confusion of Washington state with
Washington D.C., learning the wrong lexical associations due to data sparsity, and having an insufﬁciently large K.

5

Discussion

A major focus of this work is on our semantic representation, DCS, which offers a new perspective
on compositional semantics. To contrast, consider
CCG (Steedman, 2000), in which semantic parsing is driven from the lexicon. The lexicon encodes information about how each word can used in
context; for example, the lexical entry for borders
is S\NP/NP : λy.λx.border(x, y), which means
borders looks right for the ﬁrst argument and left
for the second. These rules are often too stringent,
and for complex utterances, especially in free wordorder languages, either disharmonic combinators are
employed (Zettlemoyer and Collins, 2007) or words
are given multiple lexical entries (Kwiatkowski et
al., 2010).
In DCS, we start with lexical triggers, which are
more basic than CCG lexical entries. A trigger for
borders speciﬁes only that border can be used, but
not how. The combination rules are encoded in the
features as soft preferences. This yields a more
factorized and ﬂexible representation that is easier
to search through and parametrize using features.
It also allows us to easily add new lexical triggers
without becoming mired in the semantic formalism.
Quantiﬁers and superlatives signiﬁcantly complicate scoping in lambda calculus, and often type raising needs to be employed. In DCS, the mark-execute
construct provides a ﬂexible framework for dealing

with scope variation. Think of DCS as a higher-level
programming language tailored to natural language,
which results in programs (DCS trees) which are
much simpler than the logically-equivalent lambda
calculus formulae.
The idea of using CSPs to represent semantics is
inspired by Discourse Representation Theory (DRT)
(Kamp and Reyle, 1993; Kamp et al., 2005), where
variables are discourse referents. The restriction to
trees is similar to economical DRT (Bos, 2009).
The other major focus of this work is program
induction—inferring logical forms from their denotations. There has been a fair amount of past work on
this topic: Liang et al. (2010) induces combinatory
logic programs in a non-linguistic setting. Eisenstein et al. (2009) induces conjunctive formulae and
uses them as features in another learning problem.
Piantadosi et al. (2008) induces ﬁrst-order formulae using CCG in a small domain assuming observed
lexical semantics. The closest work to ours is Clarke
et al. (2010), which we discussed earlier.
The integration of natural language with denotations computed against a world (grounding) is becoming increasingly popular. Feedback from the
world has been used to guide both syntactic parsing
(Schuler, 2003) and semantic parsing (Popescu et
al., 2003; Clarke et al., 2010). Past work has also focused on aligning text to a world (Liang et al., 2009),
using text in reinforcement learning (Branavan et al.,
2009; Branavan et al., 2010), and many others. Our
work pushes the grounded language agenda towards
deeper representations of language—think grounded
compositional semantics.

6

Conclusion

We built a system that interprets natural language
utterances much more accurately than existing systems, despite using no annotated logical forms. Our
system is based on a new semantic representation,
DCS, which offers a simple and expressive alternative to lambda calculus. Free from the burden
of annotating logical forms, we hope to use our
techniques in developing even more accurate and
broader-coverage language understanding systems.
Acknowledgments We thank Luke Zettlemoyer
and Tom Kwiatkowski for providing us with data
and answering questions.

References
J. Bos. 2009. A controlled fragment of DRT. In Workshop on Controlled Natural Language, pages 1–5.
S. Branavan, H. Chen, L. S. Zettlemoyer, and R. Barzilay.
2009. Reinforcement learning for mapping instructions to actions. In Association for Computational Linguistics and International Joint Conference on Natural
Language Processing (ACL-IJCNLP), Singapore. Association for Computational Linguistics.
S. Branavan, L. Zettlemoyer, and R. Barzilay. 2010.
Reading between the lines: Learning to map high-level
instructions to commands. In Association for Computational Linguistics (ACL). Association for Computational Linguistics.
B. Carpenter. 1998. Type-Logical Semantics. MIT Press.
J. Clarke, D. Goldwasser, M. Chang, and D. Roth.
2010. Driving semantic parsing from the world’s response. In Computational Natural Language Learning (CoNLL).
R. Dechter. 2003. Constraint Processing. Morgan Kaufmann.
J. Eisenstein, J. Clarke, D. Goldwasser, and D. Roth.
2009. Reading to learn: Constructing features from
semantic abstracts. In Empirical Methods in Natural
Language Processing (EMNLP), Singapore.
R. Ge and R. J. Mooney. 2005. A statistical semantic
parser that integrates syntax and semantics. In Computational Natural Language Learning (CoNLL), pages
9–16, Ann Arbor, Michigan.
H. Kamp and U. Reyle. 1993. From Discourse to Logic:
An Introduction to the Model-theoretic Semantics of
Natural Language, Formal Logic and Discourse Representation Theory. Kluwer, Dordrecht.
H. Kamp, J. v. Genabith, and U. Reyle. 2005. Discourse
representation theory. In Handbook of Philosophical
Logic.
R. J. Kate and R. J. Mooney. 2007. Learning language semantics from ambiguous supervision. In Association for the Advancement of Artiﬁcial Intelligence
(AAAI), pages 895–900, Cambridge, MA. MIT Press.
R. J. Kate, Y. W. Wong, and R. J. Mooney. 2005.
Learning to transform natural to formal languages. In
Association for the Advancement of Artiﬁcial Intelligence (AAAI), pages 1062–1068, Cambridge, MA.
MIT Press.
T. Kwiatkowski, L. Zettlemoyer, S. Goldwater, and
M. Steedman. 2010. Inducing probabilistic CCG
grammars from logical form with higher-order uniﬁcation. In Empirical Methods in Natural Language
Processing (EMNLP).
P. Liang, M. I. Jordan, and D. Klein. 2009. Learning semantic correspondences with less supervision. In Association for Computational Linguistics and Interna-

tional Joint Conference on Natural Language Processing (ACL-IJCNLP), Singapore. Association for Computational Linguistics.
P. Liang, M. I. Jordan, and D. Klein. 2010. Learning
programs: A hierarchical Bayesian approach. In International Conference on Machine Learning (ICML).
Omnipress.
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning accurate, compact, and interpretable tree annotation. In International Conference on Computational Linguistics and Association for Computational
Linguistics (COLING/ACL), pages 433–440. Association for Computational Linguistics.
S. T. Piantadosi, N. D. Goodman, B. A. Ellis, and J. B.
Tenenbaum. 2008. A Bayesian model of the acquisition of compositional semantics. In Proceedings of the
Thirtieth Annual Conference of the Cognitive Science
Society.
H. Poon and P. Domingos. 2009. Unsupervised semantic
parsing. In Empirical Methods in Natural Language
Processing (EMNLP), Singapore.
A. Popescu, O. Etzioni, and H. Kautz. 2003. Towards
a theory of natural language interfaces to databases.
In International Conference on Intelligent User Interfaces (IUI).
W. Schuler. 2003. Using model-theoretic semantic interpretation to guide statistical parsing and word recognition in a spoken language interface. In Association
for Computational Linguistics (ACL). Association for
Computational Linguistics.
M. Steedman. 2000. The Syntactic Process. MIT Press.
L. R. Tang and R. J. Mooney. 2001. Using multiple
clause constructors in inductive logic programming for
semantic parsing. In European Conference on Machine Learning, pages 466–477.
Y. W. Wong and R. J. Mooney. 2007. Learning synchronous grammars for semantic parsing with lambda
calculus. In Association for Computational Linguistics (ACL), pages 960–967, Prague, Czech Republic.
Association for Computational Linguistics.
M. Zelle and R. J. Mooney. 1996. Learning to parse
database queries using inductive logic proramming. In
Association for the Advancement of Artiﬁcial Intelligence (AAAI), Cambridge, MA. MIT Press.
L. S. Zettlemoyer and M. Collins. 2005. Learning to
map sentences to logical form: Structured classiﬁcation with probabilistic categorial grammars. In Uncertainty in Artiﬁcial Intelligence (UAI), pages 658–666.
L. S. Zettlemoyer and M. Collins. 2007. Online learning of relaxed CCG grammars for parsing to logical
form. In Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP/CoNLL), pages 678–687.

