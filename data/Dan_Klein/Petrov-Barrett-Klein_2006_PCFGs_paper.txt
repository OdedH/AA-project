Non-Local Modeling with a Mixture of PCFGs
Slav Petrov
Leon Barrett
Dan Klein
Computer Science Division, EECS Department
University of California at Berkeley
Berkeley, CA 94720
{petrov, lbarrett, klein}@eecs.berkeley.edu

Abstract
While most work on parsing with PCFGs
has focused on local correlations between
tree conﬁgurations, we attempt to model
non-local correlations using a ﬁnite mixture of PCFGs. A mixture grammar ﬁt
with the EM algorithm shows improvement over a single PCFG, both in parsing
accuracy and in test data likelihood. We
argue that this improvement comes from
the learning of specialized grammars that
capture non-local correlations.

1 Introduction
The probabilistic context-free grammar (PCFG) formalism is the basis of most modern statistical
parsers. The symbols in a PCFG encode contextfreedom assumptions about statistical dependencies
in the derivations of sentences, and the relative conditional probabilities of the grammar rules induce
scores on trees. Compared to a basic treebank
grammar (Charniak, 1996), the grammars of highaccuracy parsers weaken independence assumptions
by splitting grammar symbols and rules with either lexical (Charniak, 2000; Collins, 1999) or nonlexical (Klein and Manning, 2003; Matsuzaki et al.,
2005) conditioning information. While such splitting, or conditioning, can cause problems for statistical estimation, it can dramatically improve the
accuracy of a parser.
However, the conﬁgurations exploited in PCFG
parsers are quite local: rules’ probabilities may depend on parents or head words, but do not depend
on arbitrarily distant tree conﬁgurations. For example, it is generally not modeled that if one quantiﬁer

phrase (QP in the Penn Treebank) appears in a sentence, the likelihood of ﬁnding another QP in that
same sentence is greatly increased. This kind of effect is neither surprising nor unknown – for example, Bock and Loebell (1990) show experimentally
that human language generation demonstrates priming effects. The mediating variables can not only include priming effects but also genre or stylistic conventions, as well as many other factors which are not
adequately modeled by local phrase structure.
A reasonable way to add a latent variable to a
generative model is to use a mixture of estimators,
in this case a mixture of PCFGs (see Section 3).
The general mixture of estimators approach was ﬁrst
suggested in the statistics literature by Titterington
et al. (1962) and has since been adopted in machine
learning (Ghahramani and Jordan, 1994). In a mixture approach, we have a new global variable on
which all PCFG productions for a given sentence
can be conditioned. In this paper, we experiment
with a ﬁnite mixture of PCFGs. This is similar to the
latent nonterminals used in Matsuzaki et al. (2005),
but because the latent variable we use is global, our
approach is more oriented toward learning non-local
structure. We demonstrate that a mixture ﬁt with the
EM algorithm gives improved parsing accuracy and
test data likelihood. We then investigate what is and
is not being learned by the latent mixture variable.
While mixture components are difﬁcult to interpret,
we demonstrate that the patterns learned are better
than random splits.

2 Empirical Motivation
It is commonly accepted that the context freedom
assumptions underlying the PCFG model are too

14
Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X),
pages 14–20, New York City, June 2006. c 2006 Association for Computational Linguistics

VP
VBD

NP

NP

to

IN

NP

QP

increased CD NN TO
11 %

PP

PP

from

QP

# CD

CD

# 2.5 billion

# CD

CD

# 2.25 billion

Rule
QP → # CD CD
PRN → -LRB- ADJP -RRB
VP → VBD NP , PP PP
VP → VBD NP NP PP
PRN → -LRB- NP -RRBADJP → QP
PP → IN NP ADVP
NP → NP PRN
VP → VBN PP PP PP
ADVP → NP RBR

Score
131.6
77.1
33.7
28.4
17.3
13.3
12.3
12.3
11.6
10.1

Figure 1: Self-triggering: QP → # CD CD. If one British ﬁnancial occurs in the sentence, the probability of
seeing a second one in the same sentence is highly inreased. There is also a similar, but weaker, correlation
for the American ﬁnancial ($). On the right hand side we show the ten rules whose likelihoods are most
increased in a sentence containing this rule.
strong and that weakening them results in better
models of language (Johnson, 1998; Gildea, 2001;
Klein and Manning, 2003). In particular, certain
grammar productions often cooccur with other productions, which may be either near or distant in the
parse tree. In general, there exist three types of correlations: (i) local (e.g. parent-child), (ii) non-local,
and (iii) self correlations (which may be local or
non-local).
In order to quantify the strength of a correlation,
we use a likelihood ratio (LR). For two rules X → α
and Y → β, we compute
LR(X → α, Y → β) =

P(α, β|X, Y )
P(α|X, Y )P(β|X, Y )

This measures how much more often the rules occur together than they would in the case of independence. For rules that are correlated, this score will
be high (≫ 1); if the rules are independent, it will
be around 1, and if they are anti-correlated, it will be
near 0.
Among the correlations present in the Penn Treebank, the local correlations are the strongest ones;
they contribute 65% of the rule pairs with LR scores
above 90 and 85% of those with scores over 200.
Non-local and self correlations are in general common but weaker, with non-local correlations contributing approximately 85% of all correlations1 . By
adding a latent variable conditioning all productions,
1
Quantifying the amount of non-local correlation is problematic; most pairs of cooccuring rules are non-local and will,
due to small sample effects, have LR ratios greater than 1 even
if they were truly independent in the limit.

15

we aim to capture some of this interdependence between rules.
Correlations at short distances have been captured effectively in previous work (Johnson, 1998;
Klein and Manning, 2003); vertical markovization
(annotating nonterminals with their ancestor symbols) does this by simply producing a different distribution for each set of ancestors. This added context leads to substantial improvement in parsing accuracy. With local correlations already well captured, our main motivation for introducing a mixture of grammars is to capture long-range rule cooccurrences, something that to our knowledge has not
been done successfully in the past.
As an example, the rule QP → # CD CD, representing a quantity of British currency, cooccurs with itself 132 times as often as if occurrences were independent.
These cooccurrences appear in cases such as seen in Figure 1.
Similarly, the rules VP → VBD NP PP , S and
VP → VBG NP PP PP cooccur in the Penn Treebank 100 times as often as we would expect if they
were independent. They appear in sentences of a
very particular form, telling of an action and then
giving detail about it; an example can be seen in Figure 2.

3 Mixtures of PCFGs
In a probabilistic context-free grammar (PCFG),
each rule X → α is associated with a conditional
probability P(α|X) (Manning and Sch¨ tze, 1999).
u
Together, these rules induce a distribution over trees
P(T ). A mixture of PCFGs enriches the basic model

VP
VBD
hit

NP

PP

S

,

S

a record in 1998 ,

NP

VP
VBG

NP

VP

DT

PP

PP

NX

No

were present .

NX

CC

NNS

rising 1.7% after inﬂation adjustment to $13,120

or NN

lawyers

NX
NNS

tape recorders

(a)

(b)
S

S

DT

NN

VP
NNS

X

:

NP
VBP RB

These rate indications are

.

S

;
ADJP

NP
NN

n’t directly comparable

.

NNS

VBP ADVP

ADJP

.

VP

X
SYM

VBN

**

Projected

PP

lending practices vary widely by location

(c)

(d)

Figure 2: Tree fragments demonstrating coocurrences. (a) and (c) Repeated formulaic structure in one
grammar: rules VP → VBD NP PP , S and VP → VBG NP PP PP and rules VP → VBP RB ADJP
and VP → VBP ADVP PP. (b) Sibling effects, though not parallel structure, rules: NX → NNS and
NX → NN NNS. (d) A special structure for footnotes has rules ROOT → X and X → SYM coocurring
with high probability.
by allowing for multiple grammars, Gi , which we
call individual grammars, as opposed to a single
grammar. Without loss of generality, we can assume that the individual grammars share the same
set of rules. Therefore, each original rule X → α
is now associated with a vector of probabilities,
P(α|X, i). If, in addition, the individual grammars
are assigned prior probabilities P(i), then the entire
mixture induces a joint distribution over derivations
P(T, i) = P(i)P(T |i) from which we recover a distribution over trees by summing over the grammar
index i.
As a generative derivation process, we can think
of this in two ways. First, we can imagine G to be
a latent variable on which all productions are conditioned. This view emphasizes that any otherwise
unmodeled variable or variables can be captured by
the latent variable G. Second, we can imagine selecting an individual grammar Gi and then generating a sentence using that grammar. This view is
associated with the expectation that there are multiple grammars for a language, perhaps representing
different genres or styles. Formally, of course, the
two views are the same.
16

3.1

Hierarchical Estimation

So far, there is nothing in the formal mixture model
to say that rule probabilities in one component have
any relation to those in other components. However,
we have a strong intuition that many rules, such as
NP → DT NN, will be common in all mixture components. Moreover, we would like to pool our data
across components when appropriate to obtain more
reliable estimators.
This can be accomplished with a hierarchical estimator for the rule probabilities. We introduce a
shared grammar Gs . Associated to each rewrite is
now a latent variable L = {S, I } which indicates
whether the used rule was derived from the shared
grammar Gs or one of the individual grammars Gi :
P(α|X, i) =
λP(α|X, i, ℓ = I ) + (1 − λ)P(α|X, i, ℓ = S ),
where λ ≡ P (ℓ = I) is the probability of
choosing the individual grammar and can also
be viewed as a mixing coefﬁcient. Note that
P(α|X, i, ℓ = S ) = P(α|X, ℓ = S ), since the shared
grammar is the same for all individual grammars.
This kind of hierarchical estimation is analogous to
that used in hierarchical mixtures of naive-Bayes for

text categorization (McCallum et al., 1998).
The hierarchical estimator is most easily described as a generative model. First, we choose a
individual grammar Gi . Then, for each nonterminal,
we select a level from the back-off hierarchy grammar: the individual grammar Gi with probability λ,
and the shared grammar Gs with probability 1 − λ.
Finally, we select a rewrite from the chosen level. To
emphasize: the derivation of a phrase-structure tree
in a hierarchically-estimated mixture of PCFGs involves two kinds of hidden variables: the grammar
G used for each sentence, and the level L used at
each tree node. These hidden variables will impact
both learning and inference in this model.
3.2

Inference: Parsing

Parsing involves inference for a given sentence S.
One would generally like to calculate the most probable parse – that is, the tree T which has the highest probability P(T |S) ∝ i P(i)P(T |i). However, this is difﬁcult for mixture models. For a single
grammar we have:
P(T, i) = P(i)

P(α|X, i).
X→α∈T

This score decomposes into a product and it is simple to construct a dynamic programming algorithm
to ﬁnd the optimal T (Baker, 1979). However, for a
mixture of grammars we need to sum over the individual grammars:
P(T, i) =
i

P(i)
i

P(α|X, i).
X→α∈T

Because of the outer sum, this expression unfortunately does not decompose into a product over
scores of subparts. In particular, a tree which maximizes the sum need not be a top tree for any single
component.
As is true for many other grammar formalisms in
which there is a derivation / parse distinction, an alternative to ﬁnding the most probable parse is to ﬁnd
the most probable derivation (Vijay-Shankar and
Joshi, 1985; Bod, 1992; Steedman, 2000). Instead
of ﬁnding the tree T which maximizes i P(T, i),
we ﬁnd both the tree T and component i which maximize P(T, i). The most probable derivation can be
found by simply doing standard PCFG parsing once
for each component, then comparing the resulting
trees’ likelihoods.
17

3.3

Learning: Training

Training a mixture of PCFGs from a treebank is an
incomplete data problem. We need to decide which
individual grammar gave rise to a given observed
tree. Moreover, we need to select a generation path
(individual grammar or shared grammar) for each
rule in the tree. To learn estimate parameters, we
can use a standard Expectation-Maximization (EM)
approach.
In the E-step, we compute the posterior distributions of the latent variables, which are in this case
both the component G of each sentence and the hierarchy level L of each rewrite. Note that, unlike during parsing, there is no uncertainty over the actual
rules used, so the E-step does not require summing
over possible trees. Speciﬁcally, for the variable G
we have
P(T, i)
P(i|T ) =
.
j P(T, j)
For the hierarchy level L we can write
P(ℓ = I |X → α, i, T ) =
λP(α|X, ℓ = I )
,
λP(α|X, i, ℓ = I ) + (1 − λ)P(α|X, ℓ = S )
where we slightly abuse notation since the rule
X → α can occur multiple times in a tree T.
In the M-step, we ﬁnd the maximum-likelihood
model parameters given these posterior assignments; i.e., we ﬁnd the best grammars given the way
the training data’s rules are distributed between individual and shared grammars. This is done exactly
as in the standard single-grammar model using relative expected frequencies. The updates are shown in
Figure 3.3, where T = {T1 , T2 , . . . } is the training
set.
We initialize the algorithm by setting the assignments from sentences to grammars to be uniform
between all the individual grammars, with a small
random perturbation to break symmetry.

4 Results
We ran our experiments on the Wall Street Journal (WSJ) portion of the Penn Treebank using the
standard setup: We trained on sections 2 to 21,
and we used section 22 as a validation set for tuning model hyperparameters. Results are reported

Tk ∈T P(i|Tk )

P(i) ←
i

P(l = I) ←

Tk ∈T P(i|Tk )
Tk ∈T

=

X→α∈Tk

È

P(i|Tk )
k

Tk ∈T

P(ℓ = I |X → α)

Tk ∈T |Tk |
Tk ∈T

P(α|X, i, ℓ = I ) ←
α′

Tk ∈T

X→α∈Tk

P(i|Tk )P(ℓ = I|Tk , i, X → α)

X→α′ ∈Tk

P(i|Tk )P(ℓ = I|Tk , i, X → α′ )

Figure 3: Parameter updates. The shared grammar’s parameters are re-estimated in the same manner.
on all sentences of 40 words or less from section
23. We use a markovized grammar which was annotated with parent and sibling information as a
baseline (see Section 4.2). Unsmoothed maximumlikelihood estimates were used for rule probabilities as in Charniak (1996). For the tagging probabilities, we used maximum-likelihood estimates for
P(tag|word). Add-one smoothing was applied to
unknown and rare (seen ten times or less during
training) words before inverting those estimates to
give P(word|tag). Parsing was done with a simple Java implementation of an agenda-based chart
parser.
4.1

Parsing Accuracy

The EM algorithm is guaranteed to continuously increase the likelihood on the training set until convergence to a local maximum. However, the likelihood
on unseen data will start decreasing after a number
of iterations, due to overﬁtting. This is demonstrated
in Figure 4. We use the likelihood on the validation
set to stop training before overﬁtting occurs.
In order to evaluate the performance of our model,
we trained mixture grammars with various numbers
of components. For each conﬁguration, we used EM
to obtain twelve estimates, each time with a different
random initialization. We show the F1 -score for the
model with highest log-likelihood on the validation
set in Figure 4. The results show that a mixture of
grammars outperforms a standard, single grammar
PCFG parser.2
4.2

4.3

Capturing Rule Correlations

As described in Section 2, we hope that the mixture model will capture long-range correlations in
2

the data. Since local correlations can be captured
by adding parent annotation, we combine our mixture model with a grammar in which node probabilities depend on the parent (the last vertical ancestor)
and the closest sibling (the last horizontal ancestor).
Klein and Manning (2003) refer to this grammar as
a markovized grammar of vertical order = 2 and horizontal order = 1. Because many local correlations
are captured by the markovized grammar, there is a
greater hope that observed improvements stem from
non-local correlations.
In fact, we ﬁnd that the mixture does capture
non-local correlations. We measure the degree to
which a grammar captures correlations by calculating the total squared error between LR scores of the
grammar and corpus, weighted by the probability
of seeing nonterminals. This is 39422 for a single PCFG, but drops to 37125 for a mixture with
ﬁve individual grammars, indicating that the mixture model better captures the correlations present
in the corpus. As a concrete example, in the Penn
Treebank, we often see the rules FRAG → ADJP
and PRN → , SBAR , cooccurring; their LR is 134.
When we learn a single markovized PCFG from the
treebank, that grammar gives a likelihood ratio of
only 61. However, when we train with a hierarchical model composed of a shared grammar and four
individual grammars, we ﬁnd that the grammar likelihood ratio for these rules goes up to 126, which is
very similar to that of the empirical ratio.

This effect is statistically signiﬁcant.

18

Genre

The mixture of grammars model can equivalently be
viewed as capturing either non-local correlations or
variations in grammar. The latter view suggests that
the model might beneﬁt when the syntactic structure

80

Training data
Validation data
Testing data

Mixture model
Baseline: 1 grammar

Log Likelihood

79.8

F1

79.6
79.4
79.2
79
0

10

20

30

40

50

60

1

Iteration

2

3

4

5

6

7

8

9

Number of Component Grammars

(a)

(b)

Figure 4: (a) Log likelihood of training, validation, and test data during training (transformed to ﬁt on the
same plot). Note that when overﬁtting occurs the likelihood on the validation and test data starts decreasing
(after 13 iterations). (b) The accuracy of the mixture of grammars model with λ = 0.4 versus the number of
grammars. Note the improvement over a 1-grammar PCFG model.
varies signiﬁcantly, as between different genres. We
tested this with the Brown corpus, of which we used
8 different genres (f, g, k, l, m, n, p, and r). We follow Gildea (2001) in using the ninth and tenth sentences of every block of ten as validation and test
data, respectively, because a contiguous test section
might not be representative due to the genre variation.
To test the effects of genre variation, we evaluated various training schemes on the Brown corpus.
The single grammar baseline for this corpus gives
F1 = 79.75, with log likelihood (LL) on the testing
data=-242561. The ﬁrst test, then, was to estimate
each individual grammar from only one genre. We
did this by assigning sentences to individual grammars by genre, without using any EM training. This
increases the data likelihood, though it reduces the
F1 score (F1 = 79.48, LL=-242332). The increase
in likelihood indicates that there are genre-speciﬁc
features that our model can represent. (The lack of
F1 improvement may be attributed to the increased
difﬁculty of estimating rule probabilities after dividing the already scant data available in the Brown corpus. This small quantity of data makes overﬁtting
almost certain.)
However, local minima and lack of data cause difﬁculty in learning genre-speciﬁc features. If we start
with sentences assigned by genre as before, but then
train with EM, both F1 and test data log likelihood
19

drop (F1 = 79.37, LL=-242100). When we use
EM with a random initialization, so that sentences
are not assigned directly to grammars, the scores go
down even further (F1 = 79.16, LL=-242459). This
indicates that the model can capture variation between genres, but that maximum training data likelihood does not necessarily give maximum accuracy.
Presumably, with more genre-speciﬁc data available, learning would generalize better. So, genrespeciﬁc grammar variation is real, but it is difﬁcult
to capture via EM.
4.4

Smoothing Effects

While the mixture of grammars captures rule correlations, it may also enhance performance via
smoothing effects. Splitting the data randomly could
produce a smoothed shared grammar, Gs , that is
a kind of held-out estimate which could be superior to the unsmoothed ML estimates for the singlecomponent grammar.
We tested the degree of generalization by evaluating the shared grammar alone and also a mixture of the shared grammar with the known single grammar. Those shared grammars were extracted after training the mixture model with four individual grammars. We found that both the shared
grammar alone (F1 =79.13, LL=-333278) and the
shared grammar mixed with the single grammar
(F1 =79.36, LL=-331546) perform worse than a sin-

gle PCFG (F1 =79.37, LL=-327658). This indicates
that smoothing is not the primary learning effect
contributing to increased F1 .

5 Conclusions
We examined the sorts of rule correlations that may
be found in natural language corpora, discovering
non-local correlations not captured by traditional
models. We found that using a model capable of
representing these non-local features gives improvement in parsing accuracy and data likelihood. This
improvement is modest, however, primarily because
local correlations are so much stronger than nonlocal ones.

References
J. Baker. 1979. Trainable grammars for speech recognition. Speech Communication Papers for the 97th
Meeting of the Acoustical Society of America, pages
547–550.
K. Bock and H. Loebell. 1990. Framing sentences. Cognition, 35:1–39.
R. Bod. 1992. A computational model of language performance: Data oriented parsing. International Conference on Computational Linguistics (COLING).
E. Charniak. 1996. Tree-bank grammars. In Proc. of
the 13th National Conference on Artiﬁcial Intelligence
(AAAI), pages 1031–1036.
E. Charniak. 2000. A maximum–entropy–inspired
parser. In Proc. of the Conference of the North American chapter of the Association for Computational Linguistics (NAACL), pages 132–139.
M. Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, Univ. of
Pennsylvania.
Z. Ghahramani and M. I. Jordan. 1994. Supervised
learning from incomplete data via an EM approach. In
Advances in Neural Information Processing Systems
(NIPS), pages 120–127.
D. Gildea. 2001. Corpus variation and parser performance. Conference on Empirical Methods in Natural
Language Processing (EMNLP).
M. Johnson. 1998. Pcfg models of linguistic tree representations. Computational Linguistics, 24:613–632.
D. Klein and C. Manning. 2003. Accurate unlexicalized
parsing. Proc. of the 41st Meeting of the Association
for Computational Linguistics (ACL), pages 423–430.

20

C. Manning and H. Sch¨ tze. 1999. Foundations of Stau
tistical Natural Language Processing. The MIT Press,
Cambridge, Massachusetts.
T. Matsuzaki, Y. Miyao, and J. Tsujii. 2005. Probabilistic CFG with latent annotations. In Proc. of the 43rd
Meeting of the Association for Computational Linguistics (ACL), pages 75–82.
A. McCallum, R. Rosenfeld, T. Mitchell, and A. Ng.
1998. Improving text classiﬁcation by shrinkage in a
hierarchy of classes. In Int. Conf. on Machine Learning (ICML), pages 359–367.
M. Steedman. 2000. The Syntactic Process. The MIT
Press, Cambridge, Massachusetts.
D. Titterington, A. Smith, and U. Makov. 1962. Statistical Analysis of Finite Mixture Distributions. Wiley.
K. Vijay-Shankar and A. Joshi. 1985. Some computational properties of tree adjoining grammars. Proc. of
the 23th Meeting of the Association for Computational
Linguistics (ACL), pages 82–93.

