Discriminative Reordering with Chinese Grammatical Relations Features
Pi-Chuan Changa , Huihsin Tsengb , Dan Jurafskya , and Christopher D. Manninga
a Computer Science Department, Stanford University, Stanford, CA 94305
b Yahoo! Inc., Santa Clara, CA 95054
{pichuan,jurafsky,manning}@stanford.edu, huihui@yahoo-inc.com
Abstract
The prevalence in Chinese of grammatical
structures that translate into English in different word orders is an important cause of
translation difﬁculty. While previous work has
used phrase-structure parses to deal with such
ordering problems, we introduce a richer set of
Chinese grammatical relations that describes
more semantically abstract relations between
words. Using these Chinese grammatical relations, we improve a phrase orientation classiﬁer (introduced by Zens and Ney (2006))
that decides the ordering of two phrases when
translated into English by adding path features designed over the Chinese typed dependencies. We then apply the log probability of the phrase orientation classiﬁer as an
extra feature in a phrase-based MT system,
and get signiﬁcant BLEU point gains on three
test sets: MT02 (+0.59), MT03 (+1.00) and
MT05 (+0.77). Our Chinese grammatical relations are also likely to be useful for other
NLP tasks.

(a)
(ROOT
(IP
(LCP
)
(QP (CD
(CLP (M
)))
))
(LC
(PU
)
(NP
(DP (DT
))
(NP (NN
)))
(VP
(ADVP (AD
))
(VP (VV
)
(NP
(NP
(ADJP (JJ
))
(NP (NN
)))
(NP (NN
)))
(QP (CD
)
(CLP (M
)))))
(PU
)))

(b)
(ROOT
(IP
(NP
(DP (DT
))
(NP (NN
)))
(VP
(LCP
)
(QP (CD
(CLP (M
)))
(LC
))
(ADVP (AD
))
(VP (VV
)
(NP
(NP
(ADJP (JJ
))
(NP (NN
)))
(NP (NN
)))
(QP (CD
)
(CLP (M
)))))
(PU
)))

(complete)
loc

nsubj

(over; in)

advmod

(year)
nummod

range

(city)
(collectively)

lobj

dobj

det

(invest)
nn

(yuan)
nummod

(these)
(asset)

(12 billion)

amod

(three)

1

Introduction

Structural differences between Chinese and English
are a major factor in the difﬁculty of machine translation from Chinese to English. The wide variety
of such Chinese-English differences include the ordering of head nouns and relative clauses, and the
ordering of prepositional phrases and the heads they
modify. Previous studies have shown that using syntactic structures from the source side can help MT
performance on these constructions. Most of the
previous syntactic MT work has used phrase structure parses in various ways, either by doing syntaxdirected translation to directly translate parse trees
into strings in the target language (Huang et al.,
2006), or by using source-side parses to preprocess
the source sentences (Wang et al., 2007).
One intuition for using syntax is to capture different Chinese structures that might have the same

(fixed)

Figure 1: Sentences (a) and (b) have the same meaning, but different phrase structure parses. Both sentences,
however, have the same typed dependencies shown at the
bottom of the ﬁgure.

meaning and hence the same translation in English.
But it turns out that phrase structure (and linear order) are not sufﬁcient to capture this meaning relation. Two sentences with the same meaning can have
different phrase structures and linear orders. In the
example in Figure 1, sentences (a) and (b) have the
same meaning, but different linear orders and different phrase structure parses. The translation of
sentence (a) is: “In the past three years these municipalities have collectively put together investment
in ﬁxed assets in the amount of 12 billion yuan.” In
sentence (b), “in the past three years” has moved its

position. The temporal adverbial “dι d৯ dൌ” (in the
past three years) has different linear positions in the
sentences. The phrase structures are different too: in
(a) the LCP is immediately under IP while in (b) it
is under VP.
We propose to use typed dependency parses instead of phrase structure parses. Typed dependency
parses give information about grammatical relations
between words, instead of constituency information. They capture syntactic relations, such as nsubj
(nominal subject) and dobj (direct object) , but also
encode semantic information such as in the loc (localizer) relation. For the example in Figure 1, if we
look at the sentence structure from the typed dependency parse (bottom of Figure 1), “dι d৯ dൌ” is connected to the main verb dࣶdୄ (ﬁnish) by a loc (localizer) relation, and the structure is the same for
sentences (a) and (b). This suggests that this kind
of semantic and syntactic representation could have
more beneﬁt than phrase structure parses.
Our Chinese typed dependencies are automatically extracted from phrase structure parses. In English, this kind of typed dependencies has been introduced by de Marneffe and Manning (2008) and
de Marneffe et al. (2006). Using typed dependencies, it is easier to read out relations between words,
and thus the typed dependencies have been used in
meaning extraction tasks.
We design features over the Chinese typed dependencies and use them in a phrase-based MT system when deciding whether one chunk of Chinese
words (MT system statistical phrase) should appear
before or after another. To achieve this, we train a
discriminative phrase orientation classiﬁer following the work by Zens and Ney (2006), and we use
the grammatical relations between words as extra
features to build the classiﬁer. We then apply the
phrase orientation classiﬁer as a feature in a phrasebased MT system to help reordering.

2

Discriminative Reordering Model

Basic reordering models in phrase-based systems
use linear distance as the cost for phrase movements (Koehn et al., 2003). The disadvantage of
these models is their insensitivity to the content of
the words or phrases. More recent work (Tillman,
2004; Och et al., 2004; Koehn et al., 2007) has in-

troduced lexicalized reordering models which estimate reordering probabilities conditioned on the actual phrases. Lexicalized reordering models have
brought signiﬁcant gains over the baseline reordering models, but one concern is that data sparseness
can make estimation less reliable. Zens and Ney
(2006) proposed a discriminatively trained phrase
orientation model and evaluated its performance as a
classiﬁer and when plugged into a phrase-based MT
system. Their framework allows us to easily add in
extra features. Therefore we use it as a testbed to see
if we can effectively use features from Chinese typed
dependency structures to help reordering in MT.
2.1

Phrase Orientation Classiﬁer

We build up the target language (English) translation
from left to right. The phrase orientation classiﬁer
predicts the start position of the next phrase in the
source sentence. In our work, we use the simplest
class deﬁnition where we group the start positions
into two classes: one class for a position to the left of
the previous phrase (reversed) and one for a position
to the right (ordered).
Let c j, j be the class denoting the movement from
source position j to source position j of the next
phrase. The deﬁnition is:
c j, j =

reversed
ordered

if j < j
if j > j

The phrase orientation classiﬁer model is in the loglinear form:
J
pλ N (c j, j | f1 , eI , i, j)
1
1

=

J
exp ∑N λn hn ( f1 , eI , i, j, c j, j )
n=1
1
N
J , eI , i, j, c )
∑c exp ∑n=1 λn hn ( f1 1

J
i is the target position of the current phrase, and f1
and eI denote the source and target sentences respec1
tively. c represents possible categories of c j, j .
We can train this log-linear model on lots of labeled examples extracted from all of the aligned MT
training data. Figure 2 is an example of an aligned
sentence pair and the labeled examples that can be
extracted from it. Also, different from conventional
MERT training, we can have a large number of binary features for the discriminative phrase orientation classiﬁer. The experimental setting will be described in Section 4.1.

j
i

(0)
<s>

(1)

(2)

(3)

(4)

(5) (6)

(7)

(8)

(9)

(10) (11) (12) (13) (14) (15)
</s>

(0) <s>
(1) Beihai
i

(10) China

ordered

3

11

ordered

11

12

ordered

12

13

ordered

13

9

reversed

8

(9) from

ordered

3

7

(8) arising

2

2

6

(7) star

1

5

(6) bright

ordered

4

(5) a

class

1

3

(4) become

j'

0

1

(3) already

j

0

(2) has

9

10

ordered

9

7

5

reversed

5

6

ordered

6

14

ordered

20

(15) up

reversed

18

(14) opening

reversed

7

16

(13) of

8

8

15

(12) policy

10

10

(11) 's

14

15

ordered

(16) to
(17) the
(18) outside
(19) world
(20) .
(21) </s>

Figure 2: An illustration of an alignment grid between a Chinese sentence and its English translation along with the
labeled examples for the phrase orientation classiﬁer. Note that the alignment grid in this example is automatically
generated.

The basic feature functions are similar to what
Zens and Ney (2006) used in their MT experiments.
The basic binary features are source words within a
window of size 3 (d ∈ −1, 0, 1) around the current
source position j, and target words within a window
of size 3 around the current target position i. In the
classiﬁer experiments in Zens and Ney (2006) they
also use word classes to introduce generalization capabilities. In the MT setting it’s harder to incorporate the part-of-speech information on the target language. Zens and Ney (2006) also exclude word class
information in the MT experiments. In our work
we will simply use the word features as basic features for the classiﬁcation experiments as well. As
a concrete example, we look at the labeled example
(i = 4, j = 3, j = 11) in Figure 2. We include the
word features in a window of size 3 around j and i

as in Zens and Ney (2006), we also include words
around j as features. So we will have nine word
features for (i = 4, j = 3, j = 11):
Src−1 :dী
Src2−1 :dሇ
Tgt−1 :already
2.2

Src0 :dୄdϛ
Src20 :dδ
Tgt0 :become

Src1 :dϔd‫ދ‬
Src21 :dᲡ
Tgt1 :a

Path Features Using Typed Dependencies

Assuming we have parsed the Chinese sentence that
we want to translate and have extracted the grammatical relations in the sentence, we design features
using the grammatical relations. We use the path between the two words annotated by the grammatical
relations. Using this feature helps the model learn
about what the relation is between the two chunks
of Chinese words. The feature is deﬁned as follows:
for two words at positions p and q in the Chinese

sentence (p < q), we ﬁnd the shortest path in the
typed dependency parse from p to q, concatenate all
the relations on the path, normalize1 it and use that
as a feature.
A concrete example is the sentences in Figure 3,
where the alignment grid and labeled examples are
shown in Figure 2. The glosses of the Chinese words
in the sentence are in Figure 3, and the English translation is “Beihai has already become a bright star
arising from China’s policy of opening up to the outside world.” which is also listed in Figure 2.
For the labeled example (i = 4, j = 3, j = 11),
we look at the typed dependency parse to ﬁnd the
path feature between dୄ dϛ and dδ. The relevant
dependencies are: dobj(dୄdϛ, d೗dೞ), clf (d೗dೞ, dᲡ)
and nummod(dᲡ, dδ). Therefore the path feature is
PATH:dobjR-clfR-nummodR. We also use the directionality: we add an R to the dependency name if it’s
going against the direction of the arrow.

3

Chinese Grammatical Relations

Our Chinese grammatical relations are designed to
be very similar to the Stanford English typed dependencies (de Marneffe and Manning, 2008; de Marneffe et al., 2006).
3.1

Description

There are 44 named grammatical relations, and a default 45th relation dep (dependent). If a dependency
matches no patterns, it will have the most generic
relation dep. The descriptions of the 44 grammatical relations are listed in Table 2 ordered by their
frequencies in ﬁles 1–325 of CTB6 (LDC2007T36).
The total number of dependencies is 85748, and
other than the ones that fall into the 44 grammatical
relations, there are also 7470 dependencies (8.71%
of all dependencies) that do not match any patterns,
and therefore keep the generic name dep.
1 This

is a correction to the version in the proceedings. We
found that if we include features of both directions like dobj
and dobjR, these features got incorrectly over-trained. Because
these features implicitly encode the information of the order
of Chinese words. Therefore, we normalized features by only
picking one directions. For example, both features prep-dobjR
and prepR-dobj are normalized into the same feature. By doing
this, the features no longer leak information of the correct class
in the training phase, and should be more accurate when used
to predict the ordering in the testing phase.

Shared relations
nn
punct
nsubj
rcmod
dobj
advmod
conj
num/nummod
attr
tmod
ccomp
xsubj
cop
cc
amod
prep
det
pobj

Chinese
15.48%
12.71%
6.87%
2.74%
6.09%
4.93%
6.34%
3.36%
0.62%
0.79%
1.30%
0.22%
0.07%
2.06%
3.14%
3.66%
1.30%
2.82%

English
6.81%
9.64%
4.46%
0.44%
3.89%
2.73%
4.50%
1.65%
0.01%
0.25%
0.84%
0.34%
0.85%
3.73%
7.83%
10.73%
8.57%
10.49%

Table 1: The percentage of typed dependencies in ﬁles
1–325 in Chinese (CTB6) and English (English-Chinese
Translation Treebank)

3.2

Chinese Speciﬁc Structures

Although we designed the typed dependencies to
show structures that exist both in Chinese and English, there are many other syntactic structures that
only exist in Chinese. The typed dependencies we
designed also cover those Chinese speciﬁc structures. For example, the usage of “dሇ” (DE) is one
thing that could lead to different English translations. In the Chinese typed dependencies, there
are relations such as cpm (DE as complementizer)
or assm (DE as associative marker) that are used
to mark these different structures. The Chinesespeciﬁc “d୿” (BA) construction also has a relation
ba dedicated to it.
The typed dependencies annotate these Chinese
speciﬁc relations, but do not directly provide a mapping onto how they are translated into English. It
becomes more obvious how those structures affect
the ordering when Chinese sentences are translated
into English when we apply the typed dependencies
as features in the phrase orientation classiﬁer. This
will be further discussed in Section 4.4.
3.3

Comparison with English

To compare the distribution of Chinese typed dependencies with English, we extracted the English
typed dependencies from the translation of ﬁles 1–

dobj
nsubj
pobj

nsubj

Beihai

already

become

advmod

China to

lccomp

outside open

rcmod

loc

during

rising
cpm

prep

(DE) one

measure
word

nummod

bright
star

.

clf

punct

Figure 3: A Chinese example sentence labeled with typed dependencies

325 in the English Chinese Translation Treebank
1.0 (LDC2007T02), which correspond to ﬁles 1–325
in CTB6. The English typed dependencies are extracted using the Stanford Parser.
There are 116, 799 total English dependencies,
and 85, 748 Chinese ones. On the corpus we use,
there are 44 distinct dependency types (not including dep) in Chinese, and 50 in English. The coverage of named relations is 91.29% in Chinese and
90.48% in English; the remainder are the unnamed
relation dep. We looked at the 18 shared relations
between Chinese and English in Table 1. Chinese
has more nn, punct, nsubj, rcmod, dobj, advmod,
conj, nummod, attr, tmod, and ccomp while English
uses more pobj, det, prep, amod, cc, cop, and xsubj,
due mainly to grammatical differences between Chinese and English. For example, some determiners
in English (e.g., “the” in (1b)) are not mandatory in
Chinese:
(1a) dᨆdՠd‫/ح‬import and export d઱dᲦ/total value
(1b) The total value of imports and exports

In another difference, English uses adjectives
(amod) to modify a noun (“ﬁnancial” in (2b)) where
Chinese can use noun compounds (“d᫘d឵/ﬁnance”
in (2a)).
(2a) dᠲdᜦ/Tibet d᫘d឵/ﬁnance dѱdց/system dಅdᱦ/reform
(2b) the reform in Tibet ’s ﬁnancial system

We also noticed some larger differences between
the English and Chinese typed dependency distributions. We looked at speciﬁc examples and provide
the following explanations.
prep and pobj English has much more uses of prep
and pobj. We examined the data and found three
major reasons:

1. Chinese uses both prepositions and postpositions while English only has prepositions. “After” is used as a postposition in Chinese example (3a), but a preposition in English (3b):
(3a) dϲdζ/1997 dϥd੣/after
(3b) after 1997

2. Chinese uses noun phrases in some cases where
English uses prepositions. For example, “dϥ
dᯋ” (period, or during) is used as a noun phrase
in (4a), but it’s a preposition in English.
(4a) dϲdζ/1997 dտ/to dϲdԙ/1998 dϥdᯋ /period
(4b) during 1997-1998

3. Chinese can use noun phrase modiﬁcation in
situations where English uses prepositions. In
example (5a), Chinese does not use any prepositions between “apple company” and “new
product”, but English requires use of either
“of” or “from”.
(5a) dᘮd൤dԚd‫/ـ‬apple company dಱdДd‫/ڳ‬new product
(5b) the new product of (or from) Apple

The Chinese DE constructions are also often
translated into prepositions in English.
cc and punct The Chinese sentences contain more
punctuation (punct) while the English translation
has more conjunctions (cc), because English uses
conjunctions to link clauses (“and” in (6b)) while
Chinese tends to use only punctuation (“,” in (6a)).
(6a) dᨅdЌ/these dߟdে/city dየdѕ/social dᑀd༽/economic
d‫ؤ‬dॖ/development d᧻dᨱ/rapid dƥ d‫ޞ‬dಲ/local
dᑀd༽/economic dँd֤/strength d೗d೪/clearly
dࠖdਿ/enhance
(6b) In these municipalities the social and economic development has been rapid, and the local economic
strength has clearly been enhanced

rcmod and ccomp There are more rcmod and
ccomp in the Chinese sentences and less in the English translation, because of the following reasons:

abbreviation
nn
punct
nsubj
conj
dobj
advmod
prep
nummod
amod
pobj
rcmod
cpm
assm
assmod
cc
clf
ccomp
det
lobj
range
asp
tmod
plmod
attr
mmod
loc
top
pccomp
etc
lccomp
ordmod
xsubj
neg
rcomp
comod
vmod
prtmod
ba
dvpm
dvpmod
prnmod
cop
pass
nsubjpass

short description
noun compound modiﬁer
punctuation
nominal subject
conjunct (links two conjuncts)
direct object
adverbial modiﬁer
prepositional modiﬁer
number modiﬁer
adjectival modiﬁer
prepositional object
relative clause modiﬁer
complementizer
associative marker
associative modiﬁer
coordinating conjunction
classiﬁer modiﬁer
clausal complement
determiner
localizer object
dative object that is a quantiﬁer phrase
aspect marker
temporal modiﬁer
localizer modiﬁer of a preposition
attributive
modal verb modiﬁer
localizer
topic
clausal complement of a preposition
etc modiﬁer
clausal complement of a localizer
ordinal number modiﬁer
controlling subject
negative modiﬁer
resultative complement
coordinated verb compound modiﬁer
verb modiﬁer
particles such as d୛, dм, dൌ, dᓨ
“ba” construction
manner DE(d‫ )ޞ‬modiﬁer
a “XP+DEV(d‫ ”)ޞ‬phrase that modiﬁes VP
parenthetical modiﬁer
copular
passive marker
nominal passive subject

Chinese example
dദd֩ dϔdੴ
dཏdԟ dᑏdᡝ d៨d೗ dƥ
d෌dᘆ dሦdਥ
dᡷdࠩ d‫ ڔ‬dؑdൃdಥ
dཉdω d᲏dৈ dϽ dζd‫מ‬dδ dх dಞdх
d᪇dᯀ dԌ d᨜dκ dಞdх
d‫ ޗ‬dँd᥸ dϔ dᨧd๣ dࣶd‫ܐ‬
dζd‫מ‬dδ dх dಞdх
dᥲdτdᐟ dষdጫ
dටd௳ dണdԟ dᠸdࣽ
dν dട dᩂdտ d᧼ dሇ d૮dՃ
dਥd‫ ؤ‬dཉdω dሇ dᑀd༽ d༰d֬
dъdχ dሇ d۵d‫ڳ‬
dъdχ dሇ d۵d‫ڳ‬
dᡷdࠩ d‫ ڔ‬dؑdൃdಥ
dζd‫מ‬dδ dх dಞdх
d᭍d៞ dՂdࣽ dԌ d‫ئ‬d੧ dҴdᅶ d᡽dᐜ
dᨅdЌ dᑀd༽ d༰d֬
dᨀd৯ dൌ
dୄdБ dᙯd‫ ڳ‬dδdРd࠰ dԈ
d‫ؤ‬d௖ dϽ dѸdᅶ
dмd֏ dν dട dᩂdտ d᧼
d‫ ޗ‬dᨅ dႜ dၔd‫ ޕ‬dκ
dᤜd೙dᲦ dϛ dЁdህdР dᒼdԈ
dջdማ dᕜ d੧dտ dұdᰧ
d‫ ױ‬dϲdୄ dмdκ
dਢd᎐ d೤ dϜdᠳ d༰d֬
d௳ dണdԟ d᪇dᯀ dЫdᐾ
d጖d୼ dˉ dಞd಑ dᎋ dᲔdߨ
dϔd‫ ދ‬dन d࠮ dਥdಇ dϔ d‫ס‬d᥎ dሇ d೗dೞ
dᎁdζ dϑ dസd൛
d᭍d៞ dՂdࣽ dԌ d‫ئ‬d੧ dҴdᅶ d᡽dᐜ
dмd֏ dν dട dᩂdտ d᧼
d኎dፀ dୄd֧
d᲏dৈ dँd៞
dԢ d‫ ޗ‬dಀd௄ d࠮d۵ dъdχ dಲdᱤ dሇ dѸdᅶ
d‫ ޗ‬dДdχd‫ ׎‬d୛ d‫ئ‬d੧ dሇ dୄdी
d୿ d༉dଐd֤ dᦾdّ dেd‫ޡ‬
dണdಊ d‫ ޞ‬d᯶d๠ d༴d࠼
dണdಊ d‫ ޞ‬d᯶d๠ d༴d࠼
dԙdЈ dഭdᯋ dơ 1990 – 1995 dƢ
dؑ d೤ dᖪdᑉdᖪdᥙ dሇ dᑀd༽
d៿ dᡠdࣽ dϛ dᴱ d୼dഴ dДdχ
dᮛ d៿ dጣdѸ dᄕdк dষdχ dሇ dᑢdᅴdᐆ

typed dependency
nn(dϔdੴ, dദd֩)
punct(d៨d೗, dƥ)
nsubj(dሦdਥ, d෌dᘆ)
conj(dؑdൃdಥ, dᡷdࠩ)
dobj(d᲏dৈ, dಞdх)
advmod(d᨜dκ, dԌ)
prep(dࣶd‫ ,ܐ‬d‫)ޗ‬
nummod(dх, dζd‫מ‬dδ)
amod(dষdጫ, dᥲdτdᐟ)
pobj(dටd௳, dᠸdࣽ)
rcmod(d૮dՃ, dᩂdտ)
cpm(dਥd‫ ,ؤ‬dሇ)
assm(dъdχ, dሇ)
assmod(d۵d‫ ,ڳ‬dъdχ)
cc(dؑdൃdಥ, d‫)ڔ‬
clf(dಞdх, dх)
ccomp(dՂdࣽ, d‫ئ‬d੧)
det(d༰d֬, dᨅdЌ)
lobj(dൌ, dᨀd৯)
range(dୄdБ, dԈ)
asp(d‫ؤ‬d௖, dϽ)
tmod(dᩂdտ, dмd֏)
plmod(d‫ ,ޗ‬dκ)
attr(dϛ, dᒼdԈ)
mmod(d੧dտ, dᕜ)
loc(d‫ ,ױ‬dмdκ)
top(d೤, dਢd᎐)
pccomp(d௳, dЫdᐾ)
etc(dಞd಑, dᎋ)
lccomp(dϔ, dਥdಇ)
ordmod(dϑ, dᎁdζ)
xsubj(d‫ئ‬d੧, d᭍d៞)
neg(dᩂdտ, dν)
rcomp(d኎dፀ, dୄd֧)
comod(d᲏dৈ, dँd៞)
vmod(dಲdᱤ, dಀd௄)
prtmod(d‫ئ‬d੧, d୛)
ba(dᦾdّ, d୿)
dvpm(dണdಊ, d‫)ޞ‬
dvpmod(d᯶d๠, dണdಊ)
prnmod(dഭdᯋ, 1995)
cop(dᖪdᑉdᖪdᥙ, d೤)
pass(dᡠdࣽ, d៿)
nsubjpass(dጣdѸ, dᮛ)

counts
13278
10896
5893
5438
5221
4231
3138
2885
2691
2417
2348
2013
1969
1941
1763
1558
1113
1113
1010
891
857
679
630
534
497
428
380
374
295
207
199
192
186
176
150
133
124
95
73
69
67
59
53
14

percentage
15.48%
12.71%
6.87%
6.34%
6.09%
4.93%
3.66%
3.36%
3.14%
2.82%
2.74%
2.35%
2.30%
2.26%
2.06%
1.82%
1.30%
1.30%
1.18%
1.04%
1.00%
0.79%
0.73%
0.62%
0.58%
0.50%
0.44%
0.44%
0.34%
0.24%
0.23%
0.22%
0.22%
0.21%
0.17%
0.16%
0.14%
0.11%
0.09%
0.08%
0.08%
0.07%
0.06%
0.02%

Table 2: Chinese grammatical relations and examples. The counts are from ﬁles 1–325 in CTB6.

1. Some English adjectives act as verbs in Chinese. For example, dಱ (new) is an adjectival
predicate in Chinese and the relation between
dಱ (new) and dց d਌ (system) is rcmod. But
“new” is an adjective in English and the English relation between “new” and “system” is
amod. This difference contributes to more rcmod in Chinese.
(7a) dಱ/new dሇ/(DE) dඦd᭖/verify and write off
(7b) a new sales veriﬁcation system

2. Chinese has two special verbs (VC): d೤ (SHI)
and dϛ (WEI) which English doesn’t use. For
example, there is an additional relation, ccomp,

between the verb d೤/(SHI) and dᰉdѭ/reduce in
(8a). The relation is not necessary in English,
since d೤/SHI is not translated.
(8a) dЁ/second d೤/(SHI) dδdϲdϲdԛd৯/1996
dϔd‫/ދ‬China d࠵d৥d਌/substantially
dᰉdѭ/reduce dԟdጭ/tariff
(8b) Second, China reduced tax substantially in
1996.

conj There are more conj in Chinese than in English for three major reasons. First, sometimes one
complete Chinese sentence is translated into several English sentences. Our conj is deﬁned for two
grammatical roles occurring in the same sentence,

and therefore, when a sentence breaks into multiple
ones, the original relation does not apply. Second,
we deﬁne the two grammatical roles linked by the
conj relation to be in the same word class. However,
words which are in the same word class in Chinese
may not be in the same word class in English. For
example, adjective predicates act as verbs in Chinese, but as adjectives in English. Third, certain constructions with two verbs are described differently
between the two languages: verb pairs are described
as coordinations in a serial verb construction in Chinese, but as the second verb being the complement
of the ﬁrst verb in English.

rules. Documents of Gigaword released during the
epochs of MT02, MT03, MT05, and MT06 were
removed. For features in MT experiments, we incorporate Moses’ standard nine features as well as
the lexicalized reordering features. To have a more
comparable setting with (Zens and Ney, 2006), we
also have a baseline experiment with only the standard nine features. Parameter tuning is done with
Minimum Error Rate Training (MERT) (Och, 2003).
The tuning set for MERT is the NIST MT06 data
set, which includes 1664 sentences. We evaluate the
result with MT02 (878 sentences), MT03 (919 sentences), and MT05 (1082 sentences).

4

4.2

Experimental Results

4.1

Experimental Setting

We use various Chinese-English parallel corpora2
for both training the phrase orientation classiﬁer, and
for extracting statistical phrases for the phrase-based
MT system. The parallel data contains 1, 560, 071
sentence pairs from various parallel corpora. There
are 12, 259, 997 words on the English side. Chinese word segmentation is done by the Stanford Chinese segmenter (Chang et al., 2008). After segmentation, there are 11,061,792 words on the Chinese
side. The alignment is done by the Berkeley word
aligner (Liang et al., 2006) and then we symmetrized
the word alignment using the grow-diag heuristic.
For the phrase orientation classiﬁer experiments,
we extracted labeled examples using the parallel
data and the alignment as in Figure 2. We extracted
9, 194, 193 total valid examples: 86.09% of them are
ordered and the other 13.91% are reversed. To evaluate the classiﬁer performance, we split these examples into training, dev and test set (8 : 1 : 1). The
phrase orientation classiﬁer used in MT experiments
is trained with all of the available labeled examples.
Our MT experiments use a re-implementation of
Moses (Koehn et al., 2003) called Phrasal, which
provides an easier API for adding features. We
use a 5-gram language model trained on the Xinhua and AFP sections of the Gigaword corpus
(LDC2007T40) and also the English side of all the
LDC parallel data permissible under the NIST08
2 LDC2002E18,

LDC2003E07,
LDC2003E14,
LDC2005E83, LDC2005T06, LDC2006E26, LDC2006E85,
LDC2002L27 and LDC2005T34.

Phrase Orientation Classiﬁer

Feature Sets

#features

Majority class

1483696
2976108
4440492
4674291

Src
Src+Tgt
Src+Src2+Tgt
Src+Src2+Tgt+PATH

Train. Acc.
Acc. (%)
86.09
89.02
92.47
95.03
97.74

Train.
Macro-F
71.33
82.52
88.76
95.13

Dev
Acc. (%)
86.09
88.14
91.29
93.64
96.35

Dev
Macro-F
69.03
79.80
85.58
92.09

Table 3: Feature engineering of the phrase orientation
classiﬁer. Accuracy is deﬁned as (#correctly labeled examples) divided by (#all examples). The macro-F is an
average of the accuracies of the two classes.

The basic source word features described in Section 2 are referred to as Src, and the target word
features as Tgt. The feature set that Zens and Ney
(2006) used in their MT experiments is Src+Tgt. In
addition to that, we also experimented with source
word features Src2 which are similar to Src, but take
a window of 3 around j instead of j. In Table 3
we can see that adding the Src2 features increased
the total number of features by almost 50%, but also
improved the performance. The PATH features add
fewer total number of features than the lexical features, but still provide a 10% error reduction and
1.63 on the macro-F1 on the dev set. We use the best
feature sets from the feature engineering in Table 3
and test it on the test set. We get 96.38% accuracy
and 92.10 macro-F1. The overall improvement of
accuracy over the baseline is 10.09 absolute points.
4.3

MT Experiments

In the MT setting, we use the log probability from
the phrase orientation classiﬁer as an extra feature.
The weight of this discriminative reordering feature
is also tuned by MERT, along with other Moses

Setting
Moses9Features
Moses9Features+DiscrimRereorderNoPATH
Moses9Features+DiscrimRereorderWithPATH
Baseline (Moses with lexicalized reordering)
Baseline+DiscrimRereorderNoPATH
Baseline+DiscrimRereorderWithPATH

#MERT features
9
10
10
16
17
17

MT06(tune)
31.49
31.76(+0.27)
32.34(+0.85)
32.55
32.73(+0.18)
32.97(+0.42)

MT02
31.63
31.86(+0.23)
32.59(+0.96)
32.56
32.58(+0.02)
33.15(+0.59)

MT03
31.26
32.09(+0.83)
32.70(+1.44)
32.65
32.99(+0.34)
33.65(+1.00)

MT05
30.26
31.14(+0.88)
31.84(+1.58)
31.89
31.80(−0.09)
32.66(+0.77)

Table 4: MT experiments of different settings on various NIST MT evaluation datasets. All differences marked in bold
are signiﬁcant at the level of 0.05 with the approximate randomization test in (Riezler and Maxwell, 2005).
det

nn

4.4

every level product
products of all level
det
whole

nn

city this year industry total output value

gross industrial output value of the whole city this year

Figure 4: Two examples for the feature PATH:det-nn and
how the reordering occurs.

features. In order to understand how much the
PATH features add value to the MT experiments, we
trained two phrase orientation classiﬁers with different features: one with the Src+Src2+Tgt feature set,
and the other one with Src+Src2+Tgt+PATH. The results are listed in Table 4. We compared to two
different baselines: one is Moses9Features which
has a distance-based reordering model, the other is
Baseline which also includes lexicalized reordering features. From the table we can see that using
the discriminative reordering model with PATH features gives signiﬁcant improvement over both baselines. If we use the discriminative reordering model
without PATH features and only with word features,
we still get improvement over the Moses9Features
baseline, but the MT performance is not signiﬁcantly different from Baseline which uses lexicalized reordering features. From Table 4 we see that
using the Src+Src2+Tgt+PATH features signiﬁcantly
outperforms both baselines. Also, if we compare between Src+Src2+Tgt and Src+Src2+Tgt+PATH, the
differences are also statistically signiﬁcant, which
shows the effectiveness of the path features.

Analysis: Highly-weighted Features in the
Phrase Orientation Model

There are a lot of features in the log-linear phrase
orientation model. We looked at some highlyweighted PATH features to understand what kind
of grammatical constructions were informative for
phrase orientation. We found that many path features corresponded to our intuitions. For example,
the feature PATH:prep-dobjR has a high weight for
being reversed. This feature informs the model that
in Chinese a PP usually appears before VP, but in
English they should be reversed. Other features
with high weights include features related to the
DE construction that is more likely to translate to
a relative clause, such as PATH:advmod-rcmod and
PATH:rcmod. They also indicate the phrases are
more likely to be chosen in reversed order. Another
frequent pattern that has not been emphasized in the
previous literature is PATH:det-nn, meaning that a
[DT NP1 NP2 ] in Chinese is translated into English
as [NP2 DT NP1 ]. Examples with this feature are
in Figure 4. We can see that the important features
decided by the phrase orientation model are also important from a linguistic perspective.

5

Conclusion

We introduced a set of Chinese typed dependencies
that gives information about grammatical relations
between words, and which may be useful in other
NLP applications as well as MT. We used the typed
dependencies to build path features and used them to
improve a phrase orientation classiﬁer. The path features gave a 10% error reduction on the accuracy of
the classiﬁer and 1.63 points on the macro-F1 score.
We applied the log probability as an additional feature in a phrase-based MT system, which improved
the BLEU score of the three test sets signiﬁcantly
(0.59 on MT02, 1.00 on MT03 and 0.77 on MT05).

This shows that typed dependencies on the source
side are informative for the reordering component in
a phrase-based system. Whether typed dependencies can lead to improvements in other syntax-based
MT systems remains a question for future research.

Acknowledgments
The authors would like to thank Marie-Catherine de
Marneffe for her help on the typed dependencies,
and Daniel Cer for building the decoder. This work
is funded by a Stanford Graduate Fellowship to the
ﬁrst author and gift funding from Google for the
project “Translating Chinese Correctly”.

References
Pi-Chuan Chang, Michel Galley, and Christopher D.
Manning. 2008. Optimizing Chinese word segmentation for machine translation performance. In Proceedings of the Third Workshop on Statistical Machine
Translation, pages 224–232, Columbus, Ohio, June.
Association for Computational Linguistics.
Marie-Catherine de Marneffe and Christopher D. Manning. 2008. The Stanford typed dependencies representation. In Coling 2008: Proceedings of the workshop on Cross-Framework and Cross-Domain Parser
Evaluation, pages 1–8, Manchester, UK, August. Coling 2008 Organizing Committee.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed dependency parses from phrase structure parses. In Proceedings of LREC-06, pages 449–454.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In Proceedings of AMTA, Boston,
MA.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
of NAACL-HLT.
Philipp Koehn, Hieu Hoang, Alexandra Birch Mayne,
Christopher Callison-Burch, Marcello Federico,
Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar,
Alexandra Constantin, and Evan Herbst.
2007.
Moses: Open source toolkit for statistical machine
translation. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics
(ACL), Demonstration Session.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Alignment by agreement. In Proceedings of HLT-NAACL,
pages 104–111, New York City, USA, June. Association for Computational Linguistics.

Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur,
Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar
Kumar, Libin Shen, David Smith, Katherine Eng,
Viren Jain, Zhen Jin, and Dragomir Radev. 2004. A
smorgasbord of features for statistical machine translation. In Proceedings of HLT-NAACL.
Franz Josef Och. 2003. Minimum error rate training for
statistical machine translation. In ACL.
Stefan Riezler and John T. Maxwell. 2005. On some
pitfalls in automatic evaluation and signiﬁcance testing for MT. In Proceedings of the ACL Workshop on
Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, pages 57–
64, Ann Arbor, Michigan, June. Association for Computational Linguistics.
Christoph Tillman. 2004. A unigram orientation model
for statistical machine translation. In Proceedings of
HLT-NAACL 2004: Short Papers, pages 101–104.
Chao Wang, Michael Collins, and Philipp Koehn. 2007.
Chinese syntactic reordering for statistical machine
translation. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 737–745, Prague, Czech
Republic, June. Association for Computational Linguistics.
Richard Zens and Hermann Ney. 2006. Discriminative
reordering models for statistical machine translation.
In Proceedings on the Workshop on Statistical Machine Translation, pages 55–63, New York City, June.
Association for Computational Linguistics.

