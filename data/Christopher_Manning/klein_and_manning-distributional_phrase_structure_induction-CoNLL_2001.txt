Distributional Phrase Structure Induction
Dan Klein and Christopher D. Manning
Computer Science Department
Stanford University
Stanford, CA 94305-9040
klein, manning @cs.stanford.edu

 

Unsupervised grammar induction systems
commonly judge potential constituents on
the basis of their effects on the likelihood
of the data. Linguistic justiﬁcations of
constituency, on the other hand, rely on
notions such as substitutability and varying external contexts. We describe two
systems for distributional grammar induction which operate on such principles, using part-of-speech tags as the contextual
features. The advantages and disadvantages of these systems are examined, including precision/recall trade-offs, error
analysis, and extensibility.

1 Overview
While early work showed that small, artiﬁcial
context-free grammars could be induced with the
EM algorithm (Lari and Young, 1990) or with
chunk-merge systems (Stolcke and Omohundro,
1994), studies with large natural language grammars have shown that these methods of completely
unsupervised acquisition are generally ineffective.
For instance, Charniak (1993) describes experiments running the EM algorithm from random starting points, which produced widely varying grammars of extremely poor quality. Because of these
kinds of results, the vast majority of statistical parsing work has focused on parsing as a supervised
learning problem (Collins, 1997; Charniak, 2000).
It remains an open problem whether an entirely unsupervised method can either produce linguistically
sensible grammars or accurately parse free text.
However, there are compelling motivations for
unsupervised grammar induction. Building supervised training data requires considerable resources,
including time and linguistic expertise. Furthermore, investigating unsupervised methods can shed

¡

Abstract

light on linguistic phenomena which are implicitly captured within a supervised parser’s supervisory information, and, therefore, often not explicitly
modeled in such systems. For example, our system
and others have difﬁculty correctly attaching subjects to verbs above objects. For a supervised CFG
parser, this ordering is implicit in the given structure
of VP and S constituents, however, it seems likely
that to learn attachment order reliably, an unsupervised system will have to model it explicitly.
Our goal in this work is the induction of highquality, linguistically sensible grammars, not parsing accuracy. We present two systems, one which
does not do disambiguation well and one which
does not do it at all. Both take tagged but unparsed
Penn treebank sentences as input.1 To whatever degree our systems parse well, it can be taken as evidence that their grammars are sensible, but no effort
was taken to improve parsing accuracy directly.
There is no claim that human language acquisition is in any way modeled by the systems described
here. However, any success of these methods is evidence of substantial cues present in the data, which
could potentially be exploited by humans as well.
Furthermore, mistakes made by these systems could
indicate points where human acquisition is likely
not being driven by these kinds of statistics.

2 Approach
At the heart of any iterative grammar induction system is a method, implicit or explicit, for deciding
how to update the grammar. Two linguistic criteria
for constituency in natural language grammars form
the basis of this work (Radford, 1988):
1. External distribution: A constituent is a sequence of words which appears in various
structural positions within larger constituents.
1
The Penn tag and category sets used in examples in this
paper are documented in Manning and Sch¨ tze (1999, 413).
u

2. Substitutability: A constituent is a sequence of
words with (simple) variants which can be substituted for that sequence.
To make use of these intuitions, we use a distributional notion of context. Let be a part-of-speech
tag sequence. Every occurence of will be in some
context
, where and are the adjacent tags or
sentence boundaries. The distribution over contexts
in which occurs is called its signature, which we
denote by
.
Criterion 1 regards constituency itself. Consider
the tag sequences IN DT NN and IN DT. The former
is a canonical example of a constituent (of category
PP), while the later, though strictly more common,
is, in general, not a constituent. Frequency alone
does not distinguish these two sequences, but Criterion 1 points to a distributional fact which does. In
particular, IN DT NN occurs in many environments.
It can follow a verb, begin a sentence, end a sentence, and so on. On the other hand, IN DT is generally followed by some kind of a noun or adjective.
This example suggests that a sequence’s constituency might be roughly indicated by the entropy
of its signature,
. This turns out to be
somewhat true, given a few qualiﬁcations. Figure 1
shows the actual most frequent constituents along
with their rankings by several other measures. Tag
entropy by itself gives a list that is not particularly
impressive. There are two primary causes for this.
One is that uncommon but possible contexts have
little impact on the tag entropy value. Given the
skewed distribution of short sentences in the treebank, this is somewhat of a problem. To correct for
this, let
be the uniform distribution over the
observed contexts for . Using
would
have the obvious effect of boosting rare contexts,
and the more subtle effect of biasing the rankings
slightly towards more common sequences. However, while
presumably converges to some
sensible limit given inﬁnite data,
will
not, as noise eventually makes all or most counts
non-zero. Let be the uniform distribution over all
contexts. The scaled entropy

¢

¢

¥

£

¨©§
¢
¢
¥ ¢
¦¤£

 ¢¨ §¨
©

 ¢¨ §¨
" !

 ¢¨  §¨
"

 ¢ 
¨ §

¢

 ¢¨ §¨
©!
#

7 ¨  3 ¢¨ §¨ 0  ¢¨ §¨  '  ¢¨ §¨ $
865#!42 ! 1©)(©&%

turned out to be a useful quantity in practice. Multiplying entropies is not theoretically meaningful, but
given inﬁthis quantity does converge to
nite (noisy) data. The list for scaled entropy still
has notable ﬂaws, mainly relatively low ranks for
common NPs, which does not hurt system perfor-

Sequence
DT NN
NNP NNP
CD CD
JJ NNS
DT JJ NN
DT NNS
JJ NN
CD NN
IN NN
IN DT NN
NN NNS
NN NN
TO VB
DT JJ
MD VB
IN DT
PRP VBZ
PRP VBD
NNS VBP
NN VBZ
RB IN
NN IN
NNS VBD
NNS IN

Actual
1
2
3
4
5
6
7
8
9
10
-

Freq
2
1
9
7
3
8
6
4
10
5
-

Entropy
4
3
5
1
10
2
7
8
9
6

Scaled
2
3
7
9
6
10
1
4
5
8
-

Boundary
1
4
2
10
9
6
10
3
7
8
5
-

G REEDY-RE
1
2
6
4
8
10
3
7
5
9
-

Figure 1: Top non-trivial sequences by actual constituent
counts, raw frequency, raw entropy, scaled entropy, boundary
scaled entropy, and according to G REEDY-RE (see section 4.2).

mance, and overly high ranks for short subject-verb
sequences, which does.
The other fundamental problem with these
entropy-based rankings stems from the context features themselves. The entropy values will change
dramatically if, for example, all noun tags are collapsed, or if functional tags are split. This dependence on the tagset for constituent identiﬁcation is
very undesirable. One appealing way to remove this
dependence is to distinguish only two tags: one for
the sentence boundary (#) and another for words.
Scaling entropies by the entropy of this reduced signature produces the improved list labeled “Boundary.” This quantity was not used in practice because,
although it is an excellent indicator of NP, PP, and
intransitive S constituents, it gives too strong a bias
against other constituents. However, neither system
is driven exclusively by the entropy measure used,
and duplicating the above rankings more accurately
did not always lead to better end results.
Criterion 2 regards the similarity of sequences.
Assume the data were truly generated by a categorically unambiguous PCFG (i.e., whenever a token of a sequence is a constituent, its label is determined) and that we were given inﬁnite data. If so,
then two sequences, restricted to those occurrences
where they are constituents, would have the same
signatures. In practice, the data is ﬁnite, not statistically context-free, and even short sequences can be
categorically ambiguous. However, it remains true
that similar raw signatures indicate similar syntactic

 ¢¨ §¨
©

behavior. For example, DT JJ NN and DT NN have
extremely similar signatures, and both are common
NPs. Also, NN IN and NN NN IN have very similar
signatures, and both are primarily non-constituents.
For our experiments, the metric of similarity between sequences was the Jensen-Shannon divergence of the sequences’ signatures:

R P F DH G E @ §
9 4 Q2I45F46¨

KL

KL

7 PF HG E §
8 Q2D T45FSD ¨

9
D ' D §B@ §
9 0 @ ©&"C1A¨ 9
JS

Where KL is the Kullback-Leibler divergence between probability distributions. Of course, just as
various notions of context are possible, so are various metrics between signatures. The issues of tagset
dependence and data skew did not seem to matter
for the similarity measure, and unaltered JensenShannon divergence was used.
Given these ideas, section 4.1 discusses a system whose grammar induction steps are guided by
sequence entropy and interchangeability, and section 4.2 discusses a maximum likelihood system
where the objective being maximized is the quality
of the constituent/non-constituent distinction, rather
than the likelihood of the sentences.
2.1 Problems with ML/MDL
Viewing grammar induction as a search problem,
there are three principal ways in which one can induce a “bad” grammar:
Optimize the wrong objective function.
Choose bad initial conditions.
Be too sensitive to initial conditions.
Our current systems primarily attempt to address
the ﬁrst two points. Common objective functions
include maximum likelihood (ML) which asserts
that a good grammar is one which best encodes
or compresses the given data. This is potentially
undesirable for two reasons. First, it is strongly
data-dependent. The grammar which maximizes
depends on the corpus , which, in some
sense, the core of a given language’s phrase structure should not. Second, and more importantly, in
an ML approach, there is pressure for the symbols
and rules in a PCFG to align in ways which maximize the truth of the conditional independence assumptions embodied by that PCFG. The symbols
and rules of a natural language grammar, on the
other hand, represent syntactically and semantically
coherent units, for which a host of linguistic arguments have been made (Radford, 1988). None
of these arguments have anything to do with conditional independence; traditional linguistic con-

stituency reﬂects only grammatical possibilty of expansion. Indeed, there are expected to be strong
connections across phrases (such as are captured by
argument dependencies). For example, in the treebank data used, CD CD is a common object of a verb,
but a very rare subject. However, a linguist would
take this as a selectional characteristic of the data
set, not an indication that CD CD is not an NP. Of
course, it could be that the ML and linguistic criteria align, but in practice they do not always seem to,
and one should not expect that, by maximizing the
former, one will also maximize the latter.
Another common objective function is minimum
description length (MDL), which asserts that a good
analysis is a short one, in that the joint encoding of
the grammar and the data is compact. The “compact grammar” aspect of MDL is perhaps closer to
some traditional linguistic argumentation which at
times has argued for minimal grammars on grounds
of analytical (Harris, 1951) or cognitive (Chomsky
and Halle, 1968) economy. However, some CFGs
which might possibly be seen as the acquisition goal
are anything but compact; take the Penn treebank
covering grammar for an extreme example. Another
serious issue with MDL is that the target grammar
is presumably bounded in size, while adding more
and more data will on average cause MDL methods
to choose ever larger grammars.
In addition to optimizing questionable objective
functions, many systems begin their search procedure from an extremely unfavorable region of
the grammar space. For example, the randomly
weighted grammars in Carroll and Charniak (1992)
rarely converged to remotely sensible grammars. As
they point out, and quite independently of whether
ML is a good objective function, the EM algorithm
is only locally optimal, and it seems that the space
of PCFGs is riddled with numerous local maxima.
Of course, the issue of initialization is somewhat
tricky in terms of the bias given to the system; for
example, Brill (1994) begins with a uniformly rightbranching structure. For English, right-branching
structure happens to be astonishingly good both as
an initial point for grammar learning and even as a
baseline parsing model. However, it would be unlikely to perform nearly as well for a VOS language
like Malagasy or VSO languages like Hebrew.

3 Search vs. Clustering
Whether grammar induction is viewed as a search
problem or a clustering problem is a matter of per-

U
U
U

9V


YXVE 9 ¨ W

4 Systems
We present two systems. The ﬁrst, G REEDYM ERGE, learns symbolic CFGs for partial parsing.
The rules it learns are of high quality (see ﬁgures
3 and 4), but parsing coverage is relatively shallow.
The second, C ONSTITUENCY-PARSER, learns distributions over sequences representing the probabil-

TOP
#

z1
DT

VBZ

RB

#

NN

Figure 2: The possible contexts of a sequence.
ity that a constituent is realized as that sequence (see
ﬁgure 1). It produces full binary parses.
4.1 G REEDY-M ERGE
G REEDY-M ERGE is a precision-oriented system
which, to a ﬁrst approximation, can be seen as an
agglomerative clustering process over sequences.
For each pair of sequences, a normalized divergence
is calculated as follows:

qqsg g uqSpig F g
q
uqtpsF g ySpiH g F hfc xv b(¨ `
q w F q v g e d w '  a B ¢
r

spective, and the two views are certainly not mutually exclusive. The search view focuses on the recursive relationships between the non-terminals in
the grammar. The clustering view, which is perhaps more applicable to the present work, focuses
on membership of (terminal) sequences to classes
represented by the non-terminals. For example, the
non-terminal symbol NP can be thought of as a cluster of (terminal) sequences which can be generated
starting from NP. This clustering is then inherently
soft clustering, since sequences can be ambiguous.
Unlike standard clustering tasks, though, a sequence token in a given sentence need not be a constituent at all. For example, DT NN is an extremely
common NP, and when it occurs, it is a constituent
around 82% of the time in the data. However, when
it occurs as a subsequence of DT NN NN it is usually
not a constituent. In fact, the difﬁcult decisions for a
supervised parser, such as attachment level or coordination scope, are decisions as to which sequences
are constituents, not what their tags would be if they
were. For example, DT NN IN DT NN is virtually always an NP when it is a constituent, but it is only a
constituent 66% of the time, mostly because the PP,
IN DT NN , is attached elsewhere.
One way to deal with this issue is to have an explicit class for “not a constituent” (see section 4.2).
There are difﬁculties in modeling such a class,
mainly stemming from the differences between this
class and the constituent classes. In particular, this
class will not be distributionally cohesive. Also, for
example, DT NN and DT JJ NN being generally of
category NP seems to be a highly distributional fact,
while DT NN not being a constituent in the context
DT NN NN seems more properly modeled by the
competing productions of the grammar.
Another approach is to model the nonconstituents either implicitly or independently
of the clustering model (see section 4.1). The drawback to insufﬁciently modeling non-constituency is
that for acquisition systems which essentially work
bottom-up, non-constituent chunks such as NN IN
or IN DT are hard to rule out locally.

The pair with the least divergence is merged.2
Merging two sequences involves the creation of a
single new non-terminal category which rewrites as
either sequence. Once there are non-terminal categories, the deﬁnitions of sequences and contexts become slightly more complex. The input sentences
are parsed with the previous grammar state, using
a shallow parser which ties all parentless nodes together under a TOP root node. Sequences are then
the ordered sets of adjacent sisters in this parse, and
the context of a sequence can either be the preceding and following tags or a higher node in the
tree. To illustrate, in ﬁgure 2, the sequence VBZ RB
could either be considered to be in context [ Z 1. . . #]
or [ NN . . . #]. Taking the highest potential context
([Z 1. . . #] in this case) performed slightly better.3
Merging a sequence and a single non-terminal results in a rule which rewrites the non-terminal as the
sequence (i.e., that sequence is added to that nonterminal’s class), and merging two non-terminals involves collapsing the two symbols in the grammar
(i.e., those classes are merged). After the merge,
re-analysis of the grammar rule RHSs is necessary.
An important point about G REEDY-M ERGE is
that stopping the system at the correct point is critical. Since our greedy criterion is not a measure
over entire grammar states, we have no way to detect the optimal point beyond heuristics (the same
2
We required that the candidates be among the 250 most
frequent sequences. The exact threshold was not important,
but without some threshold, long singleton sequences with zero
divergence are always chosen. This suggests that we need a
greater bias towards quantity of evidence in our basic method.
3
An option which was not tried would be to consider a nonterminal as a distribution over the tags of the right or left corners of the sequences belonging to that non-terminal.

qr l
2(¢ (¨ W $  &p onig m¨ k4¦(gtd
£B E  B ¢¨ 
j id h fe
 B ¢¨
(

¨ 3  E   E  '  £ B E 

W 2£ S¨ W ¢ ¨ W (6©¢ S¨ W
 £B ¢
6©¨

As we are considering each pair independently from
the rest of the parse, this model does not correspond
to a generative model of the kind standardly associated with PCFGs, but can be seen as a random ﬁeld
over the possible parses, with the features being the
sequences and contexts (see (Abney, 1997)). However, note that we were primarily interested in the
clustering behavior, not the parsing behavior, and

5 Results
Two kinds of results are presented.
First,
we discuss the grammars learned by G REEDYM ERGE and the constituent distributions learned by
C ONSTITUENCY-PARSER. Then we apply both systems to parsing free text from the WSJ section of the
Penn treebank.
5.1

Grammars learned by G REEDY-M ERGE

Figure 3 shows a grammar learned at one stage of
a run of G REEDY-M ERGE on the sentences in the
WSJ section of up to 10 words after the removal of
punctuation ( 7500 sentences). The non-terminal
categories proposed by the systems are internally
given arbitrary designations, but we have relabeled
them to indicate the best recall match for each.
Categories corresponding to NP, VP, PP, and S are
learned, although some are split into sub-categories
(transitive and intransitive VPs, proper NPs and two

q


  E  E ¢     '  £B ¢
Q¨ W Q 5£¨ W Q ¨ W  r I(6©¨ W

by the likelihood product of
. The best tree is then

 ¢¨ §¨ $
©Q%



and we score a tree
its judgements

4.2.2 Parameters
C ONSTITUENCY-PARSER’s behavior is determined
by the initialization it is given, either by initial parameter estimates, or ﬁxed ﬁrst-round parses. We
used four methods: R ANDOM, E NTROPY, R IGHTB RANCH, and G REEDY .
For R ANDOM, we initially parsed randomly. For
E NTROPY, we weighted
proportionally to
. For R IGHT B RANCH, we forced rightbranching structures (thereby introducing a bias towards English structure). Finally, G REEDY used the
output from G REEDY-M ERGE (using the grammar
state in ﬁgure 3) to parse initially.

 E
¢ S¨ W

 £B ¢
6©¨

We use EM to maximize the likelihood of these
pairs given the hidden judgements , subject to the
constraints that the judgements for the pairs from a
given sentence must form a valid binary parse.
Initialization was either done by giving initial
seeds for the probabilities above or by forcing a certain set of parses on the ﬁrst round. To do the reestimation, we must have some method of deciding
which binary bracketing to prefer. The chance of a
pair
being a constituent is

4.2.1 Sparsity
Since this system does not postulate any nonterminal symbols, but works directly with terminal
sequences, sparsity will be extremely severe for any
reasonably long sequences. Substantial smoothing
estimates we
was done to all terms; for the
interpolated the previous counts equally with a uniform
, otherwise most sequences would remain
locked in their initial behaviors. This heavy smoothing made rare sequences behave primarily according to their contexts, removed the initial invariance
problem, and, after a few rounds of re-estimation,
had little effect on parser performance.


Q¨ W

4.2 C ONSTITUENCY-PARSER
The second system, C ONSTITUENCY-PARSER, is
recall-oriented. Unlike G REEDY-M ERGE, this system always produces a full, binary parse of each input sentence. However, its parsing behavior is secondary. It is primarily a clustering system which
views the data as the entire set of (sequence, context) pairs
that occurred in the sentences.
Each pair token comes from some speciﬁc sentence
and is classiﬁed with a binary judgement of that token’s constituency in that sentence. We assume that
these pairs are generated by the following model:

that the random ﬁeld parameters have not been ﬁt
to any distribution over trees. The parsing model is
very crude, primarily serving to eliminate systematically mutually incompatible analyses.

 E
¢ ¨ W

category appears in several merges in a row, for example) or by using a small supervision set to detect
a parse performance drop. The ﬁgures shown are
from stopping the system manually just before the
ﬁrst signiﬁcant drop in parsing accuracy.
The grammar rules produced by the system are a
strict subset of general CFG rules in several ways.
First, no unary rewriting is learned. Second, no nonterminals which have only a single rewrite are ever
proposed, though this situation can occur as a result
of later merges. The effect of these restrictions is
discussed below.

r

Transitive Verb Group
zVt VBZt VBDt VBPt
zVt MD zVBt
zVt zVt RB

CC zS
RB zS

s

S-bar
zVP

r

s

rr
r

Intransitive Verb Group
zVP
VBZ VBD VBP
MD VB
zVP
zVP zVBN
zVP

IN zS

v

zS
zS

rr

rr

s

u

s

rr
r

1 - wrong attachment level
2 - wrong result category

Figure 4: A learned grammar (with verbs split).
X X and X
(any tergrammar where X
minal). However, very incorrect merges are
sometimes made relatively early on (such as
merging VPs with PPs, or merging the sequences IN NNP IN and IN.

t

t

s

rr
r

rr
r

rr

rr
r

s

s

rr
rr
r

t

U
U
U

4
Splits often occur because unary rewrites are not learned
in the current system.

zIN zNN
zIN zNP
zIN zNNP

zNNP zVP
zNN zVP
zNP zVP
DT zVP

rr

rr

Mistakes of omission. Even though the grammar shown has correct, recursive analyses of
many categories, no rule can non-trivially incorporate a number (CD). There is also no
analysis for many common constructions.
Alternate analyses. The system almost invariably forms verb groups, merging MD VB sequences with single main verbs to form verb
group constituents (argued for at times by
some linguists (Halliday, 1994)). Also, PPs are
sometimes attached to NPs below determiners
(which is in fact a standard linguistic analysis
(Abney, 1987)). It is not always clear whether
these analyses should be considered mistakes.
Over-merging. These errors are the most serious. Since at every step two sequences are
merged, the process will eventually learn the

rr

rr

kinds of common NPs, and so on).4 Provided one is
willing to accept a verb-group analysis, this grammar seems sensible, though quite a few constructions, such as relative clauses, are missing entirely.
Figure 4 shows a grammar learned at one stage
of a run when verbs were split by transitivity. This
grammar is similar, but includes analyses of sentencial coordination and adverbials, and subordinate
clauses. The only rule in this grammar which seems
IN Z S which analyzes comoverly suspect is Z VP
plementized subordinate clauses as VPs.
In general, the major mistakes the G REEDYM ERGE system makes are of three sorts:

PP
zPP
zPP
zPP

S
zS
zS
zS
zS

rr

rr

r

Figure 3: A learned grammar.

rr

Transitive S
zSt zNNP zVP
zSt zNN zVP
zSt PRP zVP

Proper NP
zNNP zNNP zNNP
zNNP NNP

VP complementation
zVP
zVt zNP
zVt zNN
zVP

rr

rr

rr

verb groups / intransitive VPs
VBZ VBD VBP
zV
zV
MD VB
MD RB VB
zV
zV zRB
zV
zV
zV zVBG

r

Intransitive S
zS PRP zV
zS zNP zV
zS zNNP zV

zIN zNN
zIN zNP
zIN zNNP

Common NP with determiner
zNP
DT zNN
zNP
PRP$ zNN

VP adjunction
zVP
RB zVP
zVP RB
zVP
zVP
zVP zPP
zVP
zVP zJJ

rr

s

PP
zPP
zPP
zPP

Transitive VPs
(adjunction)
zVP zRB zVP
ZVP zVP zPP

rr

Proper NP
zNNP
NNP NNPS
zNNP zNNP
zNNP

s

NP with determiner
zNP
DT zNN
PRP$ zNN
zNP

Transitive VPs
(complementation)
zVP zV JJ
zVP zV zNP
zVP zV zNN
zVP zV zPP

N-bar or zero-determiner NP
NN NNS
zNN
zNN zNN zNN
zNN JJ zNN

rr

N-bar or zero determiner NP
zNN NN NNS
zNN JJ zNN
zNN zNN zNN

5.2 C ONSTITUENCY-PARSER’s Distributions
The C ONSTITUENCY-PARSER’s state is not a symbolic grammar, but estimates of constituency for terminal sequences. These distributions, while less
compelling a representation for syntactic knowledge than CFGs, clearly have signiﬁcant facts about
language embedded in them, and accurately learning them can be seen as a kind of acquisiton.
Figure 5 shows the sequences whose constituency
counts are most incorrect for the G REEDY-RE setting. An interesting analysis given by the system is
the constituency of NNP POS NN sequences as NNP
( POS NN ) which is standard in linguistic analyses
(Radford, 1988), as opposed to the treebank’s systematic ( NNP POS ) NN. Other common errors, like
the overcount of JJ NN or JJ NNS are partially due
to parsing inside NPs which are ﬂat in the treebank
(see section 5.3).
It is informative to see how re-estimation with
C ONSTITUENCY-PARSER improves and worsens
the G REEDY-M ERGE initial parses. Coverage is
improved; for example NPs and PPs involving the
CD tag are consistently parsed as constituents while
G REEDY-M ERGE did not include them in parses at
all. On the other hand, the G REEDY-M ERGE sys-

UL Recall
NP Recall
VP Recall

G
re
ed
y
re
ed
yR
E

R
an
do
m

G
re
ed
y
re
ed
yR
E
G

R
an
do
m
En
tro
R
py
ig
ht
Br
an
R
ch
an
do
m
-R
En
E
tro
py
-R
E

NCB Precision

G

UL Precision

1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
En
tro
py
R
ig
ht
Br
an
ch
R
an
do
m
-R
E
En
tro
py
-R
E

1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0

Figure 6: Unlabeled precision (left) and recall (right) values for various settings.

Sequence
NNP POS
VBD RB VBN
VB DT NN
NNP NNP POS
VB VBN
VB RB
VBD VBN
VBZ RB JJ
RB CD
VB DT JJ NN

Overcount
736
504
434
420
392
388
324
318
283
283

Estimated
1099
663
1419
453
415
405
443
355
579
799

True
363
159
985
33
23
17
119
37
296
516

Total
1385
805
2261
488
452
440
538
455
618
836

Undercount
127
59
53
42
42
39
36
33
30
29

Estimated
33
6
10
8
3
6
17
18
26
3

True
160
65
63
50
45
45
53
51
56
32

Total
224
83
137
58
141
100
202
72
117
51

Figure 5: Sequences most commonly over- and underidentiﬁed as constituents by C ONSTITUENCY-PARSER using
G REEDY-RE (E NTROPY-RE is similar). “Total” is the frequency of the sequence in the ﬂat data. “True” is the frequency
as a constituent in the treebank’s parses. “Estimated” is the
frequency as a constituent in the system’s parses.

tem had learned the standard subject-verb-object attachment order, though this has disappeared, as can
be seen in the undercounts of VP sequences. Since
many VPs did not ﬁt the conservative VP grammar
in ﬁgure 3, subjects and verbs were often grouped
together frequently even on the initial parses, and
the C ONSTITUENCY-PARSER has a further bias towards over-identifying frequent constituents.
5.3

Parsing results

Some issues impact the way the results of parsing
treebank sentences should be interpreted. Both systems, but especially the C ONSTITUENCY-PARSER,
tend to form verb groups and often attach the subject below the object for transitive verbs. Because of
this, certain VPs are systematically incorrect and VP
accuracy suffers dramatically, substantially pulling

down the overall ﬁgures.5 Secondly, the treebank’s
grammar is an imperfect standard for an unsupervised learner. For example, transitive sentences
are bracketed [subject [verb object]] (“The president [executed the law]”) while nominalizations are
bracketed [[possessive noun] complement] (“[The
president’s execution] of the law”), an arbitrary inconsistency which is unlikely to be learned automatically. The treebank is also, somewhat purposefully,
very ﬂat. For example, there is no analysis of the
inside of many short noun phrases. The G REEDYM ERGE grammars above, however, give a (correct)
analysis of the insides of NPs like DT JJ NN NN
for which it will be penalized in terms of unlabeled
precision (though not crossing brackets) when compared to the treebank.
An issue with G REEDY-M ERGE is that the grammar learned is symbolic, not probabilistic. Any disambiguation is done arbitrarily. Therefore, even
adding a linguistically valid rule can degrade numerical performance (sometimes dramatically) by
introducing ambiguity to a greater degree than it improves coverage.
In ﬁgure 6, we report summary results for
10-word sentences of the
each system on the
WSJ section. G REEDY is the above snapshot
of the G REEDY-M ERGE system. R ANDOM, E N TROPY , and R IGHT B RANCH are the behaviors
of the random-parse baseline, the right-branching
baseline, and the entropy-scored initialization for
C ONSTITUENCY-PARSER. The -RE settings are
the result of context-based re-estimation from
the respective baselines using C ONSTITUENCYPARSER.6 NCB precision is the percentage of pro-

w

Sequence
JJ NN
NN NN
NNP NNP
PRP VBZ
PRP VBD
PRP VBP
TO VB
MD VB
NN NNS
JJ NNS

5

The R IGHT B RANCH baseline is in the opposite situation.
Its high overall ﬁgures are in a large part due to extremely high
VP accuracy, while NP and PP accuracy (which is more important for tasks such as information extraction) is very low.
6
R IGHT B RANCH was invariant under re-estimation, and
R IGHT B RANCH -RE is therefore omitted.

posed brackets which do not cross a correct bracket.
Recall is also shown separately for VPs and NPs to
illustrate the VP effect noted above.
The general results are encouraging. G REEDY
is, as expected, higher precision than the other settings. Re-estimation from that initial point improves
recall at the expense of precision. In general, reestimation improves parse accuracy, despite the indirect relationship between the criterion being maximized (constituency cluster ﬁt) and parse quality.

7 Conclusion

6 Limitations of this study

References

This study presents preliminary investigations and
has several signiﬁcant limitations.

Stephen P. Abney. 1987. The English Noun Phrase in its Sentential Aspect. Ph.D. thesis, MIT.
Steven P. Abney. 1997. Stochastic attribute-value grammars.
Computational Linguistics, 23(4):597–618.
E. Brill. 1994. Automatic grammar induction and parsing free
text: A transformation-based approach. In Proc. ARPA Human Language Technology Workshop ’93, pages 237–242,
Princeton, NJ.
Glenn Carroll and Eugene Charniak. 1992. Two experiments
on learning probabilistic dependency grammars from corpora. In Carl Weir, Stephen Abney, Ralph Grishman, and
Ralph Weischedel, editors, Working Notes of the Workshop
Statistically-Based NLP Techniques, pages 1–13. AAAI
Press, Menlo Park, CA.
Eugene Charniak. 1993. Statistical Language Learning. MIT
Press, Cambridge, MA.
Eugene Charniak. 2000. A maximum-entropy-inspired parser.
In NAACL 1, pages 132–139.
Noam Chomsky and Morris Halle. 1968. The Sound Pattern of
English. Harper & Row, New York.
Michael John Collins. 1997. Three generative, lexicalised models for statistical parsing. In ACL 35/EACL 8, pages 16–23.
Steven P. Finch, Nick Chater, and Martin Redington. 1995. Acquiring syntactic information from distributional statistics.
In J. Levy, D. Bairaktaris, J. A. Bullinaria, and P. Cairns, editors, Connectionist models of memory and language, pages
229–242. UCL Press, London.
M. A. K. Halliday. 1994. An introduction to functional grammar. Edward Arnold, London, 2nd edition.
Zellig Harris. 1951. Methods in Structural Linguistics. University of Chicago Press, Chicago.
K. Lari and S. J. Young. 1990. The estimation of stochastic
context-free grammars using the inside-outside algorithm.
Computer Speech and Language, 4:35–56.
Christopher D. Manning and Hinrich Sch¨ tze. 1999. Founu
dations of Statistical Natural Language Processing. MIT
Press, Boston, MA.
Andrew Radford. 1988. Transformational Grammar. Cambridge University Press, Cambridge.
Philip Stuart Resnik. 1993. Selection and Information: A
Class-Based Approach to Lexical Relationships. Ph.D. thesis, University of Pennsylvania.
Hinrich Sch¨ tze. 1995. Distributional part-of-speech tagging.
u
In EACL 7, pages 141–148.
Andreas Stolcke and Stephen M. Omohundro. 1994. Inducing probabilistic grammars by Bayesian model merging. In
Grammatical Inference and Applications: Proceedings of
the Second International Colloquium on Grammatical Inference. Springer Verlag.

6.1

Tagged Data

A possible criticism of this work is that it relies on
part-of-speech tagged data as input. In particular,
while there has been work on acquiring parts-ofspeech distributionally (Finch et al., 1995; Sch¨ tze,
u
1995), it is clear that manually constructed tag sets
and taggings embody linguistic facts which are not
generally detected by a distributional learner. For
example, transitive and intransitive verbs are identically tagged yet distributionally dissimilar.
In principle, an acquisition system could be designed to exploit non-distributionality in the tags.
For example, verb subcategorization or selection
could be induced from the ways in which a given
lexical verb’s distribution differs from the average,
as in (Resnik, 1993). However, rather than being exploited by the systems here, the distributional nonunity of these tags appears to actually degrade performance. As an example, the systems more reliably group verbs and their objects together (rather
than verbs and their subjects) when transitive and
intransitive verbs are given separate tags.
Future experiments will investigate the impact of
distributional tagging, but, despite the degradation
in tag quality that one would expect, it is also possible that some current mistakes will be corrected.
6.2

Individual system limitations

For G REEDY-M ERGE, the primary limitations are
that there is no clear halting condition, there is
no ability to un-merge or to stop merging existing
classes while still increasing coverage, and the system is potentially very sensitive to the tagset used.
For C ONSTITUENCY-PARSER, the primary limitations are that no labels or recursive grammars are
learned, and that the behavior is highly dependent
on initialization.

We present two unsupervised grammar induction
systems, one of which is capable of producing
declarative, linguistically plausible grammars and
another which is capable of reliably identifying frequent constituents. Both parse free text with accuracy rivaling that of weakly supervised systems.
Ongoing work includes lexicalization, incorporating unary rules, enriching the models learned, and
addressing the limitations of the systems.

